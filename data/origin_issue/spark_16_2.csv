Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Container),Outward issue link (Container),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Outward issue link (Regression),Inward issue link (Required),Inward issue link (Supercedes),Inward issue link (Supercedes),Outward issue link (Supercedes),Outward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Shepherd),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Can't find keytab file when using Hive catalog,SPARK-19038,13031350,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,parente,parente,30/Dec/16 23:22,17/May/20 18:14,14/Jul/23 06:29,24/Feb/17 17:33,2.0.2,,,,,,,,2.0.3,2.1.1,2.2.0,,Spark Core,YARN,,,,,,,1,hive,kerberos,pyspark,,,"h2. Stack Trace

{noformat}
Py4JJavaErrorTraceback (most recent call last)
<ipython-input-13-c35b9cad36ad> in <module>()
----> 1 sdf = sql.createDataFrame(df)

/opt/spark2/python/pyspark/sql/context.py in createDataFrame(self, data, schema, samplingRatio, verifySchema)
    307         Py4JJavaError: ...
    308         """"""
--> 309         return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)
    310 
    311     @since(1.3)

/opt/spark2/python/pyspark/sql/session.py in createDataFrame(self, data, schema, samplingRatio, verifySchema)
    524             rdd, schema = self._createFromLocal(map(prepare, data), schema)
    525         jrdd = self._jvm.SerDeUtil.toJavaArray(rdd._to_java_object_rdd())
--> 526         jdf = self._jsparkSession.applySchemaToPythonRDD(jrdd.rdd(), schema.json())
    527         df = DataFrame(jdf, self._wrapped)
    528         df._schema = schema

/opt/spark2/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1131         answer = self.gateway_client.send_command(command)
   1132         return_value = get_return_value(
-> 1133             answer, self.gateway_client, self.target_id, self.name)
   1134 
   1135         for temp_arg in temp_args:

/opt/spark2/python/pyspark/sql/utils.py in deco(*a, **kw)
     61     def deco(*a, **kw):
     62         try:
---> 63             return f(*a, **kw)
     64         except py4j.protocol.Py4JJavaError as e:
     65             s = e.java_exception.toString()

/opt/spark2/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    317                 raise Py4JJavaError(
    318                     ""An error occurred while calling {0}{1}{2}.\n"".
--> 319                     format(target_id, ""."", name), value)
    320             else:
    321                 raise Py4JError(

Py4JJavaError: An error occurred while calling o47.applySchemaToPythonRDD.
: org.apache.spark.SparkException: Keytab file: .keytab-f0b9b814-460e-4fa8-8e7d-029186b696c4 specified in spark.yarn.keytab does not exist
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:113)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:258)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:359)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:263)
	at org.apache.spark.sql.hive.HiveSharedState.metadataHive$lzycompute(HiveSharedState.scala:39)
	at org.apache.spark.sql.hive.HiveSharedState.metadataHive(HiveSharedState.scala:38)
	at org.apache.spark.sql.hive.HiveSharedState.externalCatalog$lzycompute(HiveSharedState.scala:46)
	at org.apache.spark.sql.hive.HiveSharedState.externalCatalog(HiveSharedState.scala:45)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:50)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:666)
	at org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:656)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

h2. Steps to reproduce

1. Pass valid --principal=user@REALM and --keytab=/home/user/.keytab to spark-submit
2. Set spark.sql.catalogImplementation = 'hive'
3. Set deploy mode to yarn-client
4. Create a SparkSession and try to use session.createDataFrame()

h2. Observations

* The {{setupCredentials}} function in Client.scala sets {{spark.yarn.keytab}} to a UUID suffixed version of the base keytab filename without any path. For example, {{sparkContext.getConf().getAll()}} shows {{spark.yarn.keytab}} as having value {{.keytab-f0b9b814-460e-4fa8-8e7d-029186b696c4}}
* When listing the contents of the application staging directory on HDFS, no suffixed file exists. Rather, the keytab file appears in the listing with its original name. For instance, {{hdfs dfs -ls hdfs://home/user/.sparkStaging/appication_big_uuid/}} shows an entry {{hdfs://home/user/.sparkStaging/appication_big_uuid/.keytab}}, but not {{hdfs://home/user/.sparkStaging/appication_big_uuid/.keytab-big-uuid}}.
* The same exception noted above occurs even after I manually put a copy of the keytab with a filename matching the new value of {{spark.yarn.keytab}} onto HDFS in the staging directory.

h2. Expected Behavior

HiveClientImpl should be able to read {{spark.yarn.keytab}} to find the keytab file and initialize itself properly.

h2. References

* SPARK-8619 also noted trouble with the keytab property getting changed  after app startup.","Hadoop / YARN 2.6, pyspark, yarn-client mode",apachespark,jerryshao,liushaohui,parente,Tagar,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19588,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 14 09:13:03 UTC 2017,,,,,,,,,,"0|i385ef:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Jan/17 01:52;parente;Also, since the keytab file name in the staging directory does not match the new Spark config setting, the Kerberos ticket is not properly renewed before expiration.;;;","06/Jan/17 05:28;apachespark;User 'parente' has created a pull request for this issue:
https://github.com/apache/spark/pull/16482;;;","06/Jan/17 06:25;jerryshao;Please see the comment I made in Github(https://github.com/apache/spark/pull/16482), from my understanding the behavior is expected.;;;","14/Feb/17 03:34;Tagar;[~jerryshao] PR 16482 is for a different issue SPARK-19105? 

I still see this issue after Spark 2.0 upgrade. 
See details in https://github.com/apache/spark/pull/16482#issuecomment-279563855
We didn't have this problem in Spark 1.5 nor 1.6.

Tried to workaround by putting keytab in user's home directory in HDFS but this fails with (expected?)
{noformat}
Exception in thread ""main"" org.apache.spark.SparkException: Keytab file: hdfs:///user/svc_odiprd/.kt does not exist
        at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:555)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:158)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{noformat}
created SPARK-19588 to create this separate issue for the workaround.;;;","14/Feb/17 03:44;jerryshao;I think the issue you met is the same as this JIRA mentioned, but PR 16482 tries to use a different way to solve this problem, which is not correct for current Spark on YARN. Let me figure out a decent solution to fix this.;;;","14/Feb/17 03:54;Tagar;Thank you [~jerryshao];;;","14/Feb/17 04:06;Tagar;Another possible workaround is to pass principal and keytab to spark-submit through SPARK_SUBMIT_OPTIONS environment variable
{code}
environ[""SPARK_SUBMIT_OPTIONS""] = ""--principal %s --keytab %s"" % (kt_principal, kt_location)
{code}
instead of setting in 
{code}
SparkConf().set(""spark.yarn.keytab"", kt_location).set(""spark.yarn.principal"", kt_principal)
{code}

but again this is a breaking change / bug in Spark 2.;;;","14/Feb/17 09:13;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/16923;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HistoryServer still uses old ACLs even if ACLs are updated,SPARK-19033,13031225,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jerryshao,jerryshao,jerryshao,30/Dec/16 06:28,06/Jan/17 16:09,14/Jul/23 06:29,06/Jan/17 16:09,2.1.0,,,,,,,,2.1.1,2.2.0,,,Spark Core,,,,,,,,0,,,,,,"In the current implementation of HistoryServer, Application ACLs is picked from event log rather than configuration:

{code}
            val uiAclsEnabled = conf.getBoolean(""spark.history.ui.acls.enable"", false)
            ui.getSecurityManager.setAcls(uiAclsEnabled)
            // make sure to set admin acls before view acls so they are properly picked up
            ui.getSecurityManager.setAdminAcls(appListener.adminAcls.getOrElse(""""))
            ui.getSecurityManager.setViewAcls(attempt.sparkUser,
              appListener.viewAcls.getOrElse(""""))
            ui.getSecurityManager.setAdminAclsGroups(appListener.adminAclsGroups.getOrElse(""""))
            ui.getSecurityManager.setViewAclsGroups(appListener.viewAclsGroups.getOrElse(""""))
{code}

This will become a problem when ACLs is updated (newly added admin), only the new application can be effected, the old applications were still using the old ACLs. So these new admin still cannot check the logs of old applications.

It is hard to say this is a bug, but in our scenario this is not the expected behavior we wanted.",,apachespark,jerryshao,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 04 08:15:04 UTC 2017,,,,,,,,,,"0|i384mn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/16 06:32;jerryshao;Ping [~vanzin], I found that you made this change, would you mind explaining the purpose of doing so? Thanks very much.;;;","03/Jan/17 18:45;vanzin;I actually didn't write the code, I just moved it around a lot. It was first added in SPARK-1489 by [~tgraves].;;;","03/Jan/17 19:01;tgraves;this should actually be an improvement to allow admin acls from history server config to also apply. I've been meaning to do this for a while, thought I filed jira for it but don't see it now. 

Currently it uses what the application sets for acls, which in general is correct because you don't want the user level acls being applied differently then what the application had originally set, this would be a security hole and you would be applying one set of history acls across all applications. The exception to that is the admin level acls, the idea is that these are people with special privileges and like you mention this could change.  Although I would also encourage you to use groups which helps with this issue as you shouldn't need a huge list of individual users but its still also possible for a user to blank out the admin acls so it would be good to allow the history server to add those back.

So this jira should just add another option to append history server admin acls to application ones.  Something like spark.history.ui.admin.acls and spark.history.ui.admin.acls.groups
;;;","04/Jan/17 01:36;jerryshao;Thanks a lot [~tgraves], I will think about how to better address this issue.;;;","04/Jan/17 08:15;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/16470;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixed non-thread-safe functions used in SessionCatalog,SPARK-19028,13031134,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,29/Dec/16 18:25,01/Jan/17 00:44,14/Jul/23 06:29,31/Dec/16 11:42,2.0.2,2.1.0,,,,,,,2.2.0,,,,SQL,,,,,,,,0,,,,,,"Fixed non-thread-safe functions used in SessionCatalog.
- refreshTable
- lookupRelation",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-15135,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 29 18:30:05 UTC 2016,,,,,,,,,,"0|i3842f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Dec/16 18:30;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16437;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark does not work with Python 3.6.0,SPARK-19019,13030996,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gurwls223,gurwls223,,29/Dec/16 02:40,12/Dec/22 18:10,14/Jul/23 06:29,17/Jan/17 17:53,,,,,,,,,1.6.4,2.0.3,2.1.1,2.2.0,PySpark,,,,,,,,4,,,,,,"Currently, PySpark does not work with Python 3.6.0.

Running {{./bin/pyspark}} simply throws the error as below:

{code}
Traceback (most recent call last):
  File "".../spark/python/pyspark/shell.py"", line 30, in <module>
    import pyspark
  File "".../spark/python/pyspark/__init__.py"", line 46, in <module>
    from pyspark.context import SparkContext
  File "".../spark/python/pyspark/context.py"", line 36, in <module>
    from pyspark.java_gateway import launch_gateway
  File "".../spark/python/pyspark/java_gateway.py"", line 31, in <module>
    from py4j.java_gateway import java_import, JavaGateway, GatewayClient
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 646, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 616, in _load_backward_compatible
  File "".../spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 18, in <module>
  File ""/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/pydoc.py"", line 62, in <module>
    import pkgutil
  File ""/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/pkgutil.py"", line 22, in <module>
    ModuleInfo = namedtuple('ModuleInfo', 'module_finder name ispkg')
  File "".../spark/python/pyspark/serializers.py"", line 394, in namedtuple
    cls = _old_namedtuple(*args, **kwargs)
TypeError: namedtuple() missing 3 required keyword-only arguments: 'verbose', 'rename', and 'module'
{code}

The problem is in https://github.com/apache/spark/blob/3c68944b229aaaeeaee3efcbae3e3be9a2914855/python/pyspark/serializers.py#L386-L394 as the error says and the cause seems because the arguments of {{namedtuple}} are now completely keyword-only arguments from Python 3.6.0 (See https://bugs.python.org/issue25628).

We currently copy this function via {{types.FunctionType}} which does not set the default values of keyword-only arguments (meaning {{namedtuple.__kwdefaults__}}) and this seems causing internally missing values in the function (non-bound arguments).


This ends up as below:

{code}
import types
import collections

def _copy_func(f):
    return types.FunctionType(f.__code__, f.__globals__, f.__name__,
        f.__defaults__, f.__closure__)

_old_namedtuple = _copy_func(collections.namedtuple)

_old_namedtuple(, ""b"")
_old_namedtuple(""a"")
{code}


If we call as below:

{code}
>>> _old_namedtuple(""a"", ""b"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: namedtuple() missing 3 required keyword-only arguments: 'verbose', 'rename', and 'module'
{code}


It throws an exception as above becuase {{__kwdefaults__}} for required keyword arguments seem unset in the copied function. So, if we give explicit value for these,

{code}
>>> _old_namedtuple(""a"", ""b"", verbose=False, rename=False, module=None)
<class '__main__.a'>
{code}

It works fine.

It seems now we should properly set these into the hijected one.",,apachespark,asukhenko,davies,dongjoon,emlyn,henrytxz,kabhwan,mariusvniekerk,MrMathias,Parixit,wangchao2017,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 25 07:26:25 UTC 2019,,,,,,,,,,"0|i3837z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Dec/16 02:47;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/16429;;;","17/Jan/17 17:53;davies;Issue resolved by pull request 16429
[https://github.com/apache/spark/pull/16429];;;","18/Mar/17 17:17;zero323;[~davies] Could it be backported to 1.6 and 2.0?;;;","19/Mar/17 02:00;henrytxz;Would also be interested in the answer to Maciej's question (for 2.0) and when is 2.1.1 scheduled to be released? Thank you!;;;","19/Mar/17 04:41;gurwls223;Let me try to make a PR to backport this if this is confirmed.;;;","21/Mar/17 09:29;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/17374;;;","21/Mar/17 09:54;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/17375;;;","02/May/17 10:49;gurwls223;To solve this problem fully, I had to port cloudpickle change too in the PR. Only fixing hijected one described above dose not fully solve this issue. Please refer the discussion in the PR and the change.;;;","15/Aug/17 14:50;MrMathias;Just got this error post fix on spark 2.1:


{code:java}
Traceback (most recent call last):
    File ""/opt/anaconda3/lib/python3.6/runpy.py"", line 183, in _run_module_as_main
      mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
    File ""/opt/anaconda3/lib/python3.6/runpy.py"", line 109, in _get_module_details
      __import__(pkg_name)
    File ""/usr/hdp/current/spark-client/python/pyspark/__init__.py"", line 41, in <module>
      from pyspark.context import SparkContext
    File ""/usr/hdp/current/spark-client/python/pyspark/context.py"", line 33, in <module>
      from pyspark.java_gateway import launch_gateway
    File ""/usr/hdp/current/spark-client/python/pyspark/java_gateway.py"", line 25, in <module>
      import platform
    File ""/opt/anaconda3/lib/python3.6/platform.py"", line 886, in <module>
      ""system node release version machine processor"")
    File ""/usr/hdp/current/spark-client/python/pyspark/serializers.py"", line 381, in namedtuple
      cls = _old_namedtuple(*args, **kwargs)
  TypeError: namedtuple() missing 3 required keyword-only arguments: 'verbose', 'rename', and 'module'
{code}
;;;","15/Aug/17 14:55;gurwls223;I think this was backported into Spark 2.1.1. Was your Spark version, 2.1.1+?;;;","16/Aug/17 10:13;MrMathias;Yea. This was just a pythonpath mishab on our end. 2.1.1 is a-okay.;;;","25/Feb/19 06:41;Parixit;I am facing the same issue in Spark1.6.0? Was this fixed for Spark 1.6.0 version? If not, are there any plans to do so?;;;","25/Feb/19 07:26;kabhwan;According to fix versions, fixed version of 1.6.x version line is 1.6.4. You need to upgrade to 1.6.4, but I believe 1.6 is EOL and no more support on community. You may need to upgrade the version to 2.3.3 (if you feel more safer to have bugfix versions in minor version) or 2.4.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NOT IN subquery with more than one column may return incorrect results,SPARK-19017,13030946,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nsyca,nsyca,nsyca,28/Dec/16 20:22,14/May/19 06:27,14/Jul/23 06:29,24/Jan/17 22:32,2.0.0,2.0.1,2.0.2,2.1.0,,,,,2.1.1,2.2.0,,,SQL,,,,,,,,0,correctness,,,,,"When putting more than one column in the NOT IN, the query may not return correctly if there is a null data. We can demonstrate the problem with the following data set and query:

{code}
Seq((2,1)).toDF(""a1"",""b1"").createOrReplaceTempView(""t1"")
Seq[(java.lang.Integer,java.lang.Integer)]((1,null)).toDF(""a2"",""b2"").createOrReplaceTempView(""t2"")

sql(""select * from t1 where (a1,b1) not in (select a2,b2 from t2)"").show
+---+---+
| a1| b1|
+---+---+
+---+---+
{code}
",,apachespark,hvanhovell,nsyca,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18966,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 05 15:20:47 UTC 2017,,,,,,,,,,"0|i382wv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Dec/16 20:33;nsyca;The semantics of the NOT IN for multiple columns T1(a1, b1, ... ) NOT IN T2(a2, b2, ...) is

# For any rows of T1 if a1 <> ALL (T2.a2), those rows are returned.
# For any rows of T1 if a1 = ANY (T2.a2), take the qualified rows from T1 and T2 and compare the values from the next pair of columns with the similar condition in 1. -- if b1 <> ALL (T2.b2), those rows are returned.
# Repeat the steps until the last pair in the column list.;;;","28/Dec/16 23:10;hvanhovell;[~nsyca] Why is this incorrect?

If I rewrite the NOT IN into a WHERE statement this would become:
{noformat}
select * from t1 where (a1 <> 1 AND b1 <> NULL)
{noformat}
There WHERE would evaluate to NULL, and it would never return a result.;;;","29/Dec/16 14:33;nsyca;Using your interpretation, (2,1) not in (2,0) would be evaluated to false. Spark returns (2,1). So do many other SQL engines.
;;;","29/Dec/16 14:43;hvanhovell;Ok, that is fair. Let me correct my mistake.

{{NOT IN}} can be rewritten into, in to a sequence of NOT equals statements. Each statement contains one tuple of the subquery relation. So we would get something like:
{noformat}
WHERE (NOT (a1 = a2(1) AND  b1 = b2(1))) AND (NOT (a1 = a2(2) AND  b1 = b2(2))) AND ... AND (NOT (a1 = a2(n) AND  b1 = b2(n)))
{noformat}
Which can be rewritten into:
{noformat}
WHERE  (a1 <> a2(1) OR  b1 <> b2(1)) AND (a1 <> a2(2) OR  b1 <> b2(2)) AND ... AND (a1 <> a2(n) OR  b1 <> b2(n))
{noformat}
This would evaluate to null if one of the tuples in the subquery relation contains a null.;;;","29/Dec/16 15:13;nsyca;In 3-value logic, true OR unknown = true. Using your formula above, we will have (2,1) NOT IN (1,null) evaluated as (2 <> 1) OR (1 <> null) which is true.;;;","04/Jan/17 01:40;apachespark;User 'nsyca' has created a pull request for this issue:
https://github.com/apache/spark/pull/16467;;;","04/Jan/17 17:09;hvanhovell;Ok, my bad. Lets try this again.

If I follow the NAAJ explanation in the section 6.1 of the Enhanced Subquery Optimizations in Oracle paper (http://www.vldb.org/pvldb/2/vldb09-423.pdf). Then I can rewrite the following query {{select * from tbl_a where a not in (select b from tbl_b)}} into the following (given that tbl_b has n elements):
{noformat}
SELECT *
FROM    TBL_a
WHERE  a <> b1 AND a <> b2 AND ... AND a <> bn
{noformat}

My basic thought here is that if we are comparing a complete tuple, for instance (2, 1), to an incomplete tuple, for instance (2, null), then the result should be unknown (null). Following this it would be obvious that a conjunctive predicate containing such an incomplete comparison should also evaluate to unknown.

It now boils down to how we should compare the tuples; i.e. how is {{a <> b}} evaluated. Note that I do not have an answer here. The following two things could apply:
* {{not(a.x1 = b.x1 and a.x2 = b.x2 and ... and a.xn = b.xn)}}  - I would lean towards this option.
* {{a.x1 <> b.x1 or a.x2 <> b.x2 or ... or a.xn <> b.xn)}} - Spark currently implements this option if you compare structs.


;;;","04/Jan/17 18:59;nsyca;I think we both agree that the result of the expression {{(2, 1) = (2, null)}} is unknown (null). It is the negation of this expression that is the centre of this discussion.

If {{a = (2, 1)}} and {{b = (1, null)}}, what is the semantics of {{a <> b}}? You have suggested two formulas

{{not(a.x1 = b.x1 and a.x2 = b.x2)}}

and

{{a.x1 <> b.x1 or a.x2 <> b.x2}}

Would you agree that these two formulas are equivalent? The second formula is just the result of applying De Morgan's laws on the first formula, i.e., {{not (x and y) == (not x) or (not y)}}.

How does Spark evaluate the formulas today? Here is my test:

{code}
scala> Seq(1).toDF(""t1a"").createOrReplaceTempView(""t1"")

scala> sql(""select t1a from t1"").show
+---+
|t1a|
+---+
|  1|
+---+

scala> sql(""select t1a from t1 where not (2=1 and 1=null)"").show
+---+
|t1a|
+---+
|  1|
+---+


scala> sql(""select t1a from t1 where 2<>1 or 1<>null"").show
+---+
|t1a|
+---+
|  1|
+---+
{code}
;;;","04/Jan/17 21:05;nsyca;I also have the output from a MySQL system. Note that I changed the tuple in {{t2}} to {{(3, null)}} to avoid having the value {{1}} twice but it should not make any difference.

{code}
$ mysql -V
mysql  Ver 14.14 Distrib 5.1.73, for redhat-linux-gnu (x86_64) using readline 5.1

mysql> create table mydb.t1(t1a int, t1b int);
Query OK, 0 rows affected (0.08 sec)

mysql> create table mydb.t2(t2a int, t2b int);
Query OK, 0 rows affected (0.11 sec)

mysql> insert into mydb.t1 values(2,1);
Query OK, 1 row affected (0.00 sec)

mysql> insert into mydb.t2 values(3, null);
Query OK, 1 row affected (0.00 sec)

mysql> select * from mydb.t1 where(t1a, t1b) not in(select t2a, t2b from mydb.t2);
+------+------+
| t1a  | t1b  |
+------+------+
|    2 |    1 |
+------+------+
1 row in set (0.00 sec)
{code};;;","04/Jan/17 21:15;hvanhovell;I agree that they are equal.

It just seems weird to me that in some cases it is ok that the tuples in the subquery can have null values. That being said, I am convinced that your approach is correct.

I have also checked if comparing structs returns the same results as the underlying fields. In some cases it does not:
{noformat}
scala> sql(""select (2, 2) <> (2, cast(null as int)) as c1, 2 <> 1 or 2 <> cast(null as int) as c2"").show
+----+----+
|  c1|  c2|
+----+----+
|true|true|
+----+----+

scala> sql(""select (1, 2) <> (2, cast(null as int)) as c1, 1 <> 1 or 2 <> cast(null as int) as c2"").show
+----+----+
|  c1|  c2|
+----+----+
|true|null| <-- Result for struct is wrong.
+----+----+
{noformat}
We fortunately do not use this, but this is still a bug.;;;","04/Jan/17 21:16;hvanhovell;Thanks for checking!;;;","04/Jan/17 21:24;nsyca;In the first query, the result does not look right to me.
{code}
c1: (2 <> 2) or (2 <> null) => false or unknown => unknown <- why true?
c2: (2 <> 1) or (2 <> null) => true or unknown => true
{code}
In the second query,
{code}
c1: (1 <> 2) or (2 <> null) => true or unknown => true
c2: (1 <> 1) or (2 <> null) => false or unknown => unknown <-- This is correct, right?
{code}

In (P1 or P2), if P1 is true, we can short-circuit and conclude the result is true. In the latter case, if P1 is false, we have to continue evaluating P2 and since P2 is unknown, the result is unknown.;;;","05/Jan/17 15:20;nsyca;One way to interpret the behaviour of the comparison of tuples is it operates strictly on 2-value logic of the comparison of each element of the tuples.

That is,

Case 1:
(a1, a2) = (b1, b2)

is interpreted as

IF (a1 = b1) is true THEN true ELSE false
AND
IF (a2 = b2) is true THEN true ELSE false

Case 2:
(a1, a2) <> (b1, b2)

is interpreted as a negation of the equality operator on the 2-value logic of each element.

IF (a1 = b1) is true THEN false ELSE true
OR
IF (a2 = b2) is true THEN false ELSE true


The NOT IN semantics is different that it preserves the 3-value logic in the comparison of each element.

I don't know that we should align the two usages to the same semantics or not. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document scalable partition handling feature in the programming guide,SPARK-19016,13030940,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lian cheng,lian cheng,lian cheng,28/Dec/16 19:14,30/Dec/16 22:51,14/Jul/23 06:29,30/Dec/16 22:51,2.1.0,2.2.0,,,,,,,2.1.1,2.2.0,,,Documentation,,,,,,,,0,,,,,,"Currently, we only mention this in the migration guide. Should also document it in the programming guide.",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 30 22:51:21 UTC 2016,,,,,,,,,,"0|i382vj:",9223372036854775807,,,,,,,,,,,,,2.1.0,2.2.0,,,,,,,,,,"28/Dec/16 19:19;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/16424;;;","30/Dec/16 22:51;lian cheng;Issue resolved by pull request 16424
[https://github.com/apache/spark/pull/16424];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CreateOrReplaceTempView throws org.apache.spark.sql.catalyst.parser.ParseException when viewName first char is numerical,SPARK-19012,13030695,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,jzijlstra,jzijlstra,27/Dec/16 12:58,30/Dec/16 08:19,14/Jul/23 06:29,29/Dec/16 20:22,2.0.1,2.0.2,2.1.0,,,,,,2.2.0,,,,SQL,,,,,,,,1,,,,,,"Using a viewName where the the fist char is a numerical value on dataframe.createOrReplaceTempView(viewName: String) causes:

{code}
Exception in thread ""main"" org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '1468079114' expecting {'SELECT', 'FROM', 'ADD', 'AS', 'ALL', 'DISTINCT', 'WHERE', 'GROUP', 'BY', 'GROUPING', 'SETS', 'CUBE', 'ROLLUP', 'ORDER', 'HAVING', 'LIMIT', 'AT', 'OR', 'AND', 'IN', NOT, 'NO', 'EXISTS', 'BETWEEN', 'LIKE', RLIKE, 'IS', 'NULL', 'TRUE', 'FALSE', 'NULLS', 'ASC', 'DESC', 'FOR', 'INTERVAL', 'CASE', 'WHEN', 'THEN', 'ELSE', 'END', 'JOIN', 'CROSS', 'OUTER', 'INNER', 'LEFT', 'SEMI', 'RIGHT', 'FULL', 'NATURAL', 'ON', 'LATERAL', 'WINDOW', 'OVER', 'PARTITION', 'RANGE', 'ROWS', 'UNBOUNDED', 'PRECEDING', 'FOLLOWING', 'CURRENT', 'ROW', 'WITH', 'VALUES', 'CREATE', 'TABLE', 'VIEW', 'REPLACE', 'INSERT', 'DELETE', 'INTO', 'DESCRIBE', 'EXPLAIN', 'FORMAT', 'LOGICAL', 'CODEGEN', 'CAST', 'SHOW', 'TABLES', 'COLUMNS', 'COLUMN', 'USE', 'PARTITIONS', 'FUNCTIONS', 'DROP', 'UNION', 'EXCEPT', 'INTERSECT', 'TO', 'TABLESAMPLE', 'STRATIFY', 'ALTER', 'RENAME', 'ARRAY', 'MAP', 'STRUCT', 'COMMENT', 'SET', 'RESET', 'DATA', 'START', 'TRANSACTION', 'COMMIT', 'ROLLBACK', 'MACRO', 'IF', 'DIV', 'PERCENT', 'BUCKET', 'OUT', 'OF', 'SORT', 'CLUSTER', 'DISTRIBUTE', 'OVERWRITE', 'TRANSFORM', 'REDUCE', 'USING', 'SERDE', 'SERDEPROPERTIES', 'RECORDREADER', 'RECORDWRITER', 'DELIMITED', 'FIELDS', 'TERMINATED', 'COLLECTION', 'ITEMS', 'KEYS', 'ESCAPED', 'LINES', 'SEPARATED', 'FUNCTION', 'EXTENDED', 'REFRESH', 'CLEAR', 'CACHE', 'UNCACHE', 'LAZY', 'FORMATTED', TEMPORARY, 'OPTIONS', 'UNSET', 'TBLPROPERTIES', 'DBPROPERTIES', 'BUCKETS', 'SKEWED', 'STORED', 'DIRECTORIES', 'LOCATION', 'EXCHANGE', 'ARCHIVE', 'UNARCHIVE', 'FILEFORMAT', 'TOUCH', 'COMPACT', 'CONCATENATE', 'CHANGE', 'CASCADE', 'RESTRICT', 'CLUSTERED', 'SORTED', 'PURGE', 'INPUTFORMAT', 'OUTPUTFORMAT', DATABASE, DATABASES, 'DFS', 'TRUNCATE', 'ANALYZE', 'COMPUTE', 'LIST', 'STATISTICS', 'PARTITIONED', 'EXTERNAL', 'DEFINED', 'REVOKE', 'GRANT', 'LOCK', 'UNLOCK', 'MSCK', 'REPAIR', 'RECOVER', 'EXPORT', 'IMPORT', 'LOAD', 'ROLE', 'ROLES', 'COMPACTIONS', 'PRINCIPALS', 'TRANSACTIONS', 'INDEX', 'INDEXES', 'LOCKS', 'OPTION', 'ANTI', 'LOCAL', 'INPATH', 'CURRENT_DATE', 'CURRENT_TIMESTAMP', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 0)

== SQL ==
11111
{code}

{code}
val tableOrViewName = ""11111"" //fails
val tableOrViewName = ""a1111"" //works
sparkSession.read.orc(path).createOrReplaceTempView(tableOrViewName)
{code}

",,apachespark,dongjoon,hvanhovell,jzijlstra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 30 08:19:43 UTC 2016,,,,,,,,,,"0|i381d3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Dec/16 13:45;hvanhovell;{{""11111""}} fails because it is a numeric literal. Either add a letter to the name, or place the name between backticks, e.g.: {{""`11111`""}}

This is not a bug per se, we could however improve the name parsing.;;;","27/Dec/16 14:51;jzijlstra;The type is viewName: String so you would expect any string to work. 

When executing createOrReplaceTempView(tableOrViewName: String) an ParseException of 
{code}
== SQL ==
{tableOrViewName}
{code}
is thrown. You might not see that these are related, because it goes on about an SQL ParseException and you just defined a tempView

You might not expect that the tableOrViewName trigger some SQL parsing. A slighter more clearer exception message (viewName not supported) could be more helpful or add the identifier rules set to the documentation.

As you said prefixing the tableOrViewName with a non-numerical value does the trick (although for me this feels more like a workaround);;;","28/Dec/16 23:20;dongjoon;Hi, [~hvanhovell] and [~jzijlstra].
I'll make a PR to raise `AnalysisException` instead.;;;","28/Dec/16 23:22;hvanhovell;[~dongjoon] Could make a PR that puts the name in backticks instead? That is a bit more friendly to the end user. Or do you think we will break stuff, if we do?;;;","28/Dec/16 23:31;dongjoon;Oh, you mean always wrap the name with backticks right?;;;","28/Dec/16 23:32;hvanhovell;Yeah, maybe a bit more subtle than that (we need to escape backticks in the name).;;;","28/Dec/16 23:33;dongjoon;No problem. However, we need to raise AnalysisException on empty table table still, ``.;;;","28/Dec/16 23:33;dongjoon;+1;;;","29/Dec/16 00:10;dongjoon;BTW, [~hvanhovell]. I found the existing related issue and testcases.
{code}
test(""SPARK-12982: Add table name validation in temp table registration"") {
    val df = Seq(""foo"", ""bar"").map(Tuple1.apply).toDF(""col"")
    // invalid table name test as below
    intercept[AnalysisException](df.createOrReplaceTempView(""t~""))
    // valid table name test as below
    df.createOrReplaceTempView(""table1"")
    // another invalid table name test as below
    intercept[AnalysisException](df.createOrReplaceTempView(""#$@sum""))
    // another invalid table name test as below
    intercept[AnalysisException](df.createOrReplaceTempView(""table!#""))
  }
{code}

To be consistent with this, we should throw AnalysisException on `createOrReplaceTempView(""11111"")`.

So, what we want here is to support `createOrReplaceTempView(""`11111`"")`. Did I understand clearly?;;;","29/Dec/16 00:13;dongjoon;Ur, actually, we already support `createOrReplaceTempView(""`11111`"")`.;;;","29/Dec/16 00:21;hvanhovell;Yeah, you have a point there. I was wondering if we would hit an issue here.

The question what we want to support:
* SQL compatibility. This would be one of the more common use cases. In that case it really does not make sense to support an identifier like '11111', because that would fail in SQL.
* As much flexibility as you want.

[~jzijlstra] could you explain how you are using this?
[~dongjoon] lets just make the exception better for now.;;;","29/Dec/16 00:27;dongjoon;Thank you for decision. Yep. I'll make the PR like that.;;;","29/Dec/16 01:13;dongjoon;In API docs and many places, `createOrReplaceTempView` was assumed not to throw any Exceptions.
It seems we need to discuss on my PR.;;;","29/Dec/16 01:17;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/16427;;;","29/Dec/16 07:55;jzijlstra;Good to see that its already being discussed. 

MSSQL also has some limitation in tableOrViewNames which is described in the documentation. Maybe updating the annotation of the method would also be enough. Having an Exception with a clear reason would definitely already a fix.

[~hvanhovell]
We specify our queries inside a configuration not the code. So we have this in our config:
dataPath = ""hdfs://""
dataQuery: ""SELECT column1, column2 FROM \[TABLE] WHERE 1 = 1""

Since we have one SparkSession for the application and the tableOrViewName is coupled to that and we don't want to specify an extra config option for the tableOrViewname, I though I'd just use the hashcode from the dataquery as the tableOrViewName. Use that in the createOrReplaceTempView and replace \[TABLE] inside the query with that. 

{code}
val path = ""hdfs://....{path}""
val dataQuery = ""SELECT * FROM [TABLE] LIMIT 1""

val tableOrViewName = ""_"" + Math.abs(path.hashCode).toString + Math.abs(qry.hashCode).toString

val df = sparkSession.read.orc(path)
df.createOrReplaceTempView(tableOrViewName)

val result = sparkSession.sqlContext.sql(qry.replace(""[TABLE]"", tableOrViewName)).collect
{code}

Later I want to check If the tableOrViewName has already been created and not call createOrReplaceTempView everytime, but this is just performance improvement.;;;","29/Dec/16 20:05;dongjoon;Yep. I tried to update the annotation but unfortunately it was reverted that now. (You can see that in my PR.)

> Maybe updating the annotation of the method would also be enough. Having an Exception with a clear reason would definitely already a fix.

Changing annotation on `public` API seems to be handled in a different issue with some more discussion because it affects many other codes (e.g. examples).;;;","29/Dec/16 20:24;hvanhovell;Ok, you could also start a table name with {{tbl_}} and that would also make the problem go away.;;;","30/Dec/16 08:19;jzijlstra;[~hvanhovell] Its already working for me, I was already prefixing the tableOrViewName. 
I thought you needed an example on how a developers mind work in (mis)using other peoples code.

Its nice to see that its been resolved in just 2 days. 

;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Fix `JDBCWriteSuite.testH2Dialect` by removing `getCatalystType`,SPARK-19004,13030616,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,26/Dec/16 19:21,27/Dec/16 14:28,14/Jul/23 06:29,27/Dec/16 14:28,2.1.0,,,,,,,,2.2.0,,,,SQL,Tests,,,,,,,0,,,,,,"JDBCSuite and JDBCWriterSuite have its own testH2Dialect for their testing purposes.

This issue fixes testH2Dialect in JDBCWriterSuite by removing getCatalystType implementation in order to return correct types. Currently, it returns Some(StringType) incorrectly. For the testH2Dialect in JDBCSuite, it's intentional because of the test case Remap types via JdbcDialects.",,apachespark,dongjoon,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 27 14:28:16 UTC 2016,,,,,,,,,,"0|i380vj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Dec/16 19:25;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/16409;;;","27/Dec/16 14:28;smilegator;Issue resolved by pull request 16409
[https://github.com/apache/spark/pull/16409];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to build/compile Spark in IntelliJ due to missing Scala deps in spark-tags,SPARK-18993,13030450,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,srowen,smilegator,smilegator,24/Dec/16 00:12,28/Dec/16 12:18,14/Jul/23 06:29,28/Dec/16 12:18,,,,,,,,,2.0.3,2.1.1,2.2.0,,Build,,,,,,,,1,,,,,,"After https://github.com/apache/spark/pull/16311 is merged, I am unable to build it in my IntelliJ. Got the following compilation error:

{noformat}
Error:scalac: error while loading Object, Missing dependency 'object scala in compiler mirror', required by /Library/Java/JavaVirtualMachines/jdk1.8.0_74.jdk/Contents/Home/jre/lib/rt.jar(java/lang/Object.class)
Error:scalac: Error: object scala in compiler mirror not found.
scala.reflect.internal.MissingRequirementError: object scala in compiler mirror not found.
	at scala.reflect.internal.MissingRequirementError$.signal(MissingRequirementError.scala:17)
	at scala.reflect.internal.MissingRequirementError$.notFound(MissingRequirementError.scala:18)
	at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:53)
	at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:66)
	at scala.reflect.internal.Mirrors$RootsBase.getPackage(Mirrors.scala:173)
	at scala.reflect.internal.Definitions$DefinitionsClass.ScalaPackage$lzycompute(Definitions.scala:161)
	at scala.reflect.internal.Definitions$DefinitionsClass.ScalaPackage(Definitions.scala:161)
	at scala.reflect.internal.Definitions$DefinitionsClass.ScalaPackageClass$lzycompute(Definitions.scala:162)
	at scala.reflect.internal.Definitions$DefinitionsClass.ScalaPackageClass(Definitions.scala:162)
	at scala.reflect.internal.Definitions$DefinitionsClass.init(Definitions.scala:1395)
	at scala.tools.nsc.Global$Run.<init>(Global.scala:1215)
	at xsbt.CachedCompiler0$$anon$2.<init>(CompilerInterface.scala:105)
	at xsbt.CachedCompiler0.run(CompilerInterface.scala:105)
	at xsbt.CachedCompiler0.run(CompilerInterface.scala:94)
	at xsbt.CompilerInterface.run(CompilerInterface.scala:22)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:101)
	at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:47)
	at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
	at org.jetbrains.jps.incremental.scala.local.IdeaIncrementalCompiler.compile(IdeaIncrementalCompiler.scala:29)
	at org.jetbrains.jps.incremental.scala.local.LocalServer.compile(LocalServer.scala:26)
	at org.jetbrains.jps.incremental.scala.remote.Main$.make(Main.scala:67)
	at org.jetbrains.jps.incremental.scala.remote.Main$.nailMain(Main.scala:24)
	at org.jetbrains.jps.incremental.scala.remote.Main.nailMain(Main.scala)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.martiansoftware.nailgun.NGSession.run(NGSession.java:319)
{noformat}",,apachespark,dongjoon,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 28 12:18:06 UTC 2016,,,,,,,,,,"0|i37zun:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Dec/16 00:17;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16393;;;","27/Dec/16 21:31;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16418;;;","28/Dec/16 12:18;srowen;Issue resolved by pull request 16418
[https://github.com/apache/spark/pull/16418];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DESC TABLE should not fail with format class not found,SPARK-18989,13030380,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,23/Dec/16 13:49,26/Dec/16 19:28,14/Jul/23 06:29,26/Dec/16 19:28,,,,,,,,,2.2.0,,,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 26 19:28:22 UTC 2016,,,,,,,,,,"0|i37zf3:",9223372036854775807,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,,,,,,,"23/Dec/16 13:55;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16388;;;","26/Dec/16 19:28;smilegator;Issue resolved by pull request 16388
[https://github.com/apache/spark/pull/16388];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExternalAppendOnlyMap shouldn't fail when forced to spill before calling its iterator,SPARK-18986,13030306,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,23/Dec/16 05:45,11/Jul/17 10:03,14/Jul/23 06:29,17/Feb/17 19:28,,,,,,,,,2.2.0,,,,Spark Core,,,,,,,,0,,,,,,"{{ExternalAppendOnlyMap.forceSpill}} now uses an assert to check if an iterator is not null in the map. However, the assertion is only true after the map is asked for iterator. Before it, if another memory consumer asks more memory than currently available, {{ExternalAppendOnlyMap.forceSpill}} is also be called too. In this case, we will see failure like this:

{code}
[info]   java.lang.AssertionError: assertion failed
[info]   at scala.Predef$.assert(Predef.scala:156)
[info]   at org.apache.spark.util.collection.ExternalAppendOnlyMap.forceSpill(ExternalAppendOnlyMap.scala:196)
[info]   at org.apache.spark.util.collection.Spillable.spill(Spillable.scala:111)
[info]   at org.apache.spark.util.collection.ExternalAppendOnlyMapSuite$$anonfun$13.apply$mcV$sp(ExternalAppendOnly
MapSuite.scala:294)
{code}",,apachespark,gilinachum,samkum,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 11 10:03:48 UTC 2017,,,,,,,,,,"0|i37yyn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Dec/16 05:51;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/16387;;;","30/Dec/16 05:55;samkum;Shouldn't the priority be increased for this because I am facing this issue on almost every batch interval and the data doesn't get processed any further, which is a significant data loss for any application.;;;","30/Dec/16 09:40;srowen;(Priority really doesn't mean anything. Feel free to help review the PR.);;;","11/Jul/17 10:03;gilinachum;I'm currently using 2.11, and this was fixed in 2.2.0 (not released yet).
The workaround I used (might help others) was to increase the # of partitions to avoid the buggy spill altogether.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Netty issue may cause the shuffle client hang,SPARK-18971,13029974,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,22/Dec/16 02:05,03/Oct/17 06:10,14/Jul/23 06:29,15/Jan/17 11:15,,,,,,,,,2.1.2,2.1.3,2.2.0,,Spark Core,,,,,,,,0,,,,,,"Check https://github.com/netty/netty/issues/6153 for details

You should be able to see the following similar stack track in the executor thread dump.
{code}
""shuffle-client-7-4"" daemon prio=5 tid=97 RUNNABLE
        at io.netty.util.Recycler$Stack.scavengeSome(Recycler.java:504)
        at io.netty.util.Recycler$Stack.scavenge(Recycler.java:454)
        at io.netty.util.Recycler$Stack.pop(Recycler.java:435)
        at io.netty.util.Recycler.get(Recycler.java:144)
        at io.netty.buffer.PooledUnsafeDirectByteBuf.newInstance(PooledUnsafeDirectByteBuf.java:39)
        at io.netty.buffer.PoolArena$DirectArena.newByteBuf(PoolArena.java:727)
        at io.netty.buffer.PoolArena.allocate(PoolArena.java:140)
        at io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:271)
        at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:177)
        at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:168)
        at io.netty.buffer.AbstractByteBufAllocator.ioBuffer(AbstractByteBufAllocator.java:129)
        at io.netty.channel.AdaptiveRecvByteBufAllocator$HandleImpl.allocate(AdaptiveRecvByteBufAllocator.java:104)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:117)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
        at java.lang.Thread.run(Thread.java:745)
{code}",,andreu.urruela,apachespark,asukhenko,devaraj,emlyn,gengmao,jooseong,niccolo.becchi,roczei,sayuan,tgraves,Venkat Sambath,waldoppper,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19300,SPARK-19883,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 07 15:47:35 UTC 2017,,,,,,,,,,"0|i37wwv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/17 12:14;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16568;;;","15/Jan/17 11:15;srowen;Issue resolved by pull request 16568
[https://github.com/apache/spark/pull/16568];;;","29/Mar/17 10:14;emlyn;Will this fix go into Spark 2.1.1?;;;","05/Apr/17 02:35;zsxwing;It's not backported because there are two many changes in the new Netty version and it's too risky to upgrade in a maintenance release.. We may backport it to 2.1 if we don't see any regression after Spark 2.2.0 is released.;;;","05/May/17 16:31;tgraves;[~zsxwing]have you seen any issues with the new netty version?  We have hit this same issue.;;;","05/May/17 17:48;zsxwing;[~tgraves] No, as far as I know. But since Spark 2.2.0 has not yet been released, not sure how many people tested master or branch-2.2.;;;","11/Jul/17 11:59;andreu.urruela;Hi, we are hitting the same problem in a production system. Is there any workaround for this problem using Spark 2.1.1? Any Netty settings?;;;","11/Jul/17 18:10;zsxwing;[~andreu.urruela] Update Spark to 2.2.0 (not yet announce, but the votes passed and you can get from maven: https://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.spark%22) or build a special Spark 2.1.1 with the new Netty version by yourself? I don't know any other workarounds. This Netty issue exists for a long time. However, it's pretty hard to figure out which change makes Spark easy to hit this issue.;;;","07/Sep/17 15:47;niccolo.becchi;We are facing the same issue, and we are planning to update to Spark 2.2 but it will take weeks. Does anyone know a way to force Spark 2.1.0 to use the netty-all-4.0.43.Final.jar? The options ""spark.executor.userClassPathFirst=true"", ""spark.jars.exclude"", ""spark.jars.packages"" don't seem to work. Any help will be much appreciated!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FileSource failure during file list refresh doesn't cause an application to fail, but stops further processing",SPARK-18970,13029972,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,lev.numerify,lev.numerify,22/Dec/16 02:01,23/Mar/17 00:28,14/Jul/23 06:29,23/Mar/17 00:28,2.0.0,2.0.2,,,,,,,2.1.0,,,,SQL,Structured Streaming,,,,,,,0,,,,,,"Spark streaming application uses S3 files as streaming sources. After running for several day processing stopped even though an application continued to run. 
Stack trace:
{code}
java.io.FileNotFoundException: No such file or directory 's3n://XXXXXXXXXXXXXXXXX'
	at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:818)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:511)
	at org.apache.spark.sql.execution.datasources.HadoopFsRelation$$anonfun$7$$anonfun$apply$3.apply(fileSourceInterfaces.scala:465)
	at org.apache.spark.sql.execution.datasources.HadoopFsRelation$$anonfun$7$$anonfun$apply$3.apply(fileSourceInterfaces.scala:462)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:893)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:893)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}
I believe 2 things should (or can) be fixed:
1. Application should fail in case of such an error.
2. Allow application to ignore such failure, since there is a chance that during next refresh the error will not resurface. (In my case I believe an error was cased by S3 cleaning the bucket exactly at the same moment when refresh was running) 

My code to create streaming processing looks as the following:
{code}
      val cq = sqlContext.readStream
        .format(""json"")
        .schema(struct)
        .load(s""input"")
        .writeStream
        .option(""checkpointLocation"", s""checkpoints"")
        .foreach(new ForeachWriter[Row] {...})
        .trigger(ProcessingTime(""10 seconds"")).start()
		
	  cq.awaitTermination()	
{code}",,codingcat,lev.numerify,lwlin,marmbrus,ozawa,stevel@apache.org,uncleGen,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/16 22:15;lev.numerify;sparkerror.log;https://issues.apache.org/jira/secure/attachment/12844478/sparkerror.log",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 23 00:28:09 UTC 2017,,,,,,,,,,"0|i37wwf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/16 21:41;zsxwing;Did the Spark task fail or not? Looks like the Spark task was retried and it succeeded.

For ignoring such failure, it's done in SPARK-17850 and SPARK-18774.;;;","22/Dec/16 21:55;lev.numerify;I am not sure whether task was retried or not, but Spark application never processed any new file after this error has occurred. It appears that this error is never passed to the application code itself and application doesn't have a chance to terminate itself. Perhaps I should mention that I am running several spark structured streams in my application. 
We'll wait for 2.1 release to test SPARK-18774. I couldn't find any release date for 2.1. Any idea when it may happen?;;;","22/Dec/16 22:06;zsxwing;Where did you find the exception? If it's in the driver, could you post the full stack trace?

2.1 RC5 passed, so it should be available very soon.;;;","22/Dec/16 22:15;lev.numerify;Here is the log file;;;","22/Dec/16 22:43;zsxwing;I see. FileStreamSource ignores FileNotFoundException when trying to get a file status inside the input directory. This allows the user to clean up the old processed files without failing the query. Any reason you want to disable this behavior?;;;","23/Dec/16 01:39;lev.numerify;Actually this is exactly the behavior I want. My problem is that application appeared to be alive, but was not processing and new files after this message in the log. I intentionally included the portion of the log at the top, showing that file list was refreshed every couple of minutes before. But as you can see refreshing have stopped after the logged error.;;;","17/Jan/17 08:59;uncleGen;[~lev.numerify] Any update about this issue with 2.1.0 release?;;;","23/Mar/17 00:28;marmbrus;I'm going to close this, but please reopen if you can reproduce on 2.1.1+.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PullOutNondeterministic should work for Aggregate operator,SPARK-18969,13029965,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,rxin,rxin,22/Dec/16 01:33,12/Jan/17 12:28,14/Jul/23 06:29,12/Jan/17 12:28,,,,,,,,,2.0.3,2.1.1,2.2.0,,SQL,,,,,,,,0,,,,,,"This test case should pass:

{code}
    checkAnalysis(
      r.groupBy(rnd)(rnd),
      r.select(a, b, rnd).groupBy(rndref)(rndref)
    )
{code}",,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19035,,,,,,,,,,SPARK-19035,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 26 10:45:04 UTC 2016,,,,,,,,,,"0|i37wuv:",9223372036854775807,,,,,,,,,,,,,2.0.3,2.1.0,2.2.0,,,,,,,,,"22/Dec/16 01:36;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/16379;;;","26/Dec/16 10:45;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16404;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Locality preferences should be used when scheduling even when delay scheduling is turned off,SPARK-18967,13029848,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,irashid,irashid,irashid,21/Dec/16 18:23,17/May/20 17:48,14/Jul/23 06:29,07/Feb/17 06:38,2.1.0,,,,,,,,2.2.0,,,,Scheduler,Spark Core,,,,,,,0,,,,,,"If you turn delay scheduling off by setting {{spark.locality.wait=0}}, you effectively turn off the use the of locality preferences when there is a bulk scheduling event.  {{TaskSchedulerImpl}} will use resources based on whatever random order it decides to shuffle them, rather than taking advantage of the most local options.

This happens because {{TaskSchedulerImpl}} offers resources to a {{TaskSetManager}} one at a time, each time subject to a maxLocality constraint.  However, that constraint doesn't move through all possible locality levels -- it uses [{{tsm.myLocalityLevels}} |https://github.com/apache/spark/blob/1a64388973711b4e567f25fa33d752066a018b49/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala#L360].  And {{tsm.myLocalityLevels}} [skips locality levels completely if the wait == 0 | https://github.com/apache/spark/blob/1a64388973711b4e567f25fa33d752066a018b49/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L953].  So with delay scheduling off, {{TaskSchedulerImpl}} immediately jumps to giving tsms the offers with {{maxLocality = ANY}}.

*WORKAROUND*: instead of setting {{spark.locality.wait=0}}, use {{spark.locality.wait=1ms}}.  The one downside of this is if you have tasks that actually take less than 1ms.  You could even run into SPARK-18886.  But that is a relatively unlikely scenario for real workloads.",,apachespark,irashid,kayousterhout,rxin,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 07 21:41:52 UTC 2017,,,,,,,,,,"0|i37w4v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Dec/16 18:47;irashid;cc [~kayousterhout] [~markhamstra] [~mridulm80];;;","21/Dec/16 21:52;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/16376;;;","07/Feb/17 09:14;rxin;Please use 2.2.0, not 2.2;;;","07/Feb/17 21:41;kayousterhout;Oops this was me [~rxin] sorry!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Test Failuire on big endian; o.a.s.unsafe.types.UTF8StringSuite.writeToOutputStreamIntArray",SPARK-18963,13029692,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,robbinspg,robbinspg,robbinspg,21/Dec/16 10:33,23/Dec/16 12:16,14/Jul/23 06:29,23/Dec/16 12:15,2.1.1,,,,,,,,2.2.0,,,,Tests,,,,,,,,0,,,,,,"SPARK-18658 introduced a new test which is flipping a ByteBuffer into little endian order. This is not necessary on a big endian platform and results in:

writeToOutputStreamIntArray(org.apache.spark.unsafe.types.UTF8StringSuite)  Time elapsed: 0.01 sec  <<< FAILURE!
org.junit.ComparisonFailure: expected:<[大千世界]> but was:<[姤�䃍���]>
	at org.apache.spark.unsafe.types.UTF8StringSuite.writeToOutputStreamIntArray(UTF8StringSuite.java:609)


PR on it's way",,apachespark,robbinspg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 23 12:15:57 UTC 2016,,,,,,,,,,"0|i37v67:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Dec/16 10:48;apachespark;User 'robbinspg' has created a pull request for this issue:
https://github.com/apache/spark/pull/16375;;;","23/Dec/16 12:15;srowen;Issue resolved by pull request 16375
[https://github.com/apache/spark/pull/16375];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR should support toJSON on DataFrame,SPARK-18958,13029624,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,felixcheung,felixcheung,felixcheung,21/Dec/16 03:10,29/Mar/17 23:08,14/Jul/23 06:29,28/Dec/16 20:27,2.1.0,,,,,,,,2.2.0,,,,SparkR,,,,,,,,0,,,,,,It makes it easier to interop with other component (esp. since R does not have json support built in),,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 21 03:12:03 UTC 2016,,,,,,,,,,"0|i37ur3:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"21/Dec/16 03:12;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/16368;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flaky test: o.a.s.streaming.BasicOperationsSuite rdd cleanup - map and window,SPARK-18954,13029598,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,20/Dec/16 23:36,21/Dec/16 20:00,14/Jul/23 06:29,21/Dec/16 20:00,,,,,,,,,2.0.3,2.1.1,2.2.0,,Tests,,,,,,,,0,flaky-test,,,,,https://spark-tests.appspot.com/test-details?suite_name=org.apache.spark.streaming.BasicOperationsSuite&test_name=rdd+cleanup+-+map+and+window,,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 20 23:39:04 UTC 2016,,,,,,,,,,"0|i37ulb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/16 23:39;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16362;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
regex strings not properly escaped in codegen for aggregations,SPARK-18952,13029575,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,20/Dec/16 21:59,02/May/17 18:30,14/Jul/23 06:29,09/Jan/17 23:18,2.0.2,,,,,,,,2.1.1,2.2.0,,,SQL,,,,,,,,0,,,,,,"If I use the function regexp_extract, and then in my regex string, use `\`, i.e. escape character, this fails codegen, because the `\` character is not properly escaped when codegen'd.

Example stack trace:
{code}
/* 059 */     private int maxSteps = 2;
/* 060 */     private int numRows = 0;
/* 061 */     private org.apache.spark.sql.types.StructType keySchema = new org.apache.spark.sql.types.StructType().add(""date_format(window#325.start, yyyy-MM-dd HH:mm)"", org.apache.spark.sql.types.DataTypes.StringType)
/* 062 */     .add(""regexp_extract(source#310.description, ([a-zA-Z]+)\[.*, 1)"", org.apache.spark.sql.types.DataTypes.StringType);
/* 063 */     private org.apache.spark.sql.types.StructType valueSchema = new org.apache.spark.sql.types.StructType().add(""sum"", org.apache.spark.sql.types.DataTypes.LongType);
/* 064 */     private Object emptyVBase;

...

org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 62, Column 58: Invalid escape sequence
	at org.codehaus.janino.Scanner.scanLiteralCharacter(Scanner.java:918)
	at org.codehaus.janino.Scanner.produce(Scanner.java:604)
	at org.codehaus.janino.Parser.peekRead(Parser.java:3239)
	at org.codehaus.janino.Parser.parseArguments(Parser.java:3055)
	at org.codehaus.janino.Parser.parseSelector(Parser.java:2914)
	at org.codehaus.janino.Parser.parseUnaryExpression(Parser.java:2617)
	at org.codehaus.janino.Parser.parseMultiplicativeExpression(Parser.java:2573)
	at org.codehaus.janino.Parser.parseAdditiveExpression(Parser.java:2552)
{code}

In the codegend expression, the literal should use `\\` instead of `\`
",,apachespark,brkyvz,joshrosen,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20556,,,,,,SPARK-18866,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 09 23:18:32 UTC 2017,,,,,,,,,,"0|i37ug7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/16 23:18;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/16361;;;","09/Jan/17 20:02;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/16518;;;","09/Jan/17 23:18;joshrosen;Fixed by Burak's patch for 2.1.1 / 2.2.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade com.thoughtworks.paranamer/paranamer to 2.6,SPARK-18951,13029568,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,20/Dec/16 21:32,21/Dec/16 17:27,14/Jul/23 06:29,21/Dec/16 17:27,,,,,,,,,2.2.0,,,,Build,,,,,,,,0,,,,,,"I recently hit a bug of com.thoughtworks.paranamer/paranamer, which causes jackson fail to handle byte array defined in a case class. Then I find https://github.com/FasterXML/jackson-module-scala/issues/48, which suggests that it is caused by a bug in paranamer. Let's upgrade paranamer. 

Since we are using jackson 2.6.5 and jackson-module-paranamer 2.6.5 use com.thoughtworks.paranamer/paranamer 2.6, I suggests that we upgrade paranamer to 2.6. ",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 21 17:27:56 UTC 2016,,,,,,,,,,"0|i37uen:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/16 22:00;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/16359;;;","21/Dec/16 17:27;yhuai;Issue resolved by pull request 16359
[https://github.com/apache/spark/pull/16359];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Report conflicting fields when merging two StructTypes.,SPARK-18950,13029566,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bravozhang,lian cheng,lian cheng,20/Dec/16 21:31,01/Aug/17 03:14,14/Jul/23 06:29,01/Aug/17 03:14,,,,,,,,,2.3.0,,,,SQL,,,,,,,,0,starter,,,,,"Currently, {{StructType.merge()}} only reports data types of conflicting fields when merging two incompatible schemas. It would be nice to also report the field names for easier debugging.",,apachespark,bravozhang,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 21 00:01:04 UTC 2016,,,,,,,,,,"0|i37ue7:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"20/Dec/16 21:57;bravozhang;I'll work on this, thanks;;;","21/Dec/16 00:01;apachespark;User 'bravo-zhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/16365;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Use Mesos ""Dynamic Reservation"" resource for Spark",SPARK-18935,13029341,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,skonto,jackyoh,jackyoh,20/Dec/16 04:05,29/Nov/17 22:16,14/Jul/23 06:29,29/Nov/17 22:16,2.0.0,2.0.1,2.0.2,,,,,,2.3.0,,,,Mesos,,,,,,,,0,,,,,,"I'm running spark on Apache Mesos

Please follow these steps to reproduce the issue:

1. First, run Mesos resource reserve:

curl -i -d slaveId=c24d1cfb-79f3-4b07-9f8b-c7b19543a333-S0 -d resources='[{""name"":""cpus"",""type"":""SCALAR"",""scalar"":{""value"":20},""role"":""spark"",""reservation"":{""principal"":""""}},{""name"":""mem"",""type"":""SCALAR"",""scalar"":{""value"":4096},""role"":""spark"",""reservation"":{""principal"":""""}}]' -X POST http://192.168.1.118:5050/master/reserve

2. Then run spark-submit command:

./spark-submit --class org.apache.spark.examples.SparkPi --master mesos://192.168.1.118:5050 --conf spark.mesos.role=spark  ../examples/jars/spark-examples_2.11-2.0.2.jar 10000


And the console will keep loging same warning message as shown below: 

16/12/19 22:33:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources",,Ameenakel,apachespark,arand,devaraj,jackyoh,skonto,susanxhuynh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 29 11:56:04 UTC 2017,,,,,,,,,,"0|i37t0f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/17 13:17;arand;Are people still interested in this being fixed? I'm going to try and reproduce the problem and scope out the effort to fix it. ;;;","25/Jul/17 22:16;Ameenakel;Although I'm only one data point: I'm interested in this being fixed.;;;","28/Sep/17 11:22;skonto;I can reproduce it locally in client mode according to the example, I am looking into this.

Btw I tested this a while ago with spark latest version (master) and cluster mode (if I recall correctly) and stuck here:

{noformat}
I0829 14:05:56.342872 21756 master.cpp:6532] Sending status update TASK_ERROR for task 1 of framework f0a1a46a-e404-4faa-87f7-29479f30b57e-0009 'Total resources cpus(spark-prive)(allocated: spark-prive):1; cpus(*)(allocated: spark-prive):1; mem(spark-prive)(allocated: spark-prive):1024; mem(*)(allocated: spark-prive):384 required by task and its executor is more than available disk(spark-prive, )(allocated: spark-prive):1000; ports(spark-prive, )(allocated: spark-prive):[31000-32000]; cpus(spark-prive, )(allocated: spark-prive):1; mem(spark-prive, )(allocated: spark-prive):1024; cpus(*)(allocated: spark-prive):1; mem(*)(allocated: spark-prive):976; disk(*)(allocated: spark-prive):9000'
{noformat}


To reserve resources I used:

{code:java}
curl -i -d slaveId=cf885682-8a28-4e82-b5db-a01277edfafc-S0 -d resources='[{""name"":""disk"",""type"":""SCALAR"",""scalar"": {""value"":1000} ,""role"":""spark-prive"",""reservation"":{""principal"":""""}},{""name"":""ports"",""type"":""RANGES"",""ranges"": { ""range"": [{""begin"": 31000, ""end"": 32000}] },""role"":""spark-prive"",""reservation"":{""principal"":""""}}, {""name"":""cpus"",""type"":""SCALAR"",""scalar"": {""value"":1} ,""role"":""spark-prive"",""reservation"":{""principal"":""""}},{""name"":""mem"",""type"":""SCALAR"",""scalar"": {""value"":1024},""role"":""spark-prive"",""reservation"":{""principal"":""""}}]' -X POST http://172.17.0.1:5050/master/reserve

{code}

The error comes from here: [https://github.com/apache/mesos/blob/11ee081ee578ea12e85799e00c5fe8b89eb6ea5f/src/master/validation.cpp#L1339].
I havent checked with a principal yet, but seems optional if you have no authorization (https://github.com/apache/mesos/commit/efbdef8dfd96ff08c1342b171ef89dcb266bdce7.
However the comments here require it: https://github.com/apache/mesos/blob/master/include/mesos/mesos.proto#L274-L278
The error itself indicates that for some reason the task resources requested are less than the available on the slave. I checked the numbers does not make sense unless there is a mismatch for the principal empty vs """" or something. I will test. Since numbers are fine it seems the failure is somewhere here where the resources are compared: https://github.com/apache/mesos/blob/d47641039f5e2dd18af007250ef7ae2a34258a2d/src/common/resources.cpp#L438
;;;","28/Sep/17 18:15;skonto;I verified the example and error is the same yet the reason is as in the cluster mode case:

{noformat}

17/09/28 21:07:34 INFO MesosCoarseGrainedSchedulerBackend: Mesos task 1 is now TASK_ERROR
17/09/28 21:07:34 INFO MesosCoarseGrainedSchedulerBackend: Blacklisting Mesos slave 433038b9-80aa-43ef-b6eb-0075f5028d37-S0 due to too many failures; is Spark installed on it?
17/09/28 21:07:34 DEBUG CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove executor 1 with reason Executor finished with state LOST
17/09/28 21:07:34 INFO BlockManagerMaster: Removal of executor 1 requested
17/09/28 21:07:34 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 1
17/09/28 21:07:34 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.

{noformat}

The task is failing and the agent is blacklisted which leads to starvation. 
In the scheduler we check for task failures for a slave in order to avoid feature launches there:

{code:java}
slaves.get(slaveId).map(_.taskFailures).getOrElse(0) < MAX_SLAVE_FAILURES 
{code}

The task is failing due to:

{noformat}
I0928 21:07:34.621839  5559 master.cpp:6532] Sending status update TASK_ERROR for task 0 of framework e46985fe-1392-4d39-a3d5-e7ec77810695-0004 'Total resources cpus(spark-prive)(allocated: spark-prive):8; mem(spark-prive)(allocated: spark-prive):1408 required by task and its executor is more than available ports(spark-prive, )(allocated: spark-prive):[31000-32000]; disk(spark-prive, )(allocated: spark-prive):1000; cpus(spark-prive, )(allocated: spark-prive):8; mem(spark-prive, )(allocated: spark-prive):10024; mem(*)(allocated: spark-prive):4590; disk(*)(allocated: spark-prive):103216'
I0928 21:07:34.622593  5559 hierarchical.cpp:850] Updated allocation of framework e46985fe-1392-4d39-a3d5-e7ec77810695-0004 on agent 433038b9-80aa-43ef-b6eb-0075f5028d37-S0 from ports(spark-prive, )(allocated: spark-prive):[31000-32000]; disk(spark-prive, )(allocated: spark-prive):1000; cpus(spark-prive, )(allocated: spark-prive):8; mem(spark-prive, )(allocated: spark-prive):10024; mem(*)(allocated: spark-prive):4590; disk(*)(allocated: spark-prive):103216 to ports(spark-prive, )(allocated: spark-prive):[31000-32000]; disk(spark-prive, )(allocated: spark-prive):1000; cpus(spark-prive, )(allocated: spark-prive):8; mem(spark-prive, )(allocated: spark-prive):10024; mem(*)(allocated: spark-prive):4590; disk(*)(allocated: spark-prive):103216
I0928 21:07:34.647950  5559 master.cpp:4941] Processing REVIVE call for framework e46985fe-1392-4d39-a3d5-e7ec77810695-0004 (Spark Pi) at scheduler-df433215-b87c-4b9b-993c-a3253c5f11a8@127.0.1.1:34775

{noformat}

So again its the same reason as I have seen before.

If I try to set a principle to spark mesos framework will not be able to register because even if I set a secret the driver will be aborted:

{noformat}
I0928 21:19:54.793844  7363 sched.cpp:232] Version: 1.3.0
I0928 21:19:54.795897  7355 sched.cpp:336] New master detected at master@127.0.1.1:5050
I0928 21:19:54.796042  7355 sched.cpp:407] Authenticating with master master@127.0.1.1:5050
I0928 21:19:54.796052  7355 sched.cpp:414] Using default CRAM-MD5 authenticatee
I0928 21:19:54.796152  7361 authenticatee.cpp:97] Initializing client SASL
I0928 21:19:54.814299  7361 authenticatee.cpp:121] Creating new client SASL connection
I0928 21:19:54.815407  7357 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0928 21:19:54.815421  7357 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0928 21:19:54.815757  7360 authenticatee.cpp:259] Received SASL authentication step
E0928 21:19:54.816179  7355 sched.cpp:507] Master master@127.0.1.1:5050 refused authentication
I0928 21:19:54.816193  7355 sched.cpp:1187] Got error 'Master refused authentication'
I0928 21:19:54.816200  7355 sched.cpp:2055] Asked to abort the driver
17/09/28 21:19:54 ERROR MesosCoarseGrainedSchedulerBackend: Mesos error: Master refused authentication
Exception in thread ""Thread-12"" org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master refused authentication
	at org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:500)
	at org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend.error(MesosCoarseGrainedSchedulerBackend.scala:599)
I0928 21:19:54.817343  7355 sched.cpp:2055] Asked to abort the driver
I0928 21:19:54.817355  7355 sched.cpp:1233] Aborting framework 

{noformat}

;;;","28/Sep/17 22:05;skonto;Using a principal does not make any difference:

{noformat}
I0928 22:39:34.785213 11269 master.cpp:6532] Sending status update TASK_ERROR for task 0 of framework f20de49b-dee3-45dd-a3c1-73418b7de891-0000 'Total resources cpus(spark-prive)(allocated: spark-prive):8; mem(spark-prive)(allocated: spark-prive):1408 required by task and its executor is more than available ports(spark-prive, test)(allocated: spark-prive):[31000-32000]; disk(spark-prive, test)(allocated: spark-prive):1000; cpus(spark-prive, test)(allocated: spark-prive):8; mem(spark-prive, test)(allocated: spark-prive):10024; mem(*)(allocated: spark-prive):4590; disk(*)(allocated: spark-prive):103216'
I0928 22:39:34.785848 11270 hierarchical.cpp:850] Updated allocation of framework f20de49b-dee3-45dd-a3c1-73418b7de891-0000 on agent 433038b9-80aa-43ef-b6eb-0075f5028d37-S0 from ports(spark-prive, test)(allocated: spark-prive):[31000-32000]; disk(spark-prive, test)(allocated: spark-prive):1000; cpus(spark-prive, test)(allocated: spark-prive):8; mem(spark-prive, test)(allocated: spark-prive):10024; mem(*)(allocated: spark-prive):4590; disk(*)(allocated: spark-prive):103216 to ports(spark-prive, test)(allocated: spark-prive):[31000-32000]; disk(spark-prive, test)(allocated: spark-prive):1000; cpus(spark-prive, test)(allocated: spark-prive):8; mem(spark-prive, test)(allocated: spark-prive):10024; mem(*)(allocated: spark-prive):4590; disk(*)(allocated: spark-prive):103216
./spark-submit --verbose --class org.apache.spark.examples.SparkPi --master mesos://universe:5050 --conf spark.mesos.role=spark-prive  --conf spark.mesos.principal=test --conf spark.mesos.secret=test ../examples/jars/spark-examples_2.11-2.3.0-SNAPSHOT.jar 100000```
{noformat}

The problem was at the scheduler side because we dont propagate the reservationInfo to the used resources.
We received resources of the form:
{noformat}
name: ""disk""
type: SCALAR
scalar {
  value: 1000.0
}
role: ""spark-prive""
reservation {
  principal: ""test""
}
allocation_info {
  role: ""spark-prive""
}

{noformat}

{noformat}
And when we launch the task we pass resources of the form:
type: SCALAR
scalar {
  value: 8.0
}
role: ""spark-prive""
,name: ""mem""
type: SCALAR
scalar {
  value: 1408.0
}
role: ""spark-prive""
{noformat}

I made a fix and verified it works with and without an explicit principal. The method here: https://github.com/apache/spark/blob/d74dee1336e7152cc0fb7d2b3bf1a44f4f452025/resource-managers/mesos/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils.scala#L178 just needs to preserve reservation and allocation info.
I will have a PR shortly.;;;","29/Sep/17 03:39;jackyoh;It's clear to me now.

Thank you for your kind assistance.;;;","29/Sep/17 11:56;apachespark;User 'skonto' has created a pull request for this issue:
https://github.com/apache/spark/pull/19390;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FileScanRDD, JDBCRDD, and UnsafeSorter should support task cancellation",SPARK-18928,13029266,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,19/Dec/16 21:33,21/Dec/16 00:05,14/Jul/23 06:29,20/Dec/16 00:27,,,,,,,,,2.0.3,2.1.1,2.2.0,,Spark Core,SQL,,,,,,,0,,,,,,"Spark tasks respond to cancellation by checking {{TaskContext.isInterrupted()}}, but this check is missing on a few critical paths used in Spark SQL, including FileScanRDD, JDBCRDD, and UnsafeSorter-based sorts. This can cause interrupted / cancelled tasks to continue running and become zombies.

Here's an example: first, create a giant text file. In my case, I just concatenated /usr/share/dict/words a bunch of times to produce a 2.75 gig file. Then, run a really slow query over that file and try to cancel it:

{code}
spark.read.text(""/tmp/words"").selectExpr(""value + value + value"").collect()
{code}

This will sit and churn at 100% CPU for a minute or two because the task isn't checking the interrupted flag.

The solution here is to add InterruptedIterator-style checks to a few locations where they're currently missing in Spark SQL.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 20 19:51:04 UTC 2016,,,,,,,,,,"0|i37sjr:",9223372036854775807,,,,,,,,,,,,,2.0.3,2.1.1,,,,,,,,,,"19/Dec/16 21:41;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16340;;;","20/Dec/16 19:51;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16357;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MemorySink for StructuredStreaming can't recover from checkpoint if location is provided in conf,SPARK-18927,13029227,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,19/Dec/16 18:58,20/Dec/16 22:20,14/Jul/23 06:29,20/Dec/16 22:20,2.1.0,,,,,,,,2.1.1,2.2.0,,,Structured Streaming,,,,,,,,0,,,,,,,,apachespark,brkyvz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 19 22:29:05 UTC 2016,,,,,,,,,,"0|i37sb3:",9223372036854775807,,,,,,,,,,,,,2.1.1,,,,,,,,,,,"19/Dec/16 22:29;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/16342;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing </td> in Configuration page,SPARK-18918,13028951,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,smilegator,smilegator,smilegator,18/Dec/16 08:23,18/Dec/16 09:02,14/Jul/23 06:29,18/Dec/16 09:02,2.1.0,,,,,,,,2.1.1,,,,Documentation,,,,,,,,0,,,,,,"The configuration page looks messy now, as shown in the nightly build:

https://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/configuration.html

",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 18 09:02:45 UTC 2016,,,,,,,,,,"0|i37qlr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/16 08:29;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16327;;;","18/Dec/16 09:02;srowen;Issue resolved by pull request 16327
[https://github.com/apache/spark/pull/16327];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
append to a table with special column names should work,SPARK-18913,13028886,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,17/Dec/16 11:54,20/Dec/16 04:06,14/Jul/23 06:29,20/Dec/16 04:06,,,,,,,,,2.1.1,2.2.0,,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 17 11:57:13 UTC 2016,,,,,,,,,,"0|i37q7b:",9223372036854775807,,,,,,,,,,,,,2.1.1,,,,,,,,,,,"17/Dec/16 11:57;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16313;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
append to a non-file-based data source table should detect columns number mismatch,SPARK-18912,13028885,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,17/Dec/16 11:53,20/Dec/16 04:06,14/Jul/23 06:29,20/Dec/16 04:06,,,,,,,,,2.1.1,2.2.0,,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 17 11:57:06 UTC 2016,,,,,,,,,,"0|i37q73:",9223372036854775807,,,,,,,,,,,,,2.1.1,,,,,,,,,,,"17/Dec/16 11:57;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16313;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
It's hard for the user to see the failure if StreamExecution fails to create the logical plan,SPARK-18908,13028842,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,zsxwing,zsxwing,zsxwing,17/Dec/16 00:45,17/Jan/17 22:09,14/Jul/23 06:29,22/Dec/16 06:03,2.1.0,,,,,,,,2.1.1,2.2.0,,,Structured Streaming,,,,,,,,0,,,,,,"If the logical plan fails to create, e.g., some Source options are invalid, the user cannot use the code to detect the failure. The only place receiving this error is Thread's UncaughtExceptionHandler.

This bug is because logicalPlan is lazy, and when we try to create StreamingQueryException to wrap the exception thrown by creating logicalPlan, it calls logicalPlan agains.

",,apachespark,codingcat,lwlin,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18907,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 17 01:20:04 UTC 2016,,,,,,,,,,"0|i37pxj:",9223372036854775807,,,,,,,,,,,,,2.1.1,,,,,,,,,,,"17/Dec/16 01:20;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16322;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential Issue of Semantics of BatchCompleted,SPARK-18905,13028767,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,codingcat,codingcat,codingcat,16/Dec/16 20:32,27/Nov/17 11:02,14/Jul/23 06:29,17/Jan/17 02:34,1.5.1,1.5.2,1.6.1,1.6.2,1.6.3,2.0.0,2.0.1,2.0.2,2.1.1,2.2.0,,,DStreams,,,,,,,,0,,,,,,"the current implementation of Spark streaming considers a batch is completed no matter the results of the jobs (https://github.com/apache/spark/blob/1169db44bc1d51e68feb6ba2552520b2d660c2c0/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala#L203)

Let's consider the following case:

A micro batch contains 2 jobs and they read from two different kafka topics respectively. One of these jobs is failed due to some problem in the user defined logic, after the other one is finished successfully. 

1. The main thread in the Spark streaming application will execute the line mentioned above, 

2. and another thread (checkpoint writer) will make a checkpoint file immediately after this line is executed. 

3. Then due to the current error handling mechanism in Spark Streaming, StreamingContext will be closed (https://github.com/apache/spark/blob/1169db44bc1d51e68feb6ba2552520b2d660c2c0/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala#L214)

the user recovers from the checkpoint file, and because the JobSet containing the failed job has been removed (taken as completed) before the checkpoint is constructed, the data being processed by the failed job would never be reprocessed?


I might have missed something in the checkpoint thread or this handleJobCompletion()....or it is a potential bug ",,apachespark,codingcat,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 27 11:02:08 UTC 2017,,,,,,,,,,"0|i37pgv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/17 00:53;zsxwing;Sorry for the late reply. Yeah, good catch. However, even if it doesn't complete job, the failed job won't be run after recovery. Right? I think the root cause is that a failed job doesn't stop the StreamingContext. This might be a huge behavior change.

cc [~tdas];;;","10/Jan/17 01:05;codingcat;Hi, [~zsxwing]

Thanks for the reply, 

After testing in our environment for more times, I feel that this is not a problem anymore. The failed job would be recovered https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala#L216, as counted in the downTime

The question right now is, why we need to have pendingTime + downTime in the above method, ;;;","10/Jan/17 01:13;zsxwing;[~CodingCat] I think `pendingTime` is the jobs that have been generated but not completed.

I think you are right in the JIRA description. The failed job should not be removed so that they can be included in the `getPendingTimes`. Could you submit a PR to fix it?;;;","10/Jan/17 01:15;codingcat;yeah, but the downTime includes all batches from ""checkpoint time"" to ""restart time""

the jobs that have been generated but not completed shall be the first batch in downTime...no?;;;","10/Jan/17 02:06;codingcat;eat my words...

when we have queued up batches, we do need pendingTime, 

and yes, the original description in the JIRA still holds, ;;;","10/Jan/17 02:46;codingcat;[~zsxwing] If you agree on the conclusion above, I will file a PR;;;","10/Jan/17 18:46;zsxwing;Sure. Please go ahead.;;;","11/Jan/17 00:27;apachespark;User 'CodingCat' has created a pull request for this issue:
https://github.com/apache/spark/pull/16542;;;","27/Nov/17 11:02;apachespark;User 'victor-wong' has created a pull request for this issue:
https://github.com/apache/spark/pull/19824;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
append data to a bucketed table with mismatched bucketing should fail,SPARK-18899,13028716,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,16/Dec/16 16:59,20/Dec/16 04:05,14/Jul/23 06:29,20/Dec/16 04:05,,,,,,,,,2.1.1,2.2.0,,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 16 17:07:06 UTC 2016,,,,,,,,,,"0|i37p5j:",9223372036854775807,,,,,,,,,,,,,2.1.1,,,,,,,,,,,"16/Dec/16 17:07;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16313;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix SparkR SQL Test to drop test table,SPARK-18897,13028671,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,16/Dec/16 13:36,16/Dec/16 19:32,14/Jul/23 06:29,16/Dec/16 19:31,2.0.2,,,,,,,,2.0.3,2.1.1,2.2.0,,SparkR,Tests,,,,,,,0,,,,,,"Currently, SparkR tests, `R/run-tests.sh` succeeds only once because `test_sparkSQL.R` does not clean up the test table `people`.

As a result, the test data is accumulated at every run and the test cases fail.

The following is the failure result for the second run.
{code}
Failed -------------------------------------------------------------------------
1. Failure: create DataFrame from RDD (@test_sparkSQL.R#204) -------------------
collect(sql(""SELECT age from people WHERE name = 'Bob'""))$age not equal to c(16).
Lengths differ: 2 vs 1


2. Failure: create DataFrame from RDD (@test_sparkSQL.R#206) -------------------
collect(sql(""SELECT height from people WHERE name ='Bob'""))$height not equal to c(176.5).
Lengths differ: 2 vs 1
{code}",,apachespark,dongjoon,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 16 19:31:18 UTC 2016,,,,,,,,,,"0|i37ovj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/16 13:42;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/16310;;;","16/Dec/16 19:31;shivaram;Issue resolved by pull request 16310
[https://github.com/apache/spark/pull/16310];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Event time watermark delay threshold specified in months or years gives incorrect results,SPARK-18894,13028568,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,tdas,tdas,16/Dec/16 03:21,21/Dec/16 18:44,14/Jul/23 06:29,21/Dec/16 18:44,2.1.0,,,,,,,,2.1.1,2.2.0,,,Structured Streaming,,,,,,,,0,,,,,,"Internally we use CalendarInterval to parse the delay. Non-determinstic intervals like ""month"" and ""year"" are handled such a way that the generated delay in milliseconds is 0 delayThreshold is in months or years.
",,apachespark,codingcat,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 16 03:24:05 UTC 2016,,,,,,,,,,"0|i37o8n:",9223372036854775807,,,,,,,,,,,,,2.1.1,,,,,,,,,,,"16/Dec/16 03:24;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/16304;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Not support ""alter table .. add columns .."" ",SPARK-18893,13028565,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,zuo.tingbing9,zuo.tingbing9,16/Dec/16 03:03,04/Apr/17 00:07,14/Jul/23 06:29,04/Apr/17 00:07,2.0.1,,,,,,,,2.2.0,,,,SQL,,,,,,,,2,,,,,,"when we update spark from version 1.5.2 to 2.0.1, all cases we have need change the table use ""alter table add columns "" failed, but it is said ""All Hive DDL Functions, including: alter table"" in the official document : http://spark.apache.org/docs/latest/sql-programming-guide.html.

Is there any plan to support  sql ""alter table .. add/replace columns"" ?
",,cloud_fan,dongjoon,drwinters,licl,shenhong,shimingfei,vapira,zuo.tingbing9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19474,SPARK-19261,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 04 00:05:47 UTC 2017,,,,,,,,,,"0|i37o7z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/16 03:28;licl;spark 2.0 has disable ""alter table"".

[https://issues.apache.org/jira/browse/SPARK-14118]

[https://issues.apache.org/jira/browse/SPARK-14130]

I think this is very import feature for data warehouse 

Can spark handle it firstly?;;;","17/Jan/17 02:19;shenhong;+1, ""alter table add/replace columns"" is a very important feature. [~yhuai] [~rxin];;;","04/Apr/17 00:05;cloud_fan;https://issues.apache.org/jira/browse/SPARK-19261;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
partitionBy in DataStreamWriter in Python throws _to_seq not defined,SPARK-18888,13028483,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,brkyvz,brkyvz,brkyvz,15/Dec/16 20:03,15/Dec/16 22:31,14/Jul/23 06:29,15/Dec/16 22:31,2.0.2,,,,,,,,2.1.0,,,,PySpark,Structured Streaming,,,,,,,0,,,,,,"{code}
python/pyspark/sql/streaming.py in partitionBy(self, *cols)
    716         if len(cols) == 1 and isinstance(cols[0], (list, tuple)):
    717             cols = cols[0]
--> 718         self._jwrite = self._jwrite.partitionBy(_to_seq(self._spark._sc, cols))
    719         return self
    720 

NameError: global name '_to_seq' is not defined
{code}",,apachespark,brkyvz,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 15 20:15:05 UTC 2016,,,,,,,,,,"0|i37npr:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"15/Dec/16 20:15;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/16297;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delay scheduling should not delay some executors indefinitely if one task is scheduled before delay timeout,SPARK-18886,13028442,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nmarcott,irashid,irashid,15/Dec/16 17:22,17/May/20 17:47,14/Jul/23 06:29,13/Apr/20 06:00,2.1.0,,,,,,,,3.1.0,,,,Scheduler,Spark Core,,,,,,,1,,,,,,"Delay scheduling can introduce an unbounded delay and underutilization of cluster resources under the following circumstances:

1. Tasks have locality preferences for a subset of available resources
2. Tasks finish in less time than the delay scheduling.

Instead of having *one* delay to wait for resources with better locality, spark waits indefinitely.

As an example, consider a cluster with 100 executors, and a taskset with 500 tasks.  Say all tasks have a preference for one executor, which is by itself on one host.  Given the default locality wait of 3s per level, we end up with a 6s delay till we schedule on other hosts (process wait + host wait).

If each task takes 5 seconds (under the 6 second delay), then _all 500_ tasks get scheduled on _only one_ executor.  This means you're only using a 1% of your cluster, and you get a ~100x slowdown.  You'd actually be better off if tasks took 7 seconds.

*WORKAROUNDS*: 

(1) You can change the locality wait times so that it is shorter than the task execution time.  You need to take into account the sum of all wait times to use all the resources on your cluster.  For example, if you have resources on different racks, this will include the sum of ""spark.locality.wait.process"" + ""spark.locality.wait.node"" + ""spark.locality.wait.rack"".  Those each default to ""3s"".  The simplest way to be to set ""spark.locality.wait.process"" to your desired wait interval, and set both ""spark.locality.wait.node"" and ""spark.locality.wait.rack"" to ""0"".  For example, if your tasks take ~3 seconds on average, you might set ""spark.locality.wait.process"" to ""1s"".  *NOTE*: due to SPARK-18967, avoid setting the {{spark.locality.wait=0}} -- instead, use {{spark.locality.wait=1ms}}.

Note that this workaround isn't perfect --with less delay scheduling, you may not get as good resource locality.  After this issue is fixed, you'd most likely want to undo these configuration changes.

(2) The worst case here will only happen if your tasks have extreme skew in their locality preferences.  Users may be able to modify their job to controlling the distribution of the original input data.

(2a) A shuffle may end up with very skewed locality preferences, especially if you do a repartition starting from a small number of partitions.  (Shuffle locality preference is assigned if any node has more than 20% of the shuffle input data -- by chance, you may have one node just above that threshold, and all other nodes just below it.)  In this case, you can turn off locality preference for shuffle data by setting {{spark.shuffle.reduceLocality.enabled=false}}",,apachespark,ashish_dubey,Deng FEI,dubovsky,eyal,irashid,jazzwang,joshrosen,kayousterhout,markhamstra,mridulm80,nmarcott,tgraves,willshen,xkrt,,,,,,,,,,,,,,,,,,,,,,,SPARK-11460,,,,,,,,,,SPARK-27214,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 04 08:05:53 UTC 2019,,,,,,,,,,"0|i37ngn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Dec/16 17:29;irashid;cc [~kayousterhout] [~markhamstra] [~zsxwing];;;","15/Dec/16 17:30;irashid;Here's a failing test case: (you can check it out directly here: https://github.com/squito/spark/tree/delay_sched-SPARK-18886)

{code}
  test(""Delay scheduling checks utilization at each locality level"") {
    // Create a cluster with 100 executors, and submit 100 tasks, but each task would prefer to
    // be on the same node in the cluster.  We should not wait to schedule each task on the one
    // executor.
    sc = new SparkContext(""local"", ""test"")
    val execs = Seq((""exec0"", ""host0"")) ++ (1 to 100).map { x => (s""exec$x"", s""host$x"") }
    val sched = new FakeTaskScheduler(sc, execs: _*)
    val tasks = FakeTask.createTaskSet(500, (1 to 500).map { _ =>
      Seq(TaskLocation(TaskLocation.executorLocationTag + ""host0_exec0""))}: _*)
    val clock = new ManualClock
    val manager = new TaskSetManager(sched, tasks, MAX_TASK_FAILURES, clock)
    logInfo(""initial locality levels = "" + manager.myLocalityLevels.mkString("",""))
    assert(manager.myLocalityLevels.sameElements(Array(PROCESS_LOCAL, NODE_LOCAL, ANY)))
    // initially, the locality preferences should lead us to only schedule tasks on one executor
    logInfo(s""trying to schedule first task at ${clock.getTimeMillis()}"")
    val firstScheduledTask = execs.flatMap { case (exec, host) =>
      val schedTaskOpt = manager.resourceOffer(execId = exec, host = host, ANY)
      assert(schedTaskOpt.isDefined === (exec == ""exec0""))
      schedTaskOpt
    }.head

    // without advancing the clock, no matter how many times we make offers on the *other*
    // executors, nothing should get scheduled
    (0 until 50).foreach { _ =>
      execs.foreach { case (exec, host) =>
        if (exec != ""exec0"") {
          assert(manager.resourceOffer(execId = exec, host = host, ANY).isEmpty)
        }
      }
    }

    // now we advance the clock till just *before* the locality delay is up, and we finish the first
    // task
    val processWait = sc.getConf.getTimeAsMs(""spark.locality.wait.process"", ""3s"")
    val nodeWait = sc.getConf.getTimeAsMs(""spark.locality.wait.node"", ""3s"")
    clock.advance(processWait + nodeWait - 1)
    logInfo(s""finishing first task at ${clock.getTimeMillis()}"")
    manager.handleSuccessfulTask(firstScheduledTask.taskId,
      createTaskResult(firstScheduledTask.index))
    // if we offer all the resources again, still we should only schedule on one executor
    logInfo(s""trying to schedule second task at ${clock.getTimeMillis()}"")
    val secondScheduledTask = execs.flatMap { case (exec, host) =>
      val schedTaskOpt = manager.resourceOffer(execId = exec, host = host, ANY)
      assert(schedTaskOpt.isDefined === (exec == ""exec0""))
      schedTaskOpt
    }.head

    // Now lets advance the clock further, so that all of our other executors have been sitting
    // idle for longer than the locality wait time.  We have managed to schedule *something* at a
    // lower locality level within the time, but regardless, we *should* still schedule on the all
    // the other resources by this point
    clock.advance(10)
    // this would pass if we advanced the clock by this much instead
//    clock.advance(processWait + nodeWait + 10)
    logInfo(s""trying to schedule everyting at ${clock.getTimeMillis()}"")
    execs.foreach { case (exec, host) =>
      if (exec != ""exec0"") {
        withClue(s""trying to schedule on $exec:$host at time ${clock.getTimeMillis()}"") {
          assert(manager.resourceOffer(execId = exec, host = host, ANY).isDefined)
        }
      }
    }
  }
{code};;;","15/Dec/16 18:23;markhamstra;That's a great explanation of the issue, and nice example code, [~imranr].  I'm sure that I have seen this kind of excessive Task stickiness with many quick-to-execute Tasks, but I never got to the level of diagnosing the problem that you have.

Your listed workarounds, while interesting, aren't a complete long-term solution, of course.  Have you thought at all yet about possible paths to a solution?  One idea that comes to my mind is that speculative execution has at least the potential to get the delayed Tasks executed more quickly elsewhere -- but our prior concerns or lack of confidence with speculative execution remain.  ;;;","15/Dec/16 20:28;mridulm80;
Spark scheduler can be suboptimal for a lot of degenerate cases, not just this. But for the most part it does quite well : and for others, you can simply change the config to better suit the workload better - for example, I have used 0 for the delay timeout in some production jobs actually - you get prioritized scheduling, and fallback to ANY quickly when all tasks with preference have been scheduled to remove any waiting : when benefits of colocation are trumped by needing quicker schedules.

That is not to say we cant improve scheduling in spark - for example, treat it as a bin packing problem for schedules of a single (or across) locality preference to 'pack' more tasks - doing it without incurring high cost is why lot of these did not make it in unfortunately.
;;;","15/Dec/16 20:36;irashid;[~markhamstra] I'm not really sure yet, I can't really decide exactly what the right behavior should be.  My initial thought was that the TSM should track the last time that anything has been scheduled at *each* locality level, rather than just one overall {{lastLaunchTime}}, so that it realizes that some resources have been waiting a long time at a higher locality level.  But I wasn't exactly sure what should happen after you schedule one task at a higher locality level.  Do you reset the timer?  Or do you just keep scheduling at the locality level for the rest of the task set?  If you reset the timer, than you will only schedule one task on the other resources, before adding another locality delay, so that doesn't work.  But if you keep scheduling at that new locality level, then you've thrown away delay scheduling for the rest of the task set.  Is that OK?

To put that last question a different way -- what is the point of delay scheduling anyway?  What are we hoping will happen in that delay window?
1) the tasks run so fast that the preferred localities make it through all of the tasks in the entire task set before the delay is up
2) other tasksets are concurrently submitted with different locality preferences, so we can submit those tasks to the remaining executors (rather than sitting idle)
3) new resources will be spun up with the desired locality preferences.  (Eg., we've requested a bunch of resources from dynamic allocation, and the resources which have become available so far don't have the preferred localities, but more are still getting spun up.)

Under all the scenarios I can think of, you might as well turn off delay scheduling for the rest of the taskset.  But to be honest I don't feel like I've got good justification for delay scheduling in the first place, so I feel like I may be missing something.


Also I did a bit more digging into the case I had, and its happening because of a repartition from a small number (~10) of partitions to a much larger one.  The shuffle map stage ends up running two tasks on one host, and with a very small amount of skew, it turns out that one host has > 20% of the shuffle output, while none of the other hosts do.   I feel like there is also something else we can do here to improve the shuffle locality preferences, but I don't have any concrete ideas on what that improvement should be.;;;","15/Dec/16 20:42;irashid;[~mridulm80] good point, perhaps the right answer here is just to turn off delay scheduling completely -- not setting {{""spark.locality.wait.process""}} to a small value, as I had suggested in the initial workaround, but just turning it off completely, to avoid having to futz with tuning that value relative to task runtime.

But lemme ask you more or less the same question I just asked mark, phrased a little differently -- given the fragility of this, wouldn't it make more sense for us to turn delay scheduling *off* by default?;;;","15/Dec/16 21:34;mridulm80;[~imranr] For almost all cases, delay scheduling dramatically increases performance. The difference even between PROCESS and NODE is significantly high (between NODE and 'lower' levels, it can depend on your network config).
For both tasks with short duration and tasks processing large amounts of data, it has non trivial impact : long tasks processing small data, it is not so useful in comparison iirc, same for degenerate cases where locality preference is suboptimal to begin with. [As an aside, the ability to not specify PROCESS level locality preference actually is a drawback in our api]

The job(s) I mentioned where we set it to 0 were special cases, where we knew the costs well enough to make the decision to lower it : but I would not recommend it unless users are very sure of what they are doing. While analysing the cost, it should also be kept in mind that transferring data across nodes impacts not just spark job, but every other job in the cluster.;;;","15/Dec/16 22:40;irashid;[~mridul] sorry if I am being slow here, but do you mind spelling out for me in more detail?  I'm *not* asking about the benefits of using locality preferences -- I get that part.  I'm asking about why the *delay*.  There has to be something happening during the delay which we want to wait for.

One possibility is that you've got multiple tasksets running concurrently, with different locality preferences.  You wouldn't want the first taskset to use all the resources, you'd rather take both tasksets into account.  This is accomplished with delay scheduling, but you don't actually *need* the delay.

Another possibility is that there is such a huge gap in runtime that you expect your preferred locations will finish *all* tasks in the taskset before that delay is up, by having some executors run multiple tasks.

The reason I'm trying to figure this out is to figure out if there is a sensible fix here (and what the smallest possible fix would be).  If this is it, then the fix I suggested above to Mark should handle this case, while still working as intended in other cases.;;;","17/Dec/16 03:10;mridulm80;
- Delay 'using up' all resources - another task/taskset which has better locality preference might be available for that executor (also, see speculative exec impact)
- A delay would cause a better locality preference to become available for the task. Suboptimal schedule has a cascading effect on rest of the executors, application and cluster.
- Note that not all executors are available at the same time in resourceOffer : you have bulk reschedule periodically,  sporadic reschedules when tasks finish and periodic bulk speculative schedule updates.
;;;","19/Dec/16 21:01;irashid;Thanks [~mridul], that helps -- in particular I was only thinking about bulk scheduling, I had forgotten to take that into account.

After a closer look through the code, I think my earlier proposal makes sense -- rather than resetting the timeout as each task is scheduled, change it to start the timer as soon as there is an offer which goes unused due to the delay.  Once that timer is started, it is never reset (for that TSM).

I can think of one scenario where this would result in worse scheduling than what we currently have.  Suppose that initially, a TSM is offered one resource which only matches on rack_local.  But immediately after that, many process_local offers are made, which are all used up.  Some time later, more offers that are only rack_local come in.  They'll immediately get used, even though there may be plenty more offers that are process_local that are just about to come in (perhaps enough for all of the remaining tasks).

That wouldn't be great, but its also not nearly as bad as letting most of your cluster sit idle.

Other alternatives I can think of:

a) Turn off delay scheduling by default, and change [{{TaskSchedulerImpl.resourceOffer}}|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala#L357-L360] to go through all task sets, then advance locality levels, rather than the other way around.  Perhaps we should invert those loops anyway, just for when users turn off delay scheduling.

b) Have TSM use some knowledge about all available executors to decide whether or not it is even possible for enough resources at the right locality level to appear.  Eg., in the original case, the TSM would realize there is only one executor which is process_local, so it doesn't make sense to wait to schedule all tasks on that executor.  However, I'm pretty skeptical about doing anything like this, as it may be a somewhat complicated thing inside the scheduler, and it could just turn into a mess of heuristics which has lots of corner cases.

I think implementing my proposed solution should be relatively easy, so I'll take a stab at it, but I'd still appreciate more input on the right approach here.  Perhaps seeing an implementation will make it easier to discuss.;;;","20/Dec/16 17:50;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/16354;;;","20/Dec/16 18:26;mridulm80;I am not sure what is described will work as expected [~imranr].
Consider a taskset which has number of tasks as many multiples of number of executors (fairly common scenario).
In this case, if the timer is never reset, you will effectively make delay to 0 once it expires, across all waves for the taskset.
(I am assuming I understood the proposal right).

[~kayousterhout] and [~markhamstra] might have more comments though - in case I am missing something here.
;;;","20/Dec/16 21:43;irashid;You understand correctly -- that is precisely what I'm proposing.

The scenario with multiple waves is a good example for why I think this is a *good* change.  If only 1% of your cluster can take advantage of locality, then 99% of your cluster goes unused across all those waves.  That may be an extreme (though a case I have actually seen in practice on large clusters).  even if its 50%, then you have 50% of your cluster going unused.  Unless local tasks are more than 2x faster, it would make more sense to make the change I'm proposing.  What's the worst case after this change?  All but one executor are local -- the result is that you have one task running slower.  But the more waves there are, the less the downside.  Eg., you complete 10 waves on the local executors, and only 8 waves on the non-local one.

The worst case is if there is only one wave, there is a huge gap (multiples Xs) in runtime between local and non-local execution, and moments after you schedule on non-local resources, some local resource would become available.  I think this situation is not very common -- in particular, there normally isn't *such* an enormous gap between local and non-local that users would prefer their non-local resources sit idle indefinitely.  I'd argue that if such a use case is important, we should add a special conf for that in particular.;;;","19/Jan/17 20:14;irashid;I had another idea for how to fix this.  In addition to tracking the last time any task was launched, TaskScheduler also tracks the last time it didn't schedule anything due to locality constraints, on *each resource*.  Then when a new offer comes in, you are allowed to schedule if either the overall locality timer is up, or if the timer is up for that particular resource.

On the plus side -- I think this keeps all the properties we want.  You avoid an indefinite delay just because *one* resource is local;  but you also keep the delay if those resources get used up by another task set.

The downside -- significantly more complex.  It adds to the memory usage of TaskScheduler (though in the scheme of things, pretty nominal increase), but it will also make the code significantly more complicated.

Aside: There is also weird relationship between taskset priority, and locality scheduling.  Assuming all tasksets have cleared their locality wait timeouts, then we favor taskset priority over locality.  But if the tasksets haven't cleared those timeouts, then things get strange.  It really depends on what the current locality levels are in each taskset.  In the simple case, you end up favoring locality, by limiting the max Locality of each taskset.  A very low priority taskset easily ""steals"" the resources from a high priority one if it doesn't have locality preferences.  We should probably figure out what the desired behavior is so we can make it a little more consistent (or at least document it).;;;","18/Mar/17 02:21;kayousterhout;Sorry for the slow response here!  I realized this is the same issue as SPARK-11460 (although that JIRA proposed a slightly different solution), which stalled for reasons that are completely my fault (I neglected it because I couldn't think of a practical way of solving it).

Imran, unfortunately I don't think your latest idea will quite work.  Delay scheduling was originally intended for situations where the number of slots that a particular job could use was limited by a fairness policy.  In that case, it can be better to wait a bit for a ""better"" slot (i.e., one that satisfies locality preferences).  In particular, if you never wait, you end up with this ""sticky slot"" issue where tasks for a job keep finishing up in a ""bad"" slot (one with no locality preferences), and then they'll be re-offered to the same job, which will again accept the bad slot.  If the job just waited a bit, it could get a better slot (e.g., as a result of tasks from another job finishing). [1]

This relates to your idea because of the following situation: suppose you have a cluster with 10 machines, the job has locality preferences for 5 of them (with ids 1, 2, 3, 4, 5), and fairness dictates that the job can only use 3 slots at a time (e.g., it's sharing equally with 2 other jobs).  Suppose that for a long time, the job has been running tasks on slots 1, 2, and 3 (so local slots).  At this point, the times for machines 6, 7, 8, 9, and 10 will have expired, because the job has been running for a while.  But if the job is now offered a slot on one of those non-local machines (e.g., 6), the job hasn't been waiting long for non-local resources: until this point, it's been running it's full share of 3 slots at a time, and it's been doing so on machines that satisfy locality preferences.  So, we shouldn't accept that slot on machine 6 -- we should wait a bit to see if we can get a slot on 1, 2, 3, 4, or 5.

The solution I proposed (in a long PR comment) for the other JIRA is: if the task set is using fewer than the number of slots it could be using (where “# slots it could be using” is all of the slots in the cluster if the job is running alone, or the job’s fair share, if it’s not) for some period of time, increase the locality level.   The problem with that solution is that I thought it was completely impractical to determine the number of slots a TSM ""should"" be allowed to use.

However, after thinking about this more today, I think we might be able to do this in a practical way:
- First, I thought that we could use information about when offers are rejected to determine this (e.g., if you've been rejecting offers for a while, then you're not using your fair share).  But the problem here is that it's not easy to determine when you *are* using your fair / allowed share: accepting a single offer doesn't necessarily mean that you're now using the allowed share.  This is precisely the problem with the current approach, hence this JIRA.
- v1: one possible proxy for this is if there are slots that are currently available that haven't been accepted by any job.  The TaskSchedulerImpl could feasibly pass this information to each TaskSetManager, and the TSM could use it to update it's delay timer: something like only reset the delay timer to 0 if (a) the TSM accepts an offer and (b) the flag passed by the TSM indicates that there are no other unused slots in the cluster.  This fixes the problem described in the JIRA: in that case, the flag would indicate that there *were* other unused slots, even though a task got successfully scheduled with this offer, so the delay timer wouldn't be reset, and would eventually correctly expire.
- v2: The problem with v1 is that it doesn't correctly handle situations where e.g., you have two jobs A and B with equal shares.  B is ""greedy"" and will accept any slot (e.g., it's a reduce stage), and A is doing delay scheduling.  In this case, A might have much less than its share, but the flag from the TaskSchedulerImpl would indicate that there were no other free slots in the cluster, so the delay timer wouldn't ever expire.  I suspect we could handle this (e.g., with some logic in the TaskSchedulerImpl to detect when a particular TSM is getting starved: when it keeps rejecting offers that are later accepted by someone else) but before thinking about this further, I wanted to run the general idea by you to see what your thoughts are.

[1] There's a whole side question / discussion of how often this is useful for Spark at all.  It can be useful if you're running in a shared cluster where e.g. Yarn might be assigning you more slots over time, and it's also useful when a single Spark context is being shared across many jobs.  But often for Spark, you have one job running alone, in which case delay scheduling should arguably be turned of altogether, as you suggested earlier Imran.  But let's separate that discussion from this one, of how to make it work better.;;;","19/Mar/17 19:18;irashid;Thanks Kay for the full description (and finding the old jira, sorry I didn't notice the duplicate).  Your explanation and alternative makes sense.  One detail from v1:

bq. flag passed by the TSM indicates that there are no other unused slots in the cluster

neither the TSM nor TaskSchedulerImpl currently track this -- they know about executors, but not individual slots.  With bulk-scheduling calls to {{resourceOffer()}} that include the entire set of slots that isn't a problem, but it is for single offers.  Anyway, its still solvable, just more bookkeeping and more complex change.

bq. But often for Spark, you have one job running alone, in which case delay scheduling should arguably be turned of altogether, as you suggested earlier Imran. But let's separate that discussion from this one, of how to make it work better.

yeah, you can see that earlier in the thread I was trying to figure out what the purpose of this was anyway ... I am going to be recommending folks to turn it off more often.  But even when you have just one job running at a time, this still matters for jobs with parallel stages in the DAG, eg. a join.  Fairness doesn't matter at all between the stages, but overall efficiency does.  If you turn delay scheduling off entirely, then whichever taskset comes first will get all the resources, rather than giving both a shot at local resources.  So I feel like the right recommendation is {{1ms}}.  There is probably something else to fix and another jira here though I don't have a clear idea around it yet.

I will keep thinking about your v2.  What you are proposing makes sense, but I worry that we continue to band-aid these situations where things are really bad, but we're still stuck with a system where the delay should really be closely tuned to the task length, otherwise there is a lot of inefficiency.  This wasted time isn't even tracked anywhere (its not included in ""scheduler delay""), so users have no idea their hitting this.;;;","30/Oct/17 17:11;willshen;[~imranr],

I came across this issue because it is marked as duplicated by SPARK-11460. SPARK-11460 has affected versions of 1.0.0, 1.0.1, 1.0.2, 1.1.0, 1.1.1, 1.2.0, 1.2.1, 1.2.2, 1.3.0, 1.3.1, 1.4.0, 1.4.1, 1.5.0, 1.5.1, and this issue has affected version of 2.1.0. Do we know if this is also an issue for 1.6.0? I am observing similar behavior for our application in 1.6.0. We are also able to achieve better utilization of the executors and performance through setting the wait to 0.

Thank you;;;","30/Oct/17 17:48;irashid;Hi [~willshen],

I haven't looked closely, but I would expect this to also exist in 1.6.0.  Btw, due to SPARK-18967 (which I assume is also in 1.6.0, but again havne't looked closely), I would advise setting the wait to {{1ms}}, not {{0}}.
;;;","03/Dec/19 16:06;tgraves;Note there is discussion on this subject on prs:

[https://github.com/apache/spark/pull/26633] (hack to work around it for a particular RDD)

PR with proposed solution - but really more discussion solution:

[https://github.com/apache/spark/pull/26696]

 

My proposal I believe is similar to Kay's where we use slots and track the delay per slot.  I haven't looked at the code in specific detail, especially in the FairScheduler where most of the issues in the conversations above were mentioned.  One way around this is we have different policies and allow users to configure, or have one for FairScheduler and one for fifo.;;;","04/Dec/19 08:05;nmarcott;Thanks for mentioning the PRs here.

My proposed solution in the second [PR mentioned above|https://github.com/apache/spark/pull/26696] is what I believe Kay said was ideal in comments of this [PR|https://github.com/apache/spark/pull/9433], but seemed to think was impractical. 

*The proposed solution:*

Currently the time window that locality wait times are measuring is the time since the last task launched for a TSM. The proposed change is to instead measure the time since this TSM's available slots were fully utilized.

The number of available slots for a TSM can be determined by dividing all slots among the TSMs according to the scheduling policy (FIFO vs FAIR).

*Other possible solutions and their issues:*
 # Never reset timer: delay scheduling would likely only work on first wave*
 # Per slot timer: delay scheduling should apply per task/taskset, otherwise, timers started by one taskset could cause delay scheduling to be ignored for the next taskset,  which might lead you to try approach #3
 # Per slot per stage timer: tasks can be starved by being offered unique slots over a period of time. Possibly a taskset or other job that doesn't care about locality would use those resources.  Also too many timers/bookkeeping
 # Per task timer: you still need a way to distinguish between when a task is waiting for a slot to become available vs it has them available but is not utilizing them (which is what this PR does). To do this right seems to be this PR + more timers.

 

*wave = one round of running as many tasks as there are available slots for a taskset. imagine you have 2 slots and 10 tasks. it would take 10 / 2 = 5 waves to complete the taskset

 ;;;",,,,,,,,,,,,,,,,,,,,,,,
Unable to read given csv data. Excepion: java.lang.IllegalArgumentException: requirement failed: Decimal precision 28 exceeds max precision 20,SPARK-18877,13028314,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,Navya Krishnappa,Navya Krishnappa,15/Dec/16 07:38,26/Jan/18 10:40,14/Jul/23 06:29,03/Jan/17 15:07,2.0.2,,,,,,,,2.0.3,2.1.1,2.2.0,,SQL,,,,,,,,0,,,,,,"When reading below mentioned csv data, even though the maximum decimal precision is 38, following exception is thrown java.lang.IllegalArgumentException: requirement failed: Decimal precision 28 exceeds max precision 20


Decimal
2323366225312000000000000000
24335739714000000
23233662253000
232336622530000

",,apachespark,dongjoon,Navya Krishnappa,WeiqingYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23225,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 03 10:33:22 UTC 2017,,,,,,,,,,"0|i37mo7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Dec/16 09:20;srowen;You should include the stack trace whenever you report an exception.
How are you reading, where do you set max precision, etc? 
This could use a lot more detail.;;;","16/Dec/16 07:35;Navya Krishnappa;I'm reading through csvReader (.csv(sourceFile)) and i'm not setting any precision and scale, Spark is automatically detecting the precision and scale for the values in the source file. And precision and scale vary depending on the decimal values in the column. 

Stack trace:

Caused by: java.lang.IllegalArgumentException: requirement failed: Decimal precision 28 exceeds max precision 20
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.sql.types.Decimal.set(Decimal.scala:112)
	at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:425)
	at org.apache.spark.sql.execution.datasources.csv.CSVTypeCast$.castTo(CSVInferSchema.scala:264)
	at org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:116)
	at org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:85)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$1$$anonfun$apply$2.apply(CSVFileFormat.scala:128)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$1$$anonfun$apply$2.apply(CSVFileFormat.scala:127)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:91)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:128)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:91)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 common frames omitted;;;","16/Dec/16 07:40;Navya Krishnappa;Precision and scale vary depending on the decimal values in the column. Suppose if source file contains 

Amount(column name)
9.03E+12
1.19E+11
24335739714
1.71E+11

then spark consider Amount column as decimal(3,-9). and throws an below mentioned exception

Caused by: java.lang.IllegalArgumentException: requirement failed: Decimal precision 4 exceeds max precision 3
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.sql.types.Decimal.set(Decimal.scala:112)
	at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:425)
	at org.apache.spark.sql.execution.datasources.csv.CSVTypeCast$.castTo(CSVInferSchema.scala:264)
	at org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:116)

;;;","16/Dec/16 10:50;dongjoon;Hi, [~Navya Krishnappa].
Could you give us more details? What kind of case class did you use?
If you use simply `spark.read.format(""csv"").load(~~)` on those files, it returns `string`, doesn't it?;;;","16/Dec/16 16:27;Navya Krishnappa;I'm using SparkContext.read() to read the content. Refer the given code using to read the csv file.

Dataset dataset = getSqlContext().read()
                .option(HEADER, ""true"")
                .option(PARSER_LIB, ""commons"")
                .option(INFER_SCHEMA, ""true"")
                .option(DELIMITER, "","")
                .option(QUOTE, ""\"""")
                .option(ESCAPE, ""\\"")
                .option(MODE, Mode.PERMISSIVE)
                .csv(sourceFile);

if we collect the dataset (dataset.collect()). i will get java.lang.IllegalArgumentException: requirement failed: Decimal precision 28 exceeds max precision 20 exception.;;;","16/Dec/16 17:08;dongjoon;Thank you for the details.;;;","16/Dec/16 21:39;dongjoon;Yes. I reproduced the bug and found the root cause on schema inferencing.
I'll make a PR for this.;;;","16/Dec/16 22:35;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/16320;;;","17/Dec/16 07:28;Navya Krishnappa;Thank you for replying [~dongjoon]. Can you help me in understanding whether the above mentioned PR will resolve the below mentioned issue.

I have another issue with respect to the decimal scale. When i'm trying to read the below mentioned csv source file and creating an parquet file from that throws an java.lang.IllegalArgumentException: Invalid DECIMAL scale: -9 exception.


The source file content is 
Row(column name)
9.03E+12
1.19E+11

 Refer the given code used read the csv file and creating an parquet file:

//Read the csv file
Dataset dataset = getSqlContext().read()
.option(HEADER, ""true"")
.option(PARSER_LIB, ""commons"")
.option(INFER_SCHEMA, ""true"")
.option(DELIMITER, "","")
.option(QUOTE, ""\"""")
.option(ESCAPE, ""
"")
.option(MODE, Mode.PERMISSIVE)
.csv(sourceFile)

// create an parquet file
dataset.write().parquet(""//path.parquet"")


Stack trace:

Caused by: java.lang.IllegalArgumentException: Invalid DECIMAL scale: -9
	at org.apache.parquet.Preconditions.checkArgument(Preconditions.java:55)
	at org.apache.parquet.schema.Types$PrimitiveBuilder.decimalMetadata(Types.java:410)
	at org.apache.parquet.schema.Types$PrimitiveBuilder.build(Types.java:324)
	at org.apache.parquet.schema.Types$PrimitiveBuilder.build(Types.java:250)
	at org.apache.parquet.schema.Types$Builder.named(Types.java:228)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter.convertField(ParquetSchemaConverter.scala:412)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter.convertField(ParquetSchemaConverter.scala:321)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$$anonfun$convert$1.apply(ParquetSchemaConverter.scala:313)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$$anonfun$convert$1.apply(ParquetSchemaConverter.scala:313)
	;;;","18/Dec/16 01:04;dongjoon;Hi, [~Navya Krishnappa].

As you see in the stack trace, that is a different exception from Apache Parquet code.
{code}
Caused by: java.lang.IllegalArgumentException: Invalid DECIMAL scale: -9
	at org.apache.parquet.Preconditions.checkArgument(Preconditions.java:55)
{code}

And, this is the Parquet code [here|https://github.com/Parquet/parquet-mr/blob/master/parquet-column/src/main/java/parquet/schema/Types.java#L405-L417].

It apparently does not fully support BigDecimal.

{code}
    protected DecimalMetadata decimalMetadata() {
      DecimalMetadata meta = null;
      if (OriginalType.DECIMAL == originalType) {
        Preconditions.checkArgument(precision > 0,
            ""Invalid DECIMAL precision: "" + precision);
        Preconditions.checkArgument(scale >= 0,
            ""Invalid DECIMAL scale: "" + scale);
        Preconditions.checkArgument(scale <= precision,
            ""Invalid DECIMAL scale: cannot be greater than precision"");
        meta = new DecimalMetadata(precision, scale);
      }
      return meta;
    }
  }
{code}

I cannot make a PR for Parquet.
I hope you register another issue for that in Apache Parquet JIRA.;;;","18/Dec/16 01:06;dongjoon;For a Spark usage workaround, I think you can change the schema into a Parquet-acceptable one manually.;;;","19/Dec/16 07:50;Navya Krishnappa;Thank you [~dongjoon] and i will create an issue in  Apache Parquet JIRA.;;;","19/Dec/16 20:17;dongjoon;+1;;;","03/Jan/17 21:09;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/16463;;;","04/Jan/17 18:19;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/16472;;;","03/Jul/17 10:33;Navya Krishnappa;[~dongjoon] I have created parquet bug for the invalid scale issue in Decimal data type. But Parquet team is telling its Spark issue. Please refer https://issues.apache.org/jira/browse/PARQUET-815 and add your comments.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix R API doc generation by adding `DESCRIPTION` file,SPARK-18875,13028290,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,15/Dec/16 04:50,15/Dec/16 05:33,14/Jul/23 06:29,15/Dec/16 05:30,1.6.3,2.0.2,,,,,,,2.0.3,2.1.0,,,Documentation,SparkR,,,,,,,0,,,,,,"Since 1.4.0, R API document index page has a broken link on `DESCRIPTION file`. This issue aims to fix that.

* Official Latest Website: http://spark.apache.org/docs/latest/api/R/index.html
* Apache Spark 2.1.0-rc2: http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc2-docs/api/R/index.html
",,apachespark,dongjoon,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 15 05:30:09 UTC 2016,,,,,,,,,,"0|i37miv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Dec/16 05:02;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/16292;;;","15/Dec/16 05:30;shivaram;Issue resolved by pull request 16292
[https://github.com/apache/spark/pull/16292];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Distinct aggregates give incorrect answers on streaming dataframes,SPARK-18870,13028258,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tdas,tdas,tdas,15/Dec/16 01:22,15/Dec/16 19:54,14/Jul/23 06:29,15/Dec/16 19:54,2.0.2,,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,"Unsupported operations checking dont check whether AggregationExpression have isDistinct=true. So `streamingDf.groupBy().agg(countDistinct(""key"")) ` gives incorrect results.",,apachespark,DjvuLee,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 15 19:54:58 UTC 2016,,,,,,,,,,"0|i37mbr:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"15/Dec/16 01:44;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/16289;;;","15/Dec/16 19:54;tdas;Issue resolved by pull request 16289
[https://github.com/apache/spark/pull/16289];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Output non-aggregate expressions without GROUP BY in a subquery does not yield an error ,SPARK-18863,13028128,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nsyca,nsyca,nsyca,14/Dec/16 16:01,25/Jan/17 16:10,14/Jul/23 06:29,25/Jan/17 16:10,2.0.0,,,,,,,,2.0.3,2.1.1,2.2.0,,SQL,,,,,,,,0,,,,,,"[~smilegator] has found that the following query does not raise a syntax error (note the GROUP BY clause is commented out):

{code:sql}
SELECT pk, cv
FROM   p, c
WHERE  p.pk = c.ck
AND    c.cv = (SELECT max(avg)
               FROM   (SELECT   c1.cv, avg(c1.cv) avg
                       FROM     c c1
                       WHERE    c1.ck = p.pk
--                       GROUP BY c1.cv
                      ))

{code}

There could be multiple values of {{c1.cv}} for each value of {{avg(c1.cv)}}.",,apachespark,ioana-delaney,nsyca,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19047,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 13 11:42:04 UTC 2017,,,,,,,,,,"0|i37liv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/16 17:53;smilegator;The JIRA only shows one scenario. The expected error messages could be either
`org.apache.spark.sql.AnalysisException: grouping expressions sequence is empty, and 'c1.`cv`' is not an aggregate function. `
or
`org.apache.spark.sql.AnalysisException: expression 'c1.`cv`' is neither present in the group by, nor is it an aggregate function.`;;;","14/Dec/16 23:00;ioana-delaney;This looks like another instance of SPARK-16161.;;;","21/Dec/16 16:54;nsyca;Another case to investigate [subquery/in-subquery/in-group-by.sql TC 01.12]

{code}
Seq((1,1,1)).toDF(""t1a"", ""t1b"", ""t1c"").createOrReplaceTempView(""t1"")
Seq((1,1,1)).toDF(""t2a"", ""t2b"", ""t2c"").createOrReplaceTempView(""t2"")
Seq((1,1,1)).toDF(""t3a"", ""t3b"", ""t3c"").createOrReplaceTempView(""t3"")

— TC 01.12
select * from t1 where t1a in
(select min(t2a) from t2 where t2a = t2a and t2c >= 1 group by t2c having t2c in
(select t3c from t3 group by t3c, t3b having t2b > 6 and t3b > t2b ))

== Parsed Logical Plan ==
'Project [*]
+- 'Filter 't1a IN (list#803)
   :  +- 'Filter 't2c IN (list#802)
   :     :  +- 'Filter (('t2b > 6) && ('t3b > 't2b))
   :     :     +- 'Aggregate ['t3c, 't3b], ['t3c]
   :     :        +- 'UnresolvedRelation `t3`
   :     +- 'Aggregate ['t2c], [unresolvedalias('min('t2a), None)]
   :        +- 'Filter (('t2a = 't2a) && ('t2c >= 1))
   :           +- 'UnresolvedRelation `t2`
   +- 'UnresolvedRelation `t1`

== Analyzed Logical Plan ==
t1a: int, t1b: int, t1c: int
Project [t1a#764, t1b#765, t1c#766]
+- Filter predicate-subquery#803 [(t1a#764 = min(t2a)#816)]
   :  +- Project [min(t2a)#816]
   :     +- !Filter predicate-subquery#802 [(t2c#781 = t3c#796) && (t2b#780 > 6) && (t3b#795 > t2b#780)]
   :        :  +- Project [t3c#796, t3b#795]
   :        :     +- Aggregate [t3c#796, t3b#795], [t3c#796, t3b#795]
   :        :        +- SubqueryAlias t3, `t3`
   :        :           +- Project [_1#790 AS t3a#794, _2#791 AS t3b#795, _3#792 AS t3c#796]
   :        :              +- LocalRelation [_1#790, _2#791, _3#792]
   :        +- Aggregate [t2c#781], [min(t2a#779) AS min(t2a)#816, t2c#781]
   :           +- Filter ((t2a#779 = t2a#779) && (t2c#781 >= 1))
   :              +- SubqueryAlias t2, `t2`
   :                 +- Project [_1#775 AS t2a#779, _2#776 AS t2b#780, _3#777 AS t2c#781]
   :                    +- LocalRelation [_1#775, _2#776, _3#777]
   +- SubqueryAlias t1, `t1`
      +- Project [_1#760 AS t1a#764, _2#761 AS t1b#765, _3#762 AS t1c#766]
         +- LocalRelation [_1#760, _2#761, _3#762]

== Optimized Logical Plan ==
Project [_1#760 AS t1a#764, _2#761 AS t1b#765, _3#762 AS t1c#766]
+- Join LeftSemi, (_1#760 = min(t2a)#816)
   :- LocalRelation [_1#760, _2#761, _3#762]
   +- Aggregate [t2c#781], [min(t2a#779) AS min(t2a)#816]
      +- Project [_1#775 AS t2a#779, _3#777 AS t2c#781]
         +- Join LeftSemi, (((_3#777 = t3c#796) && (_2#776 > 6)) && (t3b#795 > _2#776))
            :- Filter (_3#777 >= 1)
            :  +- LocalRelation [_1#775, _2#776, _3#777]
            +- Aggregate [t3c#796, t3b#795], [t3c#796, t3b#795]
               +- LocalRelation [t3b#795, t3c#796]
{code}

I don't see the column t2b#780 being part of the output of the (lower) Aggregate operator. Somehow the LeftSemi join for t2b > 6 is just moved down below the Aggregate over t2. This does not look right to me. ;;;","01/Jan/17 16:12;nsyca;My further investigation concludes this problem reported by [~smilegator] is different from the one in SPARK-16161 and the one reported by me in subquery/in-subquery/in-group-by.sql TC 01.12. I have opened SPARK-19047 to track the last problem.;;;","04/Jan/17 15:58;nsyca;I made the conclusion too soon. It turns out that the scenario TC 01.12 is a variation of this problem in which we need to perform a {{CheckAnalysis}} on the subquery plan hanging of (currently limited to) a Filter operator. The original problem documented in the description is a subquery of the form of {{ScalarSubquery}} whereas TC 01.12 is of the form of {{PredicateSubquery}}.

I will close SPARK-19047 and submit a PR for this problem soon.;;;","13/Jan/17 11:42;apachespark;User 'nsyca' has created a pull request for this issue:
https://github.com/apache/spark/pull/16572;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSQL ThriftServer hangs while extracting huge data volumes in incremental collect mode,SPARK-18857,13028031,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,vishalagrwal,vishalagrwal,14/Dec/16 08:37,12/Jan/17 10:46,14/Jul/23 06:29,10/Jan/17 13:28,2.0.2,,,,,,,,2.0.3,2.1.1,2.2.0,,SQL,,,,,,,,1,,,,,,"We are trying to run a sql query on our spark cluster and extracting around 200 million records through SparkSQL ThriftServer interface. This query works fine for Spark 1.6.3 version, however for spark 2.0.2, thrift server hangs after fetching data from a few partitions (we are using incremental collect mode with 400 partitions). As per documentation max memory taken up by thrift server should be what is required by the biggest data partition. But we observed that Thrift server is not releasing the old partitions memory whenever the GC occurs even though it has moved to next partition data fetches. which is not the case with 1.6.3 version.

On further investigation we found that SparkExecuteStatementOperation.scala was modified for ""[SPARK-16563][SQL] fix spark sql thrift server FetchResults bug"" and result set iterator was duplicated to keep a reference to the first set.

+      val (itra, itrb) = iter.duplicate
+      iterHeader = itra
+      iter = itrb

We suspect that this is resulting in the memory not being cleared on GC. To confirm this we created an iterator in our test class and fetched the data once without duplicating and second time with creating a duplicate. we could see that in first instance it ran fine and fetched the entire data set while in second instance driver hanged after fetching data from a few partitions.



",,apachespark,dongjoon,vishalagrwal,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16563,,,,,,,,,,"14/Dec/16 08:42;vishalagrwal;GC-spark-1.6.3;https://issues.apache.org/jira/secure/attachment/12843184/GC-spark-1.6.3","14/Dec/16 08:42;vishalagrwal;GC-spark-2.0.2;https://issues.apache.org/jira/secure/attachment/12843185/GC-spark-2.0.2",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 11 10:28:58 UTC 2017,,,,,,,,,,"0|i37kxb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/16 08:42;vishalagrwal;GC logs for 2 spark versions while running the same query;;;","16/Dec/16 11:22;dongjoon;Thank you for reporting, [~vishalagrwal].
Then, in the Spark side, could you test on Spark 2.0.0 before SPARK-16563?;;;","19/Dec/16 05:28;vishalagrwal;We are unable to use incremental collect in a spark version before 2.0.2 due the bug spark-18009

We will have to take 2.0.2 and change this particular class and build from source code.;;;","26/Dec/16 09:48;vishalagrwal;we have built Spark from 2.0.2 source code by changing SparkExecuteStatementOperation.scala to pre SPARK-16563 version. this version works fine without causing any thrift server issues.;;;","30/Dec/16 10:36;dongjoon;Thank you for testing and sharing that information!;;;","30/Dec/16 12:56;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/16440;;;","30/Dec/16 12:58;dongjoon;Hi, [~vishalagrwal].
I agree with you. This is an important problem.
At least, I made a PR as a first attempt. In any ways, I hope this will be resolved soon.;;;","31/Dec/16 11:51;srowen;CC [~alicegugu];;;","01/Jan/17 17:25;dongjoon;Hi [~vishalagrwal].
Could you test your case with https://github.com/apache/spark/pull/16440 ?
Although I tried to address the iterator issue you mentioned, it's a memory issue.
So, I'm not sure the other parts still consume lots of memory in your case.;;;","02/Jan/17 04:07;vishalagrwal;thanks. we will test it and confirm.;;;","02/Jan/17 11:35;vishalagrwal;Thanks. its working fine now for our scenario.;;;","02/Jan/17 23:08;dongjoon;Thank you for testing and confirming!;;;","10/Jan/17 13:28;srowen;Issue resolved by pull request 16440
[https://github.com/apache/spark/pull/16440];;;","11/Jan/17 04:55;dongjoon;Hi, [~srowen].
This is a bug existing 2.0.2 and 2.1.X.
I'll create a backport for this issue.;;;","11/Jan/17 04:58;dongjoon;Or, could you cherry-pick that please?
When I try to cherry-pick for branch-2.0 and branch-2.1, there was no problem.;;;","11/Jan/17 10:28;srowen;I think it's probably OK, if it's a significant problem, and we have a targeted, tested fix here.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Newly created catalog table assumed to have 0 rows and 0 bytes,SPARK-18856,13028016,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,cloud_fan,rxin,rxin,14/Dec/16 07:55,15/Dec/16 05:03,14/Jul/23 06:29,15/Dec/16 05:03,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"{code}
scala> spark.range(100).selectExpr(""id % 10 p"", ""id"").write.partitionBy(""p"").format(""json"").saveAsTable(""testjson"")

scala> spark.table(""testjson"").queryExecution.optimizedPlan.statistics
res6: org.apache.spark.sql.catalyst.plans.logical.Statistics = Statistics(sizeInBytes=0, isBroadcastable=false)
{code}

It shouldn't be 0. The issue is that in DataSource.scala, we do:

{code}
        val fileCatalog = if (sparkSession.sqlContext.conf.manageFilesourcePartitions &&
            catalogTable.isDefined && catalogTable.get.tracksPartitionsInCatalog) {
          new CatalogFileIndex(
            sparkSession,
            catalogTable.get,
            catalogTable.get.stats.map(_.sizeInBytes.toLong).getOrElse(0L))
        } else {
          new InMemoryFileIndex(sparkSession, globbedPaths, options, Some(partitionSchema))
        }
{code}

We shouldn't use 0L as the fallback.
",,apachespark,aroberts,dongjoon,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 14 12:43:06 UTC 2016,,,,,,,,,,"0|i37ktz:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"14/Dec/16 12:43;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16280;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getNodeNumbered and generateTreeString are not consistent,SPARK-18854,13027978,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,14/Dec/16 03:15,15/Dec/16 00:15,14/Jul/23 06:29,15/Dec/16 00:15,,,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,,,,,,"This is a bug introduced by subquery handling. generateTreeString numbers trees including innerChildren (used to print subqueries), but getNodeNumbered ignores that. As a result, getNodeNumbered is not always correct.

Repro:

{code}
    val df = sql(""select * from range(10) where id not in "" +
      ""(select id from range(2) union all select id from range(2))"")

    println(""-------------------------------------------------------"")
    println(df.queryExecution.analyzed.numberedTreeString)
    println(""-------------------------------------------------------"")

    println(""-------------------------------------------------------"")
    println(df.queryExecution.analyzed(3))
    println(""-------------------------------------------------------"")
{code}

Output looks like

{noformat}
-------------------------------------------------------
00 Project [id#1L]
01 +- Filter NOT predicate-subquery#0 [(id#1L = id#2L)]
02    :  +- Union
03    :     :- Project [id#2L]
04    :     :  +- Range (0, 2, step=1, splits=None)
05    :     +- Project [id#3L]
06    :        +- Range (0, 2, step=1, splits=None)
07    +- Range (0, 10, step=1, splits=None)
-------------------------------------------------------
-------------------------------------------------------
null
-------------------------------------------------------
{noformat}

Note that 3 should be the Project node, but getNodeNumbered ignores innerChild and as a result returns the wrong one.",,apachespark,rxin,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 14 21:27:36 UTC 2016,,,,,,,,,,"0|i37klj:",9223372036854775807,,,,,,,,,,,,,2.0.3,2.1.0,,,,,,,,,,"14/Dec/16 03:15;rxin;cc [~smilegator];;;","14/Dec/16 03:16;rxin;To test this, introduce a subquery and call df.numberedTreeString() and then df(i).
;;;","14/Dec/16 06:59;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/16277;;;","14/Dec/16 21:27;smilegator;Sorry, I missed the ping.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Project (UnaryNode) is way too aggressive in estimating statistics ,SPARK-18853,13027949,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,14/Dec/16 00:27,14/Dec/16 20:33,14/Jul/23 06:29,14/Dec/16 20:24,,,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,,,,,,"We currently define statistics in UnaryNode: 

{code}
  override def statistics: Statistics = {
    // There should be some overhead in Row object, the size should not be zero when there is
    // no columns, this help to prevent divide-by-zero error.
    val childRowSize = child.output.map(_.dataType.defaultSize).sum + 8
    val outputRowSize = output.map(_.dataType.defaultSize).sum + 8
    // Assume there will be the same number of rows as child has.
    var sizeInBytes = (child.statistics.sizeInBytes * outputRowSize) / childRowSize
    if (sizeInBytes == 0) {
      // sizeInBytes can't be zero, or sizeInBytes of BinaryNode will also be zero
      // (product of children).
      sizeInBytes = 1
    }

    child.statistics.copy(sizeInBytes = sizeInBytes)
  }
{code}

This has a few issues:

1. This can aggressively underestimate the size for Project. We assume each array/map has 100 elements, which is an overestimate. If the user projects a single field out of a deeply nested field, this would lead to huge underestimation. A safer sane default is probably 1.

2. It is not a property of UnaryNode to propagate statistics this way. It should be a property of Project.



",,apachespark,kiszk,michael,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18676,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 14 20:33:18 UTC 2016,,,,,,,,,,"0|i37kf3:",9223372036854775807,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,,,,,,,"14/Dec/16 01:38;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/16274;;;","14/Dec/16 17:48;michael;Should we link this to https://issues.apache.org/jira/browse/SPARK-18676?;;;","14/Dec/16 17:49;michael;I'll just add another issue with overestimating the array type is you can get into integer overflow.;;;","14/Dec/16 17:50;rxin;Can you say more? Are you talking about deeply nested arrays?
;;;","14/Dec/16 17:51;michael;Yes, nested arrays.;;;","14/Dec/16 20:30;michael;[~rxin] [~hvanhovell] Should we move the overridden {{statistics}} method to {{Project}} before marking this issue as resolved?;;;","14/Dec/16 20:33;rxin;Let's do that separately (I thought about doing it but it might be better to be done together with the CBO work anyway).
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PageRank gives incorrect results for graphs with sinks,SPARK-18847,13027911,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,a1ray,a1ray,a1ray,13/Dec/16 21:32,03/May/17 09:00,14/Jul/23 06:29,17/Mar/17 21:23,1.0.2,1.1.1,1.2.2,1.3.1,1.4.1,1.5.2,1.6.3,2.0.2,2.2.0,,,,GraphX,,,,,,,,0,,,,,,Sink vertices (those with no outgoing edges) should evenly distribute their rank to the entire graph but in the current implementation it is just lost.,,a1ray,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18845,,,,,,,SPARK-18848,SPARK-20429,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 06 06:49:04 UTC 2017,,,,,,,,,,"0|i37k6n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/16 21:34;srowen;Before you open more can you review old JIRAs about this? ;;;","13/Dec/16 21:56;a1ray;I have and have not found any relevant. I'm currently working on a fix;;;","06/Jan/17 06:49;apachespark;User 'aray' has created a pull request for this issue:
https://github.com/apache/spark/pull/16483;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PageRank has incorrect initialization value that leads to slow convergence,SPARK-18845,13027907,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,a1ray,a1ray,a1ray,13/Dec/16 21:19,16/Dec/16 07:40,14/Jul/23 06:29,16/Dec/16 07:39,1.2.2,1.3.1,1.4.1,1.5.2,1.6.3,2.0.2,,,2.2.0,,,,GraphX,,,,,,,,0,,,,,,"All variants of PageRank in GraphX have incorrect initialization value that leads to slow convergence. In the current implementations ranks are seeded with the reset probability when it should be 1. This appears to have been introduced a long time ago in https://github.com/apache/spark/commit/15a564598fe63003652b1e24527c432080b5976c#diff-b2bf3f97dcd2f19d61c921836159cda9L90

This also hides the fact that source vertices (vertices with no incoming edges) are not updated. This is because source vertices generally* have pagerank equal to the reset probability. Therefore both need to be fixed at once.

PR will be added shortly

*when there are no sinks -- but that's a separate bug",,a1ray,ankurd,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18848,SPARK-18847,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 16 07:39:35 UTC 2016,,,,,,,,,,"0|i37k5r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/16 21:26;srowen;See https://issues.apache.org/jira/browse/SPARK-7005 ?;;;","13/Dec/16 21:49;apachespark;User 'aray' has created a pull request for this issue:
https://github.com/apache/spark/pull/16271;;;","13/Dec/16 21:52;a1ray;[~srowen] No that's a different thing just whether the result sums to 1 or n. But to expand on that, since we are using the version that sums to n our initial ranks need to sum to n or it takes a lot longer to converge. ;;;","16/Dec/16 07:39;ankurd;Issue resolved by pull request 16271
[https://github.com/apache/spark/pull/16271];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix timeout in awaitResultInForkJoinSafely,SPARK-18843,13027865,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,13/Dec/16 18:33,15/Dec/16 05:04,14/Jul/23 06:29,13/Dec/16 22:10,2.0.2,2.1.0,,,,,,,2.0.3,2.1.0,,,Spark Core,,,,,,,,0,,,,,,"Master has the fix in https://github.com/apache/spark/pull/16230. However, since we don't merge this PR into master because it's too risky, we should at least fix the timeout value for 2.0 and 2.1.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 13 18:37:09 UTC 2016,,,,,,,,,,"0|i37jwf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/16 18:37;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16268;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PushProjectionThroughUnion exception when there are same column,SPARK-18841,13027798,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,windpiger,windpiger,13/Dec/16 14:30,08/Feb/17 07:36,14/Jul/23 06:29,07/Feb/17 21:42,2.0.2,2.1.0,,,,,,,2.1.1,2.2.0,,,SQL,,,,,,,,0,,,,,,"{noformat}
DROP TABLE IF EXISTS p1 ;
DROP TABLE IF EXISTS p2 ;
DROP TABLE IF EXISTS p3 ;
CREATE TABLE p1 (col STRING) ;
CREATE TABLE p2 (col STRING) ;
CREATE TABLE p3 (col STRING) ;

set spark.sql.crossJoin.enabled = true;

SELECT
  1 as cste,
  col
FROM (
  SELECT
    col as col
  FROM (
    SELECT
      p1.col as col
    FROM p1
    LEFT JOIN p2 
    UNION ALL
    SELECT
      col
    FROM p3
  ) T1
) T2
;
{noformat}

it will throw exception:
{noformat}
key not found: col#16
java.util.NoSuchElementException: key not found: col#16
        at scala.collection.MapLike$class.default(MapLike.scala:228)
        at org.apache.spark.sql.catalyst.expressions.AttributeMap.default(AttributeMap.scala:31)
        at scala.collection.MapLike$class.apply(MapLike.scala:141)
        at org.apache.spark.sql.catalyst.expressions.AttributeMap.apply(AttributeMap.scala:31)
        at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$2.applyOrElse(Optimizer.scala:346)
        at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$2.applyOrElse(Optimizer.scala:345)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:292)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:292)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:291)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:281)
        at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$.org$apache$spark$sql$catalyst$optimizer$PushProjectionThroughUnion$$pushToRight(Optimizer.scala:345)
        at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$apply$4$$anonfun$8$$anonfun$apply$31.apply(Optimizer.scala:378)
        at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$apply$4$$anonfun$8$$anonfun$apply$31.apply(Optimizer.scala:378)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
        at scala.collection.immutable.List.map(List.scala:285)
        at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$apply$4$$anonfun$8.apply(Optimizer.scala:378)
        at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$apply$4$$anonfun$8.apply(Optimizer.scala:376)
{noformat}",,apachespark,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 07 21:46:16 UTC 2017,,,,,,,,,,"0|i37jhj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/16 14:54;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/16267;;;","01/Feb/17 14:49;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/16757;;;","07/Feb/17 21:46;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/16843;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HDFSCredentialProvider throws exception in non-HDFS security environment,SPARK-18840,13027703,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jerryshao,jerryshao,jerryshao,13/Dec/16 06:46,17/May/20 18:13,14/Jul/23 06:29,14/Dec/16 01:43,1.6.3,2.1.0,,,,,,,2.1.0,,,,Spark Core,YARN,,,,,,,0,,,,,,"Current in {{HDFSCredentialProvider}}, the code logic assumes HDFS delegation token should be existed, this is ok for HDFS environment, but for some cloud environment like Azure, HDFS is not required, so it will throw exception:

{code}
java.util.NoSuchElementException: head of empty list
        at scala.collection.immutable.Nil$.head(List.scala:337)
        at scala.collection.immutable.Nil$.head(List.scala:334)
        at org.apache.spark.deploy.yarn.Client.getTokenRenewalInterval(Client.scala:627)
{code}

We should also consider this situation.",,apachespark,jerryshao,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 14 01:43:05 UTC 2016,,,,,,,,,,"0|i37iwf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/16 06:47;jerryshao;This problem also existed in branch 1.6, but the fix is a little different compared to master.;;;","13/Dec/16 09:28;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/16265;;;","13/Dec/16 18:38;vanzin;[~jerryshao] are you planning to fix this for 2.0/1.6 also? Or should I close the bug?;;;","14/Dec/16 01:34;jerryshao;[~vanzin], is it necessary to fix it in old version (2.0/1.6), seems this problem seldom happens.;;;","14/Dec/16 01:43;vanzin;Ok. Since it's not a regression, let's do it if it becomes a problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not expose shaded types in JavaTypeInference API,SPARK-18835,13027630,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,12/Dec/16 23:53,15/Dec/16 05:05,14/Jul/23 06:29,13/Dec/16 18:02,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Currently, {{inferDataType(TypeToken)}} is called from a different maven module, and because we shade Guava, that sometimes leads to errors (e.g. when running tests using maven):

{noformat}
udf3Test(test.org.apache.spark.sql.JavaUDFSuite)  Time elapsed: 0.084 sec  <<< ERROR!
java.lang.NoSuchMethodError: org.apache.spark.sql.catalyst.JavaTypeInference$.inferDataType(Lcom/google/common/reflect/TypeToken;)Lscala/Tuple2;
        at test.org.apache.spark.sql.JavaUDFSuite.udf3Test(JavaUDFSuite.java:107)


Results :

Tests in error: 
  JavaUDFSuite.udf3Test:107 » NoSuchMethod org.apache.spark.sql.catalyst.JavaTyp...
{noformat}

Instead, we shouldn't expose Guava types in these APIs.",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 13 08:56:14 UTC 2016,,,,,,,,,,"0|i37ig7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Dec/16 23:56;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/16260;;;","13/Dec/16 08:56;srowen;(Ideally we should fix this for 2.1.0 because it causes the tests to consistently fail in at least a couple envs);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor SparkR build and test scripts,SPARK-18828,13027371,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,12/Dec/16 07:19,16/Jan/17 21:49,14/Jul/23 06:29,16/Jan/17 21:49,2.1.0,,,,,,,,2.2.0,,,,SparkR,,,,,,,,0,,,,,,"Since we are building SparkR source package we are now seeing the call tree getting more convoluted and more parts are getting duplicated.

We should try to clean this up.

One issue is with the requirement to install SparkR before building SparkR source package (ie. R CMD build) because of the loading of SparkR via ""library(SparkR)"" in the vignettes. When we refactor that part in the vignettes we should be able to further decouple the scripts.
",,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 12 07:20:04 UTC 2016,,,,,,,,,,"0|i37guv:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"12/Dec/16 07:20;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/16249;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cann't read broadcast if broadcast blocks are stored on-disk,SPARK-18827,13027354,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,12/Dec/16 05:18,23/Aug/17 03:58,14/Jul/23 06:29,18/Dec/16 09:08,2.0.1,2.0.2,2.1.0,,,,,,2.0.3,2.1.1,,,Spark Core,,,,,,,,0,,,,,,"How to reproduce it:
{code:java}
  test(""Cache broadcast to disk"") {
    val conf = new SparkConf()
      .setAppName(""Cache broadcast to disk"")
      .setMaster(""local"")
      .set(""spark.memory.useLegacyMode"", ""true"")
      .set(""spark.storage.memoryFraction"", ""0.0"")
    sc = new SparkContext(conf)
    val list = List[Int](1, 2, 3, 4)
    val broadcast = sc.broadcast(list)
    assert(broadcast.value.sum === 10)
  }
{code}

{{NoSuchElementException}} will throw since SPARK-17503 if a broadcast cannot cache in memory. The reason is that that change cannot cover {{!unrolled.hasNext}} in {{next()}} function.",,apachespark,rxin,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21794,,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/16 01:32;yumwang;NoSuchElementException4722.gif;https://issues.apache.org/jira/secure/attachment/12843685/NoSuchElementException4722.gif",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 18 09:08:55 UTC 2016,,,,,,,,,,"0|i37gr3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Dec/16 05:19;yumwang;I will create a PR later.;;;","12/Dec/16 05:50;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/16252;;;","18/Dec/16 09:08;srowen;Issue resolved by pull request 16252
[https://github.com/apache/spark/pull/16252];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
executor page fails to show log links if executors are added after an app is launched,SPARK-18816,13027174,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ajbozarth,yhuai,yhuai,10/Dec/16 05:16,15/Dec/16 05:05,14/Jul/23 06:29,13/Dec/16 21:38,,,,,,,,,2.1.0,,,,Web UI,,,,,,,,0,,,,,,"How to reproduce with standalone mode:
1. Launch a spark master
2. Launch a spark shell. At this point, there is no executor associated with this application. 
3. Launch a slave. Now, there is an executor assigned to the spark shell. However, there is no link to stdout/stderr on the executor page (please see https://issues.apache.org/jira/secure/attachment/12842649/screenshot-1.png).

",,ajbozarth,apachespark,codingcat,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/16 05:16;yhuai;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12842649/screenshot-1.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 13 21:38:25 UTC 2016,,,,,,,,,,"0|i37fn3:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"10/Dec/16 05:23;yhuai;btw, my testing was done with chrome.


I then terminated the cluster and started a new one. I first launched workers. Then, I still could not see the log links on the page. But, I can see the links from safari. ;;;","10/Dec/16 07:57;ajbozarth;That's odd, I tested my code on Safari, Chrome and FF when I made that change. I can look into it Monday if you don't have a fix.

Also this is not a blocker.;;;","12/Dec/16 21:51;yhuai;[~ajbozarth] Yea, please take a look. Thanks! 

The reasons that I set it as a blocker are (1) those log links are super important for debugging; and (2) it is a regression from 2.0.;;;","12/Dec/16 22:02;ajbozarth;Thanks for following up, I was able to recreate the issue, but I personally wont have time to fix it before my holiday vacation. It's not a blocker still because the log pages are still there, only the links to them are missing. You can still access the logs for each worker via the Worker UI links found on the Master UI.;;;","12/Dec/16 22:06;yhuai;Yea, log pages are still there. But, without those links on the executor page, it is very hard to find those pages. 

btw, is there any place that we should look at to find the cause of this problem?;;;","12/Dec/16 22:09;ajbozarth;I'm actually looking into a bit right now and I think it's an issue with the jQuery code I used when I made the column conditional. If I find a solution I'll either open a quick pr or post the info here for you to fix.;;;","12/Dec/16 22:12;ajbozarth;And I still don't think this is a blocker, but I respect that you as a committer know more about Spark than I do.;;;","12/Dec/16 22:50;ajbozarth;I have a fix, just running a few tests then I'll open a pr;;;","12/Dec/16 23:05;apachespark;User 'ajbozarth' has created a pull request for this issue:
https://github.com/apache/spark/pull/16256;;;","13/Dec/16 21:38;srowen;Issue resolved by pull request 16256
[https://github.com/apache/spark/pull/16256];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckAnalysis rejects TPCDS query 32,SPARK-18814,13027162,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,nsyca,ekhliang,ekhliang,10/Dec/16 02:50,26/Jan/17 21:16,14/Jul/23 06:29,14/Dec/16 10:11,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"It seems the CheckAnalysis rule introduced by SPARK-18504 is incorrect rejecting this TPCDS query, which ran fine in Spark 2.0. There doesn't seem to be any obvious error in the query or the check rule though: in the plan below, the scalar subquery's condition field is ""scalar-subquery#24 [(cs_item_sk#39#111 = i_item_sk#59)] "", which should reference cs_item_sk#39. Nonetheless CheckAnalysis complains that cs_item_sk#39 is not referenced by the scalar subquery predicates.

analysis error:
{code}
== Query: q32-v1.4 ==
 Can't be analyzed: org.apache.spark.sql.AnalysisException: a GROUP BY clause in a scalar correlated subquery cannot contain non-correlated columns: cs_item_sk#39;;
GlobalLimit 100
+- LocalLimit 100
   +- Aggregate [sum(cs_ext_discount_amt#46) AS excess discount amount#23]
      +- Filter ((((i_manufact_id#72 = 977) && (i_item_sk#59 = cs_item_sk#39)) && ((d_date#83 >= 2000-01-27) && (d_date#83 <= cast(cast(cast(cast(2000-01-27 as date) as timestamp) + interval 12 weeks 6 days as date) as string)))) && ((d_date_sk#81 = cs_sold_date_sk#58) && (cast(cs_ext_discount_amt#46 as decimal(14,7)) > cast(scalar-subquery#24 [(cs_item_sk#39#111 = i_item_sk#59)] as decimal(14,7)))))
         :  +- Project [(CAST(1.3 AS DECIMAL(11,6)) * CAST(avg(cs_ext_discount_amt) AS DECIMAL(11,6)))#110, cs_item_sk#39 AS cs_item_sk#39#111]
         :     +- Aggregate [cs_item_sk#39], [CheckOverflow((promote_precision(cast(1.3 as decimal(11,6))) * promote_precision(cast(avg(cs_ext_discount_amt#46) as decimal(11,6)))), DecimalType(14,7)) AS (CAST(1.3 AS DECIMAL(11,6)) * CAST(avg(cs_ext_discount_amt) AS DECIMAL(11,6)))#110, cs_item_sk#39]
         :        +- Filter (((d_date#83 >= 2000-01-27]) && (d_date#83 <= cast(cast(cast(cast(2000-01-27 as date) as timestamp) + interval 12 weeks 6 days as date) as string))) && (d_date_sk#81 = cs_sold_date_sk#58))
         :           +- Join Inner
         :              :- SubqueryAlias catalog_sales
         :              :  +- Relation[cs_sold_time_sk#25,cs_ship_date_sk#26,cs_bill_customer_sk#27,cs_bill_cdemo_sk#28,cs_bill_hdemo_sk#29,cs_bill_addr_sk#30,cs_ship_customer_sk#31,cs_ship_cdemo_sk#32,cs_ship_hdemo_sk#33,cs_ship_addr_sk#34,cs_call_center_sk#35,cs_catalog_page_sk#36,cs_ship_mode_sk#37,cs_warehouse_sk#38,cs_item_sk#39,cs_promo_sk#40,cs_order_number#41,cs_quantity#42,cs_wholesale_cost#43,cs_list_price#44,cs_sales_price#45,cs_ext_discount_amt#46,cs_ext_sales_price#47,cs_ext_wholesale_cost#48,... 10 more fields] parquet
         :              +- SubqueryAlias date_dim
         :                 +- Relation[d_date_sk#81,d_date_id#82,d_date#83,d_month_seq#84,d_week_seq#85,d_quarter_seq#86,d_year#87,d_dow#88,d_moy#89,d_dom#90,d_qoy#91,d_fy_year#92,d_fy_quarter_seq#93,d_fy_week_seq#94,d_day_name#95,d_quarter_name#96,d_holiday#97,d_weekend#98,d_following_holiday#99,d_first_dom#100,d_last_dom#101,d_same_day_ly#102,d_same_day_lq#103,d_current_day#104,... 4 more fields] parquet
         +- Join Inner
            :- Join Inner
            :  :- SubqueryAlias catalog_sales
            :  :  +- Relation[cs_sold_time_sk#25,cs_ship_date_sk#26,cs_bill_customer_sk#27,cs_bill_cdemo_sk#28,cs_bill_hdemo_sk#29,cs_bill_addr_sk#30,cs_ship_customer_sk#31,cs_ship_cdemo_sk#32,cs_ship_hdemo_sk#33,cs_ship_addr_sk#34,cs_call_center_sk#35,cs_catalog_page_sk#36,cs_ship_mode_sk#37,cs_warehouse_sk#38,cs_item_sk#39,cs_promo_sk#40,cs_order_number#41,cs_quantity#42,cs_wholesale_cost#43,cs_list_price#44,cs_sales_price#45,cs_ext_discount_amt#46,cs_ext_sales_price#47,cs_ext_wholesale_cost#48,... 10 more fields] parquet
            :  +- SubqueryAlias item
            :     +- Relation[i_item_sk#59,i_item_id#60,i_rec_start_date#61,i_rec_end_date#62,i_item_desc#63,i_current_price#64,i_wholesale_cost#65,i_brand_id#66,i_brand#67,i_class_id#68,i_class#69,i_category_id#70,i_category#71,i_manufact_id#72,i_manufact#73,i_size#74,i_formulation#75,i_color#76,i_units#77,i_container#78,i_manager_id#79,i_product_name#80] parquet
            +- SubqueryAlias date_dim
               +- Relation[d_date_sk#81,d_date_id#82,d_date#83,d_month_seq#84,d_week_seq#85,d_quarter_seq#86,d_year#87,d_dow#88,d_moy#89,d_dom#90,d_qoy#91,d_fy_year#92,d_fy_quarter_seq#93,d_fy_week_seq#94,d_day_name#95,d_quarter_name#96,d_holiday#97,d_weekend#98,d_following_holiday#99,d_first_dom#100,d_last_dom#101,d_same_day_ly#102,d_same_day_lq#103,d_current_day#104,... 4 more fields] parquet

{code}

query text:
{code}
select sum(cs_ext_discount_amt) as `excess discount amount`
 from
    catalog_sales, item, date_dim
 where
   i_manufact_id = 977
   and i_item_sk = cs_item_sk
   and d_date between '2000-01-27' and (cast('2000-01-27' as date) + interval 90 days)
   and d_date_sk = cs_sold_date_sk
   and cs_ext_discount_amt > (
          select 1.3 * avg(cs_ext_discount_amt)
          from catalog_sales, date_dim
          where cs_item_sk = i_item_sk
           and d_date between '2000-01-27]' and (cast('2000-01-27' as date) + interval 90 days)
           and d_date_sk = cs_sold_date_sk)
limit 100
{code}",,apachespark,ekhliang,kiszk,nsyca,pes2009k,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19376,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 14 01:20:24 UTC 2016,,,,,,,,,,"0|i37fkf:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"10/Dec/16 02:54;ekhliang;[~rxin];;;","10/Dec/16 02:58;rxin;cc [~hvanhovell] and [~nsyca];;;","10/Dec/16 03:01;nsyca;I am looking at this.;;;","10/Dec/16 03:49;nsyca;It looks like the {{Project}} between {{Aggregate}} and {{Filter scalar-subquery}} maps {{cs_item_sk#39}} to {{cs_item_sk#39#111}}. The logic in the code is not robust enough to recognize that the two symbols are equivalent. I tried to simplify the problem to

{code}
Seq[(java.lang.Integer, scalar.lang.BigDecimal)]((1,BigDecimal(1.0))).toDF(""k"",""v"").createOrReplaceTempView(""P"")
Seq[(java.lang.Integer, scala.math.BigDecimal)]((1,BigDecimal(1.0))).toDF(""k1"",""v1"").createOrReplaceTempView(""C"")

sql(""select * from p where v = (select 1.1 * avg(v1) from c where c.k1=p.k)"").explain(true)
{code}

This should have all the elements required to reproduce the problem but somehow I could not get the required `Project` operator so there is no mapping of the column {{p.k}} as it is in the TPCDS-Q32.

I will keep trying.;;;","10/Dec/16 03:53;ekhliang;It seems that the references of an Alias expression should include the
referenced attribute, so I would expect #39 to still show up. I could be
misunderstanding the behavior of Alias though.

On Fri, Dec 9, 2016, 7:50 PM Nattavut Sutyanyong (JIRA) <jira@apache.org>

;;;","10/Dec/16 10:27;nsyca;To get the extra `#111` of  `cs_item_sk#39#111`, we need to reference the same table in both the parent side and the subquery side (as the catalog_sales in Q35) so it will run thru the deduplicate logic to add `#111` to distinguish the same column of the two contexts.;;;","10/Dec/16 11:02;nsyca;I can reproduce with a simple script now.

{code}
Seq((1,1)).toDF(""pk"",""pv"").createOrReplaceTempView(""p"")
Seq((1,1)).toDF(""ck"",""cv"").createOrReplaceTempView(""c"")
sql(""select * from p,c where p.pk=c.ck and c.cv = (select avg(c1.cv) from c c1 where c1.ck = p.pk)"").show
{code}

The requirements are:
1. We need to reference the same table twice in both the parent and the subquery. Here is the table c.
2. We need to have a correlated predicate but to a different table. Here is from c (as c1) in the subquery to p in the parent.
3. We will then ""deduplicate"" c1.ck in the subquery to {{ck#<n1>#<n2>}} at {{Project}} above {{Aggregate}} of {{avg}}. Then when we compare {{ck#<n1>#<n2>}} and the original group by column {{ck#<n1>}} by their canonicalized form, which is #<n2> != #<n1>. That's how we trigger the exception I added.

I will continue working on a fix.;;;","10/Dec/16 11:17;nsyca;[~cloud_fan] FYI, part of the problem here is the deDuplicate logic that rewires the groupby column to a new ExprId in the middle of the checking, a similar problem tracked by SPARK-17154.

{code}
1251       // Make sure the inner and the outer query attributes do not collide.
1252       val outputSet = outer.map(_.outputSet).reduce(_ ++ _)
1253       val duplicates = basePlan.outputSet.intersect(outputSet)
1254       val (plan, deDuplicatedConditions) = if (duplicates.nonEmpty) {
1255         val aliasMap = AttributeMap(duplicates.map { dup =>
1256           dup -> Alias(dup, dup.toString)()
1257         }.toSeq)
1258         val aliasedExpressions = basePlan.output.map { ref =>
1259           aliasMap.getOrElse(ref, ref)
1260         }
1261         val aliasedProjection = Project(aliasedExpressions, basePlan)
1262         val aliasedConditions = baseConditions.map(_.transform {
1263           case ref: Attribute => aliasMap.getOrElse(ref, ref).toAttribute
1264         })
1265         (aliasedProjection, aliasedConditions)
1266       } else {
1267         (basePlan, baseConditions)
1268       }
{code};;;","10/Dec/16 13:19;nsyca;I have a potential fix; it works but it's not pretty. I want to step back and think about it more but if a fix is urgently needed, I can submit a PR for it.;;;","10/Dec/16 15:50;apachespark;User 'nsyca' has created a pull request for this issue:
https://github.com/apache/spark/pull/16246;;;","12/Dec/16 19:12;kiszk;I found the same error {{org.apache.spark.sql.AnalysisException: a GROUP BY clause in a scalar correlated subquery cannot contain non-correlated columns: ws_item_sk#1081;;}} when I ran q92 using master branch.;;;","12/Dec/16 19:25;nsyca;q92 has the same pattern as q32 and my simplified version. If it's possible, could you try patching my PR to verify the problem is resolved?;;;","14/Dec/16 01:20;nsyca;As this JIRA will be brought to close shortly, I'd like to take this opportunity to thank [~ekhliang] who reported the first case of regression, [~kiszk] the second case. You help shorten the life of a regression from my code. I also thank [~smilegator] and [~hvanhovell] who shared their ideas on the fix and reviewed my code, and lastly, to [~rxin] who nudged me to this problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SparkR install.spark does not work for RCs, snapshots",SPARK-18810,13027079,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,shivaram,shivaram,09/Dec/16 19:47,15/Dec/16 05:05,14/Jul/23 06:29,12/Dec/16 22:41,2.0.2,2.1.0,,,,,,,2.1.0,,,,SparkR,,,,,,,,0,,,,,,We publish source archives of the SparkR package now in RCs and in nightly snapshot builds. One of the problems that still remains is that `install.spark` does not work for these as it looks for the final Spark version to be present in the apache download mirrors.,,apachespark,felixcheung,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 12 22:41:15 UTC 2016,,,,,,,,,,"0|i37f1z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/16 19:53;felixcheung;I've found the same issue while testing as well, and was going to propose a change to support this.

Essentially for snapshot and RC build, since the jar is not on the Apache mirror, install.spark is unable to download it. We need to have a way to override the url (details: as it is constructing the url from a base url and a version path, it is expecting the source as a certain directory structure - currently this structure does not match how the snapshot and RC build are published, so we need a way to override the entire url)

I propose we have an environment variable instead of a parameter since we want to be able to run everything the same way without having to make code changes.
;;;","09/Dec/16 19:58;shivaram;I think the snapshot case and the RC case are probably a bit different.
- In the case of RCs the artifact name matches what would be the final release (for example http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc2-bin/) so we only need to change the base url (environment variable could work for this)
- For nightly builds the artifact name also changes and this probably needs some more thought. I guess having a way to override the entire URL would solve both the cases ? ;;;","09/Dec/16 20:06;felixcheung;For RC, it actually expects to have a subdirectory `spark-2.1.0` (==version) so it doesn't exactly match `spark-2.1.0-rc2-bin` in http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc2-bin/

https://github.com/apache/spark/blob/39e2bad6a866d27c3ca594d15e574a1da3ee84cc/R/pkg/R/install.R#L71
;;;","09/Dec/16 20:11;felixcheung;Also to expand on the earlier note above, I think the main thing to be able to run existing tests, build vignettes and so on
- without having to change any code
or
- without having to manually call install.spark in a separate session first to cache the spark jar

this is why I think it makes sense to have an environment override instead of an API parameter switch.
;;;","09/Dec/16 20:13;shivaram;Yeah I think that sounds good. This need not be an advertised feature that we tell users about but more of a flag we use for testing;;;","11/Dec/16 07:16;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/16248;;;","12/Dec/16 22:41;shivaram;Issue resolved by pull request 16248
[https://github.com/apache/spark/pull/16248];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should suppress output print for calls to JVM methods with void return values,SPARK-18807,13027070,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,felixcheung,felixcheung,felixcheung,09/Dec/16 19:26,15/Dec/16 05:05,14/Jul/23 06:29,10/Dec/16 03:07,2.0.0,2.1.0,,,,,,,2.1.0,,,,SparkR,,,,,,,,0,,,,,,"Several SparkR API calling into JVM methods that have void return values are getting printed out, especially when running in a REPL or IDE.

example:

> setLogLevel(""WARN"")
NULL

We should fix this to make the result more clear.
",,felixcheung,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 10 03:07:19 UTC 2016,,,,,,,,,,"0|i37ezz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/16 03:07;shivaram;Resolved by https://github.com/apache/spark/pull/16237;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingQueryManager should not hold a lock when starting a query,SPARK-18796,13026814,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,08/Dec/16 22:32,13/Dec/16 11:23,14/Jul/23 06:29,13/Dec/16 06:32,2.0.2,2.1.0,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,"Otherwise, the user cannot start any queries when a query is starting. If a query takes a long time to start, the user experience will be pretty bad.",,apachespark,codingcat,tdas,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 13 06:32:03 UTC 2016,,,,,,,,,,"0|i37df3:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"08/Dec/16 22:37;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16220;;;","13/Dec/16 06:32;tdas;Issue resolved by pull request 16220
[https://github.com/apache/spark/pull/16220];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Offset for FileStreamSource is not json formatted,SPARK-18776,13026543,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,tdas,tdas,08/Dec/16 03:25,09/Dec/16 01:54,14/Jul/23 06:29,09/Dec/16 01:53,2.0.2,,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,All source offset must be json formatted. ,,apachespark,codingcat,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 09 01:54:00 UTC 2016,,,,,,,,,,"0|i37bqv:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"08/Dec/16 03:49;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/16205;;;","09/Dec/16 01:54;tdas;Issue resolved by pull request 16205
[https://github.com/apache/spark/pull/16205];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unnecessary conversion try for special floats in JSON,SPARK-18772,13026508,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,NathanHowell,NathanHowell,07/Dec/16 23:28,12/Dec/22 18:10,14/Jul/23 06:29,13/May/17 13:02,2.0.2,2.2.0,,,,,,,2.2.0,,,,SQL,,,,,,,,0,,,,,,"It looks we can avoid some cases for unnecessary conversion try in special floats in JSON.

{code}
scala> import org.apache.spark.sql.types._
import org.apache.spark.sql.types._

scala> spark.read.schema(StructType(Seq(StructField(""a"", DoubleType)))).option(""mode"", ""FAILFAST"").json(Seq(""""""{""a"": ""nan""}"""""").toDS).show()
17/05/12 11:30:41 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.NumberFormatException: For input string: ""nan""
...
{code}


",,apachespark,cloud_fan,NathanHowell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 13 13:02:09 UTC 2017,,,,,,,,,,"0|i37bj3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/16 23:44;apachespark;User 'NathanHowell' has created a pull request for this issue:
https://github.com/apache/spark/pull/16199;;;","12/May/17 02:48;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/17956;;;","12/May/17 08:09;gurwls223;I am sorry for fixing the JIRA by myself - [~NathanHowell] as suggested in the PR. Please excuse me.;;;","13/May/17 13:02;cloud_fan;Issue resolved by pull request 17956
[https://github.com/apache/spark/pull/17956];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Web UI should be http:4040 instead of https:4040,SPARK-18762,13026251,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sarutak,mengxr,mengxr,07/Dec/16 06:50,08/Dec/16 07:42,14/Jul/23 06:29,07/Dec/16 19:45,2.1.0,,,,,,,,2.0.3,2.1.0,,,Spark Shell,Web UI,,,,,,,0,,,,,,"When SSL is enabled, the Spark shell shows:

{code}
Spark context Web UI available at https://192.168.99.1:4040
{code}

This is wrong because 4040 is http, not https. It redirects to the https port.

More importantly, this introduces several broken links in the UI. For example, in the master UI, the worker link is https:8081 instead of http:8081 or https:8481.",,ajbozarth,apachespark,lian cheng,mengxr,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16988,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 07 07:25:05 UTC 2016,,,,,,,,,,"0|i379xz:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"07/Dec/16 06:56;mengxr;cc [~hayashidac] [~sarutak] [~lian cheng];;;","07/Dec/16 07:11;sarutak;[~mengxr] Ah... O.K, I'll submit a PR to revert it.;;;","07/Dec/16 07:20;mengxr;Thanks! Please make sure spark history server still works  when ssl is enabled.;;;","07/Dec/16 07:22;sarutak;Yeah of course.;;;","07/Dec/16 07:25;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/16190;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Uncancellable / unkillable tasks may starve jobs of resoures,SPARK-18761,13026245,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,07/Dec/16 06:26,20/Dec/16 23:57,14/Jul/23 06:29,20/Dec/16 02:44,,,,,,,,,2.0.3,2.1.1,2.2.0,,Spark Core,,,,,,,,0,,,,,,"Spark's current task cancellation / task killing mechanism is ""best effort"" in the sense that some tasks may not be interruptible and may not respond to their ""killed"" flags being set. If a significant fraction of a cluster's task slots are occupied by tasks that have been marked as killed but remain running then this can lead to a situation where new jobs and tasks are starved of resources because zombie tasks are holding resources.

I propose to address this problem by introducing a ""task reaper"" mechanism in executors to monitor tasks after they are marked for killing in order to periodically re-attempt the task kill, capture and log stacktraces / warnings if tasks do not exit in a timely manner, and, optionally, kill the entire executor JVM if cancelled tasks cannot be killed within some timeout.",,apachespark,jonathak,joshrosen,lwlin,sarutak,viirya,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17064,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 20 20:44:05 UTC 2016,,,,,,,,,,"0|i379wn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/16 06:39;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16189;;;","07/Dec/16 07:21;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/16190;;;","07/Dec/16 09:15;sarutak;Sorry, my PR is not for this issue. Please disregard it.;;;","20/Dec/16 02:44;yhuai;Issue resolved by pull request 16189
[https://github.com/apache/spark/pull/16189];;;","20/Dec/16 20:44;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16358;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingQueryListener events from a StreamingQuery should be sent only to the listeners in the same session as the query,SPARK-18758,13026223,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,tdas,tdas,07/Dec/16 03:18,09/Dec/16 00:55,14/Jul/23 06:29,08/Dec/16 03:23,2.0.2,,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,"Listeners added with `sparkSession.streams.addListener(l)` are added to a SparkSession. So events only from queries in the same session as a listener should be posted to the listener.
Currently, all the events gets routed through the Spark's main listener bus, and therefore all StreamingQueryListener events gets posted to StreamingQueryListeners in all sessions. This is wrong.",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 08 03:23:54 UTC 2016,,,,,,,,,,"0|i379rr:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"07/Dec/16 03:43;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/16186;;;","08/Dec/16 03:23;tdas;Issue resolved by pull request 16186
[https://github.com/apache/spark/pull/16186];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent behavior after writing to parquet files,SPARK-18753,13026170,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,zsxwing,zsxwing,07/Dec/16 00:28,12/Dec/22 17:34,14/Jul/23 06:29,14/Dec/16 19:29,2.0.2,2.1.0,,,,,,,2.1.0,2.2.0,,,SQL,,,,,,,,0,,,,,,"Found an inconsistent behavior when using parquet.

{code}
scala> val ds = Seq[java.lang.Boolean](new java.lang.Boolean(true), null: java.lang.Boolean, new java.lang.Boolean(false)).toDS
ds: org.apache.spark.sql.Dataset[Boolean] = [value: boolean]

scala> ds.filter('value === ""true"").show
+-----+
|value|
+-----+
+-----+

{code}

In the above example, `ds.filter('value === ""true"")` returns nothing as ""true"" will be converted to null and the filter expression will be always null, so it drops all rows.

However, if I store `ds` to a parquet file and read it back, `filter('value === ""true"")` will return non null values.

{code}
scala> ds.write.parquet(""testfile"")
SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.

scala> val ds2 = spark.read.parquet(""testfile"")
ds2: org.apache.spark.sql.DataFrame = [value: boolean]

scala> ds2.filter('value === ""true"").show
+-----+
|value|
+-----+
| true|
|false|
+-----+

{code}",,apachespark,lian cheng,weiluo_ren123,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 14 19:29:43 UTC 2016,,,,,,,,,,"0|i379fz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/16 00:30;zsxwing;cc [~liancheng];;;","07/Dec/16 02:56;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/16184;;;","14/Dec/16 19:29;lian cheng;Issue resolved by pull request 16184
[https://github.com/apache/spark/pull/16184];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""isSrcLocal"" parameter to Hive loadTable / loadPartition should come from user",SPARK-18752,13026156,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,06/Dec/16 23:11,13/Dec/16 11:22,14/Jul/23 06:29,12/Dec/16 22:20,2.1.0,,,,,,,,2.2.0,,,,SQL,,,,,,,,0,,,,,,"We ran into an issue with the HiveShim code that calls ""loadTable"" and ""loadPartition"" while testing with some recent changes in upstream Hive.

The semantics in Hive changed slightly, and if you provide the wrong value for ""isSrcLocal"" you now can end up with an invalid table: the Hive code will move the temp directory to the final destination instead of moving its children.

The problem in Spark is that HiveShim.scala tries to figure out the value of ""isSrcLocal"" based on where the source and target directories are; that's not correct. ""isSrcLocal"" should be set based on the user query (e.g. ""LOAD DATA LOCAL"" would set it to ""true""). So we need to propagate that information from the user query down to HiveShim.",,apachespark,smilegator,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 12 23:20:05 UTC 2016,,,,,,,,,,"0|i379cv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/16 00:32;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/16179;;;","08/Dec/16 21:02;vanzin;For posterity, here's the exception we hit in our tests:

{noformat}
java.io.IOException: Not a file: file:/tmp/warehouse--ccc2e52d-5760-4cfe-84d3-79ec2bb4b03c/hivetablewitharrayvalue/-ext-10000
at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:202)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239
{noformat}

(This is trying to query a table that was populated using ""LOAD DATA"" in the unit tests.);;;","12/Dec/16 22:20;smilegator;Issue resolved by pull request 16179
[https://github.com/apache/spark/pull/16179];;;","12/Dec/16 23:20;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/16257;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark should be able to control the number of executor and should not throw stack overslow,SPARK-18750,13026134,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,neerjakhattar,neerjakhattar,06/Dec/16 21:50,18/May/20 03:24,14/Jul/23 06:29,25/Jan/17 14:19,,,,,,,,,2.0.3,2.1.1,2.2.0,,,,,,,,,,0,,,,,,"When running Sql queries on large datasets. Job fails with stack overflow warning and it shows it is requesting lots of executors.

Looks like there is no limit to number of executors or not even having an upperbound based on yarn available resources.

{noformat}
16/11/29 15:47:47 INFO impl.ContainerManagementProtocolProxy: Opening proxy : bdtcstr61n5.svr.us.jpmchase.net:8041 
16/11/29 15:47:47 INFO impl.ContainerManagementProtocolProxy: Opening proxy : bdtcstr61n8.svr.us.jpmchase.net:8041 
16/11/29 15:47:47 INFO impl.ContainerManagementProtocolProxy: Opening proxy : bdtcstr61n2.svr.us.jpmchase.net:8041 
16/11/29 15:47:47 INFO yarn.YarnAllocator: Driver requested a total number of 32770 executor(s). 
16/11/29 15:47:47 INFO yarn.YarnAllocator: Will request 24576 executor containers, each with 1 cores and 6758 MB memory including 614 MB overhead 
16/11/29 15:49:11 INFO yarn.YarnAllocator: Driver requested a total number of 52902 executor(s). 
16/11/29 15:47:47 INFO impl.ContainerManagementProtocolProxy: Opening proxy : bdtcstr61n5.svr.us.jpmchase.net:8041
16/11/29 15:47:47 INFO impl.ContainerManagementProtocolProxy: Opening proxy : bdtcstr61n8.svr.us.jpmchase.net:8041
16/11/29 15:47:47 INFO impl.ContainerManagementProtocolProxy: Opening proxy : bdtcstr61n2.svr.us.jpmchase.net:8041
16/11/29 15:47:47 INFO yarn.YarnAllocator: Driver requested a total number of 32770 executor(s).
16/11/29 15:47:47 INFO yarn.YarnAllocator: Will request 24576 executor containers, each with 1 cores and 6758 MB memory including 614 MB overhead
16/11/29 15:49:11 INFO yarn.YarnAllocator: Driver requested a total number of 52902 executor(s).
16/11/29 15:49:11 WARN yarn.ApplicationMaster: Reporter thread fails 1 time(s) in a row.
java.lang.StackOverflowError
	at scala.collection.immutable.HashMap.$plus(HashMap.scala:57)
	at scala.collection.immutable.HashMap.$plus(HashMap.scala:36)
	at scala.collection.mutable.MapBuilder.$plus$eq(MapBuilder.scala:28)
	at scala.collection.mutable.MapBuilder.$plus$eq(MapBuilder.scala:24)
	at scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:48)
	at scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:48)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:224)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:403)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:403)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.MapBuilder.$plus$plus$eq(MapBuilder.scala:24)
	at scala.collection.TraversableLike$class.$plus$plus(TraversableLike.scala:156)
	at scala.collection.AbstractTraversable.$plus$plus(Traversable.scala:105)
	at scala.collection.immutable.HashMap.$plus(HashMap.scala:60)
	at scala.collection.immutable.Map$Map4.updated(Map.scala:172)
	at scala.collection.immutable.Map$Map4.$plus(Map.scala:173)
	at scala.collection.immutable.Map$Map4.$plus(Map.scala:158)
	at scala.collection.mutable.MapBuilder.$plus$eq(MapBuilder.scala:28)
	at scala.collection.mutable.MapBuilder.$plus$eq(MapBuilder.scala:24)
	at scala.collection.TraversableLike$$anonfun$filter$1.apply(TraversableLike.scala:264)
	at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
	at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
	at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
	at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
	at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
	at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
	at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
	at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
	at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
	at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
[...]
16/11/29 15:49:11 INFO yarn.YarnAllocator: Will request 44708 executor containers, each with 1 cores and 6758 MB memory including 614 MB overhead
{noformat}

If you notice in the error above, YARN is trying to request 24576 executor containers, whereas the available cores are 1719. The Driver is requesting for 52902 executor(s), which too high. 

This exception should be fixed
",,apachespark,jelmer1,neerjakhattar,richardatcloudera,vanzin,Yibing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31746,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 25 19:09:06 UTC 2017,,,,,,,,,,"0|i3797z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/16 21:54;srowen;Hm, I am not immediately sure how these are related. Where does the overflow occur -- is it even in Spark? and I wonder why it would be caused by a lot of executor requests?

And then why is it asking for so many executors? that seems wrong but is that the bug you are reporting?  does the app ask for that many? is dynamic allocation on, etc?

This isn't enough info.;;;","07/Dec/16 16:47;neerjakhattar;[~srowen] Yes, this happens when dynamic allocation is on. Yes that's spark sql query when runs on a very large dataset.;;;","07/Dec/16 19:22;srowen;It sounds like you're focused on the StackOverflowError, and so controlling the number of executors is actually just incidental. There's not yet an indication here that the error comes from Spark though. Where is the stack trace coming from? If it's from YARN libraries the fix would have to go there.;;;","08/Dec/16 01:25;neerjakhattar;[~srowen] I added the the full stack trace.;;;","08/Dec/16 04:46;srowen;Yeah, I guess I should have realized that by nature it won't show the source of the exception, because the stack is too deep. It's not clear where the exception is coming from -- do you know and is it Spark?;;;","09/Dec/16 00:58;Yibing;[~srowen] The log says:
{noformat}
16/11/29 15:49:11 WARN yarn.ApplicationMaster: Reporter thread fails 1 time(s) in a row.
java.lang.StackOverflowError
{noformat}

It looks like that number of executors is too high for reporter thread to deal with.
;;;","09/Dec/16 02:12;srowen;Yes, so the basic question is: where is the error coming from? Is it Spark? May be simplistic question but not sure where to look for a solution and I have the impression the reporter knows.;;;","11/Dec/16 09:46;srowen;I'm going to close this as a duplicate of SPARK-18769 unless there's evidence that this is an error from Spark, and can be patched separately from the apparent underlying cause, which is in that JIRA.;;;","18/Jan/17 23:56;vanzin;Sean, this is a separate issue. Even if Spark is changed to be smarter, it could still decide to try to allocate a bunch of containers and hit this overflow error.;;;","19/Jan/17 01:38;vanzin;I haven't been able to reproduce this yet, but http://stackoverflow.com/questions/25450887/scala-map-mapvalues-stackoverflowerror seems to imply that the use of ""mapValues"" may be related. That is used in {{LocalityPreferredContainerPlacementStrategy}} and may be worth taking a look at.;;;","21/Jan/17 00:52;vanzin;Yay, I can reproduce it with a unit test against {{LocalityPreferredContainerPlacementStrategy}}.;;;","21/Jan/17 01:14;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/16667;;;","22/Jan/17 09:08;jelmer1;I am seeing the exact same issue when using dynamic allocation and doing just a basic spark sql query over a large data set;;;","25/Jan/17 19:09;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/16704;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add implicit encoders for BigDecimal, timestamp and date",SPARK-18746,13026098,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WeiqingYang,WeiqingYang,WeiqingYang,06/Dec/16 19:39,14/Dec/16 01:58,14/Jul/23 06:29,14/Dec/16 01:58,,,,,,,,,2.2.0,,,,SQL,,,,,,,,0,,,,,,"Run the code below in spark-shell, there will be an error:
{code}
scala> spark.createDataset(Seq(new java.math.BigDecimal(10)))
<console>:24: error: Unable to find encoder for type stored in a Dataset.  Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.
       spark.createDataset(Seq(new java.math.BigDecimal(10)))
                          ^

scala>
{code} 

In this PR, implicit encoders for BigDecimal, timestamp and date will be added.",,apachespark,dongjoon,WeiqingYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 06 19:49:05 UTC 2016,,,,,,,,,,"0|i378zz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/16 19:49;apachespark;User 'weiqingy' has created a pull request for this issue:
https://github.com/apache/spark/pull/16176;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.IndexOutOfBoundsException running query 68 Spark SQL on (100TB),SPARK-18745,13026093,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,kiszk,jfchen@us.ibm.com,jfchen@us.ibm.com,06/Dec/16 19:24,15/Dec/16 05:04,14/Jul/23 06:29,09/Dec/16 22:15,2.1.0,2.2.0,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,,,,,,"Running query 68 with decreased executor memory (using 12GB executors instead of 24GB) on 100TB parquet database using the Spark master dated 11/04 gave IndexOutOfBoundsException.

The query is as follows:
{noformat}
[select  c_last_name
       ,c_first_name
       ,ca_city
       ,bought_city
       ,ss_ticket_number
       ,extended_price
       ,extended_tax
       ,list_price
 from (select ss_ticket_number
             ,ss_customer_sk
             ,ca_city bought_city
             ,sum(ss_ext_sales_price) extended_price 
             ,sum(ss_ext_list_price) list_price
             ,sum(ss_ext_tax) extended_tax 
       from store_sales
           ,date_dim
           ,store
           ,household_demographics
           ,customer_address 
       where store_sales.ss_sold_date_sk = date_dim.d_date_sk
         and store_sales.ss_store_sk = store.s_store_sk  
        and store_sales.ss_hdemo_sk = household_demographics.hd_demo_sk
        and store_sales.ss_addr_sk = customer_address.ca_address_sk
        and date_dim.d_dom between 1 and 2 
        and (household_demographics.hd_dep_count = 8 or
             household_demographics.hd_vehicle_count= -1)
        and date_dim.d_year in (2000,2000+1,2000+2)
        and store.s_city in ('Plainview','Rogers')
       group by ss_ticket_number
               ,ss_customer_sk
               ,ss_addr_sk,ca_city) dn
      ,customer
      ,customer_address current_addr
 where ss_customer_sk = c_customer_sk
   and customer.c_current_addr_sk = current_addr.ca_address_sk
   and current_addr.ca_city <> bought_city
 order by c_last_name
         ,ss_ticket_number
  limit 100]
{noformat}

Spark output that showed the exception:
{noformat}
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResultInForkJoinSafely(ThreadUtils.scala:215)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:124)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:124)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:123)
	at org.apache.spark.sql.execution.exchange.ReusedExchangeExec.doExecuteBroadcast(Exchange.scala:61)
	at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:231)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:124)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:124)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:123)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:98)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:197)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:82)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:68)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.consume(SortMergeJoinExec.scala:35)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.doProduce(SortMergeJoinExec.scala:560)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.produce(SortMergeJoinExec.scala:35)
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:313)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:354)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)
	at org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:295)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$hiveResultString$4.apply(QueryExecution.scala:134)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$hiveResultString$4.apply(QueryExecution.scala:133)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.hiveResultString(QueryExecution.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:331)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:173)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.IndexOutOfBoundsException
	at java.io.ObjectOutputStream.write(ObjectOutputStream.java:707)
	at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap$$anonfun$writeExternal$4.apply(HashedRelation.scala:698)
	at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap$$anonfun$writeExternal$4.apply(HashedRelation.scala:698)
	at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap.writeLongArray(HashedRelation.scala:675)
	at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap.write(HashedRelation.scala:694)
	at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap.writeExternal(HashedRelation.scala:698)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.sql.execution.joins.LongHashedRelation.writeExternal(HashedRelation.scala:788)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:236)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:236)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1302)
	at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:237)
	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:107)
	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:86)
	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)
	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:56)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1413)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:97)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:75)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:94)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:74)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:74)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResultInForkJoinSafely(ThreadUtils.scala:215)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:124)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:124)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:123)
	at org.apache.spark.sql.execution.exchange.ReusedExchangeExec.doExecuteBroadcast(Exchange.scala:61)
	at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:231)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:124)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:124)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:123)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:98)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:197)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:82)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:68)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.consume(SortMergeJoinExec.scala:35)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.doProduce(SortMergeJoinExec.scala:560)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.produce(SortMergeJoinExec.scala:35)
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:313)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:354)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)
	at org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:295)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$hiveResultString$4.apply(QueryExecution.scala:134)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$hiveResultString$4.apply(QueryExecution.scala:133)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.hiveResultString(QueryExecution.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:331)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:173)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.IndexOutOfBoundsException
	at java.io.ObjectOutputStream.write(ObjectOutputStream.java:707)
	at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap$$anonfun$writeExternal$4.apply(HashedRelation.scala:698)
	at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap$$anonfun$writeExternal$4.apply(HashedRelation.scala:698)
	at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap.writeLongArray(HashedRelation.scala:675)
	at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap.write(HashedRelation.scala:694)
	at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap.writeExternal(HashedRelation.scala:698)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.sql.execution.joins.LongHashedRelation.writeExternal(HashedRelation.scala:788)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:236)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:236)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1302)
	at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:237)
	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:107)
	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:86)
	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)
	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:56)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1413)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:97)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:75)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:94)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:74)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:74)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

Likely an integer overflow issue?
",,apachespark,dongjoon,jfchen@us.ibm.com,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18458,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 09 15:41:04 UTC 2016,,,,,,,,,,"0|i378yv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/16 19:42;kiszk;I work with [~jfchen@us.ibm.com];;;","08/Dec/16 22:28;dongjoon;Hi, I removed the FIX VERSION because this issue is exposed as `Open` in RC2 vote links.;;;","09/Dec/16 15:21;kiszk;I identified a root cause of this {{IndexOutOfBoundsException}}. This exception happens due of negative value of {{len}} for {{ObjectOutputStream.write()}}. This {{len}} is passed as the 3rd argument of {{writeBuffer()}} at {{HasedRelation.scala}}. This value {{size}} is calculated by {{val size = Math.min(buffer.length, (end - offset).toInt)}}. Types of {{end}} and {{offset}} are long. If the result of this subtraction is {{0x0000_0000_1xxx_xxxx}}, the result of a cast {{toInt}} is negative. This is why the {{IndexOutOfBoundsException}} occurs.;;;","09/Dec/16 15:41;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/16235;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ask the build script to link to Jenkins test report page instead of full console output page when posting to GitHub,SPARK-18730,13025810,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lian cheng,lian cheng,lian cheng,06/Dec/16 00:52,14/Dec/16 18:56,14/Jul/23 06:29,14/Dec/16 18:56,,,,,,,,,2.1.0,2.2.0,,,Build,,,,,,,,0,,,,,,"Currently, the full console output page of a Spark Jenkins PR build can be as large as several megabytes. It takes a relatively long time to load and may even freeze the browser for quite a while.

I'd suggest posting the test report page link to GitHub instead, which is way more concise and is usually the first page I'd like to check when investigating a Jenkins build failure.",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 06 01:26:08 UTC 2016,,,,,,,,,,"0|i3777z:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"06/Dec/16 01:26;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/16163;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filesystem unnecessarily scanned twice during creation of non-catalog table,SPARK-18726,13025756,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,windpiger,ekhliang,ekhliang,05/Dec/16 22:11,03/Mar/17 07:54,14/Jul/23 06:29,03/Mar/17 07:54,2.1.0,,,,,,,,2.2.0,,,,SQL,,,,,,,,0,,,,,,"It seems that for non-catalog tables (e.g. spark.read.parquet(...)), we scan the filesystem twice, once for schema inference, and another to create a FileIndex class for the relation.

It would be better to combine these scans somehow, since this is the most costly step of creating a table. This is a follow-up ticket to https://github.com/apache/spark/pull/16090.

cc [~cloud_fan]",,apachespark,cloud_fan,ekhliang,mengxr,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 03 07:54:19 UTC 2017,,,,,,,,,,"0|i376vz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/17 11:49;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/17081;;;","03/Mar/17 07:54;cloud_fan;Issue resolved by pull request 17081
[https://github.com/apache/spark/pull/17081];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move no data rate limit from StreamExecution to ProgressReporter,SPARK-18722,13025742,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,05/Dec/16 21:26,08/Dec/16 23:04,14/Jul/23 06:29,06/Dec/16 02:51,,,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,So that we can also limit items in `recentProgresses`,,apachespark,tdas,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 06 02:51:42 UTC 2016,,,,,,,,,,"0|i376sv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/16 21:33;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16155;;;","06/Dec/16 02:51;tdas;Issue resolved by pull request 16155
[https://github.com/apache/spark/pull/16155];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ForeachSink breaks Watermark in append mode,SPARK-18721,13025737,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,zsxwing,copris,copris,05/Dec/16 21:10,06/Dec/16 04:35,14/Jul/23 06:29,06/Dec/16 04:35,2.1.0,,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,"The watermark is not updated in append mode with a ForeachSink

Because ForeachSink creates a separate IncrementalExecution instance, the physical plan will be recreated for the logical plan, which results in a new EventTimeWatermarkExec operator being created, that's unreachable from StreamExecution. This results in the watermark never being updated, and append mode never emits results.
",,apachespark,codingcat,copris,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18497,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 06 04:35:49 UTC 2016,,,,,,,,,,"0|i376rr:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"05/Dec/16 23:15;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16160;;;","06/Dec/16 04:35;tdas;Issue resolved by pull request 16160
[https://github.com/apache/spark/pull/16160];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Datasets - crash (compile exception) when mapping to immutable scala map,SPARK-18717,13025631,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,a1ray,Daimon,Daimon,05/Dec/16 13:09,11/Feb/17 00:11,14/Jul/23 06:29,13/Dec/16 07:50,2.0.2,2.1.0,,,,,,,2.1.1,2.2.0,,,,,,,,,,,0,,,,,,"{code}
val spark: SparkSession = ???

case class Test(id: String, map_test: Map[Long, String])

spark.sql(""CREATE TABLE xyz.map_test (id string, map_test map<int, string>) STORED AS PARQUET"")

spark.sql(""SELECT * FROM xyz.map_test"").as[Test].map(t => t).collect()
{code}

{code}
org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 307, Column 108: No applicable constructor/method found for actual parameters ""java.lang.String, scala.collection.Map""; candidates are: ""$line14.$read$$iw$$iw$Test(java.lang.String, scala.collection.immutable.Map)""
{code}",,a1ray,apachespark,Daimon,dongjoon,kiszk,WeiqingYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-14767,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 06 07:37:46 UTC 2016,,,,,,,,,,"0|i37647:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/16 22:00;a1ray;Use `scala.collection.Map` as the type in your case class definition (instead of the default `scala.collection.immutable.Map`). Supported scala types are listed in the doc here: https://spark.apache.org/docs/latest/sql-programming-guide.html#data-types although this should probably still be fixed or at least have a better error/warning.;;;","05/Dec/16 22:08;dongjoon;+1;;;","05/Dec/16 22:11;a1ray;I have a fix for this, will make a PR in a bit;;;","05/Dec/16 23:16;apachespark;User 'aray' has created a pull request for this issue:
https://github.com/apache/spark/pull/16161;;;","06/Dec/16 07:37;Daimon;Yep it's already workarounded this way in my code but usage of standard scala types is simply more natural;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix wrong AIC calculation in Binomial GLM,SPARK-18715,13025515,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,actuaryzhang,actuaryzhang,actuaryzhang,05/Dec/16 06:26,19/Jan/18 20:36,14/Jul/23 06:29,13/Dec/16 21:27,2.0.2,,,,,,,,2.2.0,,,,ML,,,,,,,30/Dec/16 00:00,0,patch,,,,,"The AIC calculation in Binomial GLM seems to be wrong when there are weights. The result is different from that in R.

The current implementation is:
{code}
      -2.0 * predictions.map { case (y: Double, mu: Double, weight: Double) =>
        weight * dist.Binomial(1, mu).logProbabilityOf(math.round(y).toInt)
      }.sum()
{code} 

Suggest changing this to 
{code}
      -2.0 * predictions.map { case (y: Double, mu: Double, weight: Double) =>
        val wt = math.round(weight).toInt
        if (wt == 0){
          0.0
        } else {
          dist.Binomial(wt, mu).logProbabilityOf(math.round(y * weight).toInt)
        }
      }.sum()
{code} 

----
----
The following is an example to illustrate the problem.
{code}
val dataset = Seq(
      LabeledPoint(0.0, Vectors.dense(18, 1.0)),
      LabeledPoint(0.5, Vectors.dense(12, 0.0)),
      LabeledPoint(1.0, Vectors.dense(15, 0.0)),
      LabeledPoint(0.0, Vectors.dense(13, 2.0)),
      LabeledPoint(0.0, Vectors.dense(15, 1.0)),
      LabeledPoint(0.5, Vectors.dense(16, 1.0))
    ).toDF().withColumn(""weight"", col(""label"") + 1.0)
val glr = new GeneralizedLinearRegression()
    .setFamily(""binomial"")
    .setWeightCol(""weight"")
    .setRegParam(0)
val model = glr.fit(dataset)
model.summary.aic
{code}

This calculation shows the AIC is 14.189026847171382. To verify whether this is correct, I run the same analysis in R but got AIC = 11.66092, -2 * LogLik = 5.660918. 
{code}
da <- scan(, what=list(y = 0, x1 = 0, x2 = 0, w = 0), sep = "","")
0,18,1,1
0.5,12,0,1.5
1,15,0,2
0,13,2,1
0,15,1,1
0.5,16,1,1.5
da <- as.data.frame(da)
f <- glm(y ~ x1 + x2 , data = da, family = binomial(), weight = w)
AIC(f)
-2 * logLik(f)
{code}

Now, I check whether the proposed change is correct. The following calculates -2 * LogLik manually and get 5.6609177228379055, the same as that in R.
{code}
val predictions = model.transform(dataset)
-2.0 * predictions.select(""label"", ""prediction"", ""weight"").rdd.map {case Row(y: Double, mu: Double, weight: Double) =>
      val wt = math.round(weight).toInt
      if (wt == 0){
        0.0
      } else {
        dist.Binomial(wt, mu).logProbabilityOf(math.round(y * weight).toInt)
      }
  }.sum()
{code}



",,actuaryzhang,apachespark,,,,,,,,,,,,,,,,,,,,,432000,432000,,0%,432000,432000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Tue Dec 13 21:27:44 UTC 2016,,,,,,,,,,"0|i375ef:",9223372036854775807,,,,,,,,,,,,,2.0.2,,,,,,,,,,,"05/Dec/16 18:42;apachespark;User 'actuaryzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/16149;;;","13/Dec/16 21:27;srowen;Issue resolved by pull request 16149
[https://github.com/apache/spark/pull/16149];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in generated SpecificMutableProjection for Aggregator,SPARK-18711,13025489,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,koert,koert,05/Dec/16 03:54,06/Dec/16 02:39,14/Jul/23 06:29,05/Dec/16 19:38,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"this is a bug in the branch-2.1, but i don't think it was in 2.1.0-rc1
code (contrived, but based on real code we run):
{noformat}
  case class Holder(i: Int)

  val agg1 = new Aggregator[Int, Tuple1[Option[Holder]], Seq[(String, Int, Int)]] {
    def zero: Tuple1[Option[Holder]] = {
      val x = Tuple1(None)
      println(s""zero ${x}"")
      x
    }

    def reduce(b: Tuple1[Option[Holder]], a: Int): Tuple1[Option[Holder]] = {
      println(s""reduce ${b} ${a}"")
      Tuple1(Some(Holder(b._1.map(_.i + a).getOrElse(a))))
    }

    def merge(b1: Tuple1[Option[Holder]], b2: Tuple1[Option[Holder]]): Tuple1[Option[Holder]] = {
      println(s""merge ${b1} ${b2}"")
      (b1._1, b2._1) match {
        case (Some(Holder(i1)), Some(Holder(i2))) => Tuple1(Some(Holder(i1 + i2)))
        case (Some(Holder(i1)), _) => Tuple1(Some(Holder(i1)))
        case (_, Some(Holder(i2))) => Tuple1(Some(Holder(i2)))
        case _ => Tuple1(None)
      }
    }

    def finish(reduction: Tuple1[Option[Holder]]): Seq[(String, Int, Int)] = {
      println(s""finish ${reduction}"")
      Seq((""ha"", reduction._1.get.i, 0))
    }

    def bufferEncoder: Encoder[Tuple1[Option[Holder]]] = ExpressionEncoder[Tuple1[Option[Holder]]]()

    def outputEncoder: Encoder[Seq[(String, Int, Int)]] = ExpressionEncoder[Seq[(String, Int, Int)]]()
  }

  val x = Seq((""a"", 1), (""a"", 2))
    .toDS
    .groupByKey(_._1)
    .mapValues(_._2)
    .agg(agg1.toColumn)
  x.printSchema
  x.show
{noformat}

result is:
{noformat}
org.apache.spark.executor.Executor: Exception in task 1.0 in stage 146.0 (TID 423)
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$generateResultProjection$1.apply(AggregationIterator.scala:223)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$generateResultProjection$1.apply(AggregationIterator.scala:221)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:159)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:29)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:232)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

the error seems to be in the code generation for the aggregator result.",,apachespark,kiszk,koert,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 06 02:39:13 UTC 2016,,,,,,,,,,"0|i3758n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/16 04:15;koert;simplified:
{noformat}
  case class Holder(i: Int)

  val agg1 = new Aggregator[Int, Tuple1[Holder], Seq[(String, Int, Int)]] {
    def zero: Tuple1[Holder] = Tuple1(Holder(1))

    def reduce(b: Tuple1[Holder], a: Int): Tuple1[Holder] = Tuple1(Holder(1))

    def merge(b1: Tuple1[Holder], b2: Tuple1[Holder]): Tuple1[Holder] = Tuple1(Holder(1))

    def finish(reduction: Tuple1[Holder]): Seq[(String, Int, Int)] = Seq((""ha"", 0, 0))

    def bufferEncoder: Encoder[Tuple1[Holder]] = ExpressionEncoder[Tuple1[Holder]]()

    def outputEncoder: Encoder[Seq[(String, Int, Int)]] = ExpressionEncoder[Seq[(String, Int, Int)]]()
  }

  val x = Seq((""a"", 1), (""a"", 2))
    .toDS
    .groupByKey(_._1)
    .mapValues(_._2)
    .agg(agg1.toColumn)
  x.printSchema
  x.show
{noformat}

error is still the same:
{noformat}
org.apache.spark.executor.Executor: Exception in task 1.0 in stage 146.0 (TID 423)
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$generateResultProjection$1.apply(AggregationIterator.scala:223)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$generateResultProjection$1.apply(AggregationIterator.scala:221)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:159)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:29)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:232)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 146.0 failed 1 times; aborting job
{noformat}

{noformat};;;","05/Dec/16 09:51;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16143;;;","06/Dec/16 02:39;koert;confirmed it resolved the issue for me. thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Automatic null conversion bug (instead of throwing error) when creating a Spark Datarame with incompatible types for fields.,SPARK-18709,13025442,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,amogh.91,amogh.91,04/Dec/16 20:50,07/Dec/16 08:26,14/Jul/23 06:29,05/Dec/16 19:30,1.6.2,1.6.3,,,,,,,2.0.0,,,,SQL,,,,,,,,0,bug,,,,,"When converting an RDD with a `float` type field to a spark dataframe with an `IntegerType` / `LongType` schema field, spark 1.6.2 and 1.6.3 silently convert the field values to nulls instead of throwing an error like `LongType can not accept object ___ in type <type 'float'>`. However, this seems to be fixed in Spark 2.0.2.


The following example should make the problem clear:
{code}
from pyspark.sql.types import StructField, StructType, LongType, DoubleType

schema = StructType([
        StructField(""0"", LongType(), True),
        StructField(""1"", DoubleType(), True),
    ])

data = [[1.0, 1.0], [nan, 2.0]]
spark_df = sqlContext.createDataFrame(sc.parallelize(data), schema)
spark_df.show()
{code}

Instead of throwing an error like:
{code}
LongType can not accept object 1.0 in type <type 'float'>
{code}

Spark converts all the values in the first column to nulls

Running `spark_df.show()` gives:
{code}
+----+---+
|   0|  1|
+----+---+
|null|1.0|
|null|1.0|
+----+---+
{code}

For the purposes of my computation, I'm doing a `mapPartitions` on a spark data frame, and for each partition, converting it into a pandas data frame, doing a few computations on this pandas dataframe and the return value will be a list of lists, which is converted to an RDD while being returned from 'mapPartitions' (for all partitions). This RDD is then converted into a spark dataframe similar to the example above, using `sqlContext.createDataFrame(rdd, schema)`. The rdd has a column that should be converted to a `LongType` in the spark data frame, but since it has missing values, it is a `float` type. When spark tries to create the data frame, it converts all the values in that column to nulls instead of throwing an error that there is a type mismatch.",,amogh.91,dongjoon,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Wed Dec 07 05:31:46 UTC 2016,,,,,,,,,,"0|i374y7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/16 22:09;dongjoon;Hi, [~amogh.91].
I removed the target version since 1.6.3 is already released.
In addition, only committers decide the target version.;;;","04/Dec/16 22:11;dongjoon;Also, I updated the fix version to 2.0.0 after testing on 2.0.0.;;;","05/Dec/16 19:24;zsxwing;[~dongjoon] Is it already resolved? If so, could you close this ticket?;;;","05/Dec/16 19:31;dongjoon;Sure. [~zsxwing]
I think the issue reporter, [~amogh.91], will agree to close this.;;;","05/Dec/16 19:50;amogh.91;[~dongjoon] Thanks for the fix. Just to clarify, does this mean that the fix will only be in 2.0.0 and not in 1.6.4 (assuming there will be a 1.6.4 release)?;;;","05/Dec/16 19:57;dongjoon;Yes. It will not be in 1.6.4 (if exists).;;;","05/Dec/16 20:09;amogh.91;Thanks, I'll close the ticket.;;;","05/Dec/16 20:10;amogh.91;The fix is in 2.0.0.;;;","06/Dec/16 02:33;srowen;BTW do you know what change fixed this, by any chance?;;;","06/Dec/16 02:56;dongjoon;I'll check which commit added the guard condition.;;;","07/Dec/16 05:31;dongjoon;Hi, [~srowen]
The type verification was introduced by https://issues.apache.org/jira/browse/SPARK-14945  when `session.py` is created in 2.0.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Insertion/CTAS against Hive Tables: Staging Directories and Data Files Not Dropped Until Normal Termination of JVM,SPARK-18703,13025381,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,smilegator,smilegator,smilegator,04/Dec/16 06:19,26/Dec/16 07:01,14/Jul/23 06:29,15/Dec/16 01:25,2.0.2,,,,,,,,2.0.3,2.1.1,2.2.0,,SQL,,,,,,,,0,,,,,,"Below are the files/directories generated for three inserts againsts a Hive table:
{noformat}
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/.hive-staging_hive_2016-12-03_20-56-29_149_4298858301766472202-1
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/.hive-staging_hive_2016-12-03_20-56-29_149_4298858301766472202-1/-ext-10000
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/.hive-staging_hive_2016-12-03_20-56-29_149_4298858301766472202-1/-ext-10000/._SUCCESS.crc
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/.hive-staging_hive_2016-12-03_20-56-29_149_4298858301766472202-1/-ext-10000/.part-00000.crc
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/.hive-staging_hive_2016-12-03_20-56-29_149_4298858301766472202-1/-ext-10000/_SUCCESS
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/.hive-staging_hive_2016-12-03_20-56-29_149_4298858301766472202-1/-ext-10000/part-00000
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/.hive-staging_hive_2016-12-03_20-56-30_454_6445008511655931341-1
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/.hive-staging_hive_2016-12-03_20-56-30_454_6445008511655931341-1/-ext-10000
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/.hive-staging_hive_2016-12-03_20-56-30_454_6445008511655931341-1/-ext-10000/._SUCCESS.crc
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/.hive-staging_hive_2016-12-03_20-56-30_454_6445008511655931341-1/-ext-10000/.part-00000.crc
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/.hive-staging_hive_2016-12-03_20-56-30_454_6445008511655931341-1/-ext-10000/_SUCCESS
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/.hive-staging_hive_2016-12-03_20-56-30_454_6445008511655931341-1/-ext-10000/part-00000
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/.hive-staging_hive_2016-12-03_20-56-30_722_3388423608658711001-1
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/.hive-staging_hive_2016-12-03_20-56-30_722_3388423608658711001-1/-ext-10000
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/.hive-staging_hive_2016-12-03_20-56-30_722_3388423608658711001-1/-ext-10000/._SUCCESS.crc
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/.hive-staging_hive_2016-12-03_20-56-30_722_3388423608658711001-1/-ext-10000/.part-00000.crc
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/.hive-staging_hive_2016-12-03_20-56-30_722_3388423608658711001-1/-ext-10000/_SUCCESS
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/.hive-staging_hive_2016-12-03_20-56-30_722_3388423608658711001-1/-ext-10000/part-00000
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/.part-00000.crc
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/part-00000
{noformat}

The first 18 files are temporary. We do not drop it until the end of JVM termination. If JVM does not appropriately terminate, these temporary files/directories will not be dropped.

Only the last two files are needed, as shown below.
{noformat}
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/.part-00000.crc
/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-41eaa5ce-0288-471e-bba1-09cc482813ff/part-00000
{noformat}

Ideally, we should drop the created staging files and temporary data files after each insert/CTAS. The temporary files/directories could accumulate a lot when we issue many inserts, since each insert generats at least six files. This could eat a lot of spaces and slow down the JVM termination.",,apachespark,codingcat,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18931,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 26 01:18:07 UTC 2016,,,,,,,,,,"0|i374kn:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"04/Dec/16 06:31;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16134;;;","17/Dec/16 19:36;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16325;;;","26/Dec/16 01:18;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16399;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Poisson GLM fails due to wrong initialization,SPARK-18701,13025371,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,actuaryzhang,actuaryzhang,actuaryzhang,04/Dec/16 03:05,19/Jan/18 20:36,14/Jul/23 06:29,07/Dec/16 08:37,2.0.2,,,,,,,,2.1.0,,,,ML,,,,,,,16/Dec/16 00:00,0,,,,,,"Poisson GLM fails for many standard data sets. The issue is incorrect initialization leading to almost zero probability and weights. The following simple example reproduces the error. 

{code:borderStyle=solid}
val datasetPoissonLogWithZero = Seq(
      LabeledPoint(0.0, Vectors.dense(18, 1.0)),
      LabeledPoint(1.0, Vectors.dense(12, 0.0)),
      LabeledPoint(0.0, Vectors.dense(15, 0.0)),
      LabeledPoint(0.0, Vectors.dense(13, 2.0)),
      LabeledPoint(0.0, Vectors.dense(15, 1.0)),
      LabeledPoint(1.0, Vectors.dense(16, 1.0)),
      LabeledPoint(0.0, Vectors.dense(10, 0.0)),
      LabeledPoint(0.0, Vectors.dense(15, 0.0)),
      LabeledPoint(0.0, Vectors.dense(12, 2.0)),
      LabeledPoint(0.0, Vectors.dense(13, 0.0)),
      LabeledPoint(1.0, Vectors.dense(15, 0.0)),
      LabeledPoint(1.0, Vectors.dense(15, 0.0)),
      LabeledPoint(0.0, Vectors.dense(15, 0.0)),
      LabeledPoint(0.0, Vectors.dense(12, 2.0)),
      LabeledPoint(1.0, Vectors.dense(12, 2.0))
    ).toDF()
    
val glr = new GeneralizedLinearRegression()
  .setFamily(""poisson"")
  .setLink(""log"")
  .setMaxIter(20)
  .setRegParam(0)

val model = glr.fit(datasetPoissonLogWithZero)
{code}

The issue is in the initialization:  the mean is initialized as the response, which could be zero. Applying the log link results in very negative numbers (protected against -Inf), which again leads to close to zero probability and weights in the weighted least squares. The fix is easy: just add a small constant, highlighted in red below. 
 

    override def initialize(y: Double, weight: Double): Double = {
      require(y >= 0.0, ""The response variable of Poisson family "" +
        s""should be non-negative, but got $y"")
      y {color:red}+ 0.1 {color}
    }

I already have a fix and test code. Will create a PR. ",,actuaryzhang,apachespark,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Wed Dec 07 08:37:59 UTC 2016,,,,,,,,,,"0|i374if:",9223372036854775807,,,,,,,,,,,,,2.0.2,,,,,,,,,,,"04/Dec/16 03:19;apachespark;User 'actuaryzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/16131;;;","07/Dec/16 08:37;srowen;Issue resolved by pull request 16131
[https://github.com/apache/spark/pull/16131];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getCached in HiveMetastoreCatalog not thread safe cause driver OOM,SPARK-18700,13025350,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,XuanYuan,XuanYuan,03/Dec/16 19:01,21/Dec/16 21:56,14/Jul/23 06:29,19/Dec/16 19:40,1.6.1,2.0.0,2.1.1,,,,,,2.0.3,2.1.1,2.2.0,,SQL,,,,,,,,0,,,,,,"    In our spark sql platform, each query use same HiveContext and independent thread, new data will append to tables as new partitions every 30min. After a new partition added to table T, we should call refreshTable to clear T’s cache in cachedDataSourceTables to make the new partition searchable. 
    For the table have more partitions and files(much bigger than spark.sql.sources.parallelPartitionDiscovery.threshold), a new query of table T will start a job to fetch all FileStatus in listLeafFiles function. Because of the huge number of files, the job will run several seconds, during the time, new queries of table T will also start new jobs to fetch FileStatus because of the function of getCache is not thread safe. Final cause a driver OOM.",,apachespark,java8964,liushaohui,rxin,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 20 10:59:03 UTC 2016,,,,,,,,,,"0|i374dr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/16 19:11;XuanYuan;Give a PR for this, add StripedLock for each table's relation in cache, not for the whole cachedDataSourceTables.
;;;","04/Dec/16 06:53;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/16135;;;","20/Dec/16 10:59;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/16350;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark CSV parsing types other than String throws exception when malformed,SPARK-18699,13025335,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,jsnowacki,jsnowacki,03/Dec/16 16:33,12/Dec/22 18:10,14/Jul/23 06:29,23/Feb/17 20:09,2.0.2,,,,,,,,2.2.0,,,,SQL,,,,,,,,1,,,,,,"If CSV is read and the schema contains any other type than String, exception is thrown when the string value in CSV is malformed; e.g. if the timestamp does not match the defined one, an exception is thrown:
{code}
Caused by: java.lang.IllegalArgumentException
	at java.sql.Date.valueOf(Date.java:143)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.stringToTime(DateTimeUtils.scala:137)
	at org.apache.spark.sql.execution.datasources.csv.CSVTypeCast$$anonfun$castTo$6.apply$mcJ$sp(CSVInferSchema.scala:272)
	at org.apache.spark.sql.execution.datasources.csv.CSVTypeCast$$anonfun$castTo$6.apply(CSVInferSchema.scala:272)
	at org.apache.spark.sql.execution.datasources.csv.CSVTypeCast$$anonfun$castTo$6.apply(CSVInferSchema.scala:272)
	at scala.util.Try.getOrElse(Try.scala:79)
	at org.apache.spark.sql.execution.datasources.csv.CSVTypeCast$.castTo(CSVInferSchema.scala:269)
	at org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:116)
	at org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:85)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$1$$anonfun$apply$2.apply(CSVFileFormat.scala:128)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$1$$anonfun$apply$2.apply(CSVFileFormat.scala:127)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:91)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258)
	... 8 more
{code}

It behaves similarly with Integer and Long types, from what I've seen.

To my understanding modes PERMISSIVE and DROPMALFORMED should just null the value or drop the line, but instead they kill the job.",,apachespark,barrybecker4,cloud_fan,jsnowacki,kubatyszko,maropu,rkamaleswaran,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18906,SPARK-16512,SPARK-20387,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 02 17:48:03 UTC 2017,,,,,,,,,,"0|i374af:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/16 12:26;maropu;you mean a query below and you'd like to load the second line only?
{code}
>> test.csv <<
1 0,2014-xx-xx
2 1,2014-01-01  

scala> import org.apache.spark.sql.types._
scala> val schema = new StructType().add(""a"", IntegerType).add(""b"", DateType)
scala> spark.read.format(""csv"").schema(schema).load(""test.csv"").show

16/12/04 21:21:56 ERROR Executor: Exception in task 0.0 in stage 32.0 (TID 32)
java.lang.NumberFormatException: For input string: ""xx""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Integer.parseInt(Integer.java:580)
        at java.lang.Integer.parseInt(Integer.java:615)
        at java.sql.Date.valueOf(Date.java:134)
        at org.apache.spark.sql.catalyst.util.DateTimeUtils$.stringToTime(DateTimeUtils.scala:137)
        at org.apache.spark.sql.execution.datasources.csv.CSVTypeCast$$anonfun$castTo$9.apply$mcI$sp(CSVInferSchema.scala:290)
        at org.apache.spark.sql.execution.datasources.csv.CSVTypeCast$$anonfun$castTo$8.apply(CSVInferSchema.scala:290)
        at org.apache.spark.sql.execution.datasources.csv.CSVTypeCast$$anonfun$castTo$8.apply(CSVInferSchema.scala:290)
        at scala.util.Try.getOrElse(Try.scala:79)
        at org.apache.spark.sql.execution.datasources.csv.CSVTypeCast$.castTo(CSVInferSchema.scala:287)
        at org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:121)
        at org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:90)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$1$$anonfun$apply$2.apply(CSVFileFormat.scala:173)
        at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$1$$anonfun$apply$2.apply(CSVFileFormat.scala:172)
{code};;;","04/Dec/16 12:43;jsnowacki;Yes, my understanding was that it should put nullify the value if it fails to parse it in PERMISSIVE mode or drop the whole row (line) in DROPMALFORMED as described in the docs: http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader, i.e.: 
* mode (default PERMISSIVE): allows a mode for dealing with corrupt records during parsing.
** PERMISSIVE : sets other fields to null when it meets a corrupted record. When a schema is set by user, it sets null for extra fields.
** DROPMALFORMED : ignores the whole corrupted records.
** FAILFAST : throws an exception when it meets corrupted records.;;;","04/Dec/16 14:02;maropu;`DROPMALFORMED` works well in this query though, `PERMISSIVE` still throws the exception. But, `PERMISSIVE` mode just fills null in case that the number of fields spark parses is lower than a given schema length (IIUC, in the spark doc., this case is called `corrupted records`). Since the current implementation does not fill null for malformed  fields (the format errors above, or something), it seems this is an expected behaviour. cc: [~hyukjin.kwon];;;","04/Dec/16 14:30;maropu;Additionally, in our basic stance, it seems this csv format keeps almost the same behavior with `com.databricks.spark.csv`. I quickly checked that the databricks one throws an exception in this query;
{code}
scala> sqlContext.read.format(""com.databricks.spark.csv"").schema(schema).option(""mode"", ""PERMISSIVE"").load(""../test.csv"").show
16/12/04 23:17:43 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NumberFormatException: For input string: ""xx""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Integer.parseInt(Integer.java:580)
        at java.lang.Integer.parseInt(Integer.java:615)
        at java.sql.Date.valueOf(Date.java:134)
        at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:74)
        at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:121)
        at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:108)
       ...
{code};;;","04/Dec/16 14:55;maropu;Anyway, we can easily fix this like this: https://github.com/apache/spark/compare/master...maropu:SPARK-18699

{code}
scala> spark.read.format(""csv"").schema(schema).option(""mode"", ""PERMISSIVE"").load(""test.csv"").show
16/12/04 23:46:55 WARN CSVRelation: Fill NULL in a field because a malformed token detected: 2014-xx-xx
+---+----------+
|  a|         b|
+---+----------+
|  0|      null|
|  1|2014-01-01|
+---+----------+
{code};;;","04/Dec/16 15:34;gurwls223;Thank you for cc'ing me. Yup, I noticed this but I decided to just leave as is because I noticed that this is not a regression comparing to the external CSV library as you said below.

I persuaded myself and was thinking that the parse mode is related with parsing itself. JSON has some standard JSON types so IIRC JSON loads the data as null in {{PERMISSIVE}} mode in this case but parsing CSV is not related with types not within plain texts so it throws an exception.

However, if this sounds odds in practice, I agree with fixing this although I still am worried of changing the exiting behaviour.

Could I ask what you think [~falaki] please?;;;","04/Dec/16 22:24;jsnowacki;While I don't argue that some other packages have similar behaviour, I think the PERMISSIVE mode should be, well, as permissive as possible, since CSVs have very little standards and no types. In ma case I had just one odd value in almost 1 TB set and the job crushed at the very end after about an hour. To go around the issue one needs to manually parse each line, which is not the end of the world, but I wanted to use CSV reader exactly for the confidence of not writing extra code. IMO the mode for error detection should be FAILFAST. Moreover, if I really need to check the data, I read it differently anyway.
BTW thanks for looking into this.;;;","09/Dec/16 04:55;maropu;yea, I'm also working on large csv files now and, certainly, I think this current behavior makes  it difficult to find incorrect records in them. Logging with meaningful waning messages (e.g., including the incorrect records and line numbers) helps much to me.;;;","14/Dec/16 17:13;rkamaleswaran;This issue is also seen in cases where a timestamp field (among many) is empty (just "" ""). If I use permissive mode, then any instances where one of those timestamp fields are empty automatically results in the entire row getting dropped. In my case that's over 90% of the rows.;;;","15/Dec/16 01:37;gurwls223;BTW, maybe you could try to set {{nullValue}} to {{"" ""}} or set {{ignoreLeadingWhiteSpace}} and {{ignoreTrailingWhiteSpace}} to {{true}} for now.;;;","15/Dec/16 02:33;rkamaleswaran;Thanks for the reply! Unfortunately neither of those options work in my case.
;;;","17/Dec/16 03:05;kubatyszko;Fixed in PR https://github.com/apache/spark/pull/16319

(not merged into the tree as of now).

Enjoy;;;","17/Dec/16 03:07;apachespark;User 'kubatyszko' has created a pull request for this issue:
https://github.com/apache/spark/pull/16319;;;","13/Feb/17 02:25;maropu;This fix makes some sensible to me and what do you think? cc: [~hyukjin.kwon]
If yes, I try to make a pr based on this (https://github.com/apache/spark/compare/master...maropu:SPARK-18699). If no, I think it's okay to set ""Won't Fix"".;;;","13/Feb/17 10:18;gurwls223;Thanks for cc'ing me. For me, it is reasonable to me too (although I am not supposed to decide what to add into Spark). Maybe, it might be even nicer if we could have some references such as similar libraries e.g., read.csv in R if applicable and/or good explanations for potential use cases.;;;","14/Feb/17 15:29;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/16928;;;","23/Feb/17 20:09;cloud_fan;Issue resolved by pull request 16928
[https://github.com/apache/spark/pull/16928];;;","02/Mar/17 17:48;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/17142;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Backward compatibility - creating a Dataframe on a new SQLContext object fails with a Derby error,SPARK-18687,13025020,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vijoshi,vijoshi,vijoshi,02/Dec/16 09:01,13/Jan/17 17:21,14/Jul/23 06:29,13/Jan/17 17:21,2.0.0,2.0.1,2.0.2,,,,,,2.0.3,2.1.1,2.2.0,,PySpark,SQL,,,,,,,0,,,,,,"With a local spark instance built with hive support, (-Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver)

The following script/sequence works in Pyspark without any error in 1.6.x, but fails in 2.x.

{code}
people = sc.parallelize([""Michael,30"", ""Andy,12"", ""Justin,19""])
peoplePartsRDD = people.map(lambda p: p.split("",""))
peopleRDD = peoplePartsRDD.map(lambda p: pyspark.sql.Row(name=p[0], age=int(p[1])))
peopleDF= sqlContext.createDataFrame(peopleRDD)
peopleDF.first()

sqlContext2 = SQLContext(sc)
people2 = sc.parallelize([""Abcd,40"", ""Efgh,14"", ""Ijkl,16""])
peoplePartsRDD2 = people2.map(lambda l: l.split("",""))
peopleRDD2 = peoplePartsRDD2.map(lambda p: pyspark.sql.Row(fname=p[0], age=int(p[1])))
peopleDF2 = sqlContext2.createDataFrame(peopleRDD2) # <==== error here
{code}

The error produced is:

{noformat}
16/12/01 22:35:36 ERROR Schema: Failed initialising database.
Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@4494053, see the next exception for details.
        at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)
.
.
------

org.datanucleus.exceptions.NucleusDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@519dabfd, see the next exception for details.
        at org.apache.derby.impl.jdb
.
.
.
NestedThrowables:
java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@519dabfd, see the next exception for details.
        at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
.
.
.
Caused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@519dabfd, see the next exception for details.
        at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)
.
.
.
16/12/01 22:48:09 ERROR Schema: Failed initialising database.
Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@519dabfd, see the next exception for details.
        at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)
.
.
.
Caused by: java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@519dabfd, see the next exception for details.
        at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)
.
.
.

Caused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@519dabfd, see the next exception for details.
        at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
        at org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)
        ... 111 more
Caused by: ERROR XSDB6: Another instance of Derby may have already booted the database /Users/vinayak/devel/spark-stc/git_repo/spark-master-x/spark/metastore_db.
        at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
        at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
        at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)
        at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)
        at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)
        at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)

{noformat}
The error goes away if sqlContext2 is replaced with sqlContext in the last (error) line. Since the SQLContext class is preserved for backward compatibility, the changes in 2.x break scripts/notebooks that follow the above pattern of calls and used to run fine with 1.6.x.

",Spark built with hive support,apachespark,vijoshi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18956,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 02 09:59:06 UTC 2016,,,,,,,,,,"0|i372cf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/16 09:09;srowen;Spark 2 isn't necessarily compatible with Spark 1, and you shouldn't be making multiple SQLContexts right? why do the second one?;;;","02/Dec/16 09:44;vijoshi;[~srowen] Understand - however, it appears we have users who have code written that way and it used to work with 1.6 but breaks with 2.0. Since SQLContext has been preserved for backward compatibility, we're looking to see if this can problem can be plugged. 

A similar sequence of calls using Scala in spark-shell remains fine and looking into the code I figure it's because of the way the scala impl of SQLContext reuses the existing SparkSession internally. I am going to submit a PR on the same lines for the python impl of SQLContext that appears to fix the problem. However, I am not an expert in this particular part of the code so hopefully my change can be reviewed and considered for this issue.;;;","02/Dec/16 09:58;vijoshi;Created pull request https://github.com/apache/spark/pull/16119;;;","02/Dec/16 09:59;apachespark;User 'vijoshi' has created a pull request for this issue:
https://github.com/apache/spark/pull/16119;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw Filtering is supported only on partition keys of type string exception,SPARK-18681,13024952,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,02/Dec/16 01:12,03/Apr/20 07:48,14/Jul/23 06:29,12/Dec/16 22:40,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Cloudera put {{/var/run/cloudera-scm-agent/process/15000-hive-HIVEMETASTORE/hive-site.xml}} as the configuration file for the Hive Metastore Server, where {{hive.metastore.try.direct.sql=false}}. But Spark reading the gateway configuration file and get default value {{hive.metastore.try.direct.sql=true}}. we should use {{getMetaConf}} or {{getMSC.getConfigValue}} method to obtain the original configuration from Hive Metastore Server.

{noformat}
spark-sql> CREATE TABLE test (value INT) PARTITIONED BY (part INT);
Time taken: 0.221 seconds
spark-sql> select * from test where part=1 limit 10;
16/12/02 08:33:45 ERROR thriftserver.SparkSQLDriver: Failed in [select * from test where part=1 limit 10]
java.lang.RuntimeException: Caught Hive MetaException attempting to get partition metadata by filter from Hive. You can set the Spark configuration setting spark.sql.hive.manageFilesourcePartitions to false to work around this problem, however this will result in degraded performance. Please report a bug: https://issues.apache.org/jira/browse/SPARK
	at org.apache.spark.sql.hive.client.Shim_v0_13.getPartitionsByFilter(HiveShim.scala:610)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitionsByFilter$1.apply(HiveClientImpl.scala:549)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitionsByFilter$1.apply(HiveClientImpl.scala:547)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:282)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:229)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:228)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:271)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getPartitionsByFilter(HiveClientImpl.scala:547)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$listPartitionsByFilter$1.apply(HiveExternalCatalog.scala:954)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$listPartitionsByFilter$1.apply(HiveExternalCatalog.scala:938)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:91)
	at org.apache.spark.sql.hive.HiveExternalCatalog.listPartitionsByFilter(HiveExternalCatalog.scala:938)
	at org.apache.spark.sql.hive.MetastoreRelation.getHiveQlPartitions(MetastoreRelation.scala:156)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec$$anonfun$10.apply(HiveTableScanExec.scala:151)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec$$anonfun$10.apply(HiveTableScanExec.scala:150)
	at org.apache.spark.util.Utils$.withDummyCallSite(Utils.scala:2435)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec.doExecute(HiveTableScanExec.scala:149)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:225)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:308)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:295)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$hiveResultString$4.apply(QueryExecution.scala:134)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$hiveResultString$4.apply(QueryExecution.scala:133)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.hiveResultString(QueryExecution.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:335)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:728)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.sql.hive.client.Shim_v0_13.getPartitionsByFilter(HiveShim.scala:597)
	... 44 more
Caused by: MetaException(message:Filtering is supported only on partition keys of type string)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_by_filter_result$get_partitions_by_filter_resultStandardScheme.read(ThriftHiveMetastore.java)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_by_filter_result$get_partitions_by_filter_resultStandardScheme.read(ThriftHiveMetastore.java)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_by_filter_result.read(ThriftHiveMetastore.java)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_partitions_by_filter(ThriftHiveMetastore.java:2216)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_partitions_by_filter(ThriftHiveMetastore.java:2200)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitionsByFilter(HiveMetaStoreClient.java:1103)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy22.listPartitionsByFilter(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByFilter(Hive.java:2254)
	... 49 more
java.lang.RuntimeException: Caught Hive MetaException attempting to get partition metadata by filter from Hive. You can set the Spark configuration setting spark.sql.hive.manageFilesourcePartitions to false to work around this problem, however this will result in degraded performance. Please report a bug: https://issues.apache.org/jira/browse/SPARK
	at org.apache.spark.sql.hive.client.Shim_v0_13.getPartitionsByFilter(HiveShim.scala:610)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitionsByFilter$1.apply(HiveClientImpl.scala:549)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitionsByFilter$1.apply(HiveClientImpl.scala:547)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:282)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:229)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:228)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:271)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getPartitionsByFilter(HiveClientImpl.scala:547)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$listPartitionsByFilter$1.apply(HiveExternalCatalog.scala:954)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$listPartitionsByFilter$1.apply(HiveExternalCatalog.scala:938)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:91)
	at org.apache.spark.sql.hive.HiveExternalCatalog.listPartitionsByFilter(HiveExternalCatalog.scala:938)
	at org.apache.spark.sql.hive.MetastoreRelation.getHiveQlPartitions(MetastoreRelation.scala:156)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec$$anonfun$10.apply(HiveTableScanExec.scala:151)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec$$anonfun$10.apply(HiveTableScanExec.scala:150)
	at org.apache.spark.util.Utils$.withDummyCallSite(Utils.scala:2435)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec.doExecute(HiveTableScanExec.scala:149)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:225)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:308)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:295)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$hiveResultString$4.apply(QueryExecution.scala:134)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$hiveResultString$4.apply(QueryExecution.scala:133)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.hiveResultString(QueryExecution.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:335)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:728)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.sql.hive.client.Shim_v0_13.getPartitionsByFilter(HiveShim.scala:597)
	... 44 more
Caused by: MetaException(message:Filtering is supported only on partition keys of type string)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_by_filter_result$get_partitions_by_filter_resultStandardScheme.read(ThriftHiveMetastore.java)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_by_filter_result$get_partitions_by_filter_resultStandardScheme.read(ThriftHiveMetastore.java)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_by_filter_result.read(ThriftHiveMetastore.java)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_partitions_by_filter(ThriftHiveMetastore.java:2216)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_partitions_by_filter(ThriftHiveMetastore.java:2200)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitionsByFilter(HiveMetaStoreClient.java:1103)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy22.listPartitionsByFilter(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByFilter(Hive.java:2254)
	... 49 more
{noformat}",,apachespark,harishk15,michael,rxin,sunnys,viirya,yumwang,小郭飞飞刀,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18680,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 07:48:57 UTC 2020,,,,,,,,,,"0|i371xb:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"02/Dec/16 01:13;yumwang;I will pull request for this issue later.;;;","02/Dec/16 03:15;viirya;Looks like you create two Jiras (SPARK-18680, SPARK-18681) for the same issue. Mind close one of them?;;;","02/Dec/16 18:18;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/16122;;;","06/Dec/16 19:32;michael;[~rxin] I think this should be a blocker for 2.1. This introduces a major regression for some common table partition schemes for a default configuration for many deployments of CDH that will fail with the (new) default Spark configuration. I think we have a fix in the PR, but the unit tests are inadequate. I'm going to work on that.;;;","03/Apr/20 07:48;小郭飞飞刀;[~michael] any news for this issue ? i meet the same issue on spark2.4.5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skewed reservoir sampling in SamplingUtils,SPARK-18678,13024893,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,BToldbod,BToldbod,01/Dec/16 20:48,07/Dec/16 09:35,14/Jul/23 06:29,07/Dec/16 09:35,2.0.2,,,,,,,,2.1.0,,,,ML,,,,,,,,0,,,,,,"The feature subsampling performed in the RandomForest-implementation from 
org.apache.spark.ml.tree.impl.RandomForest
is performed using SamplingUtils.reservoirSampleAndCount

The implementation of the sampling skews feature selection in favor of features with a higher index. 
The skewness is smaller for a large number of features, but completely dominates the feature selection for a small number of features. The extreme case is when the number of features is 2 and number of features to select is 1.

In this case the feature sampling will always pick feature 1 and ignore feature 0.
Of course this produces low quality models for few features when using subsampling.",,apachespark,BToldbod,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 07 09:35:40 UTC 2016,,,,,,,,,,"0|i371k7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/16 12:10;srowen;It's helpful if you point out specifically what you mean in the code, but I think it's this:

{code}
      var l = i.toLong
      val rand = new XORShiftRandom(seed)
      while (input.hasNext) {
        val item = input.next()
        val replacementIndex = (rand.nextDouble() * l).toLong
        if (replacementIndex < k) {
          reservoir(replacementIndex.toInt) = item
        }
        l += 1
      }
{code}

l needs to be incremented before the random choice, right?;;;","02/Dec/16 13:45;BToldbod;I'm not familiar with this subsampling technique, but your suggestion seems to solve the problem.;;;","03/Dec/16 09:43;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16129;;;","07/Dec/16 09:35;srowen;Resolved by https://github.com/apache/spark/pull/16129;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Json path implementation fails to parse ['key'],SPARK-18677,13024884,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rdblue,rdblue,rdblue,01/Dec/16 20:24,02/Dec/16 16:43,14/Jul/23 06:29,02/Dec/16 16:43,2.1.0,,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,,,,,,"The current json path parser fails to parse expressions like ['key'], which are used for named expressions with spaces.",,apachespark,rdblue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 01 20:30:07 UTC 2016,,,,,,,,,,"0|i371i7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/16 20:30;apachespark;User 'rdblue' has created a pull request for this issue:
https://github.com/apache/spark/pull/16107;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CTAS for hive serde table should work for all hive versions,SPARK-18675,13024833,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,01/Dec/16 17:20,26/Dec/16 07:02,14/Jul/23 06:29,13/Dec/16 17:47,,,,,,,,,2.0.3,2.1.1,2.2.0,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 26 01:18:10 UTC 2016,,,,,,,,,,"0|i3716v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/16 17:36;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16104;;;","13/Dec/16 17:47;yhuai;Issue resolved by pull request 16104
[https://github.com/apache/spark/pull/16104];;;","17/Dec/16 19:36;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16325;;;","26/Dec/16 01:18;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16399;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add tests to ensure stability of that all Structured Streaming log formats,SPARK-18671,13024711,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,01/Dec/16 08:37,07/Dec/16 01:46,14/Jul/23 06:29,06/Dec/16 21:06,,,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,"To be able to restart StreamingQueries across Spark version, we have already made the logs (offset log, file source log, file sink log) use json. We should added tests with actual json files in the Spark such that any incompatible changes in reading the logs is immediately caught. ",,apachespark,lwlin,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 07 01:46:05 UTC 2016,,,,,,,,,,"0|i370fr:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"03/Dec/16 02:56;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/16128;;;","07/Dec/16 01:46;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/16183;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit the number of StreamingQueryListener.StreamProgressEvent when there is no data,SPARK-18670,13024710,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,zsxwing,tdas,tdas,01/Dec/16 08:33,02/Dec/16 20:53,14/Jul/23 06:29,02/Dec/16 20:53,,,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,"When a StreamingQuery is not receiving any data, and no processing trigger is set, it attempts to run a trigger every 10 ms. This would generate StreamingQueryListener events at a very high rate. This should be limited such that as long as data is not being received, it should generate events once every X seconds.",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 02 20:53:46 UTC 2016,,,,,,,,,,"0|i370fj:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"01/Dec/16 21:32;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16108;;;","02/Dec/16 20:53;tdas;Issue resolved by pull request 16108
[https://github.com/apache/spark/pull/16108];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not auto-generate query name,SPARK-18668,13024708,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,tdas,tdas,01/Dec/16 08:30,06/Dec/16 02:22,14/Jul/23 06:29,06/Dec/16 02:22,,,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,"With SPARK-18657 we will make the StreamingQuery.id the persistently and truly unique, it does not make sense to use an auto-generated name. Rather name should be meant only as a purely optional pretty identifier set by the user, or remain as null. ",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 06 02:22:42 UTC 2016,,,,,,,,,,"0|i370f3:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"02/Dec/16 03:58;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/16113;;;","06/Dec/16 02:22;tdas;Issue resolved by pull request 16113
[https://github.com/apache/spark/pull/16113];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
input_file_name function does not work with UDF,SPARK-18667,13024686,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,gurwls223,,01/Dec/16 07:00,12/Dec/22 17:51,14/Jul/23 06:29,08/Dec/16 15:49,,,,,,,,,2.1.0,,,,PySpark,,,,,,,,0,,,,,,"{{input_file_name()}} does not return the file name but empty string instead when it is used as input for UDF in PySpark as below: 

with the data as below:

{code}
{""a"": 1}
{code}

with the codes below:

{code}
from pyspark.sql.functions import *
from pyspark.sql.types import *

def filename(path):
    return path

sourceFile = udf(filename, StringType())
spark.read.json(""tmp.json"").select(sourceFile(input_file_name())).show()
{code}

prints as below:

{code}
+---------------------------+
|filename(input_file_name())|
+---------------------------+
|                           |
+---------------------------+
{code}

but the codes below:

{code}
spark.read.json(""tmp.json"").select(input_file_name()).show()
{code}

prints correctly as below:

{code}
+--------------------+
|   input_file_name()|
+--------------------+
|file:///Users/hyu...|
+--------------------+
{code}

This seems PySpark specific issue.",,apachespark,someonehere15,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 08 15:28:47 UTC 2017,,,,,,,,,,"0|i370a7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/16 06:08;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/16115;;;","12/Jan/17 14:09;someonehere15;I still have the same problem on pySpark 2.1.0 and Python 3.5.2 with the exact same steps as described in the issue.

Additionally I also have the following problem:
I have two dataframes, one where I read files and consequently has input_file_name() containing the source path for each row, and another with other stuff and a column FILEPATH already containing the same file paths as a static value. Basically I want to join these tables on the filepath columns. So I do the following:

Add a proper filepath column to the first dataframe, based on input_file_name():
{noformat}
dataFrame = dataFrame.withColumn('FILEPATH',input_file_name())
{noformat}
or the same happens with
{noformat}
dataFrame = dataFrame.select('*', input_file_name().alias('FILEPATH'))
{noformat}

and then I join the two tables on the FILEPATH column:
{noformat}
dataFrame = dataFrame.join(df2, 'FILEPATH')
{noformat}

so until here everything works, and I don't get an empty FILEPATH column.

{noformat}
+--------------+-------+-------+
|      FILEPATH|ColumnA|ColumnB|
+--------------+-------+-------+
|file:/C:/a.xml| StuffA| StuffX|
|file:/C:/b.xml| StuffB| StuffY|
+--------------+-------+-------+
{noformat}

But then, on the resulting dataframe, if I want to apply an UDF on any column, I get a completely empty result:
e.g. I have the following UDF:
{noformat}
def f(text):
  return text
{noformat}

and I do:
{noformat}
dataFrame.selectExpr('FILEPATH AS PATH','f(ColumnA) AS A','ColumnB AS B')
{noformat}

I get:
{noformat}
+--------------+-------+-------+
|          PATH|      A|      B|
+--------------+-------+-------+
+--------------+-------+-------+
{noformat}

Am I missing something or is a bug still present?

UPDATE:
I also just tried with:
{noformat}
dataFrame.select('FILEPATH',sup('FILEPATH'))
{noformat}
and both columns come up empty
{noformat}
+--------+-----------+
|FILEPATH|f(FILEPATH)|
+--------+-----------+
|        |           |
|        |           |
|        |           |
|        |           |
|        |           |
+--------+-----------+
{noformat}
although here apparently the dataframe is not empty, just these columns, since I do get empty rows, whereas above there are no rows at all and the dataframe was empty.;;;","12/Jan/17 14:39;viirya;Hi Ben,

I've just tried the example codes in current master as described above like:
{code}
from pyspark.sql.functions import *
from pyspark.sql.types import *

def filename(path):
    return path

sourceFile = udf(filename, StringType())
spark.read.json(""tmp.json"").select(sourceFile(input_file_name())).show()
{code}

It works. Can you make sure the version you are using is 2.1 or above?

;;;","12/Jan/17 15:00;someonehere15;OK, I just tried it with a json and it worked.

To be honest I was trying it with an XML file, and I didn't think that would matter, but apparently yes.
So, the exact same code, but with an XML file through the spark-xml library is not working.

Can you confirm this?

Would this be a spark-xml issue then, or still a pyspark issue?;;;","13/Jan/17 09:43;someonehere15;So, I created a new example now, and here is the code for everything:

a.xml:
{noformat}
<root>
  <x>TEXT</x>
  <y>TEXT2</y>
</root>
{noformat}

b.xml:
{noformat}
<root>
  <file>file:/C:/Users/SSS/a.xml</file>
  <other>AAA</other>
</root>
{noformat}

code:
{noformat}
from pyspark.sql.functions import udf,input_file_name
from pyspark.sql.types import StringType
from pyspark.sql import SparkSession

def filename(path):
    return path

session = SparkSession.builder.appName('APP').getOrCreate()

session.udf.register('sameText',filename)
sameText = udf(filename, StringType())

df = session.read.format('xml').load('../../res/Other/a.xml', rowTag='root').select('*',input_file_name().alias('file'))
df.select('file').show()
df.select(sameText(df['file'])).show()

df2 = session.read.format('xml').load('../../res/Other/b.xml', rowTag='root')
df3 = df.join(df2, 'file')

df.show()
df2.show()
df3.show()
df3.selectExpr('file as FILE','x AS COL1','sameText(y) AS COL2').show()
{noformat}

and this is the console output:
{noformat}
+--------------------+
|                file|
+--------------------+
|file:/C:/Users/SS...|
+--------------------+

+--------------+
|filename(file)|
+--------------+
|              |
+--------------+

+----+-----+--------------------+
|   x|    y|                file|
+----+-----+--------------------+
|TEXT|TEXT2|file:/C:/Users/SS...|
+----+-----+--------------------+

+--------------------+-----+
|                file|other|
+--------------------+-----+
|file:/C:/Users/SS...|  AAA|
+--------------------+-----+

+--------------------+----+-----+-----+
|                file|   x|    y|other|
+--------------------+----+-----+-----+
|file:/C:/Users/SS...|TEXT|TEXT2|  AAA|
+--------------------+----+-----+-----+


[Stage 26:>                                                         (0 + 4) / 4]
                                                                                

[Stage 29:>                                                        (0 + 8) / 20]
[Stage 29:=================>                                       (6 + 8) / 20]
[Stage 29:===================>                                     (7 + 8) / 20]
[Stage 29:======================>                                  (8 + 8) / 20]
[Stage 29:============================>                           (10 + 8) / 20]
[Stage 29:====================================>                   (13 + 7) / 20]
[Stage 29:=======================================>                (14 + 6) / 20]
[Stage 29:==========================================>             (15 + 5) / 20]
                                                                                

[Stage 32:>                                                       (0 + 8) / 100]
[Stage 32:===>                                                    (7 + 8) / 100]
[Stage 32:====>                                                   (8 + 8) / 100]
[Stage 32:=======>                                               (13 + 8) / 100]
[Stage 32:========>                                              (15 + 8) / 100]
[Stage 32:===========>                                           (20 + 8) / 100]
[Stage 32:============>                                          (22 + 8) / 100]
[Stage 32:==============>                                        (27 + 8) / 100]
[Stage 32:===============>                                       (29 + 8) / 100]
[Stage 32:==================>                                    (34 + 8) / 100]
[Stage 32:===================>                                   (36 + 8) / 100]
[Stage 32:======================>                                (41 + 8) / 100]
[Stage 32:=======================>                               (42 + 8) / 100]
[Stage 32:=========================>                             (46 + 8) / 100]
[Stage 32:==========================>                            (48 + 8) / 100]
[Stage 32:==========================>                            (49 + 8) / 100]
[Stage 32:===========================>                           (50 + 8) / 100]
[Stage 32:=============================>                         (53 + 8) / 100]
[Stage 32:==============================>                        (55 + 8) / 100]
[Stage 32:==============================>                        (56 + 8) / 100]
[Stage 32:===============================>                       (57 + 8) / 100]
[Stage 32:=================================>                     (60 + 8) / 100]
[Stage 32:==================================>                    (62 + 8) / 100]
[Stage 32:==================================>                    (63 + 8) / 100]
[Stage 32:===================================>                   (65 + 8) / 100]
[Stage 32:====================================>                  (67 + 8) / 100]
[Stage 32:=====================================>                 (69 + 8) / 100]
[Stage 32:======================================>                (70 + 8) / 100]
[Stage 32:=======================================>               (72 + 8) / 100]
[Stage 32:========================================>              (74 + 8) / 100]
[Stage 32:=========================================>             (76 + 8) / 100]
[Stage 32:==========================================>            (77 + 8) / 100]
[Stage 32:===========================================>           (79 + 8) / 100]
[Stage 32:============================================>          (81 + 8) / 100]
[Stage 32:=============================================>         (83 + 8) / 100]
[Stage 32:==============================================>        (84 + 8) / 100]
[Stage 32:===============================================>       (86 + 8) / 100]
[Stage 32:================================================>      (88 + 8) / 100]
[Stage 32:=================================================>     (90 + 8) / 100]
[Stage 32:==================================================>    (91 + 8) / 100]
[Stage 32:===================================================>   (93 + 7) / 100]
[Stage 32:====================================================>  (95 + 5) / 100]
[Stage 32:=====================================================> (97 + 3) / 100]
                                                                                

[Stage 35:>                                                        (0 + 8) / 75]
[Stage 35:==>                                                     (4 + 11) / 75]
[Stage 35:=====>                                                   (7 + 8) / 75]
[Stage 35:======>                                                  (8 + 8) / 75]
[Stage 35:=========>                                              (13 + 8) / 75]
[Stage 35:==========>                                             (14 + 8) / 75]
[Stage 35:===========>                                            (15 + 8) / 75]
[Stage 35:===========>                                            (16 + 8) / 75]
[Stage 35:==============>                                         (20 + 8) / 75]
[Stage 35:===============>                                        (21 + 8) / 75]
[Stage 35:================>                                       (22 + 8) / 75]
[Stage 35:=================>                                      (23 + 8) / 75]
[Stage 35:====================>                                   (27 + 8) / 75]
[Stage 35:====================>                                   (28 + 8) / 75]
[Stage 35:=====================>                                  (29 + 8) / 75]
[Stage 35:======================>                                 (30 + 8) / 75]
[Stage 35:=========================>                              (34 + 8) / 75]
[Stage 35:==========================>                             (35 + 8) / 75]
[Stage 35:==========================>                             (36 + 8) / 75]
[Stage 35:===========================>                            (37 + 8) / 75]
[Stage 35:==============================>                         (41 + 8) / 75]
[Stage 35:===============================>                        (42 + 8) / 75]
[Stage 35:================================>                       (43 + 8) / 75]
[Stage 35:================================>                       (44 + 8) / 75]
[Stage 35:===================================>                    (48 + 8) / 75]
[Stage 35:====================================>                   (49 + 8) / 75]
[Stage 35:=====================================>                  (50 + 8) / 75]
[Stage 35:======================================>                 (51 + 8) / 75]
[Stage 35:=========================================>              (55 + 8) / 75]
[Stage 35:=========================================>              (56 + 8) / 75]
[Stage 35:==========================================>             (57 + 8) / 75]
[Stage 35:===========================================>            (58 + 8) / 75]
[Stage 35:==============================================>         (62 + 8) / 75]
[Stage 35:===============================================>        (63 + 8) / 75]
[Stage 35:================================================>       (65 + 8) / 75]
[Stage 35:===================================================>    (69 + 6) / 75]
[Stage 35:=====================================================>  (72 + 3) / 75]
+--------------------+----+-----+
|                FILE|COL1| COL2|
+--------------------+----+-----+
|file:/C:/Users/SS...|TEXT|TEXT2|
+--------------------+----+-----+

SUCCESS: The process with PID 11916 (child process of PID 3592) has been terminated.
SUCCESS: The process with PID 3592 (child process of PID 9904) has been terminated.
SUCCESS: The process with PID 9904 (child process of PID 5468) has been terminated.
{noformat}

As you can see, After I add the ""file"" column, and show it, it's working. But then if I apply an UDF on it, it returns empty.
Then I proceed with the join, and show each dataframe again. Everything is OK until the last row, where it takes a very long time considering the amount of data and the speed of the previous processes. And this worries me becauses I don't think it is supposed to take such a long time.
Additionally, although in this example it works in the end, in my actual code it is not working. So as I already wrote on the previous post, if I join two dataframes, everything is OK, until I do a select where I apply a UDF, and then the whole query returns empty. The UDF works well if I don't join.
The other thing is, that if I do a count on the dataframe after the select just mentioned, it returns the correct number of rows, and not 0, but if I try to show or write the rows, the dataframe comes up empty. I'm not sure why this is happening so I would appreciate any help, and to at least know whether it's a bug or not.;;;","13/Jan/17 14:22;viirya;Hi [~someonehere15],

Thanks for providing the info. I can reproduce the issue on spark-xml package when applying UDF on the column of input_file_name, the result will be empty.

I will take a look into this issue.

;;;","13/Jan/17 15:58;someonehere15;Happy to help.
Were you also able to reproduce the second issue, regarding the join and UDF?
As I said, in my script, the count shows a correct number, but returns an empty table. Whereas in the example I provided seems to work, but very slowly, which is also suspicious for 1 single small row.
Would this also be caused by the same bug causing the UDF on input_file_name issue or is it unrelated?;;;","14/Jan/17 02:00;viirya;[~someonehere15],

Yeah, I can reproduce that the last line {{df3.selectExpr('file as FILE','x AS COL1','sameText( y ) AS COL2').show()}} costs more time to run. At the first look, I think they are different problems. I will take a look too. Thank you.;;;","08/Mar/17 15:14;someonehere15;Any news on this? 
The issue is still there, at least wil spark-xml, so I think this issue should be reopened.;;;","08/Mar/17 15:28;viirya;I already created another JIRA SPARK-19223 for the issue of spark-xml.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Persist UUID across query restart,SPARK-18657,13024586,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,marmbrus,marmbrus,30/Nov/16 21:20,06/Dec/16 02:34,14/Jul/23 06:29,06/Dec/16 02:22,,,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,We probably also want to add an instance Id or something that changes when the query restarts,,apachespark,marmbrus,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 06 02:22:38 UTC 2016,,,,,,,,,,"0|i36znz:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"02/Dec/16 03:58;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/16113;;;","06/Dec/16 02:22;tdas;Issue resolved by pull request 16113
[https://github.com/apache/spark/pull/16113];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ignore Structured Streaming 2.0.2 logs in history server,SPARK-18655,13024541,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,zsxwing,zsxwing,zsxwing,30/Nov/16 19:05,01/Dec/16 00:18,14/Jul/23 06:29,01/Dec/16 00:18,2.1.0,,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,SPARK-18516 changes the event log format of Structured Streaming. We should make sure our changes not break the history server.,,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 30 19:11:07 UTC 2016,,,,,,,,,,"0|i36zdz:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"30/Nov/16 19:11;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16085;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"KeyValueGroupedDataset[K, V].reduceGroups cannot handle primitive for V",SPARK-18651,13024506,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,koert,koert,30/Nov/16 17:01,23/May/17 09:56,14/Jul/23 06:29,23/May/17 09:56,2.0.2,,,,,,,,2.1.1,2.2.0,,,SQL,,,,,,,,0,,,,,,"run:
{noformat}
val df = Seq(1, 2, 3)
  .toDS
  .groupByKey(x => x)
  .reduceGroups(_ + _)
df.show
{noformat}

result:
{noformat}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 143.0 failed 1 times, most recent failure: Lost task 2.0 in stage 143.0 (TID 514, localhost): java.lang.NullPointerException
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
at org.apache.spark.sql.execution.aggregate.HashAggregateExec.createHashMap(HashAggregateExec.scala:296)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
at org.apache.spark.scheduler.Task.run(Task.scala:86)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
{noformat}

the issue is the null in ReduceAggregator.zero
for a primitive type this null leads to the NPE. instead for primitive types we should do a dummy/default value (0 for int, false for boolean, etc.)",,koert,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 30 18:29:17 UTC 2016,,,,,,,,,,"0|i36z67:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Nov/16 18:28;koert;i cannot reproduce the error in master. it seems to work there.;;;","30/Nov/16 18:29;koert;Fixed in master, fixed in branch-2.1, still issue in branch-2.0

i think this can be closed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
do not put provider in table properties for Hive serde table,SPARK-18647,13024400,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,30/Nov/16 11:28,02/Dec/16 04:55,14/Jul/23 06:29,02/Dec/16 04:55,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 02 04:55:28 UTC 2016,,,,,,,,,,"0|i36yin:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"30/Nov/16 11:36;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16080;;;","02/Dec/16 04:55;cloud_fan;Issue resolved by pull request 16080
[https://github.com/apache/spark/pull/16080];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutorClassLoader for spark-shell does not honor spark.executor.userClassPathFirst,SPARK-18646,13024368,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mshen,mshen,mshen,30/Nov/16 09:30,06/Jul/18 22:28,14/Jul/23 06:29,13/Jul/17 00:35,1.6.2,,,,,,,,2.3.0,,,,Spark Shell,,,,,,,,0,,,,,,"When submitting a spark-shell application, the executor side classloader is set to be {{ExecutorClassLoader}}.
However, it appears that when {{ExecutorClassLoader}} is used, parameter {{spark.executor.userClassPathFirst}} is not honored.
It turns out that, since {{ExecutorClassLoader}} class is defined as
{noformat}
class ExecutorClassLoader(conf: SparkConf, classUri: String, parent: ClassLoader,
    userClassPathFirst: Boolean) extends ClassLoader with Logging
{noformat}
its parent classloader is actually the system default classloader (due to {{ClassLoader}} class's default constructor) rather than the ""parent"" classloader specified in {{ExecutorClassLoader}}'s constructor.
As a result, when {{spark.executor.userClassPathFirst}} is set to true, even though the ""parent"" classloader is {{ChildFirstURLClassLoader}}, {{ExecutorClassLoader.getParent()}} will return the system default classloader.
Thus, when {{ExecutorClassLoader}} tries to load a class, it will first attempt to load it through the system default classloader, and this will break the {{spark.executor.userClassPathFirst}} behavior.

A simple fix would be to define {{ExecutorClassLoader}} as:
{noformat}
class ExecutorClassLoader(conf: SparkConf, classUri: String, parent: ClassLoader,
    userClassPathFirst: Boolean) extends ClassLoader(parent) with Logging
{noformat}",,apachespark,erwaman,mshen,taroplus,zhz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19675,,,SPARK-24018,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 12 15:47:03 UTC 2017,,,,,,,,,,"0|i36ybb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/17 16:42;apachespark;User 'taroplus' has created a pull request for this issue:
https://github.com/apache/spark/pull/17074;;;","12/Jul/17 15:47;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/18614;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-daemon.sh arguments error lead to throws Unrecognized option,SPARK-18645,13024361,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,30/Nov/16 09:03,08/Dec/16 07:43,14/Jul/23 06:29,01/Dec/16 13:27,2.1.0,,,,,,,,2.1.0,,,,Deploy,,,,,,,,0,,,,,,"{{start-thriftserver.sh}} can reproduce this:
{noformat}
[root@dev spark]# ./sbin/start-thriftserver.sh --conf 'spark.driver.extraJavaOptions=-XX:+UseG1GC -XX:-HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp' 
starting org.apache.spark.sql.hive.thriftserver.HiveThriftServer2, logging to /tmp/spark-root-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-dev.out
failed to launch nice -n 0 bash /opt/cloudera/parcels/SPARK-2.1.0-cdh5.4.3.d20161129-21.04.38/lib/spark/bin/spark-submit --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 --name Thrift JDBC/ODBC Server --conf spark.driver.extraJavaOptions=-XX:+UseG1GC -XX:-HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp:
  Error starting HiveServer2 with given arguments: 
  Unrecognized option: -XX:-HeapDumpOnOutOfMemoryError
full log in /tmp/spark-root-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-dev.out
{noformat}

",,apachespark,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 01 13:27:17 UTC 2016,,,,,,,,,,"0|i36y9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Nov/16 09:04;yumwang; I will pull request for this issue later.;;;","30/Nov/16 10:51;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/16079;;;","01/Dec/16 13:27;srowen;Resolved by https://github.com/apache/spark/pull/16079;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR hangs at session start when installed as a package without SPARK_HOME set,SPARK-18643,13024325,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,felixcheung,felixcheung,felixcheung,30/Nov/16 06:08,05/Dec/16 04:25,14/Jul/23 06:29,05/Dec/16 04:25,2.1.0,,,,,,,,2.1.0,,,,SparkR,,,,,,,,0,,,,,,"1) Install SparkR from source package, ie.
R CMD INSTALL SparkR_2.1.0.tar.gz

2) Start SparkR (not from sparkR shell)
library(SparkR)
sparkR.session()

Notice SparkR hangs when it couldn't find spark-submit to launch the JVM backend.
{code}
Launching java with spark-submit command spark-submit   sparkr-shell /tmp/RtmpYbAYt5/backend_port5849dc2273
sh: 1: spark-submit: not found
{code}

If SparkR is running as a package and it has previously downloaded Spark Jar it should be able to run as before without having to set SPARK_HOME. Basically with this bug the auto install Spark will only work in the first session.

This seems to be a regression on the earlier behavior.
",,apachespark,felixcheung,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 05 04:25:41 UTC 2016,,,,,,,,,,"0|i36y1z:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"30/Nov/16 06:09;felixcheung;Related PR: https://github.com/apache/spark/pull/15888
;;;","30/Nov/16 06:22;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/16077;;;","30/Nov/16 06:23;felixcheung;A workaround is to start as sparkR.session(master=""local"") - but it might not be always correct (not if the user is going to run Spark in non-local mode);;;","05/Dec/16 04:25;shivaram;Issue resolved by pull request 16077
[https://github.com/apache/spark/pull/16077];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL: Catalyst is scanning undesired columns,SPARK-18642,13024315,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,mohitgargk,mohitgargk,30/Nov/16 05:26,13/Dec/16 11:22,14/Jul/23 06:29,05/Dec/16 06:37,1.6.2,1.6.3,,,,,,,2.0.0,,,,SQL,,,,,,,,1,performance,,,,,"When doing a left-join between two tables, say A and B,  Catalyst has information about the projection required for table B. Only the required columns should be scanned.

Code snippet below explains the scenario:

scala> val dfA = sqlContext.read.parquet(""/home/mohit/ruleA"")
dfA: org.apache.spark.sql.DataFrame = [aid: int, aVal: string]

scala> val dfB = sqlContext.read.parquet(""/home/mohit/ruleB"")
dfB: org.apache.spark.sql.DataFrame = [bid: int, bVal: string]

scala> dfA.registerTempTable(""A"")
scala> dfB.registerTempTable(""B"")

scala> sqlContext.sql(""select A.aid, B.bid from A left join B on A.aid=B.bid where B.bid<2"").explain

== Physical Plan ==
Project [aid#15,bid#17]
+- Filter (bid#17 < 2)
   +- BroadcastHashOuterJoin [aid#15], [bid#17], LeftOuter, None
      :- Scan ParquetRelation[aid#15,aVal#16] InputPaths: file:/home/mohit/ruleA
      +- Scan ParquetRelation[bid#17,bVal#18] InputPaths: file:/home/mohit/ruleB

This is a watered-down example from a production issue which has a huge performance impact.
External reference: http://stackoverflow.com/questions/40783675/spark-sql-catalyst-is-scanning-undesired-columns","Ubuntu 14.04
Spark: Local Mode",dongjoon,kmalik,mohitgargk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 11 10:41:40 UTC 2016,,,,,,,,,,"0|i36xzr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/16 21:05;dongjoon;Thank you for reporting, [~mohitgargk].
It seems to be the same with Apache Spark 1.6.3 and to be resolved since Apache Spark 2.0.0.
{code}
scala> val dfA = spark.read.parquet(""/tmp/a"")
scala> val dfB = spark.read.parquet(""/tmp/b"")
scala> dfA.createOrReplaceTempView(""A"")
scala> dfB.createOrReplaceTempView(""B"")
scala> sql(""select A.*, B.* from A left join B on A.id = B.id where B.id<2"").explain
== Physical Plan ==
*BroadcastHashJoin [id#0L], [id#3L], Inner, BuildRight
:- *Project [id#0L]
:  +- *Filter (isnotnull(id#0L) && (id#0L < 2))
:     +- *BatchedScan parquet [id#0L] Format: ParquetFormat, InputPaths: file:/tmp/a, PushedFilters: [IsNotNull(id), LessThan(id,2)], ReadSchema: struct<id:bigint>
+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]))
   +- *Project [id#3L]
      +- *Filter (isnotnull(id#3L) && (id#3L < 2))
         +- *BatchedScan parquet [id#3L] Format: ParquetFormat, InputPaths: file:/tmp/b, PushedFilters: [IsNotNull(id), LessThan(id,2)], ReadSchema: struct<id:bigint>
{code}

IMO, it will not be inside Spark 1.6.4 (if exist) since this is a performance issue.;;;","05/Dec/16 06:37;dongjoon;I close this issue as a `Fixed` because of the following reasons.
- This is already implemented in 2.0.0.
- New feature will not be backported into `branch-1.6`.;;;","09/Dec/16 00:56;srowen;[~dongjoon] ignore this if you don't know, but if you happen to know what issue fixed this, that would be great to record.;;;","09/Dec/16 01:54;dongjoon;I see. If then, I'll record that, too.;;;","11/Dec/16 10:41;mohitgargk;[~dongjoon] We will appreciate if you could share your findings in form of 'touch-points' from the source-code. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix minor synchronization issue in TaskSchedulerImpl.runningTasksByExecutors,SPARK-18640,13024299,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joshrosen,joshrosen,joshrosen,30/Nov/16 02:15,17/May/20 17:48,14/Jul/23 06:29,02/Dec/16 06:04,,,,,,,,,2.0.3,2.1.0,,,Scheduler,Spark Core,,,,,,,0,,,,,,The method TaskSchedulerImpl.runningTasksByExecutors() accesses the mutable executorIdToRunningTaskIds map without proper synchronization. We should fix this.,,andrewor14,apachespark,joshrosen,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 02 17:18:18 UTC 2016,,,,,,,,,,"0|i36xw7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Nov/16 02:18;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16073;;;","02/Dec/16 06:03;rxin;[~andrewor14] how come you didn't close the ticket?
;;;","02/Dec/16 16:04;andrewor14;my mistake;;;","02/Dec/16 17:13;joshrosen;I'm also going to backport this into branch-1.6.;;;","02/Dec/16 17:18;joshrosen;Actually, this doesn't look necessary because that method isn't in branch-1.6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stateful UDF should be considered as nondeterministic,SPARK-18637,13024250,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zhazhan,zhzhan,zhzhan,29/Nov/16 23:26,09/Dec/16 08:39,14/Jul/23 06:29,09/Dec/16 08:38,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"If the annotation UDFType of a udf is stateful, it shoudl be considered as non-deterministic. Otherwise, the catalyst may optimize the plan and return the wrong result.",,apachespark,cloud_fan,hvanhovell,zhzhan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 09 08:38:44 UTC 2016,,,,,,,,,,"0|i36xlb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/16 23:28;zhzhan;Here is the comments from UDFType 
  /**
   * If a UDF stores state based on the sequence of records it has processed, it
   * is stateful. A stateful UDF cannot be used in certain expressions such as
   * case statement and certain optimizations such as AND/OR short circuiting
   * don't apply for such UDFs, as they need to be invoked for each record.
   * row_sequence is an example of stateful UDF. A stateful UDF is considered to
   * be non-deterministic, irrespective of what deterministic() returns.
   *
   * @return true
   */
  boolean stateful() default false;;;;","29/Nov/16 23:35;hvanhovell;{{UDFType}} is a Hive construct right?;;;","29/Nov/16 23:39;apachespark;User 'zhzhan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16068;;;","29/Nov/16 23:52;zhzhan;[~hvanhovell] It is an annotation.

/**
 * UDFType annotations are used to describe properties of a UDF. This gives
 * important information to the optimizer.
 * If the UDF is not deterministic, or if it is stateful, it is necessary to
 * annotate it as such for correctness.
 *
 */;;;","09/Dec/16 08:38;cloud_fan;Issue resolved by pull request 16068
[https://github.com/apache/spark/pull/16068];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Corruption and Correctness issues with exploding Python UDFs,SPARK-18634,13024219,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,brkyvz,brkyvz,29/Nov/16 20:54,06/Dec/16 11:17,14/Jul/23 06:29,06/Dec/16 01:51,2.0.2,2.1.0,,,,,,,2.0.3,2.1.0,,,PySpark,SQL,,,,,,,0,,,,,,"There are some weird issues with exploding Python UDFs in SparkSQL.

There are 2 cases where based on the DataType of the exploded column, the result can be flat out wrong, or corrupt. Seems like something bad is happening when telling Tungsten the schema of the rows during or after applying the UDF.

Please check the code below for reproduction.

Notebook: https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6186780348633019/3425836135165635/4343791953238323/latest.html
",,apachespark,brkyvz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 06 11:17:06 UTC 2016,,,,,,,,,,"0|i36xef:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/16 15:53;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/16120;;;","06/Dec/16 11:17;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/16170;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid making data skew worse in ExchangeCoordinator,SPARK-18631,13024177,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,markhamstra,markhamstra,markhamstra,29/Nov/16 18:59,29/Nov/16 23:01,14/Jul/23 06:29,29/Nov/16 23:01,1.6.3,2.0.2,2.1.0,,,,,,2.2.0,,,,SQL,,,,,,,,0,,,,,,"The logic to resize partitions in the ExchangeCoordinator is to not start a new partition until the targetPostShuffleInputSize is equalled or exceeded.  This can make data skew problems worse since a number of small partitions can first be combined as long as the combined size remains smaller than the targetPostShuffleInputSize, and then a large, data-skewed partition can be further combined, making it even bigger than it already was.

It's a fairly simple to change the logic to create a new partition if adding a new piece would exceed the targetPostShuffleInputSize instead of only creating a new partition after the targetPostShuffleInputSize has already been exceeded.  This results in a few more partitions being created by the ExchangeCoordinator, but data skew problems are at least not made worse even though they are not made any better.",,apachespark,markhamstra,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 29 23:01:28 UTC 2016,,,,,,,,,,"0|i36x53:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/16 20:53;apachespark;User 'markhamstra' has created a pull request for this issue:
https://github.com/apache/spark/pull/16065;;;","29/Nov/16 23:01;yhuai;Issue resolved by pull request 16065
[https://github.com/apache/spark/pull/16065];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark ML memory leak,SPARK-18630,13024176,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yogeshgarg,holden,holden,29/Nov/16 18:58,05/Mar/18 23:53,14/Jul/23 06:29,05/Mar/18 23:53,,,,,,,,,2.4.0,,,,ML,PySpark,,,,,,,0,,,,,,"After SPARK-18274 is fixed by https://github.com/apache/spark/pull/15843, it would be good to follow up and address the potential memory leak for all items handled by the `JavaWrapper`, not just `JavaParams`.",,apachespark,holden,josephkb,sueann,techaddict,yogeshgarg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18274,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 05 23:53:30 UTC 2018,,,,,,,,,,"0|i36x4v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/17 23:55;sueann;will take a look at this;;;","28/Feb/18 23:42;yogeshgarg;I would like to take this. If I understand correctly, moving the `__del__` and (deep) `copy` methods to `JavaWrapper` should address this potential issue. Is there a reason why we might not want to do a deep copy of `JavaWrapper` class?;;;","01/Mar/18 23:43;yogeshgarg;After some discussion, I think it makes sense to move just the __del__ method to JavaWrapper and keep the copy method in JavaParams. The code also needs some testing.;;;","03/Mar/18 00:34;apachespark;User 'yogeshg' has created a pull request for this issue:
https://github.com/apache/spark/pull/20724;;;","05/Mar/18 23:53;josephkb;Issue resolved by pull request 20724
[https://github.com/apache/spark/pull/20724];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix numPartition of JDBCSuite Testcase,SPARK-18629,13024175,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,WeiqingYang,WeiqingYang,WeiqingYang,29/Nov/16 18:57,02/Dec/16 11:53,14/Jul/23 06:29,02/Dec/16 11:53,,,,,,,,,2.2.0,,,,SQL,,,,,,,,0,,,,,,"When running any one of the test cases in JDBCSuite, you will get the following warning.

{code}
10:34:26.389 WARN org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation: The number of partitions is reduced because the specified number of partitions is less than the difference between upper bound and lower bound. Updated number of partitions: 3; Input number of partitions: 4; Lower bound: 1; Upper bound: 4.{code}

This jira is to fix it.",,apachespark,WeiqingYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 02 11:53:33 UTC 2016,,,,,,,,,,"0|i36x4n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/16 19:07;apachespark;User 'weiqingy' has created a pull request for this issue:
https://github.com/apache/spark/pull/16062;;;","02/Dec/16 11:53;srowen;Issue resolved by pull request 16062
[https://github.com/apache/spark/pull/16062];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing Reference in Multi Union Clauses Cause by TypeCoercion,SPARK-18622,13024022,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,sunyerui,sunyerui,29/Nov/16 09:14,18/Aug/20 10:00,14/Jul/23 06:29,30/Nov/16 07:26,1.6.3,2.0.2,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"{code}
spark-sql> explain extended
         > select a
         > from
         > (
         >   select 0 a, 0 b
         > union all
         >   select sum(1) a, cast(0 as bigint) b
         > union all
         >   select 0 a, 0 b
         > )t;
 
== Parsed Logical Plan ==
'Project ['a]
+- 'SubqueryAlias t
   +- 'Union
      :- 'Union
      :  :- Project [0 AS a#0, 0 AS b#1]
      :  :  +- OneRowRelation$
      :  +- 'Project ['sum(1) AS a#2, cast(0 as bigint) AS b#3L]
      :     +- OneRowRelation$
      +- Project [0 AS a#4, 0 AS b#5]
         +- OneRowRelation$
 
== Analyzed Logical Plan ==
a: int
Project [a#0]
+- SubqueryAlias t
   +- Union
      :- !Project [a#0, b#9L]
      :  +- Union
      :     :- Project [cast(a#0 as bigint) AS a#11L, b#9L]
      :     :  +- Project [a#0, cast(b#1 as bigint) AS b#9L]
      :     :     +- Project [0 AS a#0, 0 AS b#1]
      :     :        +- OneRowRelation$
      :     +- Project [a#2L, b#3L]
      :        +- Project [a#2L, b#3L]
      :           +- Aggregate [sum(cast(1 as bigint)) AS a#2L, cast(0 as bigint) AS b#3L]
      :              +- OneRowRelation$
      +- Project [a#4, cast(b#5 as bigint) AS b#10L]
         +- Project [0 AS a#4, 0 AS b#5]
            +- OneRowRelation$
 
== Optimized Logical Plan ==
org.apache.spark.sql.AnalysisException: resolved attribute(s) a#0 missing from a#11L,b#9L in operator !Project [a#0, b#9L];;
Project [a#0]
+- SubqueryAlias t
   +- Union
      :- !Project [a#0, b#9L]
      :  +- Union
      :     :- Project [cast(a#0 as bigint) AS a#11L, b#9L]
      :     :  +- Project [a#0, cast(b#1 as bigint) AS b#9L]
      :     :     +- Project [0 AS a#0, 0 AS b#1]
      :     :        +- OneRowRelation$
      :     +- Project [a#2L, b#3L]
      :        +- Project [a#2L, b#3L]
      :           +- Aggregate [sum(cast(1 as bigint)) AS a#2L, cast(0 as bigint) AS b#3L]
      :              +- OneRowRelation$
      +- Project [a#4, cast(b#5 as bigint) AS b#10L]
         +- Project [0 AS a#4, 0 AS b#5]
            +- OneRowRelation$
 
== Physical Plan ==
org.apache.spark.sql.AnalysisException: resolved attribute(s) a#0 missing from a#11L,b#9L in operator !Project [a#0, b#9L];;
Project [a#0]
+- SubqueryAlias t
   +- Union
      :- !Project [a#0, b#9L]
      :  +- Union
      :     :- Project [cast(a#0 as bigint) AS a#11L, b#9L]
      :     :  +- Project [a#0, cast(b#1 as bigint) AS b#9L]
      :     :     +- Project [0 AS a#0, 0 AS b#1]
      :     :        +- OneRowRelation$
      :     +- Project [a#2L, b#3L]
      :        +- Project [a#2L, b#3L]
      :           +- Aggregate [sum(cast(1 as bigint)) AS a#2L, cast(0 as bigint) AS b#3L]
      :              +- OneRowRelation$
      +- Project [a#4, cast(b#5 as bigint) AS b#10L]
         +- Project [0 AS a#4, 0 AS b#5]
            +- OneRowRelation$
{code}

Key Points to re-produce issue:
* 3 or more union clauses;
* One column is sum aggregate in one union clause, and is Integer type in other union clause;
* Another column has different date types in union clauses;

The reason of issue:
- Step 1: Apply TypeCoercion.WidenSetOperationTypes, add project with cast since the union clauses has different datatypes for one column; With 3 union clauses, the inner union clause also be projected with cast;
- Step 2: Apply TypeCoercion.FunctionArgumentConversion, the return type of sum(int) will be extended to BigInt, meaning one column in union clauses changed datatype;
- Step 3: Apply TypeCoercion.WidenSetOperationTypes again, another cast project added in inner union clause, since sum(int) datatype changed; at this point, the reference of project ON inner union will be missed, since the project IN inner union is newly added, see the  Analyzed Logical Plan;

Solutions to fix:
* Since set operation type coercion should be applied after inner clause be stabled, apply WidenSetOperationTypes at last will fix the issue;
* To avoiding multi level projects on set operation clause, handle the existing cast project carefully in WidenSetOperationTypes should be also work;

Appreciate for any comments.",,apachespark,kkyong,maropu,sunyerui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 18 10:00:18 UTC 2020,,,,,,,,,,"0|i36w6n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/16 19:48;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/16063;;;","18/Aug/20 10:00;kkyong;I have the same issue in spark 2.3.4

https://issues.apache.org/jira/browse/SPARK-32638

the issue seem not be resolved . ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySQL SQL Types (aka Dataframa Schema) have __repr__() with Scala and not Python representation,SPARK-18621,13024020,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,romik,romik,romik,29/Nov/16 09:05,23/Mar/22 14:01,14/Jul/23 06:29,23/Mar/22 14:01,1.6.2,2.0.2,,,,,,,3.3.0,,,,PySpark,,,,,,,,0,,,,,,"When using Python's repr() on an object, the expected result is a string that Python can evaluate to construct the object.
See: https://docs.python.org/2/library/functions.html#func-repr

However, when getting a DataFrame schema in PySpark, the code (in ""__repr()__"" overload methods) returns the string representation for Scala, rather than for Python.

Relevant code in PySpark:
https://github.com/apache/spark/blob/5f02d2e5b4d37f554629cbd0e488e856fffd7b6b/python/pyspark/sql/types.py#L442

Python Code:

{code}
# 1. define object
struct1 = StructType([StructField(""f1"", StringType(), True)])
# 2. print representation, expected to be like above
print(repr(struct1))
# 3. actual result:
# StructType(List(StructField(f1,StringType,true)))
# 4. try to use result in code
struct2 = StructType(List(StructField(f1,StringType,true)))
# 5. get bunch of errors
# Unresolved reference 'List'
# Unresolved reference 'f1'
# StringType is class, not constructed object
# Unresolved reference 'true'
{code}",,apachespark,romik,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 23 14:01:06 UTC 2022,,,,,,,,,,"0|i36w67:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/21 02:43;apachespark;User 'crflynn' has created a pull request for this issue:
https://github.com/apache/spark/pull/34320;;;","23/Mar/22 14:01;srowen;Resolved by https://github.com/apache/spark/pull/34320;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Streaming + Kinesis : Receiver MaxRate is violated,SPARK-18620,13024018,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maropu,dav009,dav009,29/Nov/16 09:04,10/May/18 08:32,14/Jul/23 06:29,09/Dec/16 21:32,2.0.2,,,,,,,,2.2.0,,,,DStreams,,,,,,,,0,kinesis,,,,,"I am calling spark-submit passing maxRate, I have a single kinesis receiver, and batches of 1s

spark-submit  --conf spark.streaming.receiver.maxRate=10 ....

however a single batch can greatly exceed the stablished maxRate. i.e: Im getting 300 records.

it looks like Kinesis is completely ignoring the spark.streaming.receiver.maxRate configuration.

If you look inside KinesisReceiver.onStart, you see:

val kinesisClientLibConfiguration =
  new KinesisClientLibConfiguration(checkpointAppName, streamName, awsCredProvider, workerId)
  .withKinesisEndpoint(endpointUrl)
  .withInitialPositionInStream(initialPositionInStream)
  .withTaskBackoffTimeMillis(500)
  .withRegionName(regionName)

This constructor ends up calling another constructor which has a lot of default values for the configuration. One of those values is DEFAULT_MAX_RECORDS which is constantly set to 10,000 records.",,apachespark,bruce_zhao11,dav009,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23294,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/16 16:21;maropu;Apply_limit in_spark_with_my_patch.png;https://issues.apache.org/jira/secure/attachment/12841307/Apply_limit+in_spark_with_my_patch.png","01/Dec/16 16:21;maropu;Apply_limit in_vanilla_spark.png;https://issues.apache.org/jira/secure/attachment/12841306/Apply_limit+in_vanilla_spark.png","01/Dec/16 16:20;maropu;Apply_no_limit.png;https://issues.apache.org/jira/secure/attachment/12841305/Apply_no_limit.png",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 10 08:32:57 UTC 2018,,,,,,,,,,"0|i36w5r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/16 07:27;maropu;I quickly checked and I found that that's not enough to set max records in Kinesis workers because
the kinesis workers cannot limit the number of aggregate messages (http://docs.aws.amazon.com/streams/latest/dev/kinesis-kpl-concepts.html#d0e5184).
For example, if we set 10 to the number of max records in workers and a producer aggregates two records into one message,
it seems kinesis workers actually 20 records per callback function called.
My hunch is that we need to control #records to push them into a receiver in KinesisRecordProcessor#processRecords(https://github.com/apache/spark/blob/master/external/kinesis-asl/src/main/scala/org/apache/spark/streaming/kinesis/KinesisRecordProcessor.scala#L68).;;;","01/Dec/16 16:24;maropu;I tried to fix this issue: https://github.com/apache/spark/compare/master...maropu:SPARK-18620.
Also, I did some tests with three different conditions: no limit, applying limits on vanilla spark, and applying limits on spark with my patch (See attached above).
Obviously, my patch could limit the number of input records more naturally.;;;","02/Dec/16 03:25;dav009;Looks good to me.
Thanks for the prompt answer;;;","02/Dec/16 03:27;maropu;yea, I'll make a pr in a day;;;","02/Dec/16 05:29;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/16114;;;","09/Dec/16 21:32;srowen;Issue resolved by pull request 16114
[https://github.com/apache/spark/pull/16114];;;","10/May/18 08:32;bruce_zhao11;This PR makes the input rate flat, but the limit of get-records is still out of control. 

In the onStart(), it will initialize a configuration for KCL worker. 

val baseClientLibConfiguration = new KinesisClientLibConfiguration(
checkpointAppName,
streamName,
kinesisProvider,
dynamoDBCreds.map(_.provider).getOrElse(kinesisProvider),
cloudWatchCreds.map(_.provider).getOrElse(kinesisProvider),
workerId)

In the KCL library, it will use default value DEFAULT_MAX_RECORDS(10,000) for getRecords. 

As Kinesis only supports a data read rate of 2 MB per second per shard, it will be easily to get exception ProvisionedThroughputExceededException, especially when we restart the application after a long stop. 

 

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Close ""kryo auto pick"" feature for Spark Streaming",SPARK-18617,13023975,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,uncleGen,uncleGen,uncleGen,29/Nov/16 06:10,30/Dec/16 01:40,14/Jul/23 06:29,30/Nov/16 07:45,2.0.2,,,,,,,,2.0.3,2.1.0,,,,,,,,,,,0,,,,,,"[PR-15992| https://github.com/apache/spark/pull/15992] provided a solution to fix the bug, i.e. {{receiver data can not be deserialized properly}}. As [~zsxwing] said, it is a critical bug, but we should not break APIs between maintenance releases. It may be a rational choice to close {{auto pick kryo serializer}} for Spark Streaming in the first step.",,apachespark,uncleGen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18560,SPARK-17766,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 01 19:25:07 UTC 2016,,,,,,,,,,"0|i36vw7:",9223372036854775807,,,,,,,,,,,,,2.0.3,2.1.0,,,,,,,,,,"29/Nov/16 06:15;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16052;;;","01/Dec/16 00:17;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16091;;;","01/Dec/16 05:16;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16096;;;","01/Dec/16 19:25;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16105;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect predicate pushdown from ExistenceJoin,SPARK-18614,13023865,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nsyca,nsyca,nsyca,28/Nov/16 21:19,29/Nov/16 23:28,14/Jul/23 06:29,29/Nov/16 23:28,2.0.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,This is a follow-up work from SPARK-18597 to close a potential incorrect rewrite in {{PushPredicateThroughJoin}} rule of the Optimizer phase.,,apachespark,nsyca,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18597,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 28 21:48:04 UTC 2016,,,,,,,,,,"0|i36v7r:",9223372036854775807,,,,,hvanhovell,,,,,,,,,,,,,,,,,,,"28/Nov/16 21:28;nsyca;{{ExistenceJoin}} should be treated the same as {{LeftOuter}} and {{LeftAnti}}, not {{InnerLike}} and {{LeftSemi}}. This is not currently exposed because the rewrite of {{\[NOT\] EXISTS OR ...}} to {{ExistenceJoin}} happens in rule {{RewritePredicateSubquery}}, which is in a separate rule set and placed after the rule {{PushPredicateThroughJoin}}. During the transformation in the rule {{PushPredicateThroughJoin}}, an ExistenceJoin never exists.

The semantics of {{ExistenceJoin}} says we need to preserve all the rows from the left table through the join operation as if it is a regular {{LeftOuter}} join. The {{ExistenceJoin}} augments the {{LeftOuter}} operation with a new column called {{exists}}, set to true when the join condition in the ON clause is true and false otherwise. The filter of any rows will happen in the {{Filter}} operation above the {{ExistenceJoin}}.

Example:

A(c1, c2): \{ (1, 1), (1, 2) \}
// B can be any value as it is irrelevant in this example
B(c1): \{ (NULL) \}

{code:SQL}
select A.*
from   A
where  exists (select 1 from B where A.c1 = A.c2)
       or A.c2=2
{code}

In this example, the correct result is all the rows from A. If the pattern {{ExistenceJoin}} at line 935 in {{Optimizer.scala}} added by the work in SPARK-18597 is indeed active, the code will push down the predicate A.c1 = A.c2 to be a {{Filter}} on relation A, which will filter the row (1,2) from A.
;;;","28/Nov/16 21:31;smilegator;Since it does not affect the correctness of the query results, I remove the label `correctness`;;;","28/Nov/16 21:48;apachespark;User 'nsyca' has created a pull request for this issue:
https://github.com/apache/spark/pull/16044;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[SQL] column mixup with CROSS JOIN,SPARK-18609,13023712,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,fpin,fpin,28/Nov/16 11:33,08/Feb/17 10:31,14/Jul/23 06:29,07/Feb/17 21:41,2.0.2,2.1.0,,,,,,,2.1.1,2.2.0,,,SQL,,,,,,,,0,,,,,,"Reproduced on spark-sql v2.0.2 and on branch master.

{code}
DROP TABLE IF EXISTS p1 ;
DROP TABLE IF EXISTS p2 ;

CREATE TABLE p1 (col TIMESTAMP) ;
CREATE TABLE p2 (col TIMESTAMP) ;

set spark.sql.crossJoin.enabled = true;

-- EXPLAIN
WITH CTE AS (
  SELECT
    s2.col as col
  FROM p1
  CROSS JOIN (
    SELECT
      e.col as col
    FROM p2 E
  ) s2
)
SELECT
  T1.col as c1,
  T2.col as c2
FROM CTE T1
CROSS JOIN CTE T2
;
{code}

This returns the following stacktrace :
{code}
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: col#21
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:88)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:279)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:279)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:268)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:87)
	at org.apache.spark.sql.execution.ProjectExec$$anonfun$4.apply(basicPhysicalOperators.scala:55)
	at org.apache.spark.sql.execution.ProjectExec$$anonfun$4.apply(basicPhysicalOperators.scala:54)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:54)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:218)
	at org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:244)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:218)
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:40)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:30)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:309)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:347)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.joins.CartesianProductExec.doExecute(CartesianProductExec.scala:107)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:233)
	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:361)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:240)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:287)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:310)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$hiveResultString$3.apply(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$hiveResultString$3.apply(QueryExecution.scala:128)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.hiveResultString(QueryExecution.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:331)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.RuntimeException: Couldn't find col#21 in []
	at scala.sys.package$.error(package.scala:27)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:94)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:88)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	... 85 more
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: col#21
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:88)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:279)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:279)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:268)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:87)
	at org.apache.spark.sql.execution.ProjectExec$$anonfun$4.apply(basicPhysicalOperators.scala:55)
	at org.apache.spark.sql.execution.ProjectExec$$anonfun$4.apply(basicPhysicalOperators.scala:54)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:54)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:218)
	at org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:244)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:218)
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:40)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:30)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:309)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:347)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.joins.CartesianProductExec.doExecute(CartesianProductExec.scala:107)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:233)
	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:361)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:240)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:287)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:310)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$hiveResultString$3.apply(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$hiveResultString$3.apply(QueryExecution.scala:128)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.hiveResultString(QueryExecution.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:331)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.RuntimeException: Couldn't find col#21 in []
	at scala.sys.package$.error(package.scala:27)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:94)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:88)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	... 85 more
{code}

The EXPLAIN works though:
{code}
== Physical Plan ==
*Project [col#33 AS c1#26, col#36 AS c2#27]
+- CartesianProduct
   :- CartesianProduct
   :  :- HiveTableScan MetastoreRelation default, p1
   :  +- HiveTableScan [col#33], MetastoreRelation default, p2, E
   +- *!Project [col#33 AS col#36]
      +- CartesianProduct
         :- HiveTableScan MetastoreRelation default, p1
         +- HiveTableScan MetastoreRelation default, p2, E
{code}",,apachespark,fpin,hvanhovell,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 08 10:31:57 UTC 2017,,,,,,,,,,"0|i36u9r:",9223372036854775807,,,,,,,,,,,,,2.1.1,,,,,,,,,,,"02/Dec/16 11:41;fpin;Hi,

I found another minimal example that yields a similar, yet different error.
I report it here in case it is related, but perhaps it should be moved in another JIRA.

Reproduced on branch Spark 2.0.2 and on branch master:

{code}
DROP TABLE IF EXISTS p1 ;
DROP TABLE IF EXISTS p2 ;
DROP TABLE IF EXISTS p3 ;
CREATE TABLE p1 (col TIMESTAMP) ;
CREATE TABLE p2 (col TIMESTAMP) ;
CREATE TABLE p3 (col TIMESTAMP) ;

-- EXPLAIN
DROP TABLE IF EXISTS p1 ;
DROP TABLE IF EXISTS p2 ;
DROP TABLE IF EXISTS p3 ;
CREATE TABLE p1 (col TIMESTAMP) ;
CREATE TABLE p2 (col TIMESTAMP) ;
CREATE TABLE p3 (col TIMESTAMP) ;

set spark.sql.crossJoin.enabled = true;

-- EXPLAIN
SELECT
  1 as cste,
  col
FROM (
  SELECT
    col as col
  FROM (
    SELECT
      p1.col as col
    FROM p1
    LEFT JOIN p2 
    UNION ALL
    SELECT
      col
    FROM p3
  ) T1
) T2
;
{code}

This returns the following stacktrace :
{code}
java.util.NoSuchElementException: key not found: col#41
	at scala.collection.MapLike$class.default(MapLike.scala:228)
	at org.apache.spark.sql.catalyst.expressions.AttributeMap.default(AttributeMap.scala:31)
	at scala.collection.MapLike$class.apply(MapLike.scala:141)
	at org.apache.spark.sql.catalyst.expressions.AttributeMap.apply(AttributeMap.scala:31)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$2.applyOrElse(Optimizer.scala:297)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$2.applyOrElse(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:292)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:292)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:291)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$.org$apache$spark$sql$catalyst$optimizer$PushProjectionThroughUnion$$pushToRight(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$apply$4$$anonfun$8$$anonfun$apply$29.apply(Optimizer.scala:329)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$apply$4$$anonfun$8$$anonfun$apply$29.apply(Optimizer.scala:329)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$apply$4$$anonfun$8.apply(Optimizer.scala:329)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$apply$4$$anonfun$8.apply(Optimizer.scala:327)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$apply$4.applyOrElse(Optimizer.scala:327)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$apply$4.applyOrElse(Optimizer.scala:320)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:292)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:292)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:291)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$.apply(Optimizer.scala:320)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$.apply(Optimizer.scala:280)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
	at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)
	at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:73)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:73)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:84)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$simpleString$1.apply(QueryExecution.scala:213)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$simpleString$1.apply(QueryExecution.scala:213)
	at org.apache.spark.sql.execution.QueryExecution.stringOrError(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:213)
	at org.apache.spark.sql.execution.command.ExplainCommand.run(commands.scala:117)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:62)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:335)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:728)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
java.util.NoSuchElementException: key not found: col#41
	at scala.collection.MapLike$class.default(MapLike.scala:228)
	at org.apache.spark.sql.catalyst.expressions.AttributeMap.default(AttributeMap.scala:31)
	at scala.collection.MapLike$class.apply(MapLike.scala:141)
	at org.apache.spark.sql.catalyst.expressions.AttributeMap.apply(AttributeMap.scala:31)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$2.applyOrElse(Optimizer.scala:297)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$2.applyOrElse(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:292)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:292)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:291)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$.org$apache$spark$sql$catalyst$optimizer$PushProjectionThroughUnion$$pushToRight(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$apply$4$$anonfun$8$$anonfun$apply$29.apply(Optimizer.scala:329)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$apply$4$$anonfun$8$$anonfun$apply$29.apply(Optimizer.scala:329)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$apply$4$$anonfun$8.apply(Optimizer.scala:329)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$apply$4$$anonfun$8.apply(Optimizer.scala:327)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$apply$4.applyOrElse(Optimizer.scala:327)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$$anonfun$apply$4.applyOrElse(Optimizer.scala:320)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:292)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:292)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:291)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$.apply(Optimizer.scala:320)
	at org.apache.spark.sql.catalyst.optimizer.PushProjectionThroughUnion$.apply(Optimizer.scala:280)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
	at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)
	at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:73)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:73)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:84)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$simpleString$1.apply(QueryExecution.scala:213)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$simpleString$1.apply(QueryExecution.scala:213)
	at org.apache.spark.sql.execution.QueryExecution.stringOrError(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:213)
	at org.apache.spark.sql.execution.command.ExplainCommand.run(commands.scala:117)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:62)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:335)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:728)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}

This time however, the EXPLAIN gives the same error.
Also, renaming the columns with distinct names solves the issue, but this workaround cannot be used for generated queries...

Perhaps the same column mix-up at some point causes both errors at different later points.

;;;","02/Dec/16 12:12;hvanhovell;Those are two separate problems, the first one occurs during execution and the second during optimization. They might have the same root cause. We should take a look at that. Targeting for 2.11 (not 2.1).;;;","07/Dec/16 07:52;windpiger;I'm working on this~;;;","12/Dec/16 14:52;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/16255;;;","13/Dec/16 14:37;windpiger;open another jira SPARK-18841;;;","31/Jan/17 16:48;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/16757;;;","07/Feb/17 21:46;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/16843;;;","08/Feb/17 10:31;hvanhovell;I am not fixing this for 2.0. The PR is to invasive and we cause more harm than good by merging it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark ML algorithms that check RDD cache level for internal caching double-cache data,SPARK-18608,13023677,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,podongfeng,mlnick,mlnick,28/Nov/16 09:28,20/Jun/19 05:26,14/Jul/23 06:29,12/Sep/17 18:37,,,,,,,,,2.2.1,2.3.0,,,ML,,,,,,,,0,,,,,,"Some algorithms in Spark ML (e.g. {{LogisticRegression}}, {{LinearRegression}}, and I believe now {{KMeans}}) handle persistence internally. They check whether the input dataset is cached, and if not they cache it for performance.

However, the check is done using {{dataset.rdd.getStorageLevel == NONE}}. This will actually always be true, since even if the dataset itself is cached, the RDD returned by {{dataset.rdd}} will not be cached.

Hence if the input dataset is cached, the data will end up being cached twice, which is wasteful.

To see this:

{code}
scala> import org.apache.spark.storage.StorageLevel
import org.apache.spark.storage.StorageLevel

scala> val df = spark.range(10).toDF(""num"")
df: org.apache.spark.sql.DataFrame = [num: bigint]

scala> df.storageLevel == StorageLevel.NONE
res0: Boolean = true

scala> df.persist
res1: df.type = [num: bigint]

scala> df.storageLevel == StorageLevel.MEMORY_AND_DISK
res2: Boolean = true

scala> df.rdd.getStorageLevel == StorageLevel.MEMORY_AND_DISK
res3: Boolean = false

scala> df.rdd.getStorageLevel == StorageLevel.NONE
res4: Boolean = true
{code}

Before SPARK-16063, there was no way to check the storage level of the input {{DataSet}}, but now we can, so the checks should be migrated to use {{dataset.storageLevel}}.",,apachespark,bryanc,jbrock,josephkb,mlnick,peng.meng@intel.com,podongfeng,wm624,yanboliang,yuhaoyan,zahili,,,,,,,,,,,,,,,,,,,,,SPARK-19422,,,,,,SPARK-21799,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 13 11:33:04 UTC 2017,,,,,,,,,,"0|i36u1z:",9223372036854775807,,,,,,,,,,,,,2.2.1,2.3.0,,,,,,,,,,"28/Nov/16 09:31;srowen;Agree, I had long since meant to note this. This would be great to fix.;;;","28/Nov/16 12:21;mlnick;I've also been meaning to log this for a little while now. 

It's actually not a simple fix - there is now some automated label casting to Double in {{predictor.fit}} that will throw away the dataset storage level info. We could perhaps centralize the handle persistence logic in {{fit}}.;;;","30/Nov/16 06:05;yuhaoyan;Agree. we can just add an extra parameter handlePersistence: Boolean to the train method in Predictor. ;;;","19/Dec/16 13:59;zahili;Hi, 
[~mlnick], can I join this discussion?
Could you please explain to me why do you believe that the generated rdd is cached ?
As you can see in https://github.com/apache/spark/blob/master/python/pyspark/sql/dataframe.py 

we generate a new rdd, so it's normal that this rdd is not cached

    def rdd(self):
        """"""Returns the content as an :class:`pyspark.RDD` of :class:`Row`.
        """"""
        if self._lazy_rdd is None:
            jrdd = self._jdf.javaToPython()
            self._lazy_rdd = RDD(jrdd, self.sql_ctx._sc, BatchedSerializer(PickleSerializer()))
        return self._lazy_rdd;;;","22/Dec/16 09:56;srowen;I don't think this has to do with Pyspark. The situation is different: input is cached, but the intermediate RDD created internally is not, and so is cached again.;;;","22/Dec/16 10:31;zahili;[~srowen], Now , I understand what you mean, your purpose is to optimize the memory,however, I think that all we need is to add an extra check,
if the dataframe is cached, we call the mllib algo directly,
if not, we have to cache the internal rdd

But the problem of this solution is: if we do not cache the internal rdd, we will get a lot of warnings from mllib package (RDD is not cached)
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/clustering/KMeans.scala#L216
because df.rdd.getStorageLevel == StorageLevel.NONE is true.

So maybe we will need to create a new public class in mllib methods which can take the handlePersistence as parameter : if it has true there no need to recheck the input RDD;;;","02/Feb/17 11:52;podongfeng;[~yuhaoyan] Agree that it's nice to add an extra parameter:

in {{Predictor}}
{code}
  override def fit(dataset: Dataset[_]): M = {
    val handlePersistence = dataset.storageLevel == StorageLevel.NONE
  ...
    copyValues(train(casted, handlePersistence).setParent(this))
  }

protected def train(dataset: Dataset[_], handlePersistence: Boolean): M

~~protected def train(dataset: Dataset[_]): M~~  //delete this
{code}

In each classification and regression algorithms, override the new {{train}} api instead of old one.

For clustering algorithms, directly modify the {{fit}} method.

Since SPARK-16063 was already resolved, is there someone working on this?;;;","21/Feb/17 06:55;mlnick;[~podongfeng] [~yuhaoyan] I'm not aware of anyone working on this now, either of you want to take it?;;;","21/Feb/17 06:59;podongfeng;[~mlnick] I will send a PR for this according to the above discussion:  use {{train(dataset: Dataset[_], handlePersistence: Boolean)}} instead of {{train(dataset: Dataset[_])}};;;","21/Feb/17 07:10;yuhaoyan;Thanks [~podongfeng] ;;;","21/Feb/17 09:30;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/17014;;;","03/Mar/17 02:24;podongfeng;[~mlnick] [~yuhaoyan] [~srowen] I think if we use {{train(dataset: Dataset[_], handlePersistence: Boolean)}} instead of {{train(dataset: Dataset[_])}} may result in extra problems for external implementers, because the existing external algorithms overriding {{Predictor.train}} will not work. 

I think we can do it in another way:
{code}
abstract class Predictor[
    FeaturesType,
    Learner <: Predictor[FeaturesType, Learner, M],
    M <: PredictionModel[FeaturesType, M]]
  extends Estimator[M] with PredictorParams {

  protected var storageLevel = StorageLevel.NONE // 

  override def fit(dataset: Dataset[_]): M = {
    storageLevel = dataset.storageLevel
...
  }

  protected def train(dataset: Dataset[_]): M
{code}

so in algorithm implementations we can use the orignial storageLevel of the input dataset.;;;","13/Mar/17 03:33;podongfeng;[~mlnick][~srowen] [~yuhaoyan] What's your opinions about what I commented above?;;;","13/Mar/17 09:01;srowen;Is the point here that the .ml implementation can check the storage level of its input, and pass that information in some way, internally, to the .mllib implementation that it delegates to? Yes I think that's the most feasible answer to this particular problem, as long as it doesn't change a public API. The .mllib implementations would have to have some internal mechanism for getting this information about parents' storage level, if applicable.

It does raise the more general question of whether these implementation can meaningfully decide about caching anyway, and whether they should try, rather than just warn. More generally it's hard for any library function to reason about whether to persist its input or not, or even, to reason about when a data structure can be unpersisted. Those are much bigger and separate questions, but it's why this type of question keeps popping up and is hard to solve.;;;","13/Mar/17 23:14;yuhaoyan;Thanks [~podongfeng] I'd say it's a better solution as it avoids API change. In the long term, this should be a temporary workaround until we migrate all the implementations from RDD to DataFrame. 

Also FYI, Nick mentioned something related [here|https://issues.apache.org/jira/browse/SPARK-19071?focusedCommentId=15834232&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15834232]. I think the new solution can adapt to it with a setter method. Right now, we can just focus on resolving the double-caching issue.;;;","11/Sep/17 21:11;josephkb;Hi all, it looks like there has been confusion about what has been agreed on.  This is my current understanding:

There are 2 issues:
1. This JIRA [SPARK-18608], which discusses the bug of double-caching because of misuse of {{dataset.rdd.getStorageLevel}}.  Note that [SPARK-21799] is just a special case of this bug.
2. [SPARK-21972], which discusses adding a parameter handlePersistence to allow user control over whether to cache the input data.

I recommend:
1. We should fix the current double-caching bug in master and branch-2.2.  Going from Spark 2.1 to 2.2, I've only seen a performance regression with K-Means, but I recommend we fix the bug for all cases.  This fix would be like [~podongfeng]'s original PR for https://github.com/apache/spark/pull/17014 (before adding in handlePersistence).
2. We can work on adding handlePersistence to master.  No backporting there of course.  Note that [SPARK-19422] is also related, and it may be blocked by decisions on [SPARK-21972].;;;","12/Sep/17 04:59;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/19197;;;","12/Sep/17 18:37;josephkb;Issue resolved by pull request 19197
[https://github.com/apache/spark/pull/19197];;;","13/Sep/17 11:33;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/19220;;;",,,,,,,,,,,,,,,,,,,,,,,,
Dependency list still shows that the version of org.codehaus.janino:commons-compiler is 2.7.6,SPARK-18602,13023610,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,27/Nov/16 23:25,28/Nov/16 18:10,14/Jul/23 06:29,28/Nov/16 18:10,2.1.0,,,,,,,,2.1.0,,,,Build,SQL,,,,,,,0,,,,,,"org.codehaus.janino:janino:3.0.0 depends on org.codehaus.janino:commons-compiler:3.0.0.

However, https://github.com/apache/spark/blob/branch-2.1/dev/deps/spark-deps-hadoop-2.7 still shows that commons-compiler from janino is 2.7.6. This is probably because hive module depends on calcite-core, which depends on commons-compiler 2.7.6.",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 28 18:10:06 UTC 2016,,,,,,,,,,"0|i36tn3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/16 23:47;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/16025;;;","28/Nov/16 18:10;yhuai;Issue resolved by pull request 16025
[https://github.com/apache/spark/pull/16025];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not push down filters for LEFT ANTI JOIN,SPARK-18597,13023579,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,hvanhovell,hvanhovell,hvanhovell,27/Nov/16 13:48,28/Nov/16 21:24,14/Jul/23 06:29,28/Nov/16 15:15,,,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,correctness,,,,,"The optimizer pushes down filters for left anti joins. This unfortunately has the opposite effect. For example:
{noformat}
sql(""create or replace temporary view tbl_a as values (1, 5), (2, 1), (3, 6) as t(c1, c2)"")
sql(""create or replace temporary view tbl_b as values 1 as t(c1)"")
sql(""""""
select *
from   tbl_a
       left anti join tbl_b on ((tbl_a.c1 = tbl_a.c2) is null or tbl_a.c1 = tbl_a.c2)
"""""")
{noformat}

Should return rows [1, 5], [2, 1] & [3, 6], but returns no rows.

The upside is that this will only happen when you use a really weird anti-join (only referencing the table on the left hand side).",,apachespark,codingcat,dindin5258,hvanhovell,nsyca,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18614,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 28 21:20:24 UTC 2016,,,,,,,,,,"0|i36tg7:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"28/Nov/16 00:31;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/16026;;;","28/Nov/16 01:23;nsyca;It's nice to learn through this JIRA that Spark supports {{left anti join}} externally rather just being an internal form when rewriting {{NOT EXISTS}} or {{NOT IN}}. If this is to follow the traditional join semantics, why does Spark choose not to output the columns from the right table of a {{left anti join}}? I guess the same question applies to {{left semi}} as well.;;;","28/Nov/16 16:03;hvanhovell;[~nsyca] LEFT SEMI and LEFT ANTI are both SEMI joins (half joins). A semi join only returns rows from the first table when it matches one or more rows from the second table (I got this from http://www.slideshare.net/alokeparnachoudhury/semi-joins). The anti join is the opposite, and only returns a row form the first table when it does not match any row in the second table.

Hive also supports LEFT SEMI JOIN: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins
Impala supports both LEFT SEMI and LEFT ANTI JOIN: https://www.cloudera.com/documentation/enterprise/5-7-x/topics/impala_joins.html;;;","28/Nov/16 16:07;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/16039;;;","28/Nov/16 16:48;nsyca;[~hvanhovell], [~dongjoon], [~smilegator] FYI:

I edited the description to reflect the correct behaviour of the {{LeftAnti}} in the example. The correct answer is all the rows from tbl_a as there is no row in tbl_a that satisfies the predicate tbl_a.c1 = tbl_a.c2 in the ON clause. By the LeftAnti semantics of tbl_a except rows that satisfy the predicate in the ON clause, all rows from tbl_a are returned. ;;;","28/Nov/16 16:53;hvanhovell;Thanks!;;;","28/Nov/16 17:06;nsyca;I also left another comment in the PR that {{ExistenceJoin}} should be treated the same as {{LeftOuter}} and {{LeftAnti}}, not {{InnerLike}} and {{LeftSemi}}. This is not currently exposed because the rewrite of {{\[NOT\] EXISTS OR ...}} to {{ExistenceJoin}} happens in rule {{RewritePredicateSubquery}}, which is in a separate rule set and placed after the rule {{PushPredicateThroughJoin}}. During the transformation in the rule {{PushPredicateThroughJoin}}, an ExistenceJoin never exists.

The semantics of {{ExistenceJoin}} says we need to preserve all the rows from the left table through the join operation as if it is a regular {{LeftOuter}} join. The {{ExistenceJoin}} augments the {{LeftOuter}} operation with a new column called {{exists}}, set to true when the join condition in the ON clause is true and false otherwise. The filter of any rows will happen in the {{Filter}} operation above the {{ExistenceJoin}}.

Example:

A(c1, c2): { (1, 1), (1, 2) }
B(c1): { (NULL) }   // can be any value as it is irrelevant in this example

{code:SQL}
select A.*
from   A
where  exists (select 1 from B where A.c1 = A.c2)
       or A.c2=2
{code}

In this example, the correct result is all the rows from A. If the pattern {{ExistenceJoin}} at line 935 in {{Optimizer.scala}} added by the PR of this JIRA is indeed active, the code will push down the predicate A.c1 = A.c2 to be a {{Filter}} on relation A, which will filter the row (1,2) from A.

If you agree with my analysis above, I can open a JIRA/a PR to move this piece of code from {{InnerLike}} to {{LeftOuter}}.
;;;","28/Nov/16 21:10;hvanhovell;That is a correct assessment. We will lose rows if we push down filters into the left side of an {{ExistanceJoin}}. [~nsyca] could you open a JIRA/PR for this?;;;","28/Nov/16 21:20;nsyca;SPARK-18614 is opened to deliver a fix to close this potential, not-yet-exposed, rewrite problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Name Validation of Databases/Tables,SPARK-18594,13023537,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,26/Nov/16 22:24,23/Aug/17 18:30,14/Jul/23 06:29,28/Nov/16 03:50,2.0.2,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Currently, the name validation checks are limited to table creation. It is enfored by Analyzer rule: `PreWriteCheck`.  

However, table renaming and database creation have the same issues. It makes more sense to do the checks in `SessionCatalog`. ",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 26 22:25:08 UTC 2016,,,,,,,,,,"0|i36t6v:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"26/Nov/16 22:25;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16018;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBCRDD returns incorrect results for filters on CHAR of PostgreSQL,SPARK-18593,13023525,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maropu,DurgaPrasad16,DurgaPrasad16,26/Nov/16 17:50,06/Dec/16 02:48,14/Jul/23 06:29,03/Dec/16 05:19,1.6.2,1.6.3,,,,,,,2.0.0,,,,SQL,,,,,,,,1,correctness,,,,,"In Apache Spark 1.6.x, JDBCRDD returns incorrect results for a query with filters on CHAR column with PostgreSQL CHAR type. The root cause is PostgreSQL returns `space padded string` for a result. So, the post processing filter `Filter (a#0 = A)` is evaluated false. Spark 2.0.0 removes the post filter because it is already handled in the database by `PushedFilters: [EqualTo(a,A)]`.

{code}
scala> val t_char = sqlContext.read.option(""user"", ""postgres"").option(""password"", ""rootpass"").jdbc(""jdbc:postgresql://localhost:5432/postgres"", ""t_char"", new java.util.Properties())
t_char: org.apache.spark.sql.DataFrame = [a: string]

scala> val t_varchar = sqlContext.read.option(""user"", ""postgres"").option(""password"", ""rootpass"").jdbc(""jdbc:postgresql://localhost:5432/postgres"", ""t_varchar"", new java.util.Properties())
t_varchar: org.apache.spark.sql.DataFrame = [a: string]

scala> t_char.show
+----------+
|         a|
+----------+
|A         |
|AA        |
|AAA       |
+----------+


scala> t_varchar.show
+---+
|  a|
+---+
|  A|
| AA|
|AAA|
+---+


scala> t_char.filter(t_char(""a"")===""A"").show
+---+
|  a|
+---+
+---+


scala> t_char.filter(t_char(""a"")===""A         "").show
+----------+
|         a|
+----------+
|A         |
+----------+


scala> t_varchar.filter(t_varchar(""a"")===""A"").show
+---+
|  a|
+---+
|  A|
+---+


scala> t_char.filter(t_char(""a"")===""A"").explain
== Physical Plan ==
Filter (a#0 = A)
+- Scan JDBCRelation(jdbc:postgresql://localhost:5432/postgres,t_char,[Lorg.apache.spark.Partition;@2f65c341,{user=postgres, password=rootpass})[a#0] PushedFilters: [EqualTo(a,A)]
{code}",,apachespark,dongjoon,DurgaPrasad16,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 06 02:48:43 UTC 2016,,,,,,,,,,"0|i36t47:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/16 20:01;dongjoon;Hi, [~DurgaPrasad16]
Does this happen only on JDBC?;;;","26/Nov/16 21:03;dongjoon;For me, Spark `master` (2.x) and `branch-1.6` seems to have no problem like this.
Could you try latest official Spark 1.6.3 release? It's the same with `branch-1.6` master.;;;","26/Nov/16 21:58;dongjoon;Oh, sorry. I reproduced this in `branch-1.6`.

- When I added a testcase in JDBCWriter. There was no problem in both version. It doesn't use PostgreSQL driver.

- However, when I tried in spark-shell with postgresql driver. Only `branch-1.6` has the same problem.
{code}
scala> val df = sqlContext.read.option(""user"", ""postgres"").option(""password"", ""test"").jdbc(""jdbc:postgresql://localhost:5432/postgres"", ""t1"", new java.util.Properties())
df: org.apache.spark.sql.DataFrame = [a: string]

scala> df.show
+----+
|   a|
+----+
|A   |
+----+


scala> df.filter(df(""a"")===""A"").show
+---+
|  a|
+---+
+---+
{code}

I'll take a look at this.;;;","26/Nov/16 22:06;dongjoon;Maybe, it's not an issue about *data length is one*.
{code}
scala> df.show
+----+
|   a|
+----+
|A   |
|A   |
|AA  |
+----+


scala> df.filter(df(""a"")===""AA"").show
+---+
|  a|
+---+
+---+
{code};;;","27/Nov/16 04:59;dongjoon;In addition, this does not occur with MySQL.;;;","27/Nov/16 10:18;dongjoon;Although this is correctness issue, there is a workaround for this issue. Users can use TEXT or VARCHAR.
In addition, I'm not sure we will have Apache Spark 1.6.4.

BTW, the only correct way to fix this issue is a backport of related feature. So, I'll make a PR for this.
;;;","27/Nov/16 11:32;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/16021;;;","03/Dec/16 05:19;dongjoon;This was fixed since 2.0.0 by several issue (SPARK-12409, SPARK-12387, SPARK-12391).;;;","03/Dec/16 05:20;dongjoon;This will not be backported. For the discussion and the corresponding patches for Spark 1.6.x, please refer the above PR.;;;","06/Dec/16 02:48;dongjoon;Oops. Thank you for correction.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"persist() resolves ""java.lang.RuntimeException: Invalid PythonUDF <lambda>(...), requires attributes from more than one child""",SPARK-18589,13023447,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,nchammas,nchammas,25/Nov/16 21:27,23/Mar/17 14:06,14/Jul/23 06:29,21/Jan/17 00:12,2.0.2,2.1.0,,,,,,,2.1.1,2.2.0,,,PySpark,SQL,,,,,,,1,,,,,,"Smells like another optimizer bug that's similar to SPARK-17100 and SPARK-18254. I'm seeing this on 2.0.2 and on master at commit {{fb07bbe575aabe68422fd3a31865101fb7fa1722}}.

I don't have a minimal repro for this yet, but the error I'm seeing is:

{code}
py4j.protocol.Py4JJavaError: An error occurred while calling o247.count.
: java.lang.RuntimeException: Invalid PythonUDF <...>(...), requires attributes from more than one child.
    at scala.sys.package$.error(package.scala:27)
    at org.apache.spark.sql.execution.python.ExtractPythonUDFs$$anonfun$org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract$2.apply(ExtractPythonUDFs.scala:150)
    at org.apache.spark.sql.execution.python.ExtractPythonUDFs$$anonfun$org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract$2.apply(ExtractPythonUDFs.scala:149)
    at scala.collection.immutable.Stream.foreach(Stream.scala:594)
    at org.apache.spark.sql.execution.python.ExtractPythonUDFs$.org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(ExtractPythonUDFs.scala:149)
    at org.apache.spark.sql.execution.python.ExtractPythonUDFs$$anonfun$apply$2.applyOrElse(ExtractPythonUDFs.scala:114)
    at org.apache.spark.sql.execution.python.ExtractPythonUDFs$$anonfun$apply$2.applyOrElse(ExtractPythonUDFs.scala:113)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$2.apply(TreeNode.scala:312)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$2.apply(TreeNode.scala:312)
    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:311)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
    at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
    at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
    at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
    at org.apache.spark.sql.execution.python.ExtractPythonUDFs$.apply(ExtractPythonUDFs.scala:113)
    at org.apache.spark.sql.execution.python.ExtractPythonUDFs$.apply(ExtractPythonUDFs.scala:93)
    at org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:93)
    at org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:93)
    at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
    at scala.collection.immutable.List.foldLeft(List.scala:84)
    at org.apache.spark.sql.execution.QueryExecution.prepareForExecution(QueryExecution.scala:93)
    at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:83)
    at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:83)
    at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2555)
    at org.apache.spark.sql.Dataset.count(Dataset.scala:2226)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:280)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:214)
    at java.lang.Thread.run(Thread.java:745)
{code}

The extended plan (cleaned of field names) is as follows:

{code}
== Parsed Logical Plan ==
'Filter NOT ('expected_prediction = 'prediction)
+- Project [p1, p2, pair_features, rawPrediction, probability, prediction, cast((p1._testing_universal_key = p2._testing_universal_key) as float) AS expected_prediction]
   +- Project [p1, p2, pair_features, rawPrediction, probability, UDF(rawPrediction) AS prediction]
      +- Project [p1, p2, pair_features, rawPrediction, UDF(rawPrediction) AS probability]
         +- Project [p1, p2, pair_features, UDF(pair_features) AS rawPrediction]
            +- Project [p1, p2, <lambda>(p1.person, p2.person) AS pair_features]
               +- Project [struct(...) AS p1, struct(...) AS p2]
                  +- Project [_blocking_key, ..., ...]
                     +- Join Inner, (_blocking_key = _blocking_key)
                        :- SubqueryAlias p1
                        :  +- Project [..., <lambda>(dataset_name, primary_key, person) AS _blocking_key]
                        :     +- Project [...]
                        :        +- Project [primary_key, universal_key, _testing_universal_key, struct(...) AS person]
                        :           +- Project [...]
                        :              +- Project [_testing_universal_key, primary_key, struct(...) AS person]
                        :                 +- LogicalRDD [...]
                        +- SubqueryAlias p2
                           +- Project [..., <lambda>(dataset_name, primary_key, person) AS _blocking_key]
                              +- Project [...]
                                 +- Project [primary_key, universal_key, _testing_universal_key, struct(...) AS person]
                                    +- Project [...]
                                       +- Project [_testing_universal_key, primary_key, struct(...) AS person]
                                          +- LogicalRDD [...]

== Analyzed Logical Plan ==
p1: struct<...>, p2: struct<...>, pair_features: vector, rawPrediction: vector, probability: vector, prediction: double, expected_prediction: float
Filter NOT (cast(expected_prediction as double) = prediction)
+- Project [p1, p2, pair_features, rawPrediction, probability, prediction, cast((p1._testing_universal_key = p2._testing_universal_key) as float) AS expected_prediction]
   +- Project [p1, p2, pair_features, rawPrediction, probability, UDF(rawPrediction) AS prediction]
      +- Project [p1, p2, pair_features, rawPrediction, UDF(rawPrediction) AS probability]
         +- Project [p1, p2, pair_features, UDF(pair_features) AS rawPrediction]
            +- Project [p1, p2, <lambda>(p1.person, p2.person) AS pair_features]
               +- Project [struct(...) AS p1, struct(...) AS p2]
                  +- Project [_blocking_key, ..., ...]
                     +- Join Inner, (_blocking_key = _blocking_key)
                        :- SubqueryAlias p1
                        :  +- Project [..., <lambda>(dataset_name, primary_key, person) AS _blocking_key]
                        :     +- Project [...]
                        :        +- Project [primary_key, universal_key, _testing_universal_key, struct(...) AS person]
                        :           +- Project [...]
                        :              +- Project [_testing_universal_key, primary_key, struct(...) AS person]
                        :                 +- LogicalRDD [...]
                        +- SubqueryAlias p2
                           +- Project [..., <lambda>(dataset_name, primary_key, person) AS _blocking_key]
                              +- Project [...]
                                 +- Project [primary_key, universal_key, _testing_universal_key, struct(...) AS person]
                                    +- Project [...]
                                       +- Project [_testing_universal_key, primary_key, struct(...) AS person]
                                          +- LogicalRDD [...]

== Optimized Logical Plan ==
Project [struct(...) AS p1, struct(...) AS p2, <lambda>(struct(...).person, struct(...).person) AS pair_features, UDF(<lambda>(struct(...).person, struct(...).person)) AS rawPrediction, UDF(UDF(<lambda>(struct(...).person, struct(...).person))) AS probability, UDF(UDF(<lambda>(struct(...).person, struct(...).person))) AS prediction, cast((struct(...)._testing_universal_key = struct(...)._testing_universal_key) as float) AS expected_prediction]
+- Join Inner, (NOT (cast(cast((struct(...)._testing_universal_key = struct(...)._testing_universal_key) as float) as double) = UDF(UDF(<lambda>(struct(...).person, struct(...).person)))) && (_blocking_key = _blocking_key))
   :- Project [..., <lambda>(dataset_name, primary_key, person) AS _blocking_key]
   :  +- Filter isnotnull(<lambda>(dataset_name, primary_key, person))
   :     +- InMemoryRelation [...], true, 10000, StorageLevel(memory, 1 replicas)
   :        :  +- *Project [primary_key, struct(...) AS person, test_people AS dataset_name]
   :        :     +- Scan ExistingRDD[...]
   +- Project [..., <lambda>(dataset_name, primary_key, person) AS _blocking_key]
      +- Filter isnotnull(<lambda>(dataset_name, primary_key, person))
         +- InMemoryRelation [...], true, 10000, StorageLevel(memory, 1 replicas)
            :  +- *Project [primary_key, struct(...) AS person, test_people AS dataset_name]
            :     +- Scan ExistingRDD[...]

== Physical Plan ==
java.lang.RuntimeException: Invalid PythonUDF <lambda>(struct(...).person, struct(...).person), requires attributes from more than one child.
{code}

Note the error at the end when Spark tries to print the physical plan. I've scrubbed some Project fields from the plan to simplify the display, but if I've scrubbed anything you think is important let me know.

I can get around this problem by adding a {{persist()}} right before the operation that fails. The failing operation is a filter.

Any clues on how I can boil this down to a minimal repro? Any clues about where the problem is?
","Python 3.5, Java 8",apachespark,felixcheung,franklynDsouza,glenn.strycker@gmail.com,nchammas,nealhu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19728,,,,,,,,,,,,,,SPARK-17100,SPARK-18254,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 13 20:05:01 UTC 2017,,,,,,,,,,"0|i36smv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/16 21:28;nchammas;cc [~davies] [~hvanhovell];;;","19/Dec/16 17:20;franklynDsouza;The sequence of steps that causes this are:

{code}
join two dataframes A and B > make a udf that uses one column from A and another from B > filter on column produced by udf > java.lang.RuntimeException: Invalid PythonUDF <lambda>(b#1L, c#6L), requires attributes from more than one child.
{code}

Here are some minimum steps to reproduce this issue in pyspark

{code}
from pyspark.sql import types
from pyspark.sql import functions as F
df1 = sqlCtx.createDataFrame([types.Row(a=1, b=2), types.Row(a=1, b=4)])
df2 = sqlCtx.createDataFrame([types.Row(a=1, c=12)])
joined = df1.join(df2, df1['a'] == df2['a'])
extra = joined.withColumn('sum', F.udf(lambda a,b : a+b, types.IntegerType())(joined['b'], joined['c']))
filtered = extra.where(extra['sum'] < F.lit(10)).collect()
{code}

*doing extra.cache() before the filtering will fix the issue* but obviously isn't a solution.
;;;","13/Jan/17 20:05;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/16581;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSourceStressForDontFailOnDataLossSuite is flaky,SPARK-18588,13023408,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,hvanhovell,hvanhovell,25/Nov/16 14:26,21/Dec/16 23:40,14/Jul/23 06:29,21/Dec/16 23:40,,,,,,,,,2.1.1,2.2.0,,,Structured Streaming,,,,,,,,0,,,,,,https://spark-tests.appspot.com/test-details?suite_name=org.apache.spark.sql.kafka010.KafkaSourceStressForDontFailOnDataLossSuite&test_name=stress+test+for+failOnDataLoss%3Dfalse,,apachespark,codingcat,dongjoon,hvanhovell,tdas,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 21 23:40:34 UTC 2016,,,,,,,,,,"0|i36se7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/16 14:26;hvanhovell;cc [~zsxwing];;;","29/Nov/16 04:21;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16051;;;","01/Dec/16 21:48;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16109;;;","07/Dec/16 21:48;tdas;Issue resolved by pull request 16109
[https://github.com/apache/spark/pull/16109];;;","09/Dec/16 18:03;zsxwing;It's still flaky;;;","14/Dec/16 02:10;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16275;;;","16/Dec/16 18:03;dongjoon;This was merged at the following commit in RC5.

https://github.com/apache/spark/commit/019d1fa3d421b5750170429fc07b204692b7b58e;;;","16/Dec/16 18:35;zsxwing;[~dongjoon] This commit just disabled the test. We still need to fix the test and I'm working on it.;;;","16/Dec/16 18:37;dongjoon;Oh, thank you for fixing this!;;;","19/Dec/16 22:57;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16282;;;","21/Dec/16 23:40;tdas;Issue resolved by pull request 16282
[https://github.com/apache/spark/pull/16282];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
netty-3.8.0.Final.jar has vulnerability CVE-2014-3488  and CVE-2014-0193,SPARK-18586,13023319,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,meiyoula,meiyoula,25/Nov/16 07:50,03/Dec/16 09:54,14/Jul/23 06:29,03/Dec/16 09:54,,,,,,,,,2.2.0,,,,Build,,,,,,,,0,,,,,,,,apachespark,meiyoula,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 03 09:54:52 UTC 2016,,,,,,,,,,"0|i36ruf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/16 09:56;srowen;Spark doesn't use netty 3, but it is pulled in as a transitive dependency. We can't get rid of it, but, it also isn't even necessarily exposed. 
Do these CVEs even affect Spark? We can try managing the version up to 3.8.3 to resolve one, or 3.9.x to resolve both, but this won't change the version of Netty that ends up on the classpath if deploying on an existing cluster.;;;","01/Dec/16 16:01;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16102;;;","03/Dec/16 09:54;srowen;Issue resolved by pull request 16102
[https://github.com/apache/spark/pull/16102];;;","03/Dec/16 09:54;srowen;I don't think the CVE actually affected Spark, as Netty 3 isn't directly used, but I updated it anyway.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix nullability of InputFileName.,SPARK-18583,13023280,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ueshin,ueshin,ueshin,25/Nov/16 03:02,26/Nov/16 04:24,14/Jul/23 06:29,26/Nov/16 04:24,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,The nullability of {{InputFileName}} should be {{false}}.,,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 25 03:06:04 UTC 2016,,,,,,,,,,"0|i36rlr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/16 03:06;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/16007;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-csv strips whitespace (pyspark) ,SPARK-18579,13023230,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,abridgett,abridgett,24/Nov/16 16:08,12/Dec/22 18:11,14/Jul/23 06:29,23/Mar/17 07:25,2.0.2,,,,,,,,2.2.0,,,,Input/Output,,,,,,,,0,,,,,,"ignoreLeadingWhiteSpace and ignoreTrailingWhiteSpace are supported on CSV reader (and defaults to false).

However these are not supported options on the CSV writer and so the library defaults take place which strips the whitespace.

I think it would make the most sense if the writer semantics matched the reader (and did not alter the data) however this would be a change in behaviour.  In any case it'd be great to have the _option_ to strip or not.",,514793425@qq.com,abridgett,apachespark,tenstriker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19942,SPARK-21442,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 19 04:49:02 UTC 2017,,,,,,,,,,"0|i36ran:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"26/Jan/17 13:57;gurwls223;Can we just strip them within the dataframe/dataset? Available options for read/write are well documented. IMHO, we should not just add options/APIs just for consistency.;;;","28/Feb/17 23:33;gurwls223;Oh, I overlooked. You meant it always strips the white spaces when writing out via CSV datasource?;;;","28/Feb/17 23:36;abridgett;Yep, that's right. Sorry - not sure why I didn't reply to your initial comment!;;;","14/Mar/17 00:51;tenstriker;I am having same issue. Created SPARK-19942. It seems like this was a issue in spark-csv module as well and they fixed it but not sure it got ported in spark module.;;;","14/Mar/17 00:56;gurwls223;It is not implemented in spark-csv library (see https://github.com/databricks/spark-csv/pull/306) I raised this PR but at that time CSV datasource was ported into Spark. So, I decided to close that.

For CSV datasource in Spark, I think it is worth checking it. I will check this when I have some time.

BTW, let me close  SPARK-19942 as a duplicate as you said.;;;","16/Mar/17 13:54;gurwls223;I submitted a PR for this https://github.com/apache/spark/pull/17310 but it seems not adding a link automatically for now.;;;","19/Mar/17 04:49;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/17310;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Full outer join in correlated subquery returns incorrect results,SPARK-18578,13023204,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nsyca,nsyca,nsyca,24/Nov/16 14:49,14/May/19 06:27,14/Jul/23 06:29,24/Nov/16 20:10,2.0.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,correctness,,,,,"Full outer join with a correlated predicate in the left operand in a subquery may return incorrect results.

Example:
{code}
Seq(1).toDF(""c1"").createOrReplaceTempView(""t1"")
Seq(2).toDF(""c1"").createOrReplaceTempView(""t2"")
Seq(1).toDF(""c1"").createOrReplaceTempView(""t3"")

// Test case: 01 EXISTS subquery context
// Expected result: 1 row
// Actual result:   0 row
sql(""select * from t1 where exists (select 1 from (select c1 from t2 where t1.c1 = 2) t2 full join t3 on t2.c1=t3.c1)"").show

// Test case: 02 Scalar subquery context
// Expected result: 1 row of value 1
// Actual result:   1 row of NULL
// Note adding the meaningless equi-join T1.c1=T2.c1 to get through the CROSS JOIN restriction
sql(""select (select max(1) from (select c1 from t2 where t1.c1 = 2 and t1.c1=t2.c1) t2 full join t3 on t2.c1=t3.c1) from t1"").show
{code}",,apachespark,nsyca,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18455,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 24 16:01:04 UTC 2016,,,,,,,,,,"0|i36r4v:",9223372036854775807,,,,,hvanhovell,,,,,,,,,,,,,,,,,,,"24/Nov/16 14:57;nsyca;Note that correlated predicates in the right operand of the FOJ is not a problem because it is protected in the block of code below in 

{code}
private def pullOutCorrelatedPredicates( ... )
...
        case j @ Join(_, right, jt, _) if !jt.isInstanceOf[InnerLike] =>
          failOnOuterReference(j)
          failOnOuterReferenceInSubTree(right, ""a LEFT (OUTER) JOIN"")
          j
{code}

However, the error message is misleading as it reports an error on a LEFT JOIN.

{code}
Accessing outer query column is not allowed in a LEFT (OUTER) JOIN: 
{code}

I propose to block the incorrect result case in the FOJ at this moment and change the misleading error message case.;;;","24/Nov/16 16:01;apachespark;User 'nsyca' has created a pull request for this issue:
https://github.com/apache/spark/pull/16005;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix HLL++ with small relative error,SPARK-18559,13022781,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ZenWzh,ZenWzh,ZenWzh,23/Nov/16 08:27,12/Dec/22 18:10,14/Jul/23 06:29,18/Jan/17 15:41,,,,,,,,,2.1.1,2.2.0,,,SQL,,,,,,,,0,,,,,,"In HyperLogLogPlusPlus, if the relative error is so small that p >= 19, it will cause ArrayIndexOutOfBoundsException in THRESHOLDS(p-4) . We should check p and when p >= 19, regress to the original HLL result and use the small range correction they use.",,apachespark,ZenWzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 18 15:23:41 UTC 2017,,,,,,,,,,"0|i36oiv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/16 08:37;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/15990;;;","18/Jan/17 15:23;gurwls223;[~srowen], it seems mistakenly not resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
na.fill miss up original values in long integers,SPARK-18555,13022730,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,windpiger,mahmoudr,mahmoudr,23/Nov/16 03:58,11/Apr/17 00:38,14/Jul/23 06:29,06/Dec/16 02:39,2.0.0,2.0.1,2.0.2,,,,,,2.0.3,2.1.1,2.2.0,,SQL,,,,,,,,1,,,,,,"Manly the issue is clarified in the following example:
Given a Dataset: 

scala> data.show

|                  a|                  b|
|                  1|                  2|
|                 -1|                 -2|
|9123146099426677101|9123146560113991650|


theoretically when we call na.fill(0) nothing should change, while the current result is:

scala> data.na.fill(0).show

|                  a|                  b|
|                  1|                  2|
|                 -1|                 -2|
|9123146099426676736|9123146560113991680|
",,apachespark,mahmoudr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19375,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 11 00:38:05 UTC 2017,,,,,,,,,,"0|i36o7j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/16 13:46;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/15994;;;","11/Apr/17 00:38;apachespark;User 'dbtsai' has created a pull request for this issue:
https://github.com/apache/spark/pull/17600;;;","11/Apr/17 00:38;apachespark;User 'dbtsai' has created a pull request for this issue:
https://github.com/apache/spark/pull/17601;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executor loss may cause TaskSetManager to be leaked,SPARK-18553,13022718,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,23/Nov/16 03:08,17/May/20 17:46,14/Jul/23 06:29,01/Dec/16 19:28,1.6.0,2.0.0,2.1.0,,,,,,1.6.4,2.0.3,2.1.0,2.2.0,Scheduler,Spark Core,,,,,,,0,,,,,,"Due to a bug in TaskSchedulerImpl, the complete sudden loss of an executor may cause a TaskSetManager to be leaked, causing ShuffleDependencies and other data structures to be kept alive indefinitely, leading to various types of resource leaks (including shuffle file leaks).

In a nutshell, the problem is that TaskSchedulerImpl did not maintain its own mapping from executorId to running task ids, leaving it unable to clean up taskId to taskSetManager maps when an executor is totally lost.",,apachespark,java8964,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 30 00:59:06 UTC 2016,,,,,,,,,,"0|i36o4v:",9223372036854775807,,,,,,,,,,,,,1.6.4,2.0.3,2.1.0,,,,,,,,,"23/Nov/16 03:30;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15986;;;","28/Nov/16 22:01;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16045;;;","30/Nov/16 00:59;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16070;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to Uncache a View that References a Dropped Table.,SPARK-18549,13022662,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,cloud_fan,smilegator,smilegator,22/Nov/16 23:20,17/Mar/17 02:59,14/Jul/23 06:29,07/Mar/17 17:22,2.0.2,,,,,,,,2.1.1,2.2.0,,,SQL,,,,,,,,0,,,,,,"{code}
      spark.range(1, 10).toDF(""id1"").write.format(""json"").saveAsTable(""jt1"")
      spark.range(1, 10).toDF(""id2"").write.format(""json"").saveAsTable(""jt2"")
      sql(""CREATE VIEW testView AS SELECT * FROM jt1 JOIN jt2 ON id1 == id2"")
      // Cache is empty at the beginning
      assert(spark.sharedState.cacheManager.isEmpty)
      sql(""CACHE TABLE testView"")
      assert(spark.catalog.isCached(""testView""))
      // Cache is not empty
      assert(!spark.sharedState.cacheManager.isEmpty)
{code}

{code}
      // drop a table referenced by a cached view
      sql(""DROP TABLE jt1"")

-- So far everything is fine

      // Failed to unache the view
      val e = intercept[AnalysisException] {
        sql(""UNCACHE TABLE testView"")
      }.getMessage
      assert(e.contains(""Table or view not found: `default`.`jt1`""))

      // We are unable to drop it from the cache
      assert(!spark.sharedState.cacheManager.isEmpty)
{code}",,apachespark,dongjoon,glenn.strycker@gmail.com,hvanhovell,jiangxb1987,rxin,smilegator,umesh9794@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18169,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 07 06:41:03 UTC 2017,,,,,,,,,,"0|i36nsf:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"22/Nov/16 23:24;smilegator;cc [~hvanhovell] [~dongjoon] [~rxin] [~marmbrus] [~cloud_fan]

Any suggestion is welcomed. ;;;","22/Nov/16 23:27;dongjoon;`InMemoryRelation` of `CachedData` has `tableName: Option[String]`. Can we use that?;;;","23/Nov/16 17:14;smilegator;Are you suggesting name-based resolution? This is risky. You know, cache manager is cross session. It is very hard for us to determine which temporary views, views, tables, or global temporary tables are the original cached data.

Cross session unique ID might be needed. However, I am not sure this is a right direction. 

BTW, this could become worse if we re-implement the views in Spark 2.2. 

;;;","23/Nov/16 18:46;dongjoon;Yes, I meant it as a the fallback operation for the dropped table case.;;;","23/Nov/16 21:06;smilegator;Using name resolution is not a clean fix. We might uncache the ones that should not be uncached.;;;","23/Nov/16 21:12;smilegator;Another way is to uncache all the impacted cached data when we drop a table? Is it desired?;;;","23/Nov/16 23:47;hvanhovell;That only works for the hive client which is dropping the table. You can have multiple hive clients talking to the same database.;;;","24/Nov/16 01:28;smilegator;True.;;;","24/Nov/16 08:51;jiangxb1987;Do you think we can resolve this problem after we have added the dependency management? On drop table `jt1`, we can invalid any view that references the table.;;;","26/Nov/16 22:50;smilegator;Could you explain more about the design of dependency management?;;;","27/Nov/16 06:27;jiangxb1987;Currently we barely don't manage a view's references, when a table/view is changed, all views that reference(directly or in-directly) the table/view are not notified.
Perhaps a better approach is to keep the references of a view in a variable, say 'referenceTables'. On DDL commands, like Create/Drop/Change tables, we search for the views that references the current table, and examine whether they are valid after the operation, if not, we should invalidate them as soon as possible.
I haven't generate a detailed plan to this, but I'm sure it will be useful. Any further disscussion is welcomed. Thank you!;;;","27/Nov/16 06:54;smilegator;Many RDBMS stores the dependency between tables/views. See 
- DB2: http://www.ibm.com/support/knowledgecenter/SSEPGG_9.7.0/com.ibm.db2.luw.sql.ref.doc/doc/r0001067.html
- Oracle: https://docs.oracle.com/cd/B19306_01/server.102/b14237/statviews_1041.htm#i1576452

My first question is how to physically store the table/view dependency in Hive metastore? ;;;","28/Nov/16 15:42;jiangxb1987;I'd suggest we store the view dependency in the view's catalogTable, note that we only do this for permenant views, temp/global temp views don't participant in the dependency management issue.;;;","29/Nov/16 14:57;dongjoon;Shall we retarget this into 2.2.0?;;;","05/Dec/16 20:10;hvanhovell;Yeah, lets retarget this.;;;","07/Mar/17 06:41;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/17097;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsafeShuffleWriter corrupts encrypted shuffle files when merging,SPARK-18546,13022634,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,vanzin,vanzin,vanzin,22/Nov/16 21:19,08/Dec/16 07:42,14/Jul/23 06:29,30/Nov/16 22:11,2.1.0,,,,,,,,2.1.0,,,,Spark Core,,,,,,,,0,,,,,,"The merging algorithm in {{UnsafeShuffleWriter}} does not consider encryption, and when it tries to merge encrypted files the result data cannot be read, since data encrypted with different initial vectors is interleaved in the same partition data. This leads to exceptions when trying to read the files during shuffle:

{noformat}
com.esotericsoftware.kryo.KryoException: com.ning.compress.lzf.LZFException: Corrupt input data, block did not start with 2 byte signature ('ZV') followed by type byte, 2-byte length)
	at com.esotericsoftware.kryo.io.Input.fill(Input.java:142)
	at com.esotericsoftware.kryo.io.Input.require(Input.java:155)
	at com.esotericsoftware.kryo.io.Input.readInt(Input.java:337)
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:109)
	at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:610)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:721)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:228)
	at org.apache.spark.serializer.DeserializationStream.readKey(Serializer.scala:169)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap$DiskMapIterator.readNextItem(ExternalAppendOnlyMap.scala:512)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap$DiskMapIterator.hasNext(ExternalAppendOnlyMap.scala:533)
...
{noformat}

(This is our internal branch so don't worry if lines don't necessarily match.)
",,apachespark,codingcat,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18547,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 22 21:29:08 UTC 2016,,,,,,,,,,"0|i36nm7:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"22/Nov/16 21:22;vanzin;I'm marking this as blocked by SPARK-18547 because that feature allows adding the necessary unit tests for making sure this bug is fixed.;;;","22/Nov/16 21:29;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/15982;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot filter by nonexisting column in parquet file,SPARK-18539,13022481,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,dongjoon,v-gerasimov,v-gerasimov,22/Nov/16 11:55,23/Aug/17 18:09,14/Jul/23 06:29,03/Feb/17 19:12,2.0.1,2.0.2,,,,,,,2.2.0,,,,,,,,,,,,2,,,,,,"{code}
  import org.apache.spark.SparkConf
  import org.apache.spark.sql.SparkSession
  import org.apache.spark.sql.types.DataTypes._
  import org.apache.spark.sql.types.{StructField, StructType}

  val sc = SparkSession.builder().config(new SparkConf().setMaster(""local"")).getOrCreate()

  val jsonRDD = sc.sparkContext.parallelize(Seq(""""""{""a"":1}""""""))

  sc.read
    .schema(StructType(Seq(StructField(""a"", IntegerType))))
    .json(jsonRDD)
    .write
    .parquet(""/tmp/test"")

  sc.read
    .schema(StructType(Seq(StructField(""a"", IntegerType), StructField(""b"", IntegerType, nullable = true))))
    .load(""/tmp/test"")
    .createOrReplaceTempView(""table"")

  sc.sql(""select b from table where b is not null"").show()
{code}
returns:
{code}
16/11/22 17:43:47 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.IllegalArgumentException: Column [b] was not found in schema!
	at org.apache.parquet.Preconditions.checkArgument(Preconditions.java:55)
	at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.getColumnDescriptor(SchemaCompatibilityValidator.java:190)
	at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumn(SchemaCompatibilityValidator.java:178)
	at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumnFilterPredicate(SchemaCompatibilityValidator.java:160)
	at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:100)
	at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:59)
	at org.apache.parquet.filter2.predicate.Operators$NotEq.accept(Operators.java:194)
	at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validate(SchemaCompatibilityValidator.java:64)
	at org.apache.parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:59)
	at org.apache.parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:40)
	at org.apache.parquet.filter2.compat.FilterCompat$FilterPredicateCompat.accept(FilterCompat.java:126)
	at org.apache.parquet.filter2.compat.RowGroupFilter.filterRowGroups(RowGroupFilter.java:46)
	at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:110)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:109)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1.apply(ParquetFileFormat.scala:367)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1.apply(ParquetFileFormat.scala:341)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:116)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:91)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}
expected result:
{code}
+---+
|  b|
+---+
+---+
{code}

It works fine in 2.0.0 and 1.6.2. However, if I only select the nonexisting column (without filter) it also works fine.

Query plan:
{code}
== Parsed Logical Plan ==
'Project ['b]
+- 'Filter isnotnull('b)
   +- 'UnresolvedRelation `table`

== Analyzed Logical Plan ==
b: int
Project [b#8]
+- Filter isnotnull(b#8)
   +- SubqueryAlias table
      +- Relation[a#7,b#8] parquet

== Optimized Logical Plan ==
Project [b#8]
+- Filter isnotnull(b#8)
   +- Relation[a#7,b#8] parquet

== Physical Plan ==
*Project [b#8]
+- *Filter isnotnull(b#8)
   +- *BatchedScan parquet [b#8] Format: ParquetFormat, InputPaths: file:/tmp/test, PartitionFilters: [], PushedFilters: [IsNotNull(b)], ReadSchema: struct<b:int>
{code}",,apachespark,dongjoon,java8964,lian cheng,rxin,smilegator,v-gerasimov,viirya,xwu0226,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PARQUET-389,SPARK-19409,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 03 19:11:39 UTC 2017,,,,,,,,,,"0|i36mo7:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"26/Nov/16 11:42;dongjoon;Interesting. Is it valid for invalid `b`?
{code}
sc.read
    .schema(StructType(Seq(StructField(""a"", IntegerType), StructField(""b"", IntegerType, nullable = true))))
    .load(""/tmp/test"")
    .createOrReplaceTempView(""table"")
{code};;;","27/Nov/16 09:23;v-gerasimov;[~dongjoon] I don't know. However, it works fine in previous versions of spark including Spark 2.0.0. I think this is normal to apply schema with nullable fields.;;;","04/Dec/16 22:36;dongjoon;It looks like the predicates are pushed down to Parquet and Parquet is screaming about that.
I'm not sure the original behavior is desirable.

[~smilegator]. How do you think about the previous behavior?;;;","05/Dec/16 01:48;smilegator;The parquet filter push-down of Spark 2.x is different from the Spark 1.x. Since Spark 2.0, parquet scan starts using vectorization. Unfortunately, Spark 2.0.0 had a serious bug in filter push-down : https://issues.apache.org/jira/browse/SPARK-15639 

After the fix was merged into Spark 2.0.1, you hit the behavior difference. We always respect user-specified schemas. When user-specified schema does not match the actual data schema, the behaviors are not well-defined. You might hit different errors or unexpected results. The behaviors could be different when you choose different formats.

Let me dig it deeper about this specific issues and might provide you a better answer later;;;","05/Dec/16 03:04;dongjoon;Thank you so much!;;;","05/Dec/16 05:40;v-gerasimov;Thank you for your reply. I think we need to make user-specified schemas (that do not match the actual data schema) work correctly e.g. I have parquet files that are generated every day and have field `a`, in some time I add nullable field `b`, so I don't want to add that field to previous files, but I want my application can read both of them. It's like schema merging.;;;","05/Dec/16 05:46;dongjoon;The use case makes sense. I see now!;;;","05/Dec/16 05:50;smilegator;Could you turn off `spark.sql.parquet.filterPushdown`? Is that acceptable in your use case?;;;","05/Dec/16 06:24;v-gerasimov;If I turn off `spark.sql.parquet.filterPushdown` it works correctly. But I would prefer to use filter pushdown optimization in my case.;;;","05/Dec/16 06:27;smilegator;Basically, we have to know whether a column exists or not before executing/processing a query. Otherwise, we are unable to know whether a filter can be pushed down or not. 

To completely resolve the issue, we need to infer the schema every time when we need to read the external file. Schema inference is very expensive when we need to scan many many (small) files. Thus, it does not make sense for Spark to do it. Right?;;;","05/Dec/16 06:32;v-gerasimov;Hmm.. How it works when we use schema merging? Does it also very expensive when we need to scan many files, doesn't it?;;;","05/Dec/16 06:35;smilegator;Yeah. It is very slow when you have many many small parquet files.;;;","05/Dec/16 06:47;smilegator;FYI, I checked the other formats, CSV and JSON work as expected. No error is issued. Just an empty data set. However, ORC is even worse. It does not tolerate it if users specify a schema with extra non-existent columns, no matter whether it is used in the filter or not.;;;","05/Dec/16 06:55;v-gerasimov;If we can neglect the performance when we use schema merging, maybe we can neglect the performance when we use user-specified schemas, possibly we should make a new parameter for using this. I think small files are not very big problem, in this case, so the problem is I can't normally use user-specified schemas with filter pushdown optimization. What do you think?;;;","05/Dec/16 06:58;v-gerasimov;I think this is another reason why we should make user-specified schemas working correctly with parquet and ORC, it would be great have the same behavior for all formats.;;;","05/Dec/16 07:32;smilegator;The default of `spark.sql.parquet.mergeSchema` is false. To figure out the schema of parquet, the default behavior is based on a much cheaper solution

{noformat}
    // Always tries the summary files first if users don't require a merged schema.  In this case,
    // ""_common_metadata"" is more preferable than ""_metadata"" because it doesn't contain row
    // groups information, and could be much smaller for large Parquet files with lots of row
    // groups.  If no summary file is available, falls back to some random part-file.
{noformat}

It might not always resolve your case.

If we introduce such a parameter to always infer the schema and compare the inferred schema with user-specified schemas, we can do extra checking and processing on the schema (e.g., type promotion, schema merging between user-specified schema and actual data schema). The scope will be much bigger. This is a design decision. cc [~rxin] [~marmbrus] [~lian cheng] [~cloud_fan] [~ekhliang]



;;;","05/Dec/16 07:36;rxin;Why don't we fix the parquet reader so it can tolerate non-existent columns?
;;;","05/Dec/16 07:41;smilegator;The error is from Parquet.
{noformat}
16/11/22 17:43:47 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.IllegalArgumentException: Column [b] was not found in schema!
	at org.apache.parquet.Preconditions.checkArgument(Preconditions.java:55)
	at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.getColumnDescriptor(SchemaCompatibilityValidator.java:190)
	at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumn(SchemaCompatibilityValidator.java:178)
{noformat}

If users specify the schema, we do not check whether the user-specified schema is right or wrong. We always respect the schema. If the schema contains the non-existent columns, we push down the filters to the Parquet, and then Parquet returns the above error to us.

Below is the test case you can try.

{noformat}
    Seq(""parquet"").foreach { format =>

      withTempPath { path =>
        Seq((1, ""abc""), (2, ""hello"")).toDF(""a"", ""b"").write.format(format).save(path.toString)

        // user-specified schema contains nonexistent columns
        val schema = StructType(
          Seq(StructField(""a"", IntegerType),
            StructField(""b"", StringType),
            StructField(""c"", IntegerType)))
        val readDf = spark.read.schema(schema).format(format).load(path.toString)

        // Read the table without any filter
        checkAnswer(readDf, Row(1, ""abc"", null) :: Row(2, ""hello"", null) :: Nil)
        // Read the table with a filter on existing columns
        checkAnswer(readDf.filter(""a < 2""), Row(1, ""abc"", null) :: Nil)

        val e = intercept[SparkException] {
          // Read the table with a filter on nonexistent columns
          readDf.filter(""c < 2"").show()
        }.getMessage
        assert(e.contains(""Column [c] was not found in schema""))

        withSQLConf(SQLConf.PARQUET_FILTER_PUSHDOWN_ENABLED.key -> ""false"") {
          checkAnswer(readDf.filter(""c < 2""), Nil)
        }
      }
    }
{noformat};;;","05/Dec/16 17:59;lian cheng;Haven't looked deeply into this issue, but my hunch is that this is related to https://issues.apache.org/jira/browse/PARQUET-389, which was fixed in parquet-mr 1.9.0, while Spark is still using 1.8 (in 2.1) and 1.7 (in 2.0).;;;","05/Dec/16 20:16;smilegator;[~lian cheng] [~rxin] We might be able to capture and process the Parquet-issued exception in our reader. [~xwu0226] will submit a very tiny PR to resolve this issue.   ;;;","05/Dec/16 20:31;xwu0226;Yes. I have the fix and will submit PR and cc everyone for review.;;;","05/Dec/16 21:52;apachespark;User 'xwu0226' has created a pull request for this issue:
https://github.com/apache/spark/pull/16156;;;","05/Dec/16 23:26;lian cheng;As commented on GitHub, there're two issues right now:
# This bug also affects the normal Parquet reader code path, where {{ParquetRecordReader}} is a 3rd party class closed for modification. Therefore, we can't capture the exception there.
# [PR #9940|https://github.com/apache/spark/pull/9940] should have already fixed this issue. But somehow it is broken right now.;;;","05/Dec/16 23:40;lian cheng;[~v-gerasimov], [~smilegator], and [~xwu0226], after some investigation, I don't think this is a bug now.

Just tested the master branch using the following test cases:
{code}
  for {
    useVectorizedReader <- Seq(true, false)
    mergeSchema <- Seq(true, false)
  } {
    test(s""foo - mergeSchema: $mergeSchema - vectorized: $useVectorizedReader"") {
      withSQLConf(SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key -> useVectorizedReader.toString) {
        withTempPath { dir =>
          val path = dir.getCanonicalPath
          val df = spark.range(1).coalesce(1)

          df.selectExpr(""id AS a"", ""id AS b"").write.parquet(path)
          df.selectExpr(""id AS a"").write.mode(""append"").parquet(path)

          assertResult(0) {
            spark.read
              .option(""mergeSchema"", mergeSchema.toString)
              .parquet(path)
              .filter(""b < 0"")
              .count()
          }
        }
      }
    }
  }
{code}
It turned out that this issue only happens when schema merging is turned off. This also explains why PR #9940 doesn't prevent PARQUET-389: because the trick PR #9940 employs happens during schema merging phase. On the other hand, you can't expect missing columns to be properly read when schema merging is turned off. Therefore, I don't think it's a bug.

The fix for the snippet mentioned in the ticket description is easy, just add {{.option(""mergeSchema"", ""true"")}} to enable schema merging.;;;","05/Dec/16 23:53;lian cheng;Please remind me if I missed anything important, otherwise, we can resolve this ticket as ""Not a Problem"".;;;","06/Dec/16 00:45;xwu0226;I think we will hit the issue if we use user-specified schema. Here is what I tried in spark-shell built from master branch:
{code}
val df = spark.range(1).coalesce(1)
df.selectExpr(""id AS a"").write.parquet(""/Users/xinwu/spark-test/data/spark-18539"")
val schema = StructType(Seq(StructField(""a"", IntegerType), StructField(""b"", IntegerType)))
spark.read.option(""mergeSchema"", ""true"").schema(schema).parquet(""/Users/xinwu/spark-test/data/spark-18539"").filter(""b is null"").count()
{code}

The exception is 
{code}
Caused by: java.lang.IllegalArgumentException: Column [b] was not found in schema!
  at org.apache.parquet.Preconditions.checkArgument(Preconditions.java:55)
  at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.getColumnDescriptor(SchemaCompatibilityValidator.java:181)
  at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumn(SchemaCompatibilityValidator.java:169)
  at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumnFilterPredicate(SchemaCompatibilityValidator.java:151)
  at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:91)
  at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:58)
  at org.apache.parquet.filter2.predicate.Operators$NotEq.accept(Operators.java:194)
  at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:121)
  at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:58)
  at org.apache.parquet.filter2.predicate.Operators$And.accept(Operators.java:308)
  at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validate(SchemaCompatibilityValidator.java:63)
  at org.apache.parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:59)
  at org.apache.parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:40)
  at org.apache.parquet.filter2.compat.FilterCompat$FilterPredicateCompat.accept(FilterCompat.java:126)
  at org.apache.parquet.filter2.compat.RowGroupFilter.filterRowGroups(RowGroupFilter.java:46)
  at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:110)
  at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:109)
  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1.apply(ParquetFileFormat.scala:377)
{code}

Here I have one parquet file missing column b and query with user-specified schema (a, b). 

;;;","06/Dec/16 01:22;viirya;Because we respect user-specified schema, we won't infer schema and schema merging won't step in of course.;;;","06/Dec/16 01:26;viirya;Actually I am not sure if this is a valid usage. I tend to think it is not. As you specify a non existing column, and the system reports the column was not found in schema. It looks reasonable to me.
;;;","06/Dec/16 01:55;lian cheng;[~xwu0226], thanks for the new use case!

[~viirya], I do think this is a valid use case as long as all the missing columns are nullable. The only reason that this use case doesn't work right now is PARQUET-389.

I got some vague idea about a possible cleaner fix for this issue. Will post it later.;;;","06/Dec/16 02:37;viirya;That's cool.;;;","07/Dec/16 03:22;viirya;[~lian cheng], in Parquet's code, looks like a null column still can have its ColumnChunkMetaData. It won't cause problem even before PARQUET-389, because Parquet will check if all values in the chunk are null.

PARQUET-389 resolves the case there is no ColumnChunkMetaData for a column, i.e., the column is missing from the Parquet file.

So I am not sure is, in a Parquet file, can a nullable column have no ColumnChunkMetaData like you said?

Appreciate if you can clarify it. Thanks.;;;","26/Jan/17 18:34;lian cheng;[~viirya], sorry for the (super) late reply. What I mentioned was a *nullable* column instead of a *null* column. To be more specific, say we have two Parquet files:

- File {{A}} has columns {{<a, b>}}
- File {{B}} has columns {{<a, b, c>}}, where {{c}} is marked as nullable (or {{optional}} in the term of Parquet)

Then it should be fine to treat these two files as a single dataset with a merged schema {{<a, b, c>}} and you should be able to push down predicates involving {{c}}.

BTW, the Parquet community just made a patch release 1.8.2 that includes a fix for PARQUET-389 and we probably will upgrade to 1.8.2 in 2.2.0. Then we'll have a proper fix for this issue and remove the workaround we did while doing schema merging.;;;","27/Jan/17 02:26;viirya;[~lian cheng] Yea, I see. The term {{optional}} is more proper and won't cause misunderstanding. If we can upgrade to 1.8.2, that is great we can remove the workaround. The workaround is actually a hacky solution.;;;","03/Feb/17 19:11;lian cheng;SPARK-19409 upgrades parquet-mr to 1.8.2 and fixed this issue.;;;",,,,,,,,,
Concurrent Fetching DataFrameReader JDBC APIs Do Not Work,SPARK-18538,13022400,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,smilegator,smilegator,smilegator,22/Nov/16 05:55,02/Dec/16 03:16,14/Jul/23 06:29,02/Dec/16 03:16,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"{code}
  def jdbc(
      url: String,
      table: String,
      columnName: String,
      lowerBound: Long,
      upperBound: Long,
      numPartitions: Int,
      connectionProperties: Properties): DataFrame
{code}

{code}
  def jdbc(
      url: String,
      table: String,
      predicates: Array[String],
      connectionProperties: Properties): DataFrame
{code}

The above two DataFrameReader JDBC APIs ignore the user-specified parameters of parallelism degree",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 01 23:53:05 UTC 2016,,,,,,,,,,"0|i36m67:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"22/Nov/16 06:03;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15975;;;","01/Dec/16 07:49;cloud_fan;already merged to master, will resolve this ticket once we backport it to 2.1;;;","01/Dec/16 23:53;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16111;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Redact sensitive information from Spark logs and UI,SPARK-18535,13022351,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgrover,mgrover,mgrover,22/Nov/16 00:11,12/Dec/22 18:10,14/Jul/23 06:29,28/Nov/16 17:00,2.1.0,,,,,,,,2.1.2,2.2.0,,,Spark Core,Web UI,YARN,,,,,,0,,,,,,"A Spark user may have to provide a sensitive information for a Spark configuration property, or a source out an environment variable in the executor or driver environment that contains sensitive information. A good example of this would be when reading/writing data from/to S3 using Spark. The S3 secret and S3 access key can be placed in a [hadoop credential provider|https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/CredentialProviderAPI.html]. However, one still needs to provide the password for the credential provider to Spark, which is typically supplied as an environment variable to the driver and executor environments. This environment variable shows up in logs, and may also show up in the UI.

1. For logs, it shows up in a few places:
  1A. Event logs under {{SparkListenerEnvironmentUpdate}} event.
  1B. YARN logs, when printing the executor launch context.
2. For UI, it would show up in the _Environment_ tab, but it is redacted if it contains the words ""password"" or ""secret"" in it. And, these magic words are [hardcoded|https://github.com/apache/spark/blob/a2d464770cd183daa7d727bf377bde9c21e29e6a/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala#L30] and hence not customizable.

This JIRA is to track the work to make sure sensitive information is redacted from all logs and UIs in Spark, while still being passed on to all relevant places it needs to get passed on to.",,ajbozarth,apachespark,diogo.mvieira,mgrover,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19720,,,,,,,,,,,,,,"22/Nov/16 00:30;mgrover;redacted.png;https://issues.apache.org/jira/secure/attachment/12839921/redacted.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 02 19:21:52 UTC 2017,,,,,,,,,,"0|i36lvb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/16 00:29;apachespark;User 'markgrover' has created a pull request for this issue:
https://github.com/apache/spark/pull/15971;;;","22/Nov/16 00:29;mgrover;I just issued a PR for this, that adds a new customizable property for determining what configuration properties are sensitive. Attached is an image from the UI with this change.
Here's the text in the YARN logs, with this change:
{{HADOOP_CREDSTORE_PASSWORD -> *********(redacted)}}

Here's the text in the event logs, with this change:
{code}
...,""spark.executorEnv.HADOOP_CREDSTORE_PASSWORD"":""*********(redacted)"",""spark.yarn.appMasterEnv.HADOOP_CREDSTORE_PASSWORD"":""*********(redacted)"",...
{code};;;","02/Aug/17 19:21;gurwls223;User 'dmvieira' has created a pull request for this issue:
https://github.com/apache/spark/pull/18802;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Raise correct error upon specification of schema for datasource tables created through CTAS,SPARK-18533,13022331,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dkbiswal,dkbiswal,dkbiswal,21/Nov/16 22:56,22/Nov/16 23:58,14/Jul/23 06:29,22/Nov/16 23:57,2.0.2,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Currently hive serde tables created through CTAS does not allow explicit specification of schema as its inferred from the select clause.  Currently a semantic error is raised for this case. However for data source tables currently we raise a parser error which is not as informative. We should raise consistent error for both forms of tables.
",,apachespark,dkbiswal,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 22 23:57:45 UTC 2016,,,,,,,,,,"0|i36lqv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Nov/16 23:03;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/15968;;;","22/Nov/16 23:57;smilegator;Issue resolved by pull request 15968
[https://github.com/apache/spark/pull/15968];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka timestamp should be TimestampType,SPARK-18530,13022257,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,zsxwing,marmbrus,marmbrus,21/Nov/16 17:54,23/Nov/16 00:49,14/Jul/23 06:29,23/Nov/16 00:49,,,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,Otherwise every time you try to use it you have to do a manual conversion.,,apachespark,codingcat,lwlin,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 21 23:09:04 UTC 2016,,,,,,,,,,"0|i36laf:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"21/Nov/16 23:09;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/15969;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Timeouts shouldn't be AssertionErrors,SPARK-18529,13022252,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,marmbrus,marmbrus,21/Nov/16 17:32,22/Nov/16 22:17,14/Jul/23 06:29,22/Nov/16 22:17,,,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,"A timeout should inherit from {{RuntimeException}} as its not a fatal error.
{code}
java.lang.AssertionError: assertion failed: Failed to get records for spark-kafka-source-26d6f51c-0781-45a4-ab8e-bc6bd6603917-86874470-executor service-log-0 49350201 after polling for 1000
	at scala.Predef$.assert(Predef.scala:170)
	at org.apache.spark.sql.kafka010.CachedKafkaConsumer.get(CachedKafkaConsumer.scala:65)
	at org.apache.spark.sql.kafka010.KafkaSourceRDD$$anon$1.next(KafkaSourceRDD.scala:146)
	at org.apache.spark.sql.kafka010.KafkaSourceRDD$$anon$1.next(KafkaSourceRDD.scala:142)
{code}",,apachespark,marmbrus,tdas,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 22 22:17:54 UTC 2016,,,,,,,,,,"0|i36l9b:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"21/Nov/16 20:57;zsxwing;This will be fixed in https://github.com/apache/spark/pull/15820 after refactoring codes.;;;","22/Nov/16 18:34;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/15820;;;","22/Nov/16 22:17;tdas;Issue resolved by pull request 15820
[https://github.com/apache/spark/pull/15820];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
limit + groupBy leads to java.lang.NullPointerException,SPARK-18528,13022251,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,correedsh,correedsh,21/Nov/16 17:31,10/May/17 01:19,14/Jul/23 06:29,22/Dec/16 00:54,2.0.1,,,,,,,,2.0.3,2.1.1,2.2.0,,PySpark,SQL,,,,,,,3,,,,,,"Using limit on a DataFrame prior to groupBy will lead to a crash. Repartitioning will avoid the crash.

*will crash:* {{df.limit(3).groupBy(""user_id"").count().show()}}
*will work:* {{df.limit(3).coalesce(1).groupBy('user_id').count().show()}}
*will work:* {{df.limit(3).repartition('user_id').groupBy('user_id').count().show()}}

Here is a reproducible example along with the error message:

{quote}
>>> df = spark.createDataFrame([ (1, 1), (1, 3), (2, 1), (3, 2), (3, 3) ], [""user_id"", ""genre_id""])
>>>
>>> df.show()
+-------+--------+
|user_id|genre_id|
+-------+--------+
|      1|       1|
|      1|       3|
|      2|       1|
|      3|       2|
|      3|       3|
+-------+--------+
>>> df.groupBy(""user_id"").count().show()
+-------+-----+
|user_id|count|
+-------+-----+
|      1|    2|
|      3|    2|
|      2|    1|
+-------+-----+
>>> df.limit(3).groupBy(""user_id"").count().show()
[Stage 8:===================================================>(1964 + 24) / 2000]16/11/21 01:59:27 WARN TaskSetManager: Lost task 0.0 in stage 9.0 (TID 8204, lvsp20hdn012.stubprod.com): java.lang.NullPointerException
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
    at org.apache.spark.scheduler.Task.run(Task.scala:86)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
{quote}

","CentOS release 6.6, Linux 2.6.32-504.el6.x86_64",apachespark,correedsh,DjvuLee,eaubin234,ekundin,kamprath,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18851,SPARK-19037,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 10 01:19:57 UTC 2017,,,,,,,,,,"0|i36l93:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/16 02:26;DjvuLee;I just test your example, but it works.

>>> df.limit(3).groupBy(""user_id"").count().show()
+-------+-----+
|user_id|count|
+-------+-----+
|      1|    2|
|      2|    1|
+-------+-----+


I test on a the spark2.0.1-hadoop2.6 version downloaded from the spark website.

can you reproduce your error?

;;;","22/Nov/16 02:57;maropu;I reproduced this in master;
{code}
scala> val df = Seq((""a"", 1), (""b"", 2), (""a"", 1), (""c"", 5), (""b"", 2)).toDF(""id"", ""value"")
scala> df.limit(3).groupBy(""id"").count().show()
{code}
I checked code and I found this exception happened when execution was early stopped by the limit and hashmap (https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala#L603) was not initialized.
I'm making pr to fix this. Thanks!;;;","22/Nov/16 19:44;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/15980;;;","09/May/17 18:38;ekundin;I have just found the same bug with Spark 2.10, even though it's marked as fixed.  I believe you need to add 2.1.0 to the ""Affected Versions"" list.

I have a Dataframe df with a few columns that I read in from json, and running:
df.select('foo').distinct().count()
works fine.
df.limit(100).select('foo').distinct().count()
throws a NPE.
df.limit(100).repartition('foo').select('foo').distinct().count()
works fine too.  Seems like the same bug, still broken.;;;","10/May/17 01:19;maropu;You tried spark-v2.1.1? This issue is fixed in 2.0.3, 2.1.1, 2.2.0 (See Fix Versions).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"UDAFPercentile (bigint, array<double>) needs explicity cast to double",SPARK-18527,13022222,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiangxb1987,fabboe,fabboe,21/Nov/16 15:28,29/Nov/16 10:40,14/Jul/23 06:29,28/Nov/16 22:59,2.0.0,2.0.1,,,,,,,2.1.0,,,,SQL,,,,,,,,1,,,,,,"Same bug as SPARK-16228 but 
{code}_FUNC_(bigint, array<double>) {code}
instead of 
{code}_FUNC_(bigint, double){code}

Fix of SPARK-16228 only fixes the non-array case that was hit.

{code}
sql(""select percentile(value, array(0.5,0.99)) from values 1,2,3 T(value)"")
{code}
fails in Spark 2 shell.


Longer example
{code}
case class Record(key: Long, value: String)
val recordsDF = spark.createDataFrame((1 to 100).map(i => Record(i.toLong, s""val_$i"")))

recordsDF.createOrReplaceTempView(""records"")
sql(""SELECT percentile(key, Array(0.95, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1)) AS test FROM records"")
org.apache.spark.sql.AnalysisException: No handler for Hive UDF 'org.apache.hadoop.hive.ql.udf.UDAFPercentile': org.apache.hadoop.hive.ql.exec.NoMatchingMethodException: No matching method for class org.apache.had
oop.hive.ql.udf.UDAFPercentile with (bigint, array<decimal(38,18)>). Possible choices: _FUNC_(bigint, array<double>)  _FUNC_(bigint, double)  ; line 1 pos 7
  at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getMethodInternal(FunctionRegistry.java:1164)
  at org.apache.hadoop.hive.ql.exec.DefaultUDAFEvaluatorResolver.getEvaluatorClass(DefaultUDAFEvaluatorResolver.java:83)
  at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge.getEvaluator(GenericUDAFBridge.java:56)
  at org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver.getEvaluator(AbstractGenericUDAFResolver.java:47){code}","spark-2.0.1-bin-hadoop2.7/bin/spark-shell
",apachespark,fabboe,hvanhovell,maropu,thomastechs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 29 10:40:31 UTC 2016,,,,,,,,,,"0|i36l2n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/16 20:08;thomastechs;I am interested to work on this.;;;","26/Nov/16 13:52;hvanhovell;The implementation of our own percentile function is tracked in SPARK-16282. I don't think that will make 2.1, so it would be great if you can create a PR.;;;","28/Nov/16 11:11;fabboe;Interesting [~hvanhovell], good to see that 
{code}percentile(a, array<double>){code}
is aimed to be [covered|https://github.com/apache/spark/pull/14136/files#diff-a15a6f87f9676612c69435953a13ddd3R127] in the own implementation. Indeed that PR seems quite big for a soon release.

Maybe [~dongjoon] could give starting points for this one, related to the very close PR: https://github.com/apache/spark/pull/13930

Thanks!;;;","28/Nov/16 12:06;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/16034;;;","28/Nov/16 22:59;hvanhovell;This has been fixed by merging the Percentile UDAF into branch-2.1;;;","29/Nov/16 10:38;fabboe;Thanks [~hvanhovell]. Will be interesting to compare this fix to the native implementation which was also merged into branch-2.1.;;;","29/Nov/16 10:40;hvanhovell;We did not merge my PR (which was a hack TBH), but the Percentile implementation. That is the reason why I closed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOM killer may leave SparkContext in broken state causing Connection Refused errors,SPARK-18523,13022143,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kxepal,kxepal,kxepal,21/Nov/16 09:53,31/Aug/17 09:14,14/Jul/23 06:29,29/Nov/16 02:29,1.6.1,2.0.0,,,,,,,2.1.0,,,,PySpark,,,,,,,,0,,,,,,"When you run some memory-heavy spark job, Spark driver may consume more memory resources than host available to provide.

In this case OOM killer comes on scene and successfully kills a spark-submit process.

The pyspark.SparkContext is not able to handle such state of things and becomes completely broken. 

You cannot stop it as on stop it tries to call stop method of bounded java context (jsc) and fails with Py4JError, because such process no longer exists as like as the connection to it. 

You cannot start new SparkContext because you have your broken one as active one and pyspark still is not able to not have SparkContext as sort of singleton.

The only thing you can do is shutdown your IPython Notebook and start it over. Or dive into SparkContext internal attributes and reset them manually to initial None state.

The OOM killer case is just one of the many: any reason of spark-submit crash in the middle of something leaves SparkContext in a broken state.

Example on error log on {{sc.stop()}} in broken state:
{code}
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/site-packages/py4j/java_gateway.py"", line 883, in send_command
    response = connection.send_command(command)
  File ""/usr/local/lib/python2.7/site-packages/py4j/java_gateway.py"", line 1040, in send_command
    ""Error while receiving"", e, proto.ERROR_ON_RECEIVE)
Py4JNetworkError: Error while receiving
ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:59911)
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/site-packages/py4j/java_gateway.py"", line 963, in start
    self.socket.connect((self.address, self.port))
  File ""/usr/local/lib/python2.7/socket.py"", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 61] Connection refused

---------------------------------------------------------------------------
Py4JError                                 Traceback (most recent call last)
<ipython-input-2-f154e069615b> in <module>()
----> 1 sc.stop()

/usr/local/share/spark/python/pyspark/context.py in stop(self)
    360         """"""
    361         if getattr(self, ""_jsc"", None):
--> 362             self._jsc.stop()
    363             self._jsc = None
    364         if getattr(self, ""_accumulatorServer"", None):

/usr/local/lib/python2.7/site-packages/py4j/java_gateway.pyc in __call__(self, *args)
   1131         answer = self.gateway_client.send_command(command)
   1132         return_value = get_return_value(
-> 1133             answer, self.gateway_client, self.target_id, self.name)
   1134 
   1135         for temp_arg in temp_args:

/usr/local/share/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
     43     def deco(*a, **kw):
     44         try:
---> 45             return f(*a, **kw)
     46         except py4j.protocol.Py4JJavaError as e:
     47             s = e.java_exception.toString()

/usr/local/lib/python2.7/site-packages/py4j/protocol.pyc in get_return_value(answer, gateway_client, target_id, name)
    325             raise Py4JError(
    326                 ""An error occurred while calling {0}{1}{2}"".
--> 327                 format(target_id, ""."", name))
    328     else:
    329         type = answer[1]

Py4JError: An error occurred while calling o47.stop
{code}",,aaronsteers,apachespark,kadeng,kxepal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21881,,,,SPARK-18876,SPARK-21881,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 31 09:14:50 UTC 2017,,,,,,,,,,"0|i36kl3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Nov/16 10:58;apachespark;User 'kxepal' has created a pull request for this issue:
https://github.com/apache/spark/pull/15961;;;","31/Aug/17 07:05;kadeng;In PySpark 2.2.0 this issue was not really fixed for me. While I could close the SparkContext (with an Exception message), I could not reopen any new spark contexts.

If I resetted the global SparkContext variables like this, it worked:


{code:none}
def reset_spark():
    import pyspark
    from threading import RLock
    pyspark.SparkContext._jvm = None
    pyspark.SparkContext._gateway = None
    pyspark.SparkContext._next_accum_id = 0
    pyspark.SparkContext._active_spark_context = None
    pyspark.SparkContext._lock = RLock()
    pyspark.SparkContext._python_includes = None
reset_spark()
{code}
;;;","31/Aug/17 09:14;kxepal;[~kadeng]
I don't have a 2.2.0 in production for now (shame on me!), but will check this. Thanks for report!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
map type can not be used in EqualTo,SPARK-18519,13022101,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,21/Nov/16 06:25,23/Nov/16 12:21,14/Jul/23 06:29,22/Nov/16 17:18,,,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 23 07:17:04 UTC 2016,,,,,,,,,,"0|i36kbr:",9223372036854775807,,,,,,,,,,,,,2.0.3,2.1.0,,,,,,,,,,"21/Nov/16 06:29;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/15956;;;","23/Nov/16 07:17;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/15988;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DROP TABLE IF EXISTS should not warn for non-existing tables,SPARK-18517,13022089,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,21/Nov/16 04:51,21/Nov/16 21:16,14/Jul/23 06:29,21/Nov/16 21:16,2.0.2,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Currently, `DROP TABLE IF EXISTS` shows warning for non-existing tables. However, it had better be quiet for this case by definition of the command.

{code}
scala> sql(""DROP TABLE IF EXISTS nonexist"")
16/11/20 20:48:26 WARN DropTableCommand: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'nonexist' not found in database 'default';
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 21 04:57:06 UTC 2016,,,,,,,,,,"0|i36k93:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"21/Nov/16 04:57;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/15953;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partition schema inference corrupts data,SPARK-18510,13021974,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,brkyvz,brkyvz,brkyvz,20/Nov/16 03:12,28/Nov/16 10:09,14/Jul/23 06:29,23/Nov/16 19:51,2.1.0,,,,,,,,2.1.0,,,,SQL,Structured Streaming,,,,,,,0,,,,,,"Not sure if this is a regression from 2.0 to 2.1. I was investigating this for Structured Streaming, but it seems it affects batch data as well.

Here's the issue:
If I specify my schema when doing
{code}
spark.read
  .schema(someSchemaWherePartitionColumnsAreStrings)
{code}

but if the partition inference can infer it as IntegerType or I assume LongType or DoubleType (basically fixed size types), then once UnsafeRows are generated, your data will be corrupted.

Reproduction:
{code}
val createArray = udf { (length: Long) =>
    for (i <- 1 to length.toInt) yield i.toString
}
spark.range(10).select(createArray('id + 1) as 'ex, 'id, 'id % 4 as 'part).coalesce(1).write
        .partitionBy(""part"", ""id"")
        .mode(""overwrite"")
        .parquet(src.toString)
val schema = new StructType()
        .add(""id"", StringType)
        .add(""part"", IntegerType)
        .add(""ex"", ArrayType(StringType))
spark.read
      .schema(schema)
      .format(""parquet"")
      .load(src.toString)
      .show()
{code}
The UDF is useful for creating a row long enough so that you don't hit other weird NullPointerExceptions caused for the same reason I believe.
Output:
{code}
+---------+----+--------------------+
|       id|part|                  ex|
+---------+----+--------------------+
|�|   1|[1, 2, 3, 4, 5, 6...|
| |   0|[1, 2, 3, 4, 5, 6...|
|  |   3|[1, 2, 3, 4, 5, 6...|
|   |   2|[1, 2, 3, 4, 5, 6...|
|    |   1|  [1, 2, 3, 4, 5, 6]|
|     |   0|     [1, 2, 3, 4, 5]|
|      |   3|        [1, 2, 3, 4]|
|       |   2|           [1, 2, 3]|
|        |   1|              [1, 2]|
|         |   0|                 [1]|
+---------+----+--------------------+
{code}

I was hoping to fix the issue as part of SPARK-18407 but it seems it's not only applicable to StructuredStreaming and deserves it's own JIRA.",,apachespark,brkyvz,codingcat,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18407,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 23 21:54:05 UTC 2016,,,,,,,,,,"0|i36jjj:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"20/Nov/16 03:15;brkyvz;cc [~rxin@databricks.com] I marked this as a blocker as it is pretty nasty. Feel free to downgrade if you don't think so, or feel like the semantics of what I'm doing is wrong.;;;","20/Nov/16 05:40;rxin;Does your pull request for SPARK-18407 fix this one as well?
;;;","20/Nov/16 19:10;brkyvz;No. Working on a separate fix;;;","20/Nov/16 22:36;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/15951;;;","23/Nov/16 21:54;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/15997;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix documentation for DateDiff,SPARK-18508,13021921,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,19/Nov/16 06:39,20/Nov/16 05:57,14/Jul/23 06:29,20/Nov/16 05:57,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"The current documentation for DateDiff does not make it clear which one is the start date, and which is the end date. The example is also wrong about the direction.
",,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 19 06:40:05 UTC 2016,,,,,,,,,,"0|i36j7r:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"19/Nov/16 06:40;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/15937;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scalar subquery with extra group by columns returning incorrect result,SPARK-18504,13021856,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nsyca,nsyca,nsyca,18/Nov/16 21:11,14/May/19 06:28,14/Jul/23 06:29,22/Nov/16 20:07,2.0.0,,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,1,correctness,,,,,,,apachespark,hvanhovell,nsyca,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18455,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 22 02:32:07 UTC 2016,,,,,,,,,,"0|i36itb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/16 21:13;nsyca;// Incorrect result
Seq(1).toDF(""c1"").createOrReplaceTempView(""t1"")
Seq((1,1),(1,2)).toDF(""c1"",""c2"").createOrReplaceTempView(""t2"")
sql(""select (select sum(-1) from t2 where t1.c1=t2.c1 group by t2.c2) from t1"").show

// How can selecting a scalar subquery from a 1-row table return 2 rows?;;;","18/Nov/16 21:45;hvanhovell;Is this a valid correlated scalar subquery?

They only way to prevent this is to add a check in the analyzer (as long as we don't have a subquery processing node which would be able to enforce a single row result per key at runtime). What makes this a bit more complicated that this only applies scalar subqueries and not to predicate subqueries.
;;;","18/Nov/16 21:45;hvanhovell;Could you open a PR?;;;","18/Nov/16 22:41;apachespark;User 'nsyca' has created a pull request for this issue:
https://github.com/apache/spark/pull/15936;;;","18/Nov/16 22:45;nsyca;While we have SPARK-18455 to track the progress of a better solution to process Subquery, I feel we should close this loophole as soon as possible. The fix simply blocks the scenario where there are extra GROUP BY columns beyond the ones in the correlated predicates of the same scalar subquery.;;;","22/Nov/16 02:32;nsyca;Revise the reproduction script to be more concise

{code}
Seq((1,1),(1,2)).toDF(""c1"",""c2"").createOrReplaceTempView(""t"")
sql(""select (select sum(-1) from t t2 where t1.c2=t2.c1 group by t2.c2) from t t1"").show

+---------------------------+
|scalarsubquery((c2 = c1#5))|
+---------------------------+
|                         -1|
|                         -1|
|                       null|
+---------------------------+
{code}

The result should return only one ""-1"" row, not two.

This JIRA will block this type of query where the column(s) in the GROUP BY clause are not part of the WHERE clause.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR spark.glm error on collinear data ,SPARK-18501,13021704,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yanboliang,yanboliang,yanboliang,18/Nov/16 08:28,23/Nov/16 03:22,14/Jul/23 06:29,23/Nov/16 03:22,,,,,,,,,2.1.0,,,,ML,SparkR,,,,,,,0,,,,,,"Spark {{GeneralizedLinearRegression}} can handle collinear data since the underlying {{WeightedLeastSquares}} can be solved by local ""l-bfgs""(rather than ""normal""). But the SparkR wrapper {{spark.glm}} throw errors when fitting on collinear data:
{code}
> df <- read.df(""data/mllib/sample_binary_classification_data.txt"", source = ""libsvm"")
> model <- spark.glm(df, label ~ features, family = binomial(link = ""logit”))
> summary(model)
Error in `rownames<-`(`*tmp*`, value = c(""(Intercept)"", ""features_0"",  :
  length of 'dimnames' [1] not equal to array extent
{code}
After depth study of this error, I found it was caused the standard error of coefficients, t value and p value are not available when the underlying {{WeightedLeastSquares}} was solved by local ""l-bfgs"". So the coefficients matrix was generated failed.",,apachespark,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 18 09:29:06 UTC 2016,,,,,,,,,,"0|i36hvj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/16 09:29;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/15930;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ForeachSink fails with ""assertion failed: No plan for EventTimeWatermark""",SPARK-18497,13021647,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,zsxwing,ilikerps,ilikerps,18/Nov/16 01:13,05/Dec/16 21:13,14/Jul/23 06:29,19/Nov/16 00:34,2.1.0,,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,"I have a pretty standard stream. I call "".writeStream.foreach(...).start()"" and get

{code}
java.lang.AssertionError: assertion failed: No plan for EventTimeWatermark timestamp#39688: timestamp, interval 1 days
	at scala.Predef$.assert(Predef.scala:170)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:77)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:74)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:74)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:66)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:77)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:74)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:74)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:66)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:84)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$toString$3.apply(QueryExecution.scala:232)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$toString$3.apply(QueryExecution.scala:232)
	at org.apache.spark.sql.execution.QueryExecution.stringOrError(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution.toString(QueryExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:54)
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2751)
	at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:2290)
	at org.apache.spark.sql.execution.streaming.ForeachSink.addBatch(ForeachSink.scala:70)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch(StreamExecution.scala:449)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$1.apply$mcZ$sp(StreamExecution.scala:227)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$1.apply(StreamExecution.scala:215)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$1.apply(StreamExecution.scala:215)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$reportTimeTaken(StreamExecution.scala:687)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1.apply$mcZ$sp(StreamExecution.scala:214)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:43)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches(StreamExecution.scala:210)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:144)
{code}

What do?",,apachespark,codingcat,copris,ilikerps,marmbrus,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18721,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 19 00:34:59 UTC 2016,,,,,,,,,,"0|i36hiv:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"18/Nov/16 22:16;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/15934;;;","19/Nov/16 00:34;tdas;Issue resolved by pull request 15934
[https://github.com/apache/spark/pull/15934];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Underlying integer overflow when create ChunkedByteBufferOutputStream in MemoryStore,SPARK-18485,13021380,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,uncleGen,uncleGen,uncleGen,17/Nov/16 08:23,17/Dec/16 13:22,14/Jul/23 06:29,17/Dec/16 13:19,1.6.3,2.0.2,,,,,,,2.2.0,,,,,,,,,,,,0,,,,,,,,apachespark,uncleGen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 17 13:19:50 UTC 2016,,,,,,,,,,"0|i36fvj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/16 08:33;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15915;;;","17/Dec/16 13:19;srowen;Issue resolved by pull request 15915
[https://github.com/apache/spark/pull/15915];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Link validation for ML guides,SPARK-18480,13021338,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,podongfeng,podongfeng,podongfeng,17/Nov/16 03:00,17/Nov/16 13:40,14/Jul/23 06:29,17/Nov/16 13:40,,,,,,,,,2.1.0,,,,Documentation,,,,,,,,0,,,,,,"I did a skim about the link validation of ML and found following link issues:
1, There are two {{[Graph.partitionBy]}} in {{graphx-programming-guide.md}}, the first one had no effert.
2, {{DataFrame}}, {{Transformer}}, {{Pipeline}} and {{Parameter}}  in {{ml-pipeline.md}} were linked to {{ml-guide.html}} by mistake.
3, {{PythonMLLibAPI}} in {{mllib-linear-methods.md}} was not accessable, because class {{PythonMLLibAPI}} is private.
4, Other links updates.",,apachespark,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18322,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 17 13:40:38 UTC 2016,,,,,,,,,,"0|i36fm7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/16 03:01;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/15912;;;","17/Nov/16 13:40;srowen;Issue resolved by pull request 15912
[https://github.com/apache/spark/pull/15912];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR Logistic Regression should output original label,SPARK-18476,13021322,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wm624,wm624,wm624,17/Nov/16 00:32,14/Dec/16 22:08,14/Jul/23 06:29,01/Dec/16 04:36,2.1.0,,,,,,,,2.1.0,,,,SparkR,,,,,,,,0,,,,,,"Similar to [SPARK-18401], as a classification algorithm, logistic regression should support output original label instead of supporting index label.",,apachespark,josephkb,wm624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18291,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 02 00:22:35 UTC 2016,,,,,,,,,,"0|i36fin:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/16 00:36;apachespark;User 'wangmiao1981' has created a pull request for this issue:
https://github.com/apache/spark/pull/15910;;;","02/Dec/16 00:22;wm624;spark.logit predict should output original label instead of a numerical value as the prediction column. Example:

> training <- suppressWarnings(createDataFrame(iris))
> binomial_training <- training[training$Species %in% c(""versicolor"", ""virginica""), ]
> binomial_model <- spark.logit(binomial_training, Species ~ Sepal_Length + Sepal_Width)
> prediction <- predict(binomial_model, binomial_training)
> showDF(prediction)

Output:

+------------+-----------+------------+-----------+----------+--------------------+--------------------+----------+
|Sepal_Length|Sepal_Width|Petal_Length|Petal_Width|   Species|       rawPrediction|         probability|prediction|
+------------+-----------+------------+-----------+----------+--------------------+--------------------+----------+
|         7.0|        3.2|         4.7|        1.4|versicolor|[-1.5655042626435...|[0.17285823940230...| virginica|
|         6.4|        3.2|         4.5|        1.5|versicolor|[-0.4240802660720...|[0.39554079174312...| virginica|
|         6.9|        3.1|         4.9|        1.5|versicolor|[-1.3348014339322...|[0.20836626079909...| virginica|
|         5.5|        2.3|         4.0|        1.3|versicolor|[1.65224519232947...|[0.83919426374389...|versicolor|
|         6.5|        2.8|         4.6|        1.5|versicolor|[-0.4524556150364...|[0.38877708044707...| virginica|
|         5.7|        2.8|         4.5|        1.3|versicolor|[1.06944304705877...|[0.74449098435029...|versicolor|
|         6.3|        3.3|         4.7|        1.6|versicolor|[-0.2743084292595...|[0.43184968922729...| virginica|
|         4.9|        2.4|         3.3|        1.0|versicolor|[2.75320369295153...|[0.94009402758065...|versicolor|
|         6.6|        2.9|         4.6|        1.3|versicolor|[-0.6831584437477...|[0.33555673563505...| virginica|
|         5.2|        2.7|         3.9|        1.4|versicolor|[2.06109520681768...|[0.88706393592062...|versicolor|
|         5.0|        2.0|         3.5|        1.0|versicolor|[2.72482834398713...|[0.93847590782569...|versicolor|
|         5.9|        3.0|         4.2|        1.5|versicolor|[0.60803738963620...|[0.64749297424084...|versicolor|
|         6.0|        2.2|         4.0|        1.0|versicolor|[0.74152402446931...|[0.67732902849243...|versicolor|
|         6.1|        2.9|         4.7|        1.4|versicolor|[0.26802822006176...|[0.56660877197498...|versicolor|
|         5.6|        2.9|         3.6|        1.3|versicolor|[1.21921488387130...|[0.77192535405997...|versicolor|
|         6.7|        3.1|         4.4|        1.4|versicolor|[-0.9543267684084...|[0.27801550694056...| virginica|
|         5.6|        3.0|         4.5|        1.5|versicolor|[1.17874938792192...|[0.76472286588073...|versicolor|
|         5.8|        2.7|         4.1|        1.0|versicolor|[0.91967121024624...|[0.71497510778599...|versicolor|
|         6.2|        2.2|         4.5|        1.5|versicolor|[0.36104935894550...|[0.58929443051304...|versicolor|
|         5.6|        2.5|         3.9|        1.1|versicolor|[1.38107686766881...|[0.79916389423163...|versicolor|
+------------+-----------+------------+-----------+----------+--------------------+--------------------+----------+

The `prediction` column should be the original label as shown above.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correctness issue in INNER join result with window functions,SPARK-18473,13021248,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,peay,peay,16/Nov/16 20:07,14/May/19 06:28,14/Jul/23 06:29,17/Nov/16 00:39,2.0.1,,,,,,,,,,,,PySpark,Spark Core,SQL,,,,,,0,correctness,,,,,"I have stumbled onto a corner case where an INNER join appears to return incorrect results. I believe the join should behave as the identity, but instead, some values are shuffled around, and some are just plain wrong.

This can be reproduced as follows: joining

{code}
+-----+---------+------+--------+--------+----------+------+
|index|timeStamp|hasOne|hasFifty|oneCount|fiftyCount|sessId|
+-----+---------+------+--------+--------+----------+------+
|    1|        1|     1|       0|       1|         0|     1|
|    2|        2|     0|       0|       1|         0|     1|
|    1|        3|     1|       0|       2|         0|     2|
+-----+---------+------+--------+--------+----------+------+
{code}

with

{code}
+------+
|sessId|
+------+
|     1|
|     2|
+------+
{code}

The result is

{code}
+------+-----+---------+------+--------+--------+----------+
|sessId|index|timeStamp|hasOne|hasFifty|oneCount|fiftyCount|
+------+-----+---------+------+--------+--------+----------+
|     1|    2|        2|     0|       0|       1|         0|
|     2|    1|        1|     1|       0|       1|        -1|
|     2|    1|        3|     1|       0|       2|         0|
+------+-----+---------+------+--------+--------+----------+
{code}

Note how two rows have a sessId of 2 (instead of one row as expected), and how `fiftyCount` can now be negative while always zero in the original dataframe.

The first dataframe uses two windows:
- `hasOne` uses a `window.rowsBetween(-10, 0)`.
- `hasFifty` uses a `window.rowsBetween(-10, -1)`.

The result is *correct* if:
- `hasFifty` is changed to `window.rowsBetween(-10, 0)` instead of  `window.rowsBetween(-10, -1)`.
- I add {code}.fillna({ 'numOnesBefore': 0 }) {code} after the other call to `fillna` -- although there are no visible effect on the dataframe as shown by `show` as far as I can tell.
- I use a LEFT OUTER join instead of INNER JOIN.
- I write both dataframes to Parquet, read them back and join these.

This can be reproduced in pyspark using:

{code}
import pyspark.sql.functions as F
from pyspark.sql.functions import col
from pyspark.sql.window import Window

df1 = sql_context.createDataFrame(
    pd.DataFrame({""index"": [1, 2, 1], ""timeStamp"": [1, 2, 3]})
)

window = Window.partitionBy(F.lit(1)).orderBy(""timeStamp"", ""index"")

df2 = (
    df1
    .withColumn(""hasOne"", (col(""index"") == 1).cast(""int""))
    .withColumn(""hasFifty"", (col(""index"") == 50).cast(""int""))
    .withColumn(""numOnesBefore"", F.sum(col(""hasOne"")).over(window.rowsBetween(-10, 0)))
    .withColumn(""numFiftyStrictlyBefore"", F.sum(col(""hasFifty"")).over(window.rowsBetween(-10, -1)))
    .fillna({ 'numFiftyStrictlyBefore': 0 })
    .withColumn(""sessId"", col(""numOnesBefore"") - col(""numFiftyStrictlyBefore""))
)

df_selector = sql_context.createDataFrame(pd.DataFrame({""sessId"": [1, 2]}))
df_joined = df_selector.join(df2, ""sessId"", how=""inner"")

df2.show()
df_selector.show()
df_joined.show()
{code}",,dongjoon,hvanhovell,peay,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 17 00:45:38 UTC 2016,,,,,,,,,,"0|i36f27:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/16 20:59;hvanhovell;This has been fixed in spark 2.0.2.

See the following notebook: https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2728434780191932/3885409687420673/6987336228780374/latest.html;;;","16/Nov/16 21:04;peay;Ah, great, thanks. I had checked out the CHANGELOG but couldn't find anything relevant, any reference to the corresponding issue?;;;","17/Nov/16 00:37;hvanhovell;This is probably caused by SPARK-17981. That bug creates incorrect nullability flags, which messes with the result of coalesce (fillna) in your query.;;;","17/Nov/16 00:39;hvanhovell;Fixed by gatorsmile's PR for SPARK-17981/SPARK-17957;;;","17/Nov/16 00:42;dongjoon;Hi, [~peay].
Maybe, the relevant one is https://issues.apache.org/jira/browse/SPARK-17806 .

I think we can close this issue since it's already proven by the example.;;;","17/Nov/16 00:43;dongjoon;Wow. Please forget about my comment. :)
I didn't read [~hvanhovell]'s comments. Sorry.;;;","17/Nov/16 00:45;peay;Ok, I see, thanks. The fix is in 2.0.3 though, not 2.0.2, correct? (edit: nevermind, the fix appears to be in 2.0.2 indeed).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL fails to load tables created without providing a schema,SPARK-18464,13021006,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,cloud_fan,yhuai,yhuai,16/Nov/16 01:58,19/Nov/16 09:04,14/Jul/23 06:29,17/Nov/16 08:00,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"I have a old table that was created without providing a schema. Seems branch 2.1 fail to load it and says that the schema is corrupt. 

With {{spark.sql.debug}} enabled, I get the metadata by using {{describe formatted}}.
{code}
[col,array<string>,from deserializer]
[,,]
[# Detailed Table Information,,]
[Database:,mydb,]
[Owner:,root,]
[Create Time:,Fri Jun 17 11:55:07 UTC 2016,]
[Last Access Time:,Thu Jan 01 00:00:00 UTC 1970,]
[Location:,mylocation,]
[Table Type:,EXTERNAL,]
[Table Parameters:,,]
[  transient_lastDdlTime,1466164507,]
[  spark.sql.sources.provider,parquet,]
[,,]
[# Storage Information,,]
[SerDe Library:,org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe,]
[InputFormat:,org.apache.hadoop.mapred.SequenceFileInputFormat,]
[OutputFormat:,org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat,]
[Compressed:,No,]
[Storage Desc Parameters:,,]
[  path,/myPatch,]
[  serialization.format,1,]
{code}",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 16 06:18:06 UTC 2016,,,,,,,,,,"0|i36dkf:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"16/Nov/16 01:59;yhuai;cc [~cloud_fan];;;","16/Nov/16 06:18;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/15900;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkListenerDriverAccumUpdates event does not deserialize properly in history server,SPARK-18462,13020998,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,16/Nov/16 01:23,18/Nov/16 02:45,14/Jul/23 06:29,18/Nov/16 02:45,2.0.0,,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,,,,,,"The following test fails with a ClassCastException due to oddities in how Jackson object mapping works, breaking the SQL tab in the history server:

{code}
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/ui/SQLListenerSuite.scala
@@ -19,6 +19,7 @@ package org.apache.spark.sql.execution.ui

 import java.util.Properties

+import org.json4s.jackson.JsonMethods._
 import org.mockito.Mockito.mock

 import org.apache.spark._
@@ -35,7 +36,7 @@ import org.apache.spark.sql.execution.{LeafExecNode, QueryExecution, SparkPlanIn
 import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}
 import org.apache.spark.sql.test.SharedSQLContext
 import org.apache.spark.ui.SparkUI
-import org.apache.spark.util.{AccumulatorMetadata, LongAccumulator}
+import org.apache.spark.util.{AccumulatorMetadata, JsonProtocol, LongAccumulator}


 class SQLListenerSuite extends SparkFunSuite with SharedSQLContext {
@@ -416,6 +417,20 @@ class SQLListenerSuite extends SparkFunSuite with SharedSQLContext {
     assert(driverUpdates(physicalPlan.longMetric(""dummy"").id) == expectedAccumValue)
   }

+  test(""roundtripping SparkListenerDriverAccumUpdates through JsonProtocol"") {
+    val event = SparkListenerDriverAccumUpdates(1L, Seq((2L, 3L)))
+    val actualJsonString = compact(render(JsonProtocol.sparkEventToJson(event)))
+    val newEvent = JsonProtocol.sparkEventFromJson(parse(actualJsonString))
+    newEvent match {
+      case SparkListenerDriverAccumUpdates(executionId, accums) =>
+        assert(executionId == 1L)
+        accums.foreach { case (a, b) =>
+          assert(a == 2L)
+          assert(b == 3L)
+        }
+    }
+  }
+
{code}",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 18 00:11:06 UTC 2016,,,,,,,,,,"0|i36din:",9223372036854775807,,,,,,,,,,,,,2.0.3,2.1.0,,,,,,,,,,"16/Nov/16 01:32;joshrosen;{code}
[info] - roundtripping SparkListenerDriverAccumUpdates through JsonProtocol (SPARK-18462) *** FAILED *** (547 milliseconds)
[info]   java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long
[info]   at scala.runtime.BoxesRunTime.unboxToLong(BoxesRunTime.java:105)
[info]   at scala.Tuple2._1$mcJ$sp(Tuple2.scala:20)
[info]   at org.apache.spark.sql.execution.ui.SQLListenerSuite$$anonfun$8$$anonfun$apply$mcV$sp$28.apply(SQLListenerSuite.scala:429)
[info]   at org.apache.spark.sql.execution.ui.SQLListenerSuite$$anonfun$8$$anonfun$apply$mcV$sp$28.apply(SQLListenerSuite.scala:429)
[info]   at scala.collection.immutable.List.foreach(List.scala:381)
[info]   at org.apache.spark.sql.execution.ui.SQLListenerSuite$$anonfun$8.apply$mcV$sp(SQLListenerSuite.scala:429)
[info]   at org.apache.spark.sql.execution.ui.SQLListenerSuite$$anonfun$8.apply(SQLListenerSuite.scala:420)
[info]   at org.apache.spark.sql.execution.ui.SQLListenerSuite$$anonfun$8.apply(SQLListenerSuite.scala:420)
{code};;;","18/Nov/16 00:11;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15922;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
core dumped running Spark SQL on large data volume (100TB),SPARK-18458,13020980,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,kiszk,jfchen@us.ibm.com,jfchen@us.ibm.com,16/Nov/16 00:04,06/Dec/16 19:24,14/Jul/23 06:29,20/Nov/16 05:50,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,core,dump,,,,"Running a query on 100TB parquet database using the Spark master dated 11/04 dump cores on Spark executors.

The query is TPCDS query 82 (though this query is not the only one can produce this core dump, just the easiest one to re-create the error).

Spark output that showed the exception:
{noformat}
16/11/14 10:38:51 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_e68_1478924651089_0018_01_000074 on host: mer05x.svl.ibm.com. Exit status: 134. Diagnostics: Exception from container-launch.
Container id: container_e68_1478924651089_0018_01_000074
Exit code: 134
Exception message: /bin/bash: line 1: 4031216 Aborted                 (core dumped) /usr/jdk64/java-1.8.0-openjdk-1.8.0.77-0.b03.el7_2.x86_64/bin/java -server -Xmx24576m -Djava.io.tmpdir=/data4/hadoop/yarn/local/usercache/spark/appcache/application_1478924651089_0018/container_e68_1478924651089_0018_01_000074/tmp '-Dspark.history.ui.port=18080' '-Dspark.driver.port=39855' -Dspark.yarn.app.container.log.dir=/data4/hadoop/yarn/log/application_1478924651089_0018/container_e68_1478924651089_0018_01_000074 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@192.168.10.101:39855 --executor-id 73 --hostname mer05x.svl.ibm.com --cores 2 --app-id application_1478924651089_0018 --user-class-path file:/data4/hadoop/yarn/local/usercache/spark/appcache/application_1478924651089_0018/container_e68_1478924651089_0018_01_000074/__app__.jar --user-class-path file:/data4/hadoop/yarn/local/usercache/spark/appcache/application_1478924651089_0018/container_e68_1478924651089_0018_01_000074/com.databricks_spark-csv_2.10-1.3.0.jar --user-class-path file:/data4/hadoop/yarn/local/usercache/spark/appcache/application_1478924651089_0018/container_e68_1478924651089_0018_01_000074/org.apache.commons_commons-csv-1.1.jar --user-class-path file:/data4/hadoop/yarn/local/usercache/spark/appcache/application_1478924651089_0018/container_e68_1478924651089_0018_01_000074/com.univocity_univocity-parsers-1.5.1.jar > /data4/hadoop/yarn/log/application_1478924651089_0018/container_e68_1478924651089_0018_01_000074/stdout 2> /data4/hadoop/yarn/log/application_1478924651089_0018/container_e68_1478924651089_0018_01_000074/stderr

Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1: 4031216 Aborted                 (core dumped) /usr/jdk64/java-1.8.0-openjdk-1.8.0.77-0.b03.el7_2.x86_64/bin/java -server -Xmx24576m -Djava.io.tmpdir=/data4/hadoop/yarn/local/usercache/spark/appcache/application_1478924651089_0018/container_e68_1478924651089_0018_01_000074/tmp '-Dspark.history.ui.port=18080' '-Dspark.driver.port=39855' -Dspark.yarn.app.container.log.dir=/data4/hadoop/yarn/log/application_1478924651089_0018/container_e68_1478924651089_0018_01_000074 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@192.168.10.101:39855 --executor-id 73 --hostname mer05x.svl.ibm.com --cores 2 --app-id application_1478924651089_0018 --user-class-path file:/data4/hadoop/yarn/local/usercache/spark/appcache/application_1478924651089_0018/container_e68_1478924651089_0018_01_000074/__app__.jar --user-class-path file:/data4/hadoop/yarn/local/usercache/spark/appcache/application_1478924651089_0018/container_e68_1478924651089_0018_01_000074/com.databricks_spark-csv_2.10-1.3.0.jar --user-class-path file:/data4/hadoop/yarn/local/usercache/spark/appcache/application_1478924651089_0018/container_e68_1478924651089_0018_01_000074/org.apache.commons_commons-csv-1.1.jar --user-class-path file:/data4/hadoop/yarn/local/usercache/spark/appcache/application_1478924651089_0018/container_e68_1478924651089_0018_01_000074/com.univocity_univocity-parsers-1.5.1.jar > /data4/hadoop/yarn/log/application_1478924651089_0018/container_e68_1478924651089_0018_01_000074/stdout 2> /data4/hadoop/yarn/log/application_1478924651089_0018/container_e68_1478924651089_0018_01_000074/stderr

        at org.apache.hadoop.util.Shell.runCommand(Shell.java:545)
        at org.apache.hadoop.util.Shell.run(Shell.java:456)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:722)
        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)


Container exited with a non-zero exit code 134

{noformat}

According to the source code, exit code 134 is 128+6, and 6 is        SIGABRT       6       Core    Abort signal from abort(3). The external signal killed executors. 

On the YARN side, the log is more clear:
{noformat}
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007fffe29e6bac, pid=3694385, tid=140735430203136
#
# JRE version: OpenJDK Runtime Environment (8.0_77-b03) (build 1.8.0_77-b03)
# Java VM: OpenJDK 64-Bit Server VM (25.77-b03 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# J 10342% C2 org.apache.spark.util.collection.unsafe.sort.RadixSort.sortKeyPrefixArrayAtByte(Lorg/apache/spark/unsafe/array/LongArray;I[JIIIZZ)V (386 bytes) @ 0x00007fffe29e6bac [0x00007fffe29e43c0+0x27ec]
#
# Core dump written. Default location: /data2/hadoop/yarn/local/usercache/spark/appcache/application_1479156026828_0006/container_e69_1479156026828_0006_01_000825/core or core.3694385
#
# An error report file with more information is saved as:
# /data2/hadoop/yarn/local/usercache/spark/appcache/application_1479156026828_0006/container_e69_1479156026828_0006_01_000825/hs_err_pid3694385.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#
{noformat}

And the  hs_err_pid3694385.log shows the stack:
{noformat}
Stack: [0x00007fff85432000,0x00007fff85533000],  sp=0x00007fff85530ce0,  free space=1019k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
J 3896 C1 org.apache.spark.unsafe.Platform.putLong(Ljava/lang/Object;JJ)V (10 bytes) @ 0x00007fffe1d3cdec [0x00007fffe1d3cde0+0xc]
j  org.apache.spark.util.collection.unsafe.sort.RadixSort.sortKeyPrefixArrayAtByte(Lorg/apache/spark/unsafe/array/LongArray;I[JIIIZZ)V+138
j  org.apache.spark.util.collection.unsafe.sort.RadixSort.sortKeyPrefixArray(Lorg/apache/spark/unsafe/array/LongArray;IIIIZZ)I+209
j  org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.getSortedIterator()Lorg/apache/spark/util/collection/unsafe/sort/UnsafeSorterIterator;+56
j  org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator()Lorg/apache/spark/util/collection/unsafe/sort/UnsafeSorterIterator;+62
j  org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort()Lscala/collection/Iterator;+4
j  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext()V+24
j  org.apache.spark.sql.execution.BufferedRowIterator.hasNext()Z+11
j  org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext()Z+4
j  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Lorg/apache/spark/sql/catalyst/expressions/GeneratedClass$GeneratedIterator;Lscala/collection/Iterator;Lscala/collection/Iterator;)Z+147
j  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Lorg/apache/spark/sql/catalyst/expressions/GeneratedClass$GeneratedIterator;)V+552
j  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext()V+17
J 3849 C1 org.apache.spark.sql.execution.BufferedRowIterator.hasNext()Z (30 bytes) @ 0x00007fffe1d5679c [0x00007fffe1d56520+0x27c]
j  org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext()Z+4
j  scala.collection.Iterator$$anon$11.hasNext()Z+4
j  org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(Lscala/collection/Iterator;)V+3
j  org.apache.spark.scheduler.ShuffleMapTask.runTask(Lorg/apache/spark/TaskContext;)Lorg/apache/spark/scheduler/MapStatus;+222
j  org.apache.spark.scheduler.ShuffleMapTask.runTask(Lorg/apache/spark/TaskContext;)Ljava/lang/Object;+2
j  org.apache.spark.scheduler.Task.run(JILorg/apache/spark/metrics/MetricsSystem;)Ljava/lang/Object;+152
j  org.apache.spark.executor.Executor$TaskRunner.run()V+423
j  java.util.concurrent.ThreadPoolExecutor.runWorker(Ljava/util/concurrent/ThreadPoolExecutor$Worker;)V+95
j  java.util.concurrent.ThreadPoolExecutor$Worker.run()V+5
j  java.lang.Thread.run()V+11
v  ~StubRoutines::call_stub
V  [libjvm.so+0x63d6ba]
V  [libjvm.so+0x63ab74]
V  [libjvm.so+0x63b189]
V  [libjvm.so+0x67e6a1]
V  [libjvm.so+0x9b3f5a]
V  [libjvm.so+0x869722]
C  [libpthread.so.0+0x7dc5]  start_thread+0xc5
{noformat}

This is not easily reproducible on smaller data volumes, e.g., 1TB or 10TB, but easily reproducible on 100TB+...so look into data types that may not be big enough to handle hundreds of billion.

",,DjvuLee,gurmukhd,hvanhovell,jfchen@us.ibm.com,kiszk,nchammas,nsyca,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18745,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 16 19:05:46 UTC 2016,,,,,,,,,,"0|i36den:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"16/Nov/16 01:14;kiszk;I work for this.;;;","16/Nov/16 15:43;hvanhovell;[~kiszk] Could you keep us posted of the progress you make. I would like to fix this for 2.1;;;","16/Nov/16 16:04;kiszk;I see. I will do that.;;;","16/Nov/16 18:35;kiszk;I worked with [~jfchen@us.ibm.com]. Then, I identified that a root cause is an integer overflow in a Java expression.

When SIGSEGV occurred, we identified {{dest}} is less than 0 at [RadixSort.java|https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RadixSort.java#L253-L254]. It should be greater than 0. It means that {{offsets\[bucket\]}} is less than 0.
Next, when we checked [transformCountsToOffsets()|https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RadixSort.java#L148-L168], we knew that {{outputOffset}} is less than 0.
Then, when we checked [RadixSort.java|https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RadixSort.java#L244-L245], we knew that {{outindex = 0x10\?\?\?\?\?\?}}. Since the result of multiplication is {{0x80\?\?\?\?\?\?}}, it means that less than 0 as int. This is because {{outIndex}} and {{8}} are integer type. Finally, we understood integer overflow occurs.

Here, the expression must be {{array.getBaseOffset() + outIndex * 8L}}.;;;","16/Nov/16 19:05;hvanhovell;Nice find!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ORC and other columnar formats using HiveShim read all columns when doing a simple count,SPARK-18457,13020965,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,a1ray,a1ray,a1ray,15/Nov/16 23:12,12/Dec/22 18:10,14/Jul/23 06:29,18/Nov/16 19:19,1.6.3,2.0.2,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Doing a `select count(*) from a_orc_table` reads all columns and thus is slower than a query selecting one like `select count(a_column) from a_orc_table`. Data read can be seen in the UI (appears to only be accurate for Hadoop 2.5+ based on comment in FileScanRDD.scala line 80).

I will create a PR shortly.",Hadoop 2.7.0,a1ray,apachespark,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 16 04:30:51 UTC 2016,,,,,,,,,,"0|i36dbb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/16 02:38;gurwls223;Hm, don't we push down no column when performing

{code}
count(*)
{code} 

?

{code}
val data = (0 to 255).zip(0 to 255).toDF(""a"", ""b"")
val path = ""/tmp/aa""
data.write.orc(path)
spark.read.orc(path).createOrReplaceTempView(""a_orc_table"")
spark.sql(""select count(*) from a_orc_table"").explain(true)
{code}

{code}
== Parsed Logical Plan ==
'Project [unresolvedalias('count(1), None)]
+- 'UnresolvedRelation `a_orc_table`

== Analyzed Logical Plan ==
count(1): bigint
Aggregate [count(1) AS count(1)#28L]
+- SubqueryAlias a_orc_table
   +- Relation[a#16,b#17] orc

== Optimized Logical Plan ==
Aggregate [count(1) AS count(1)#28L]
+- Project
   +- Relation[a#16,b#17] orc

== Physical Plan ==
*HashAggregate(keys=[], functions=[count(1)], output=[count(1)#28L])
+- Exchange SinglePartition
   +- *HashAggregate(keys=[], functions=[partial_count(1)], output=[count#30L])
      +- *FileScan orc [] Batched: false, Format: ORC, Location: InMemoryFileIndex[file:/tmp/aa], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<>
{code}

It seems ORC datasource does not try to read all columns and I just verified this by debugging with IDE.;;;","16/Nov/16 04:17;apachespark;User 'aray' has created a pull request for this issue:
https://github.com/apache/spark/pull/15898;;;","16/Nov/16 04:30;gurwls223;Ah, I see {{HiveShim.appendReadColumns}} is not called when no columns are set and it reads all columns because {{hive.io.file.read.all.columns}} is {{null}} but get {{true}} by default.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR running in yarn-cluster mode should not download Spark package,SPARK-18444,13020751,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,yanboliang,yanboliang,yanboliang,15/Nov/16 09:57,22/Nov/16 08:10,14/Jul/23 06:29,22/Nov/16 08:10,,,,,,,,,2.0.3,2.1.0,,,SparkR,,,,,,,,0,,,,,,"When running SparkR job in yarn-cluster mode, it will download Spark package from apache website which is not necessary.
{code}
./bin/spark-submit --master yarn-cluster ./examples/src/main/r/dataframe.R
{code}
The following is output:
{code}
Attaching package: ‘SparkR’

The following objects are masked from ‘package:stats’:

    cov, filter, lag, na.omit, predict, sd, var, window

The following objects are masked from ‘package:base’:

    as.data.frame, colnames, colnames<-, drop, endsWith, intersect,
    rank, rbind, sample, startsWith, subset, summary, transform, union

Spark not found in SPARK_HOME:
Spark not found in the cache directory. Installation will start.
MirrorUrl not provided.
Looking for preferred site from apache website...
......
{code} 
There's no {{SPARK_HOME}} in yarn-cluster mode since the R process is in a remote host of the yarn cluster rather than in the client host. The JVM comes up first and the R process then connects to it. So in such cases we should never have to download Spark as Spark is already running.",,apachespark,tgraves,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 15 10:06:07 UTC 2016,,,,,,,,,,"0|i36bzr:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"15/Nov/16 10:06;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/15888;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix nullability of WrapOption.,SPARK-18442,13020703,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ueshin,ueshin,ueshin,15/Nov/16 05:49,17/Nov/16 03:22,14/Jul/23 06:29,17/Nov/16 03:22,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,The nullability of {{WrapOption}} should be {{false}}.,,apachespark,cloud_fan,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 17 03:22:25 UTC 2016,,,,,,,,,,"0|i36bp3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Nov/16 05:54;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/15887;;;","17/Nov/16 03:22;cloud_fan;Issue resolved by pull request 15887
[https://github.com/apache/spark/pull/15887];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
isin causing SQL syntax error with JDBC,SPARK-18436,13020503,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiangxb1987,MiniMizer,MiniMizer,14/Nov/16 13:38,25/Nov/16 20:45,14/Jul/23 06:29,25/Nov/16 20:45,2.0.1,,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,jdbc,sql,,,,"When using a JDBC data source, the ""isin"" function generates invalid SQL syntax when called with an empty array, which causes the JDBC driver to throw an exception.
If the array is not empty, it works fine.

In the below example you can assume that SOURCE_CONNECTION, SQL_DRIVER and TABLE are all correctly defined.
{noformat}
scala> val filter = Array[String]()
filter: Array[String] = Array()

scala> val sortDF = spark.read.format(""jdbc"").options(Map(""url"" -> SOURCE_CONNECTION, ""driver"" -> SQL_DRIVER, ""dbtable"" -> TABLE)).load().filter($""cl_ult"".isin(filter:_*))
sortDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ibi_bulk_id: bigint, ibi_row_id: int ... 174 more fields]

scala> sortDF.show()
16/11/14 15:35:46 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 205)
com.microsoft.sqlserver.jdbc.SQLServerException: Incorrect syntax near ')'.
        at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:216)
        at com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1515)
        at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(SQLServerPreparedStatement.java:404)
        at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecCmd.doExecute(SQLServerPreparedStatement.java:350)
        at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:5696)
        at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:1715)
        at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:180)
        at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:155)
        at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeQuery(SQLServerPreparedStatement.java:285)
        at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$$anon$1.<init>(JDBCRDD.scala:408)
        at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:379)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
        at org.apache.spark.scheduler.Task.run(Task.scala:86)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{noformat}","Linux, SQL Server 2012",apachespark,hvanhovell,jiangxb1987,MiniMizer,tsuresh,zgl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 22 10:28:31 UTC 2016,,,,,,,,,,"0|i36agn:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"14/Nov/16 14:15;srowen;You're saying that "" ... IN () "" is a SQL error in MSSQL?

I wonder if it would be smarter to handle this much farther upstream, because this condition is always false anyway.

It's not my area, but is it something you might comment on, [~hvanhovell] ? It seems like it might belong in Analyzer.;;;","14/Nov/16 19:29;MiniMizer;That's right, IN () is invalid. IN ('') is the correct syntax for an empty list.;;;","14/Nov/16 19:39;hvanhovell;An empty {{IN}} is currently not folded during optimization (because the {{cl_ult}} is not foldable). I do think this worth adding.
;;;","16/Nov/16 16:42;jiangxb1987;I created  a simpler example for this issue:
{code}
val filter = Array[Int]()
val df = Seq((1, (1, 1))).toDF(""a"", ""b"")
df.filter($""a"".isin(filter:_*)).explain
{code}

The `queryExecution` of this DF is:
{code}
== Parsed Logical Plan ==
'Filter 'a IN ()
+- Project [_1#2 AS a#5, _2#3 AS b#6]
   +- LocalRelation [_1#2, _2#3]

== Analyzed Logical Plan ==
a: int, b: struct<_1:int,_2:int>
Filter a#5 IN ()
+- Project [_1#2 AS a#5, _2#3 AS b#6]
   +- LocalRelation [_1#2, _2#3]

== Optimized Logical Plan ==
Project [_1#2 AS a#5, _2#3 AS b#6]
+- Filter _1#2 IN ()
   +- LocalRelation [_1#2, _2#3]

== Physical Plan ==
*Project [_1#2 AS a#5, _2#3 AS b#6]
+- *Filter _1#2 IN ()
   +- LocalTableScan [_1#2, _2#3]
{code}

In SQL we will throw a `ParseException` for
{code}
SELECT * FROM t WHERE a IN ()
{code}

Seems we have different behavior between DF and SQL. Should we throw a `AnalyzeException` in `checkAnalyze`? Or should we deal with empty `IN` in `OptimizeIn`? Should we change the behavior of SQL too?
After we have defined the right behavior for this, I would like to send a PR. Thank you!;;;","18/Nov/16 02:09;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/15925;;;","18/Nov/16 09:25;MiniMizer;Would it not make more sense to push an empty list with the correct syntax into the JDBC, rather than throwing an exception that the list cannot be empty?
From a SQL perspective, the query SELECT * FROM TABLE WHERE Column IN ('') is a perfectly valid query.
In my specific use case, the array which I pass to isin is dynamically generated in the code and can sometimes be empty, in which case I would expect the dataframe to simply contain no rows.;;;","18/Nov/16 09:32;srowen;Yes, see the PR. This is perfectly valid and should not be an error in any API.;;;","22/Nov/16 10:28;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/15977;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Returned Message Null when Hitting an Invocation Exception of Function Lookup.,SPARK-18430,13020430,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,14/Nov/16 06:37,16/Nov/16 14:03,14/Jul/23 06:29,15/Nov/16 05:21,2.0.0,2.1.0,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,,,,,,"When the exception is an invocation exception during function lookup, we return a useless/confusing error message:

For example, 
{code}
      df.selectExpr(""format_string()"")
{code}
or 
{code}
      df.selectExpr(""concat_ws()"")
{code}

Below is the error message we got:
{code}
null; line 1 pos 0
org.apache.spark.sql.AnalysisException: null; line 1 pos 0
{code}",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 16 07:37:07 UTC 2016,,,,,,,,,,"0|i36a0f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/16 06:46;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15878;;;","16/Nov/16 07:37;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15902;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`JDBCRelation.insert` should not remove Spark options,SPARK-18419,13020277,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,12/Nov/16 13:44,02/Dec/16 13:50,14/Jul/23 06:29,02/Dec/16 13:50,2.0.2,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Currently, `JDBCRelation.insert` removes Spark options too early by mistakenly using `asConnectionProperties`. Spark options like `numPartitions` should be passed into `DataFrameWriter.jdbc` correctly. This bug have been hidden because `JDBCOptions.asConnectionProperties` fails to filter out the mixed-case options. This issue aims to fix both.

*JDBCRelation.insert*
{code}
override def insert(data: DataFrame, overwrite: Boolean): Unit = {
  val url = jdbcOptions.url
  val table = jdbcOptions.table
- val properties = jdbcOptions.asConnectionProperties
+ val properties = jdbcOptions.asProperties
  data.write
    .mode(if (overwrite) SaveMode.Overwrite else SaveMode.Append)
    .jdbc(url, table, properties)
{code}

*JDBCOptions.asConnectionProperties*
{code}
scala> import org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions
import org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions

scala> import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap
import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap

scala> new JDBCOptions(Map(""url"" -> ""jdbc:mysql://localhost:3306/temp"", ""dbtable"" -> ""t1"", ""numPartitions"" -> ""10"")).asConnectionProperties
res0: java.util.Properties = {numpartitions=10}

scala> new JDBCOptions(new CaseInsensitiveMap(Map(""url"" -> ""jdbc:mysql://localhost:3306/temp"", ""dbtable"" -> ""t1"", ""numPartitions"" -> ""10""))).asConnectionProperties
res1: java.util.Properties = {numpartitions=10}
{code}",,apachespark,dongjoon,maropu,WeiqingYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 01 19:27:29 UTC 2016,,,,,,,,,,"0|i3692f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/16 13:48;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/15863;;;","01/Dec/16 19:04;srowen;[~dongjoon] given your comment on the PR, is this resolved?;;;","01/Dec/16 19:27;dongjoon;Sorry, I thought it's resolved.
But, when I tried to verify it, I found it's not.
So, I reopen and refresh the testcase and PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make release script hadoop profiles aren't correctly specified.,SPARK-18418,13020247,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,holden,holden,holden,12/Nov/16 04:10,12/Nov/16 22:51,14/Jul/23 06:29,12/Nov/16 22:51,2.1.0,,,,,,,,2.1.0,2.2.0,,,Build,Project Infra,,,,,,,0,,,,,,Split from https://github.com/apache/spark/pull/15659/files,,apachespark,holden,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16967,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 12 22:51:28 UTC 2016,,,,,,,,,,"0|i368vr:",9223372036854775807,,,,,,,,,,,,,2.1.0,2.2.0,,,,,,,,,,"12/Nov/16 04:13;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/15860;;;","12/Nov/16 22:49;joshrosen;For reference: this patch fixes a bug which was introduced in SPARK-16967 and affects both {{master}} and {{branch-2.1}}.;;;","12/Nov/16 22:51;joshrosen;Issue resolved by pull request 15860
[https://github.com/apache/spark/pull/15860];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Weird Plan Output when CTE used in RunnableCommand,SPARK-18415,13020203,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,11/Nov/16 21:21,23/Aug/17 18:24,14/Jul/23 06:29,16/Nov/16 16:26,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Currently, when CTE is used in RunnableCommand, the Analyzer does not replace the logical node `With`. The child plan of RunnableCommand is not resolved. However, the output of the `With` plan node looks very confusing.

For example, 
{code}
sql(""CREATE VIEW cte_view AS WITH w AS (SELECT 1 AS n) SELECT n FROM w"").explain()
{code}

The output is like
{code}
ExecutedCommand
   +- CreateViewCommand `w`, WITH w AS (SELECT 1 AS n) SELECT n FROM w, false, false, org.apache.spark.sql.execution.command.PersistedView$@2251b87b
         +- 'With [(w,SubqueryAlias w
               +- Project [1 AS n#16]
                  +- OneRowRelation$
               )]
            +- 'Project ['n]
               +- 'UnresolvedRelation `w`
{code}",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 11 21:27:06 UTC 2016,,,,,,,,,,"0|i368lz:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"11/Nov/16 21:27;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15854;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR spark.randomForest classification throws exception when training on libsvm data,SPARK-18412,13020067,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yanboliang,yanboliang,yanboliang,11/Nov/16 10:20,14/Nov/16 04:27,14/Jul/23 06:29,14/Nov/16 04:27,,,,,,,,,2.1.0,,,,ML,SparkR,,,,,,,0,,,,,,"{{spark.randomForest}} classification throws exception when training on libsvm data. It can be reproduced as following:
{code}
df <- read.df(""data/mllib/sample_multiclass_classification_data.txt"", source = ""libsvm"")
model <- spark.randomForest(df, label ~ features, ""classification"")
{code}
The exception is:
{code}
Error in handleErrors(returnStatus, conn) :
  java.lang.IllegalArgumentException: requirement failed: If label column already exists, forceIndexLabel can not be set with true.
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.ml.feature.RFormula.transformSchema(RFormula.scala:205)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:70)
	at org.apache.spark.ml.feature.RFormula.fit(RFormula.scala:136)
	at org.apache.spark.ml.r.RandomForestClassifierWrapper$.fit(RandomForestClassificationWrapper.scala:86)
	at org.apache.spark.ml.r.RandomForestClassifierWrapper.fit(RandomForestClassificationWrapper.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:172)
{code}

This error is caused by the label column of the R formula already exists, we can not force to index label. However, it must index the label for classification algorithms, so we need to rename the RFormula.labelCol to a new value and then we can index the original label.
This issue also appears at other algorithms: spark.naiveBayes, spark.glm(only for binomial family) and spark.gbt (only for classification).",,apachespark,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 11 11:57:04 UTC 2016,,,,,,,,,,"0|i367rr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/16 11:57;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/15851;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inferred partition columns cause assertion error,SPARK-18407,13019954,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,brkyvz,marmbrus,marmbrus,10/Nov/16 21:32,28/Nov/16 10:09,14/Jul/23 06:29,28/Nov/16 10:09,2.0.2,,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,"[This assertion|https://github.com/apache/spark/blob/16eaad9daed0b633e6a714b5704509aa7107d6e5/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala#L408] fails when you run a stream against json data that is stored in partitioned folders, if you manually specify the schema and that schema omits the partitioned columns.

My hunch is that we are inferring those columns even though the schema is being passed in manually and adding them to the end.

While we are fixing this bug, it would be nice to make the assertion better.  Truncating is not terribly useful as, at least in my case, it truncated the most interesting part.  I changed it to this while debugging:

{code}
          s""""""
             |Batch does not have expected schema
             |Expected: ${output.mkString("","")}
             |Actual: ${newPlan.output.mkString("","")}
             |
             |== Original ==
             |$logicalPlan
             |
             |== Batch ==
             |$newPlan
           """""".stripMargin
{code}

I also tried specifying the partition columns in the schema and now it appears that they are filled with corrupted data.",,apachespark,brkyvz,codingcat,lwlin,marmbrus,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18510,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 25 17:17:58 UTC 2016,,,,,,,,,,"0|i3672n:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"20/Nov/16 03:35;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/15942;;;","25/Nov/16 17:17;brkyvz;This is also resolved as part of https://issues.apache.org/jira/browse/SPARK-18510;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race between end-of-task and completion iterator read lock release,SPARK-18406,13019949,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiangxb1987,joshrosen,joshrosen,10/Nov/16 21:11,22/May/19 02:25,14/Jul/23 06:29,24/May/17 07:55,2.0.0,2.0.1,,,,,,,2.0.3,2.1.2,2.2.0,,Block Manager,Spark Core,,,,,,,2,,,,,,"The following log comes from a production streaming job where executors periodically die due to uncaught exceptions during block release:


{code}
16/11/07 17:11:06 INFO CoarseGrainedExecutorBackend: Got assigned task 7921
16/11/07 17:11:06 INFO Executor: Running task 0.0 in stage 2390.0 (TID 7921)
16/11/07 17:11:06 INFO CoarseGrainedExecutorBackend: Got assigned task 7922
16/11/07 17:11:06 INFO Executor: Running task 1.0 in stage 2390.0 (TID 7922)
16/11/07 17:11:06 INFO CoarseGrainedExecutorBackend: Got assigned task 7923
16/11/07 17:11:06 INFO Executor: Running task 2.0 in stage 2390.0 (TID 7923)
16/11/07 17:11:06 INFO TorrentBroadcast: Started reading broadcast variable 2721
16/11/07 17:11:06 INFO CoarseGrainedExecutorBackend: Got assigned task 7924
16/11/07 17:11:06 INFO Executor: Running task 3.0 in stage 2390.0 (TID 7924)
16/11/07 17:11:06 INFO MemoryStore: Block broadcast_2721_piece0 stored as bytes in memory (estimated size 5.0 KB, free 4.9 GB)
16/11/07 17:11:06 INFO TorrentBroadcast: Reading broadcast variable 2721 took 3 ms
16/11/07 17:11:06 INFO MemoryStore: Block broadcast_2721 stored as values in memory (estimated size 9.4 KB, free 4.9 GB)
16/11/07 17:11:06 INFO BlockManager: Found block rdd_2741_1 locally
16/11/07 17:11:06 INFO BlockManager: Found block rdd_2741_3 locally
16/11/07 17:11:06 INFO BlockManager: Found block rdd_2741_2 locally
16/11/07 17:11:06 INFO BlockManager: Found block rdd_2741_4 locally
16/11/07 17:11:06 INFO PythonRunner: Times: total = 2, boot = -566, init = 567, finish = 1
16/11/07 17:11:06 INFO PythonRunner: Times: total = 7, boot = -540, init = 541, finish = 6
16/11/07 17:11:06 INFO Executor: Finished task 2.0 in stage 2390.0 (TID 7923). 1429 bytes result sent to driver
16/11/07 17:11:06 INFO PythonRunner: Times: total = 8, boot = -532, init = 533, finish = 7
16/11/07 17:11:06 INFO Executor: Finished task 3.0 in stage 2390.0 (TID 7924). 1429 bytes result sent to driver
16/11/07 17:11:06 ERROR Executor: Exception in task 0.0 in stage 2390.0 (TID 7921)
java.lang.AssertionError: assertion failed
	at scala.Predef$.assert(Predef.scala:165)
	at org.apache.spark.storage.BlockInfo.checkInvariants(BlockInfoManager.scala:84)
	at org.apache.spark.storage.BlockInfo.readerCount_$eq(BlockInfoManager.scala:66)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$releaseAllLocksForTask$2$$anonfun$apply$2.apply(BlockInfoManager.scala:362)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$releaseAllLocksForTask$2$$anonfun$apply$2.apply(BlockInfoManager.scala:361)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$releaseAllLocksForTask$2.apply(BlockInfoManager.scala:361)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$releaseAllLocksForTask$2.apply(BlockInfoManager.scala:356)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:356)
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:646)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:281)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/11/07 17:11:06 INFO CoarseGrainedExecutorBackend: Got assigned task 7925
16/11/07 17:11:06 INFO Executor: Running task 0.1 in stage 2390.0 (TID 7925)
16/11/07 17:11:06 INFO BlockManager: Found block rdd_2741_1 locally
16/11/07 17:11:06 INFO PythonRunner: Times: total = 41, boot = -536, init = 576, finish = 1
16/11/07 17:11:06 INFO Executor: Finished task 1.0 in stage 2390.0 (TID 7922). 1429 bytes result sent to driver
16/11/07 17:11:06 ERROR Utils: Uncaught exception in thread stdout writer for /databricks/python/bin/python
java.lang.AssertionError: assertion failed: Block rdd_2741_1 is not locked for reading
	at scala.Predef$.assert(Predef.scala:179)
	at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:294)
	at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:630)
	at org.apache.spark.storage.BlockManager$$anonfun$1.apply$mcV$sp(BlockManager.scala:434)
	at org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:46)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:35)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1882)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
16/11/07 17:11:06 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[stdout writer for /databricks/python/bin/python,5,main]
java.lang.AssertionError: assertion failed: Block rdd_2741_1 is not locked for reading
	at scala.Predef$.assert(Predef.scala:179)
	at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:294)
	at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:630)
	at org.apache.spark.storage.BlockManager$$anonfun$1.apply$mcV$sp(BlockManager.scala:434)
	at org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:46)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:35)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1882)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala
{code}

I think that there's some sort of internal race condition between a task finishing (TID 7921) and automatically releasing locks and between some ""automatically release locks on hitting the end of an iterator"" logic running in a separate thread. The log above came from a production streaming job where executors periodically died with this type of error.",,apachespark,bruce_zhao11,cloud_fan,codingcat,jiangxb1987,joshrosen,lwlin,myjay610,taichi.sano,yxiao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25139,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 22 02:25:50 UTC 2019,,,,,,,,,,"0|i3671j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/17 17:24;myjay610;Similar issue in doing basic RDD operations (like checking for empty RDD's) within streaming jobs:

17/01/19 16:43:45 WARN BlockManager: Block input-0-1484840671538 replicated to only 0 peer(s) instead of 1 peers
17/01/19 16:43:46 WARN MetricsHelper: No metrics scope set in thread RecurringTimer - Kinesis Checkpointer - Worker localhost:65fe618d-c0a7-4fea-b710-0f1b5c6498f2, getMetricsScope returning NullMetricsScope.
17/01/19 16:44:51 ERROR Executor: Exception in task 0.0 in stage 212.0 (TID 4180)
java.lang.AssertionError: assertion failed
	at scala.Predef$.assert(Predef.scala:156)
	at org.apache.spark.storage.BlockInfo.checkInvariants(BlockInfoManager.scala:84)
	at org.apache.spark.storage.BlockInfo.readerCount_$eq(BlockInfoManager.scala:66)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$releaseAllLocksForTask$2$$anonfun$apply$2.apply(BlockInfoManager.scala:362)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$releaseAllLocksForTask$2$$anonfun$apply$2.apply(BlockInfoManager.scala:361)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$releaseAllLocksForTask$2.apply(BlockInfoManager.scala:361)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$releaseAllLocksForTask$2.apply(BlockInfoManager.scala:356)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:356)
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:646)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:281)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
17/01/19 16:44:51 WARN TaskSetManager: Lost task 0.0 in stage 212.0 (TID 4180, localhost): java.lang.AssertionError: assertion failed
	at scala.Predef$.assert(Predef.scala:156)
	at org.apache.spark.storage.BlockInfo.checkInvariants(BlockInfoManager.scala:84)
	at org.apache.spark.storage.BlockInfo.readerCount_$eq(BlockInfoManager.scala:66)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$releaseAllLocksForTask$2$$anonfun$apply$2.apply(BlockInfoManager.scala:362)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$releaseAllLocksForTask$2$$anonfun$apply$2.apply(BlockInfoManager.scala:361)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$releaseAllLocksForTask$2.apply(BlockInfoManager.scala:361)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$releaseAllLocksForTask$2.apply(BlockInfoManager.scala:356)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:356)
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:646)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:281)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/01/19 16:44:51 ERROR TaskSetManager: Task 0 in stage 212.0 failed 1 times; aborting job
17/01/19 16:44:51 ERROR JobScheduler: Error running job streaming job 1484844270000 ms.0
org.apache.spark.SparkException: An exception was raised by Python:
Traceback (most recent call last):
  File ""/root/spark/python/lib/pyspark.zip/pyspark/streaming/util.py"", line 65, in call
    r = self.func(t, *rdds)
  File ""/root/spark/python/lib/pyspark.zip/pyspark/streaming/dstream.py"", line 159, in <lambda>
    func = lambda t, rdd: old_func(rdd)
  File ""/root/streamtest.py"", line 519, in multiplex
    if not flow_rdd.isEmpty():
  File ""/root/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1343, in isEmpty
    return self.getNumPartitions() == 0 or len(self.take(1)) == 0
  File ""/root/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1310, in take
    res = self.context.runJob(self, takeUpToNumLeft, p)
  File ""/root/spark/python/lib/pyspark.zip/pyspark/context.py"", line 933, in runJob
    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)
  File ""/root/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File ""/root/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco
    return f(*a, **kw)
  File ""/root/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value
    format(target_id, ""."", name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.;;;","14/Apr/17 18:51;yxiao;The same JIRA is found under https://issues-test.apache.org/jira/browse/SPARK-18406.
Same issue observed in spark 2.1.0 as well.

The issue is observed on some simple spark query that compiles into 3 stages. I have some custom RDD being used in this case, and it is registered for persistence. In the compute() method of the custom RDD, I spawn a new thread to compute the data in the background, and then immediately return an abstract iterator (wrapped under a InterruptibleIterator) that gets data from the background computation on demand. The assertion happens when iterator of the parent RDD reaches the end of the data. This issue doesn't always happen when the custom RDD is used in the query, regardless being used once or multiple times.

The issue is related to the new thread I created which accesses data from the input iterator of parent RDD. The new thread is missing the TSS(thread-specific-storage) for TaskContext. I see BlockInfoManager is using this TSS TaskContext as key to search the storage.

Here is log showing the task ID being unset in this thread: Line 1819: 2017/04/13 15:01:01.674 [Thread-33]: TRACE storage.BlockInfoManager: Task -1024 releasing lock for rdd_25_0

However, I have no way to set TSS for my thread now because the method is made protected as below:
object TaskContext {
 ...
private[this] val taskContext: ThreadLocal[TaskContext] = new ThreadLocal[TaskContext]
// Note: protected[spark] instead of private[spark] to prevent the following two from
 // showing up in JavaDoc.
 /** Set the thread local TaskContext. Internal to Spark. */
 protected[spark] def setTaskContext(tc: TaskContext): Unit = taskContext.set(tc)

Just to confirm my theory, I made the TaskContext.setTaskContext public, and called it in the beginning of my thread. The use cases that were failing consistently with assertion on lock-release now run successful in all scenarios I have tried, which include having different number of src/shuffle partitions, number of executors, async vs. sequential execution (for having multiple downstream custom RDDs pulling data from upstream RDD).;;;","16/Apr/17 04:33;joshrosen;I can see how allowing user-level code to call setTaskContext() can fix this issue but it's not ideal because it still places the burden on the end users to call the setTaskContext() method in their code.

Instead, I think a cleaner fix would be to have the CompletionIterator record the task ID when it's instantiated so that the same task ID can be used even if the completion occurs in a different thread (the idea is to reduce our reliance on thread locals: there are reasons why we couldn't completely remove them (API changes), but there are parts of the internals where we can propagate more efficiently).

To move forward here, my suggestion is that we write a failing regression test based on the description provided by [~yxiao], then experiment on my suggested approach of more explicit threading of task ids into closeable objects when they're first created.

I'm on vacation this week and won't be able to help with this until Monday, April 24th, so someone else will need to help / review if this is urgent.;;;","16/Apr/17 19:16;yxiao;Thanks Josh for the quick response!
This issue is critical to my company's use cases, where for the purpose of performance we have to use custom RDD to take input from multiple parent RDDs, and use existing computation logic (in a black box) in the background to pull the result.
;;;","23/May/17 23:24;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/18076;;;","23/May/17 23:29;yxiao;Thanks for the fix. What spark release will have it?
Can we get a patch on top of spark 2.1.0?;;;","24/May/17 07:55;cloud_fan;we will backport this to 2.1 and 2.0 later;;;","24/May/17 19:06;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/18096;;;","24/May/17 21:54;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/18099;;;","03/Aug/17 23:38;taichi.sano;Hello,
I am experiencing an issue very similar to this. I am currently trying to do a groupByKeyAndWindow() with batch size of 1, window size of 80, and shift size of 1 from data that is being streamed from Kafka (ver 0.10) with Direct Streaming. Every once in a while, I encounter the AssertionError like so:

17/08/03 22:32:19 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 20936.0 (TID 4409)
java.lang.AssertionError: assertion failed
	at scala.Predef$.assert(Predef.scala:156)
	at org.apache.spark.storage.BlockInfo.checkInvariants(BlockInfoManager.scala:84)
	at org.apache.spark.storage.BlockInfo.readerCount_$eq(BlockInfoManager.scala:66)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$releaseAllLocksForTask$2$$anonfun$apply$2.apply(BlockInfoManager.scala:367)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$releaseAllLocksForTask$2$$anonfun$apply$2.apply(BlockInfoManager.scala:366)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$releaseAllLocksForTask$2.apply(BlockInfoManager.scala:366)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$releaseAllLocksForTask$2.apply(BlockInfoManager.scala:361)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:361)
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:736)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:342)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/08/03 22:32:19 ERROR org.apache.spark.executor.Executor: Exception in task 0.1 in stage 20936.0 (TID 4410)
java.lang.AssertionError: assertion failed
	at scala.Predef$.assert(Predef.scala:156)
	at org.apache.spark.storage.BlockInfo.checkInvariants(BlockInfoManager.scala:84)
	at org.apache.spark.storage.BlockInfo.readerCount_$eq(BlockInfoManager.scala:66)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$releaseAllLocksForTask$2$$anonfun$apply$2.apply(BlockInfoManager.scala:367)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$releaseAllLocksForTask$2$$anonfun$apply$2.apply(BlockInfoManager.scala:366)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$releaseAllLocksForTask$2.apply(BlockInfoManager.scala:366)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$releaseAllLocksForTask$2.apply(BlockInfoManager.scala:361)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:361)
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:736)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:342)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/08/03 22:32:19 ERROR org.apache.spark.util.Utils: Uncaught exception in thread stdout writer for /opt/conda/bin/python
java.lang.AssertionError: assertion failed: Block rdd_30291_0 is not locked for reading
	at scala.Predef$.assert(Predef.scala:170)
	at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:299)
	at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:720)
	at org.apache.spark.storage.BlockManager$$anonfun$1.apply$mcV$sp(BlockManager.scala:516)
	at org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:46)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:35)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:509)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:333)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/08/03 22:32:19 ERROR org.apache.spark.util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[stdout writer for /opt/conda/bin/python,5,main]
java.lang.AssertionError: assertion failed: Block rdd_30291_0 is not locked for reading
	at scala.Predef$.assert(Predef.scala:170)
	at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:299)
	at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:720)
	at org.apache.spark.storage.BlockManager$$anonfun$1.apply$mcV$sp(BlockManager.scala:516)
	at org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:46)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:35)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:509)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:333)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)

which also kills the executor. Sometimes a new executor is spawned to pick up where the dead executor left off but sometimes the whole Spark job also crashes due to this error. I'm running version 2.2.0 on Google Dataproc on a single node cluster. ;;;","18/Sep/17 05:00;hadoopqa;
    [ https://issues-test.apache.org/jira/browse/SPARK-18406?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16167180#comment-16167180 ] 

Yongqin Xiao commented on SPARK-18406:
--------------------------------------

[~cloud_fan], I see there are 3 check-ins for this issue, touching multiple files. You mentioned the fix will be backport to spark2.1.0. Can you let me know which single submission in spark2.1.0 will address the issue?
The reason I am asking is that my company may not update spark version to 2.2 very soon, I will have to port your fix to our company's version of spark 2.1.0 and 2.0.1. I cannot just use latest spark 2.1.0 even after you backport the fix because we have other patches on top of spark 2.1.0, some were fixed by ourselves.
Thanks for your help.




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: issues-unsubscribe@spark.apache.org
For additional commands, e-mail: issues-help@spark.apache.org

;;;","18/Sep/17 06:56;hadoopqa;
    [ https://issues-test.apache.org/jira/browse/SPARK-18406?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16167181#comment-16167181 ] 

Wenchen Fan commented on SPARK-18406:
-------------------------------------

https://github.com/apache/spark/pull/18099 is the PR that backported the fix to 2.1




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: issues-unsubscribe@spark.apache.org
For additional commands, e-mail: issues-help@spark.apache.org

;;;","16/Nov/17 06:51;david_zhang;we still find this issue in Spark 2.2,  I check this case change code and find spark version 2.2 contain these change. but our error log:
17/11/03 08:00:10 ERROR Utils: Uncaught exception in thread stdout writer for python
java.lang.AssertionError: assertion failed: Block input-0-1508745006978 is not locked for reading
at scala.Predef$.assert(Predef.scala:170)
at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:299)
at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:720)
at org.apache.spark.storage.BlockManager$$anonfun$1.apply$mcV$sp(BlockManager.scala:516)
at org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:46)
at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:35)
at scala.collection.Iterator$class.foreach(Iterator.scala:893)
at org.apache.spark.util.CompletionIterator.foreach(CompletionIterator.scala:26)
at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:509)
at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:333)
at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
;;;","01/May/19 00:12;jiangxb1987;This problem still exists in PythonRunner, since python side uses a pre-fetch model to consume the upstream data, and open another thread to serve output data to downstream operators, thus it's possible the Task finishes first and trigger the task cleanup logic, and then the CompletionIterator try to release the write lock it holds on some blocks and found the lock has already been released. I'll submit a PR to bypass the issue later.;;;","06/May/19 23:39;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/24542;;;","06/May/19 23:39;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/24542;;;","08/May/19 06:13;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/24552;;;","22/May/19 02:25;apachespark;User 'rezasafi' has created a pull request for this issue:
https://github.com/apache/spark/pull/24670;;;",,,,,,,,,,,,,,,,,,,,,,,,,
ObjectHashAggregateSuite is being flaky (occasional OOM errors),SPARK-18403,13019922,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,10/Nov/16 19:34,22/Nov/16 06:54,14/Jul/23 06:29,10/Nov/16 21:44,2.2.0,,,,,,,,2.2.0,,,,SQL,,,,,,,,1,,,,,,"This test suite fails occasionally on Jenkins due to OOM errors. I've already reproduced it locally but haven't figured out the root cause.

We should probably disable it temporarily before getting it fixed so that it doesn't break the PR build too often.",,apachespark,dongjoon,hvanhovell,lian cheng,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 22 06:41:05 UTC 2016,,,,,,,,,,"0|i366vj:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"10/Nov/16 20:29;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/15845;;;","10/Nov/16 21:44;rxin;Please make sure we enable it.
;;;","21/Nov/16 20:40;lian cheng;Here is a minimal test case (add it to {{ObjectHashAggregateSuite}}) that can be used to reproduce this issue steadily:
{code}
test(""oom"") {
  withSQLConf(
    SQLConf.USE_OBJECT_HASH_AGG.key -> ""true"",
    SQLConf.OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD.key -> ""1""
  ) {
    Seq(Tuple1(Seq.empty[Int]))
      .toDF(""c0"")
      .groupBy(lit(1))
      .agg(typed_count($""c0""), max($""c0""))
      .show()
  }
}
{code}
What I observed is that the partial aggregation phase produces a malformed {{UnsafeRow}} after applying the {{resultProjection}} [here|https://github.com/apache/spark/blob/07beb5d21c6803e80733149f1560c71cd3cacc86/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggregationIterator.scala#L254].

When printed, the malformed {{UnsafeRow}} is always
{noformat}
[0,0,2000000008,2800000008,100000000000000,5a5a5a5a5a5a5a5a]
{noformat}
The {{5a5a5a5a5a5a5a5a}} is interpreted as the length of an {{ArrayData}}. Therefore, the JVM blows up when trying to allocate a huge array to deep copy this {{ArrayData}} at a later phase.

[~sameer] and [~davies], would you mind to have a look at this issue? Thanks!;;;","21/Nov/16 22:21;hvanhovell;The 5a5a5a5a5a5a means that the page has been freed clean. This has been added in https://github.com/apache/spark/commit/44c7c62bcfca74c82ffc4e3c53997fff47bfacac

This means that we are freeing an in-use buffer page.

;;;","22/Nov/16 01:53;lian cheng;Figured it out. It's caused by a false sharing issue inside {{ObjectAggregationIterator}}. In short, after setting an {{UnsafeArrayData}} to an aggregation buffer, which is a safe row, the underlying buffer of the {{UnsafeArrayData}} gets overwritten when iterator steps forward.

Have to say that this issue is pretty hard to debug. The large array allocation blows up the JVM right away and you can't really find the large array in the heap dump since the allocation itself fails. Therefore, all the heap dumps are super small (~70MB) compared to the heap size (3GB for default SBT tests) and you can't find anything useful in the heap dumps.

I'm opening a PR to fix this issue.;;;","22/Nov/16 06:41;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/15976;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR random forest should support output original label,SPARK-18401,13019861,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yanboliang,yanboliang,yanboliang,10/Nov/16 15:20,01/Dec/16 22:47,14/Jul/23 06:29,11/Nov/16 01:14,,,,,,,,,2.1.0,,,,ML,SparkR,,,,,,,0,,,,,,SparkR {{spark.randomForest}} classification prediction should output original label rather than the indexed label. This issue is very similar with SPARK-18291.,,apachespark,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16137,,,,SPARK-18291,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 10 15:31:05 UTC 2016,,,,,,,,,,"0|i366hz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/16 15:31;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/15842;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when resharding Kinesis Stream,SPARK-18400,13019854,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,boneill42,boneill42,10/Nov/16 14:55,16/Nov/16 10:17,14/Jul/23 06:29,16/Nov/16 10:17,1.6.2,,,,,,,,2.0.3,2.1.0,,,DStreams,,,,,,,,0,,,,,,"Occasionally, we see an NPE when we reshard our streams:

{code}
java.lang.NullPointerException
	at java.util.concurrent.ConcurrentHashMap.replaceNode(ConcurrentHashMap.java:1106) ~[?:1.8.0_60]
	at java.util.concurrent.ConcurrentHashMap.remove(ConcurrentHashMap.java:1097) ~[?:1.8.0_60]
	at org.apache.spark.streaming.kinesis.KinesisCheckpointer.removeCheckpointer(KinesisCheckpointer.scala:66) ~[spark-streaming-kinesis-asl_2.11-1.6.4-SNAPSHOT.jar:1.6.4-SNAPSHOT]
	at org.apache.spark.streaming.kinesis.KinesisReceiver.removeCheckpointer(KinesisReceiver.scala:245) ~[spark-streaming-kinesis-asl_2.11-1.6.4-SNAPSHOT.jar:1.6.4-SNAPSHOT]
	at org.apache.spark.streaming.kinesis.KinesisRecordProcessor.shutdown(KinesisRecordProcessor.scala:124) ~[spark-streaming-kinesis-asl_2.11-1.6.4-SNAPSHOT.jar:1.6.4-SNAPSHOT]
	at com.amazonaws.services.kinesis.clientlibrary.lib.worker.V1ToV2RecordProcessorAdapter.shutdown(V1ToV2RecordProcessorAdapter.java:48) ~[amazon-kinesis-client-1.6.2.jar:?]
	at com.amazonaws.services.kinesis.clientlibrary.lib.worker.ShutdownTask.call(ShutdownTask.java:100) [amazon-kinesis-client-1.6.2.jar:?]
	at com.amazonaws.services.kinesis.clientlibrary.lib.worker.MetricsCollectingTaskDecorator.call(MetricsCollectingTaskDecorator.java:49) [amazon-kinesis-client-1.6.2.jar:?]
	at com.amazonaws.services.kinesis.clientlibrary.lib.worker.MetricsCollectingTaskDecorator.call(MetricsCollectingTaskDecorator.java:24) [amazon-kinesis-client-1.6.2.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_60]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_60]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_60]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]
{code}",Spark 1.6 streaming from AWS Kinesis,apachespark,boneill42,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 16 10:17:13 UTC 2016,,,,,,,,,,"0|i366gf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/16 15:03;srowen;Looks like an easy fix, do you want to try it? the shardId in KinesisRecordProcessor could be null, I suppose, if shutdown happens before initalize completes or is called for some reason. ;;;","10/Nov/16 15:19;boneill42;Sure thing.  I have the patch: null check with log.info when we see a null shard.

I'm just double checking things... 

It appears that workers might be caught in a loop on ZOMBIE shards.  I see the following lines repeated.  I'm trying to track down the shard allocation logic that might pull it out of the loop.

{code}
2016-11-10 10:05:37 INFO  KinesisRecordProcessor:58 - Shutdown:  Shutting down workerId localhost:4bac2666-b7c9-4ecd-8c1a-15c790588443 with reason ZOMBIE
2016-11-10 10:05:38 INFO  KinesisRecordProcessor:58 - Initialized workerId localhost:4bac2666-b7c9-4ecd-8c1a-15c790588443 with shardId shardId-000000000257
{code}

;;;","12/Nov/16 10:11;srowen;OK, open a pull request with that change?;;;","14/Nov/16 11:43;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15882;;;","16/Nov/16 10:17;srowen;Issue resolved by pull request 15882
[https://github.com/apache/spark/pull/15882];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executing the same query twice in a row results in CodeGenerator cache misses,SPARK-18394,13019715,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,jwserencsa,jwserencsa,10/Nov/16 01:20,29/Oct/17 04:25,14/Jul/23 06:29,17/Aug/17 20:48,,,,,,,,,2.3.0,,,,SQL,,,,,,,,0,,,,,,"Executing the query:
{noformat}
select
    l_returnflag,
    l_linestatus,
    sum(l_quantity) as sum_qty,
    sum(l_extendedprice) as sum_base_price,
    sum(l_extendedprice * (1 - l_discount)) as sum_disc_price,
    sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,
    avg(l_quantity) as avg_qty,
    avg(l_extendedprice) as avg_price,
    avg(l_discount) as avg_disc,
    count(*) as count_order
from
    lineitem_1_row
where
    l_shipdate <= date_sub('1998-12-01', '90')
group by
    l_returnflag,
    l_linestatus
;
{noformat}
twice (in succession), will result in CodeGenerator cache misses in BOTH executions. Since the query is identical, I would expect the same code to be generated. 

Turns out, the generated code is not exactly the same, resulting in cache misses when performing the lookup in the CodeGenerator cache. Yet, the code is equivalent. 

Below is (some portion of the) generated code for two runs of the query:

run-1
{noformat}
import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import scala.collection.Iterator;
import org.apache.spark.sql.types.DataType;
import org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder;
import org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter;
import org.apache.spark.sql.execution.columnar.MutableUnsafeRow;

public SpecificColumnarIterator generate(Object[] references) {
return new SpecificColumnarIterator();
}

class SpecificColumnarIterator extends org.apache.spark.sql.execution.columnar.ColumnarIterator {

private ByteOrder nativeOrder = null;
private byte[][] buffers = null;
private UnsafeRow unsafeRow = new UnsafeRow(7);
private BufferHolder bufferHolder = new BufferHolder(unsafeRow);
private UnsafeRowWriter rowWriter = new UnsafeRowWriter(bufferHolder, 7);
private MutableUnsafeRow mutableRow = null;

private int currentRow = 0;
private int numRowsInBatch = 0;

private scala.collection.Iterator input = null;
private DataType[] columnTypes = null;
private int[] columnIndexes = null;

private org.apache.spark.sql.execution.columnar.DoubleColumnAccessor accessor;
private org.apache.spark.sql.execution.columnar.DoubleColumnAccessor accessor1;
private org.apache.spark.sql.execution.columnar.DoubleColumnAccessor accessor2;
private org.apache.spark.sql.execution.columnar.StringColumnAccessor accessor3;
private org.apache.spark.sql.execution.columnar.DoubleColumnAccessor accessor4;
private org.apache.spark.sql.execution.columnar.StringColumnAccessor accessor5;
private org.apache.spark.sql.execution.columnar.StringColumnAccessor accessor6;

public SpecificColumnarIterator() {
this.nativeOrder = ByteOrder.nativeOrder();
this.buffers = new byte[7][];
this.mutableRow = new MutableUnsafeRow(rowWriter);
}

public void initialize(Iterator input, DataType[] columnTypes, int[] columnIndexes) {
this.input = input;
this.columnTypes = columnTypes;
this.columnIndexes = columnIndexes;
}



public boolean hasNext() {
if (currentRow < numRowsInBatch) {
return true;
}
if (!input.hasNext()) {
return false;
}

org.apache.spark.sql.execution.columnar.CachedBatch batch = (org.apache.spark.sql.execution.columnar.CachedBatch) input.next();
currentRow = 0;
numRowsInBatch = batch.numRows();
for (int i = 0; i < columnIndexes.length; i ++) {
buffers[i] = batch.buffers()[columnIndexes[i]];
}
accessor = new org.apache.spark.sql.execution.columnar.DoubleColumnAccessor(ByteBuffer.wrap(buffers[0]).order(nativeOrder));
accessor1 = new org.apache.spark.sql.execution.columnar.DoubleColumnAccessor(ByteBuffer.wrap(buffers[1]).order(nativeOrder));
accessor2 = new org.apache.spark.sql.execution.columnar.DoubleColumnAccessor(ByteBuffer.wrap(buffers[2]).order(nativeOrder));
accessor3 = new org.apache.spark.sql.execution.columnar.StringColumnAccessor(ByteBuffer.wrap(buffers[3]).order(nativeOrder));
accessor4 = new org.apache.spark.sql.execution.columnar.DoubleColumnAccessor(ByteBuffer.wrap(buffers[4]).order(nativeOrder));
accessor5 = new org.apache.spark.sql.execution.columnar.StringColumnAccessor(ByteBuffer.wrap(buffers[5]).order(nativeOrder));
accessor6 = new org.apache.spark.sql.execution.columnar.StringColumnAccessor(ByteBuffer.wrap(buffers[6]).order(nativeOrder));

return hasNext();
}

public InternalRow next() {
currentRow += 1;
bufferHolder.reset();
rowWriter.zeroOutNullBytes();
accessor.extractTo(mutableRow, 0);
accessor1.extractTo(mutableRow, 1);
accessor2.extractTo(mutableRow, 2);
accessor3.extractTo(mutableRow, 3);
accessor4.extractTo(mutableRow, 4);
accessor5.extractTo(mutableRow, 5);
accessor6.extractTo(mutableRow, 6);
unsafeRow.setTotalSize(bufferHolder.totalSize());
return unsafeRow;
}
}
{noformat}

run-2:
{noformat}
import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import scala.collection.Iterator;
import org.apache.spark.sql.types.DataType;
import org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder;
import org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter;
import org.apache.spark.sql.execution.columnar.MutableUnsafeRow;

public SpecificColumnarIterator generate(Object[] references) {
return new SpecificColumnarIterator();
}

class SpecificColumnarIterator extends org.apache.spark.sql.execution.columnar.ColumnarIterator {

private ByteOrder nativeOrder = null;
private byte[][] buffers = null;
private UnsafeRow unsafeRow = new UnsafeRow(7);
private BufferHolder bufferHolder = new BufferHolder(unsafeRow);
private UnsafeRowWriter rowWriter = new UnsafeRowWriter(bufferHolder, 7);
private MutableUnsafeRow mutableRow = null;

private int currentRow = 0;
private int numRowsInBatch = 0;

private scala.collection.Iterator input = null;
private DataType[] columnTypes = null;
private int[] columnIndexes = null;

private org.apache.spark.sql.execution.columnar.DoubleColumnAccessor accessor;
private org.apache.spark.sql.execution.columnar.StringColumnAccessor accessor1;
private org.apache.spark.sql.execution.columnar.DoubleColumnAccessor accessor2;
private org.apache.spark.sql.execution.columnar.StringColumnAccessor accessor3;
private org.apache.spark.sql.execution.columnar.DoubleColumnAccessor accessor4;
private org.apache.spark.sql.execution.columnar.StringColumnAccessor accessor5;
private org.apache.spark.sql.execution.columnar.DoubleColumnAccessor accessor6;

public SpecificColumnarIterator() {
this.nativeOrder = ByteOrder.nativeOrder();
this.buffers = new byte[7][];
this.mutableRow = new MutableUnsafeRow(rowWriter);
}

public void initialize(Iterator input, DataType[] columnTypes, int[] columnIndexes) {
this.input = input;
this.columnTypes = columnTypes;
this.columnIndexes = columnIndexes;
}



public boolean hasNext() {
if (currentRow < numRowsInBatch) {
return true;
}
if (!input.hasNext()) {
return false;
}

org.apache.spark.sql.execution.columnar.CachedBatch batch = (org.apache.spark.sql.execution.columnar.CachedBatch) input.next();
currentRow = 0;
numRowsInBatch = batch.numRows();
for (int i = 0; i < columnIndexes.length; i ++) {
buffers[i] = batch.buffers()[columnIndexes[i]];
}
accessor = new org.apache.spark.sql.execution.columnar.DoubleColumnAccessor(ByteBuffer.wrap(buffers[0]).order(nativeOrder));
accessor1 = new org.apache.spark.sql.execution.columnar.StringColumnAccessor(ByteBuffer.wrap(buffers[1]).order(nativeOrder));
accessor2 = new org.apache.spark.sql.execution.columnar.DoubleColumnAccessor(ByteBuffer.wrap(buffers[2]).order(nativeOrder));
accessor3 = new org.apache.spark.sql.execution.columnar.StringColumnAccessor(ByteBuffer.wrap(buffers[3]).order(nativeOrder));
accessor4 = new org.apache.spark.sql.execution.columnar.DoubleColumnAccessor(ByteBuffer.wrap(buffers[4]).order(nativeOrder));
accessor5 = new org.apache.spark.sql.execution.columnar.StringColumnAccessor(ByteBuffer.wrap(buffers[5]).order(nativeOrder));
accessor6 = new org.apache.spark.sql.execution.columnar.DoubleColumnAccessor(ByteBuffer.wrap(buffers[6]).order(nativeOrder));

return hasNext();
}

public InternalRow next() {
currentRow += 1;
bufferHolder.reset();
rowWriter.zeroOutNullBytes();
accessor.extractTo(mutableRow, 0);
accessor1.extractTo(mutableRow, 1);
accessor2.extractTo(mutableRow, 2);
accessor3.extractTo(mutableRow, 3);
accessor4.extractTo(mutableRow, 4);
accessor5.extractTo(mutableRow, 5);
accessor6.extractTo(mutableRow, 6);
unsafeRow.setTotalSize(bufferHolder.totalSize());
return unsafeRow;
}
}
{noformat}

Diff-ing the two files reveals that the ""accessor*"" variable definitions are permuted. 
",HiveThriftServer2 running on branch-2.0 on Mac laptop,cloud_fan,codingcat,djalova,hvanhovell,jwserencsa,kiszk,maropu,vish741,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 29 04:25:42 UTC 2017,,,,,,,,,,"0|i365lj:",9223372036854775807,,,,,hvanhovell,,,,,,,,2.3.0,,,,,,,,,,,"22/Nov/16 22:06;hvanhovell;I am not able to reproduce this. Could you also explain to me why this is a major issue?

I have used the following script:
{noformat}
sc.setLogLevel(""INFO"")
spark.sql(""create database if not exists tpc"")
spark.sql(""drop table if exists tpc.lineitem"")
spark.sql(""""""
create table tpc.lineitem (
  L_ORDERKEY bigint,
  L_PARTKEY bigint,
  L_SUPPKEY bigint,
  L_LINENUMBER bigint,
  L_QUANTITY double,
  L_EXTENDEDPRICE double,
  L_DISCOUNT double,
  L_TAX double,
  L_RETURNFLAG string,
  L_LINESTATUS string,
  L_SHIPDATE string,
  L_COMMITDATE string,
  L_RECEIPTDATE string,
  L_SHIPINSTRUCT string,
  L_SHIPMODE string,
  L_COMMENT string
) using parquet
"""""")

spark.sql(s""""""
insert into tpc.lineitem
select id as L_ORDERKEY,
       id % 10 as L_PARTKEY,
       id % 50 as L_SUPPKEY,
       id as L_LINENUMBER,
       rand(3) * 10 as L_QUANTITY,
       rand(5) * 50 as L_EXTENDEDPRICE,
       rand(7) * 20 as L_DISCOUNT,
       0.18d as L_TAX,
       case when rand(11) < 0.7d then 'Y' else 'N' end as L_RETURNFLAG,
       case when rand(13) < 0.4d then 'A' when rand(17) < 0.2d then 'B' else 'C' end as L_LINESTATUS,
       date_format(date_add(date '1998-08-05', id % 365), 'yyyy-MM-dd') as L_SHIPDATE,
       date_format(date_add(date '1998-08-01', id % 365), 'yyyy-MM-dd') as L_COMMITDATE,
       date_format(date_add(date '1998-08-03', id % 365), 'yyyy-MM-dd') as L_RECEIPTDATE,
       'DUMMY' as L_SHIPINSTRUCT,
       case when rand(19) < 0.7d then 'AIR' else 'LAND' end as L_SHIPMODE,
       'DUMMY' as L_COMMENT
from   range(100)
"""""")


val df = spark.sql(""""""
select
    l_returnflag,
    l_linestatus,
    sum(l_quantity) as sum_qty,
    sum(l_extendedprice) as sum_base_price,
    sum(l_extendedprice * (1 - l_discount)) as sum_disc_price,
    sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,
    avg(l_quantity) as avg_qty,
    avg(l_extendedprice) as avg_price,
    avg(l_discount) as avg_disc,
    count(*) as count_order
from
    tpc.lineitem
where
    l_shipdate <= date_sub('1998-12-01', '90')
group by
    l_returnflag,
    l_linestatus
"""""")

df.show()

df.show()
{noformat}

;;;","22/Nov/16 22:32;jwserencsa;This problem was discovered during high concurrency experiments where I was running the aforementioned query thousands of times repeatedly via many concurrent clients. Eventually, after 5K-10K executions, the JVM level CodeGenCache was having to be purged resulting in 10 second long pauses.

My expectation was that since the exact same query is being executed, Spark would not have to re-generate the byte code (because of it's own CodeGenCache). After removing the WHERE clauses from the query, this was in fact the case and the JVM level cache purging disappeared.  

I made this a Major issue because it didn't seem like Spark's CodeGenCache was working as expected. Perhaps I am mistaken. 

My above repro of the issue required me to set breakpoints through the debugger. 

;;;","22/Nov/16 23:04;hvanhovell;Ok, that is fair.

What strikes me as odd is that the column order that the columnar cache produces is different the two both plans. This is what causes the code generator to create two different 'programs' and what in the end causes the your caching problems . Could you re-run this without the in-memory cache, and see if you are still hitting this problem.

I'll have a look on my end to see what is going on in the in-memory cache.;;;","22/Nov/16 23:12;jwserencsa;Already did that. The problem happens with both HiveScanExec and InMemoryScanExec. The indeterminate ordering is an artifact of the hash code for AttributeEquals  involving a hash code of the exprId. Thus, when you iterate through an AttributeSet, the order or the attributes is not consistent. ;;;","22/Nov/16 23:49;hvanhovell;Nice, this is a good find.

I think we either need to make AttributeSet iteration deterministic, or make a change to the following line in the SparkPlanner: https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala#L96

Would you be interested in working on this? 

;;;","22/Nov/16 23:51;jwserencsa;Yes I would. ;;;","22/Nov/16 23:56;hvanhovell;Great! Ping me if you need any assistance.;;;","16/Aug/17 03:16;maropu;Any update? I checked and I found the master still has this issue; I just run the query above and dump output names in https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala#L102.
{code}
17/08/16 02:13:13 WARN BaseSessionStateBuilder$$anon$3: L_QUANTITY#9015,L_RETURNFLAG#9019,l_shipdate#9021,L_TAX#9018,L_DISCOUNT#9017,L_LINESTATUS#9020,L_EXTENDEDPRICE#9016
17/08/16 02:13:13 WARN BaseSessionStateBuilder$$anon$3: L_RETURNFLAG#9142,L_DISCOUNT#9140,L_EXTENDEDPRICE#9139,L_QUANTITY#9138,L_LINESTATUS#9143,l_shipdate#9144,L_TAX#9141
17/08/16 02:13:13 WARN BaseSessionStateBuilder$$anon$3: L_QUANTITY#9305,L_TAX#9308,l_shipdate#9311,L_DISCOUNT#9307,L_RETURNFLAG#9309,L_LINESTATUS#9310,L_EXTENDEDPRICE#9306
17/08/16 02:13:13 WARN BaseSessionStateBuilder$$anon$3: L_EXTENDEDPRICE#9451,L_QUANTITY#9450,L_RETURNFLAG#9454,L_TAX#9453,L_DISCOUNT#9452,l_shipdate#9456,L_LINESTATUS#9455
17/08/16 02:13:13 WARN BaseSessionStateBuilder$$anon$3: L_LINESTATUS#9600,l_shipdate#9601,L_DISCOUNT#9597,L_TAX#9598,L_EXTENDEDPRICE#9596,L_RETURNFLAG#9599,L_QUANTITY#9595
17/08/16 02:13:13 WARN BaseSessionStateBuilder$$anon$3: L_QUANTITY#9740,L_TAX#9743,l_shipdate#9746,L_DISCOUNT#9742,L_EXTENDEDPRICE#9741,L_LINESTATUS#9745,L_RETURNFLAG#9744
...
{code}
The attribute order is different, and then Spark generates different  code in `GenerateColumnAccessor`.
Also, I quickly checked `AttributeSet.toSeq` output attributes with a different order;
{code}
scala> val attr1 = AttributeReference(""c1"", IntegerType)(exprId = ExprId(1098))
scala> val attr2 = AttributeReference(""c2"", IntegerType)(exprId = ExprId(107))
scala> val attr3 = AttributeReference(""c3"", IntegerType)(exprId = ExprId(838))
scala> val attrSetA = AttributeSet(attr1 :: attr2 :: attr3 :: Nil)
scala> val attr4 = AttributeReference(""c4"", IntegerType)(exprId = ExprId(389))
scala> val attr5 = AttributeReference(""c5"", IntegerType)(exprId = ExprId(89329))
scala> val attrSetB = AttributeSet(attr4 :: attr5 :: Nil)
scala> (attrSetA ++ attrSetB).toSeq
res1: Seq[org.apache.spark.sql.catalyst.expressions.Attribute] = WrappedArray(c3#838, c4#389, c2#107, c5#89329, c1#1098)

scala> val attr1 = AttributeReference(""c1"", IntegerType)(exprId = ExprId(392))
scala> val attr2 = AttributeReference(""c2"", IntegerType)(exprId = ExprId(92))
scala> val attr3 = AttributeReference(""c3"", IntegerType)(exprId = ExprId(87))
scala> val attrSetA = AttributeSet(attr1 :: attr2 :: attr3 :: Nil)
scala> val attr4 = AttributeReference(""c4"", IntegerType)(exprId = ExprId(9023920))
scala> val attr5 = AttributeReference(""c5"", IntegerType)(exprId = ExprId(522))
scala> val attrSetB = AttributeSet(attr4 :: attr5 :: Nil)
scala> (attrSetA ++ attrSetB).toSeq
res2: Seq[org.apache.spark.sql.catalyst.expressions.Attribute] = WrappedArray(c3#87, c1#392, c5#522, c2#92, c4#9023920)
{code}

As suggested, to fix this, `Attribute.toSeq` need to output attributes with a consistent order like;
https://github.com/apache/spark/compare/master...maropu:SPARK-18394
 ;;;","29/Oct/17 04:25;cloud_fan;resolved by https://github.com/apache/spark/pull/18959;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test that expressions can be serialized,SPARK-18387,13019610,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,rdblue,rdblue,rdblue,09/Nov/16 18:58,11/Nov/16 21:52,14/Jul/23 06:29,11/Nov/16 21:52,2.0.1,2.1.0,,,,,,,2.0.3,2.1.0,,,,,,,,,,,0,,,,,,"SPARK-18368 fixes regexp_replace when it is serialized. One of the reviews requested updating the tests so that all expressions that are tested using checkEvaluation are first serialized. That caused several new [test failures], so this issue is to add serialization to the tests ([pick commit 10805059|https://github.com/apache/spark/commit/10805059]) and fix the bugs serialization exposes.",,apachespark,hvanhovell,rdblue,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18368,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 11 00:26:05 UTC 2016,,,,,,,,,,"0|i364y7:",9223372036854775807,,,,,,,,,,,,,2.0.3,2.1.0,,,,,,,,,,"10/Nov/16 21:37;hvanhovell;[~rdblue] Are you working on this one? I'd be happy to pick this one up.;;;","10/Nov/16 22:58;rdblue;Yeah, I'm working on it. Thanks!

On Thu, Nov 10, 2016 at 1:37 PM, Herman van Hovell (JIRA) <jira@apache.org>




-- 
Ryan Blue
;;;","11/Nov/16 00:26;apachespark;User 'rdblue' has created a pull request for this issue:
https://github.com/apache/spark/pull/15847;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Utils.isBindCollision does not properly handle all possible address-port collisions when binding,SPARK-18383,13019565,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gq,gq,gq,09/Nov/16 15:28,12/Nov/16 09:50,14/Jul/23 06:29,12/Nov/16 09:49,1.6.2,2.0.1,,,,,,,2.1.0,,,,Spark Core,,,,,,,,0,,,,,,"When the IO mode is set to epoll, Netty uses {{io.netty.channel.unix.Socket}} class, and {{Socket.bind}}  throws an exception that is a {{io.netty.channel.unix.Errors.NativeIoException}}  instead of a {{java.net.BindException}} instance",,apachespark,gq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 12 09:49:48 UTC 2016,,,,,,,,,,"0|i364o7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/16 15:31;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/15830;;;","12/Nov/16 09:49;srowen;Issue resolved by pull request 15830
[https://github.com/apache/spark/pull/15830];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""run at null:-1"" in UI when no file/line info in call site info",SPARK-18382,13019538,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,srowen,EmAm05,EmAm05,09/Nov/16 13:38,14/Nov/16 07:59,14/Jul/23 06:29,14/Nov/16 07:59,2.0.0,,,,,,,,2.0.3,2.1.0,,,Spark Core,Web UI,,,,,,,0,,,,,,"From my Apache WEB UI dashboard. I've seen a lot of this run at null:-1 jobs, several actually in my particular project that comprises basically of: connect to a JDBC PostgreSQL Server, fetch some tables, creating some temp tables and do some aggregations with org.apache.spark.sql.Cube() method.

Link to image: http://i.stack.imgur.com/UEfgM.png","Windows 10, Scala Eclipse Luna, Intel i3, 6gb RAM",ajbozarth,apachespark,EmAm05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,http://stackoverflow.com/questions/39796003/what-does-run-at-null-1-mean-in-apache-spark-web-ui,,,,,,,,,,9223372036854775807,,,Sat Nov 12 10:06:04 UTC 2016,,,,,,,,,,"0|i364i7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/16 16:42;srowen;It means it doesn't know the call-site of this job for some reason. I think it's a cosmetic bug (not Major) which you can open a PR to patch;;;","12/Nov/16 10:05;srowen;This is easy to touch up cosmetically so it shows what it's ""supposed"" to, the default of ""<unknown>:0"" instead of ""null:-1"". 

It looks like it happens when there are no debug symbols. Do you build Spark yourself and maybe strip these with flags like '-optimize'?;;;","12/Nov/16 10:06;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15862;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect words in StopWords/english.txt,SPARK-18374,13019439,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yuhaoyan,tenstriker,tenstriker,09/Nov/16 02:08,15/Dec/16 01:31,14/Jul/23 06:29,06/Dec/16 21:12,2.0.1,,,,,,,,2.2.0,,,,ML,,,,,,,,0,releasenotes,,,,,"I was just double checking english.txt for list of stopwords as I felt it was taking out valid tokens like 'won'. I think issue is english.txt list is missing apostrophe character and all character after apostrophe. So ""won't"" becam ""won"" in that list; ""wouldn't"" is ""wouldn"" .

Here are some incorrect tokens in this list:

won
wouldn
ma
mightn
mustn
needn
shan
shouldn
wasn
weren

I think ideal list should have both style. i.e. won't and wont both should be part of english.txt as some tokenizer might remove special characters. But 'won' is obviously shouldn't be in this list.

Here's list of snowball english stop words:
http://snowball.tartarus.org/algorithms/english/stop.txt",,apachespark,josephkb,mengxr,tenstriker,yuhaoyan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 15 01:31:19 UTC 2016,,,,,,,,,,"0|i363w7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/16 06:32;yuhaoyan;Just to provide some history info for the issue: https://github.com/apache/spark/pull/11871#issuecomment-199832509, according to which english stopwords were argumented.;;;","09/Nov/16 12:27;srowen;I think the idea is that it's applied post-tokenization, after which ""won't"" becomes ""won"" and ""t"". Removing them would cause a different perhaps bigger problem. It does seem more ideal to not split ""won't"" of course and maybe that's a way forward, but I'm not sure how to do that reliably in a language-agnostic way without implementing a lot more sophisticated logic.;;;","09/Nov/16 19:17;tenstriker;[~srowen] Do you mean how to tokenize words in language agnostic way? means there are some language where it make sense to break ""won't"" into ""won"" and ""t"" ? 
At least other tokenizer like lucene families doesn't do this. Seems plainly incorrect . Other language where it make sense to tokenize on ""'"" instead of removing it there should be different tokenizer and they can then follow their stopwords list correctly in that regards. Tokenizer in that regards should not be generic same as stopwrods.;;;","09/Nov/16 21:25;srowen;It's a fair point indeed, because it would be much better to omit ""won't"" than incorrectly omit ""won"", rare as that might be. Right now, simply removing those words causes a different problem because, I presume, you'd find ""won"" and ""t"" in your tokens. (Worth testing?) If you can see an easy way to improve the tokenization, that could become the topic of this issue.;;;","13/Nov/16 20:10;yuhaoyan;With the default behavior of the _Tokenizer_ and _RegexTokenizer_, I think it's more reasonable to directly include words like _won't_, _haven't_ in the stop words lists, as shown in the list on http://www.ranks.nl/stopwords.

More specifically, if a user is using the default _Tokenizer_ and _RegexTokenizer_ in spark.ml without customization, then _weren_, _wasn_ in current stop words list are useless，whereas _weren't_ and _wasn't_ can be helpful. The default behavior of ml transformers should be consistent and effective.;;;","14/Nov/16 09:27;srowen;Adding the stop-words is fine, however, if the upstream transformers are splitting on a single quote, it won't have the desired effect. I haven't looked into it, sounds like you have -- they _don't_ split on quote? then sure, this should be fixed. A simple test to show it would be great to verify the change.;;;","14/Nov/16 17:40;yuhaoyan;Thanks for the response. By default, _Tokenizer_ equals to {noformat}split(""\\s""){noformat} and _RegexTokenizer_ equals to {noformat}split(""\\s+""){noformat}, which means no split on apostrophes or quotes. _RegexTokenizer_ can surely support customized regex pattern for split.;;;","14/Nov/16 19:40;srowen;[~whisper] do you have a comment here? it does seem like stopwords like ""wouldn"" should be ""wouldn't"" for example. Does that seem right to you, given your change in SPARK-14050?;;;","30/Nov/16 05:04;yuhaoyan;cc [~mengxr] to see if he recalls any specific reason.;;;","30/Nov/16 05:59;mengxr;See the discussion here: https://github.com/nltk/nltk_data/issues/22. Including `won` is apparently a mistake.;;;","30/Nov/16 06:04;yuhaoyan;Yes. Currently we're discussing if we should put ""wouldn't"" (rather than ""wonldnt"") directly into MLlib's stop words list, because by default Tokenizer in Spark does not split on apostrophes or quotes.;;;","30/Nov/16 10:32;srowen;I think you can proceed to remove things like ""won"" but also ""wouldn"";;;","30/Nov/16 23:32;yuhaoyan;I checked with some other lists of stopwords and got a list to add, but it's longer than I thought:
i'll
you'll
he'll
she'll
we'll
they'll
i'd
you'd
he'd
she'd
we'd
they'd
i'm
you're
he's
she's
it's
we're
they're
i've
we've
you've
they've
isn't
aren't
wasn't
weren't
haven't
hasn't
hadn't
don't
doesn't
didn't
won't
wouldn't
shan't
shouldn't
mustn't
can't
couldn't

any concern?;;;","01/Dec/16 10:55;srowen;Seems OK to me and to remove the stems like won.;;;","01/Dec/16 17:22;apachespark;User 'hhbyyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/16103;;;","06/Dec/16 21:12;srowen;Issue resolved by pull request 16103
[https://github.com/apache/spark/pull/16103];;;","14/Dec/16 18:49;josephkb;I noted this change of behavior in [SPARK-18864];;;","14/Dec/16 18:56;srowen;Yeah I tagged as 'releasenotes' for that reason -- not sure how official that is as our mechanism. You might look at anything recent tagged that way as a possible candidate.;;;","15/Dec/16 01:31;josephkb;Oh nice, I didn't realize that was in use.  I'll start doing that.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Make KafkaSource's failOnDataLoss=false work with Spark jobs,SPARK-18373,13019427,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,09/Nov/16 01:06,22/Nov/16 22:17,14/Jul/23 06:29,22/Nov/16 22:17,,,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,Right now failOnDataLoss=false doesn't affect Spark jobs launched by KafkaSource. The job may still fail the query when some topics are deleted or some data is aged out. We should handle these corner cases in Spark jobs as well.,,apachespark,codingcat,lwlin,tdas,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 22 22:17:46 UTC 2016,,,,,,,,,,"0|i363tr:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"09/Nov/16 01:11;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/15820;;;","22/Nov/16 22:17;tdas;Issue resolved by pull request 15820
[https://github.com/apache/spark/pull/15820];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Streaming backpressure bug - generates a batch with large number of records,SPARK-18371,13019420,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,seb.arzt,mapreduced,mapreduced,09/Nov/16 00:26,29/Sep/19 13:14,14/Jul/23 06:29,16/Mar/18 17:33,2.0.0,,,,,,,,2.4.0,,,,DStreams,,,,,,,,2,,,,,,"When the streaming job is configured with backpressureEnabled=true, it generates a GIANT batch of records if the processing time + scheduled delay is (much) larger than batchDuration. This creates a backlog of records like no other and results in batches queueing for hours until it chews through this giant batch.
Expectation is that it should reduce the number of records per batch in some time to whatever it can really process.
Attaching some screen shots where it seems that this issue is quite easily reproducible.",,apachespark,codingcat,feestend,jse,koeninger,mapreduced,r_anirban,rkarthikeyan,seb.arzt,uncleGen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/17 15:10;seb.arzt;01.png;https://issues.apache.org/jira/secure/attachment/12865156/01.png","26/Apr/17 15:10;seb.arzt;02.png;https://issues.apache.org/jira/secure/attachment/12865158/02.png","09/Nov/16 00:28;mapreduced;GiantBatch2.png;https://issues.apache.org/jira/secure/attachment/12838094/GiantBatch2.png","09/Nov/16 00:29;mapreduced;GiantBatch3.png;https://issues.apache.org/jira/secure/attachment/12838095/GiantBatch3.png","09/Nov/16 00:29;mapreduced;Giant_batch_at_23_00.png;https://issues.apache.org/jira/secure/attachment/12838096/Giant_batch_at_23_00.png","09/Nov/16 00:27;mapreduced;Look_at_batch_at_22_14.png;https://issues.apache.org/jira/secure/attachment/12838093/Look_at_batch_at_22_14.png","18/Sep/19 07:28;rkarthikeyan;Screen Shot 2019-09-16 at 12.27.25 PM.png;https://issues.apache.org/jira/secure/attachment/12980581/Screen+Shot+2019-09-16+at+12.27.25+PM.png",,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 29 13:14:41 UTC 2019,,,,,,,,,,"0|i363s7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/16 01:01;mapreduced;I worked the math for [PIDRateEstimator|https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/PIDRateEstimator.scala] and I found that when there are long scheduling delays it increases the 'historicalError' by a LOT, which even when multiplied by integral (=0.2 default), results in a large negative in the formula:
val newRate = (latestRate - proportional * error - integral * historicalError - derivative * dError).max(minRate)

As a result, minRate (=100 default) becomes the newRate. Now, when it comes in [DirectKafkaInputDStream|https://github.com/apache/spark/blob/master/external/kafka-0-8/src/main/scala/org/apache/spark/streaming/kafka/DirectKafkaInputDStream.scala#L107] , if you have more than 100 partitions in your kafka topics, you're almost guaranteed to get Math.rounded backpressureRate = 0.  Which then [here|https://github.com/apache/spark/blob/master/external/kafka-0-8/src/main/scala/org/apache/spark/streaming/kafka/DirectKafkaInputDStream.scala#L114] leads to returning None. That as a result returns [leaderOffsets|https://github.com/apache/spark/blob/master/external/kafka-0-8/src/main/scala/org/apache/spark/streaming/kafka/DirectKafkaInputDStream.scala#L149] for that batch - hence the giant batch with all records from last batch till head of the queue (leaderOffsets).

Proposed solution: Not sure, maybe the minRate could be defaulted to Total number of Partitions in all your kafka topics + some constant. Not sure if anyone has any suggestions to changes in PIDRateEstimator itself.

Here's the math I did from the example in Look_at_batch_at_22_14.png : 

*Run1* :

latestRate = -1
latestTime = -1
latestError = -1
time = 1478587297000
processingDelay = 342000
delaySinceUpdate = 1478587298

processingRate = (10800000/342000) * 1000 = 31578.94

error = -1 -31579 = -31580
historicalError = 8 *  31579 / 60000 = 4.21
dError = (-31580 - 4.21)/ 1478587298 = 0.00002136107254

newRate =  (-1 -(1*-31580) - (0.2*4.21) - (0 * 0.00002136107254)).max(100) = (31578.158).max(100) = 31578.158

latestTime = 1478587297
latestError = 0
latestRate = 31578.94
Returns None - which results in picking up maxRatePerPartition

*Run 2* :
time = 1478587615000
processingDelay = 5.3 * 60 * 1000 = 318000
schedulingDelay = 282000

delaySinceUpdate = (1478587615000 - 1478587297000) = 318
processingRate = 10800000/318000 * 1000 = 33962.2
error = 31578 - 33962 = -2384
historicalError = 282000 * 33962 / 60000 = 159621.4

dError = doesnt matter since multiplied by 0

newRate = (31578 - (1*-2384) - (0.2*159621) - (0 * dError)).max(100) = (2037.72).max(100) = 2037.72

latestRate = 2037.72
latestError = -2384
latestTime = 1478587615000
Returns newRate = 2037.72

*Run 3* :
totalLag = 1972830183
perpartition lag = 6576100.61
backpressureRate = 126000 - 129000

time = 1478587795000
delaySinceUpdate = 1478587795000 - 1478587615000 = 180

processingRate = 10800000/180000 * 1000 = 60000
error = 2037 - 60000 = -57963
historicalError = 540000 * 60000 / 60000 = 540000
dError = doesntMatter

newRate = (2037.72 - (1*-57963) -(0.2*540000) - 0).max(100) = (-48000).max(100) = 100

latestTime = 1478587795000
latestRate = 100
latestError = -62384

Returns newRate = 100;;;","09/Nov/16 02:59;koeninger;Thanks for digging into this.  The other thing I noticed when working on

https://github.com/apache/spark/pull/15132

is that the return value of getLatestRate was cast to Int, which seems wrong and possibly subject to overflow.

If you have the ability to test that PR (shouldn't require a spark redeploy, since the kafka jar is standalone), may want to test it out.;;;","09/Nov/16 05:48;mapreduced;I'll try to test it out hopefully soon.;;;","26/Apr/17 13:22;seb.arzt;I deep dived into it and found a simple solution. The problem is that [maxRateLimitPerPartition|https://github.com/apache/spark/blob/branch-2.0/external/kafka-0-8/src/main/scala/org/apache/spark/streaming/kafka/DirectKafkaInputDStream.scala#L94] returns {{None}} for an unintended case. {{None}} should only be returned if there is no lag as indicated by this [condition|https://github.com/apache/spark/blob/branch-2.0/external/kafka-0-8/src/main/scala/org/apache/spark/streaming/kafka/DirectKafkaInputDStream.scala#L114]. However, this condition is also true if all backpressureRates are [rounded|https://github.com/apache/spark/blob/branch-2.0/external/kafka-0-8/src/main/scala/org/apache/spark/streaming/kafka/DirectKafkaInputDStream.scala#L107] to zero. I propose a solution, where rounding is omitted at all. This has the nice side-effect that backpressure is more fine-grained and not only an integral multiple of the [batchDuration|https://github.com/apache/spark/blob/branch-2.0/external/kafka-0-8/src/main/scala/org/apache/spark/streaming/kafka/DirectKafkaInputDStream.scala#L117] in seconds. I will open a pull request for it soon.;;;","26/Apr/17 15:08;apachespark;User 'arzt' has created a pull request for this issue:
https://github.com/apache/spark/pull/17774;;;","26/Apr/17 15:13;seb.arzt;Screenshots:

[before|https://issues.apache.org/jira/secure/attachment/12865156/01.png]
[after|https://issues.apache.org/jira/secure/attachment/12865158/02.png];;;","18/Sep/19 07:28;rkarthikeyan;[~seb.arzt] Isnt this the same problem will come with kinesis connectors too?. Looks like your fix is only on the kafka part. We are using kinesis with spark streaming and we are seeing the exact same problem. attaching screenshot for reference.  !Screen Shot 2019-09-16 at 12.27.25 PM.png!;;;","29/Sep/19 13:14;seb.arzt;[~rkarthikeyan] at a first glace I cannot find back pressure support in the kinesis receiver yet. I think your problem should be investigated independently. I suggest to create a new ticket with instructions to reproduce your findings.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InsertIntoHadoopFsRelationCommand should keep track of its table,SPARK-18370,13019416,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,hvanhovell,hvanhovell,hvanhovell,09/Nov/16 00:15,08/Dec/16 23:03,14/Jul/23 06:29,09/Nov/16 20:25,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,When we plan a {{InsertIntoHadoopFsRelationCommand}} we drop the {{Table}} name. This is quite annoying when debugging plans.,,apachespark,hvanhovell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 09 16:56:07 UTC 2016,,,,,,,,,,"0|i363rb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/16 16:56;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/15832;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regular expression replace throws NullPointerException when serialized,SPARK-18368,13019399,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rdblue,rdblue,rdblue,08/Nov/16 22:56,23/May/17 10:54,14/Jul/23 06:29,09/Nov/16 19:02,2.0.1,2.1.0,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,,,,,,"This query fails with a [NullPointerException on line 247|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala#L247]:

{code}
SELECT POSEXPLODE(SPLIT(REGEXP_REPLACE(ranks, '[\\[ \\]]', ''), ',')) AS (rank, col0) FROM table;
{code}

The problem is that POSEXPLODE is causing the REGEXP_REPLACE to be serialized after it is instantiated. The null value is a transient StringBuffer that should hold the result. The fix is to make the result value lazy.",,apachespark,rdblue,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-13819,,,,,,,,,,,,,,SPARK-18387,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 11 00:24:06 UTC 2016,,,,,,,,,,"0|i363nj:",9223372036854775807,,,,,,,,,,,,,2.0.3,2.1.0,,,,,,,,,,"08/Nov/16 23:03;apachespark;User 'rdblue' has created a pull request for this issue:
https://github.com/apache/spark/pull/15816;;;","09/Nov/16 18:54;apachespark;User 'rdblue' has created a pull request for this issue:
https://github.com/apache/spark/pull/15834;;;","09/Nov/16 19:02;yhuai;Issue resolved by pull request 15834
[https://github.com/apache/spark/pull/15834];;;","11/Nov/16 00:24;apachespark;User 'rdblue' has created a pull request for this issue:
https://github.com/apache/spark/pull/15847;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve Documentation for Sample Methods,SPARK-18365,13019372,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bill_chambers,bill_chambers,bill_chambers,08/Nov/16 21:25,17/Nov/16 11:35,14/Jul/23 06:29,17/Nov/16 11:35,,,,,,,,,2.1.0,,,,,,,,,,,,0,,,,,,"The documentation for sample is a little unintuitive. It was difficult to understand why I wasn't getting exactly the fraction specified of my total DataFrame rows. The PR clarifies the documentation for  Scala, Python, and R to explain that that is expected behavior.",,apachespark,bill_chambers,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 17 11:35:21 UTC 2016,,,,,,,,,,"0|i363hj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/16 21:29;apachespark;User 'anabranch' has created a pull request for this issue:
https://github.com/apache/spark/pull/15815;;;","17/Nov/16 11:35;srowen;Issue resolved by pull request 15815
[https://github.com/apache/spark/pull/15815];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
default table path of tables in default database should depend on the location of default database,SPARK-18360,13019285,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,08/Nov/16 17:49,18/Nov/16 01:33,14/Jul/23 06:29,18/Nov/16 01:33,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,release_notes,releasenotes,,,,,,apachespark,cloud_fan,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 18 01:33:31 UTC 2016,,,,,,,,,,"0|i362y7:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"08/Nov/16 17:54;srowen;Oh boy, more on the warehouse path. Since i've wrestled with it a lot recently (if it's about what i think it's about) let me know if I can provide input or review.;;;","08/Nov/16 18:08;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/15812;;;","18/Nov/16 01:33;yhuai;Issue resolved by pull request 15812
[https://github.com/apache/spark/pull/15812];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARN --files/--archives broke,SPARK-18357,13019248,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,kishorvpatil,tgraves,tgraves,08/Nov/16 15:43,17/May/20 18:13,14/Jul/23 06:29,08/Nov/16 18:14,2.1.0,,,,,,,,2.1.0,2.2.0,,,Spark Core,YARN,,,,,,,0,,,,,,"SPARK-18099 broke --files and --archives options.  The check should be ==null instead of !=:

 if (localizedPath != null) {
 +            throw new IllegalArgumentException(s""Attempt to add ($file) multiple times"" +
 +              "" to the distributed cache."")
 +          }",,apachespark,kishorvpatil,tgraves,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18099,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 08 17:19:04 UTC 2016,,,,,,,,,,"0|i362pz:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"08/Nov/16 15:57;tgraves;[~kishorvpatil] can you fix please;;;","08/Nov/16 16:03;kishorvpatil;My apologies for breaking the functionality. I will put up the patch soon.;;;","08/Nov/16 17:18;kishorvpatil;The patch is up with unit tests: https://github.com/apache/spark/pull/15810
;;;","08/Nov/16 17:19;apachespark;User 'kishorvpatil' has created a pull request for this issue:
https://github.com/apache/spark/pull/15810;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL fails to read data from a ORC hive table that has a new column added to it,SPARK-18355,13019153,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,Sandeep Nemuri,Sandeep Nemuri,08/Nov/16 09:45,03/Nov/17 04:44,14/Jul/23 06:29,13/Oct/17 15:25,1.6.2,2.1.0,2.2.0,,,,,,2.2.1,2.3.0,,,SQL,,,,,,,,1,,,,,,"*PROBLEM*:

Spark SQL fails to read data from a ORC hive table that has a new column added to it.

Below is the exception:
{code}
scala> sqlContext.sql(""select click_id,search_id from testorc"").show
16/11/03 22:17:53 INFO ParseDriver: Parsing command: select click_id,search_id from testorc
16/11/03 22:17:54 INFO ParseDriver: Parse Completed
java.lang.AssertionError: assertion failed
	at scala.Predef$.assert(Predef.scala:165)
	at org.apache.spark.sql.execution.datasources.LogicalRelation$$anonfun$1.apply(LogicalRelation.scala:39)
	at org.apache.spark.sql.execution.datasources.LogicalRelation$$anonfun$1.apply(LogicalRelation.scala:38)
	at scala.Option.map(Option.scala:145)
	at org.apache.spark.sql.execution.datasources.LogicalRelation.<init>(LogicalRelation.scala:38)
	at org.apache.spark.sql.execution.datasources.LogicalRelation.copy(LogicalRelation.scala:31)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.org$apache$spark$sql$hive$HiveMetastoreCatalog$$convertToOrcRelation(HiveMetastoreCatalog.scala:588)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog$OrcConversions$$anonfun$apply$2.applyOrElse(HiveMetastoreCatalog.scala:647)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog$OrcConversions$$anonfun$apply$2.applyOrElse(HiveMetastoreCatalog.scala:643)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:335)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:335)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:334)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:332)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:332)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:281)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)

{code}


*STEPS TO SIMULATE THIS ISSUE*:

1) Create table in hive.

{code}
CREATE TABLE `testorc`( 
`click_id` string, 
`search_id` string, 
`uid` bigint)
PARTITIONED BY ( 
`ts` string, 
`hour` string) 
STORED AS ORC; 
{code}

2) Load data into table :

{code}
INSERT INTO TABLE testorc PARTITION (ts = '98765',hour = '01' ) VALUES (12,2,12345);
{code}

3) Select through spark shell (This works)

{code}
sqlContext.sql(""select click_id,search_id from testorc"").show
{code}

4) Now add column to hive table

{code}
ALTER TABLE testorc ADD COLUMNS (dummy string);
{code}

5) Now again select from spark shell

{code}
scala> sqlContext.sql(""select click_id,search_id from testorc"").show
16/11/03 22:17:53 INFO ParseDriver: Parsing command: select click_id,search_id from testorc
16/11/03 22:17:54 INFO ParseDriver: Parse Completed
java.lang.AssertionError: assertion failed
	at scala.Predef$.assert(Predef.scala:165)
	at org.apache.spark.sql.execution.datasources.LogicalRelation$$anonfun$1.apply(LogicalRelation.scala:39)
	at org.apache.spark.sql.execution.datasources.LogicalRelation$$anonfun$1.apply(LogicalRelation.scala:38)
	at scala.Option.map(Option.scala:145)
	at org.apache.spark.sql.execution.datasources.LogicalRelation.<init>(LogicalRelation.scala:38)
	at org.apache.spark.sql.execution.datasources.LogicalRelation.copy(LogicalRelation.scala:31)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.org$apache$spark$sql$hive$HiveMetastoreCatalog$$convertToOrcRelation(HiveMetastoreCatalog.scala:588)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog$OrcConversions$$anonfun$apply$2.applyOrElse(HiveMetastoreCatalog.scala:647)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog$OrcConversions$$anonfun$apply$2.applyOrElse(HiveMetastoreCatalog.scala:643)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:335)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:335)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:334)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:332)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:332)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:281)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
{code}

",Centos6,apachespark,cloud_fan,dongjoon,mgaido,Sandeep Nemuri,tgraves,thomastechs,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,SPARK-21686,,,,,,,,,,,,,,SPARK-16628,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 13 15:25:51 UTC 2017,,,,,,,,,,"0|i3624v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/17 20:17;dongjoon;I confirmed that this happens only with `spark.sql.hive.convertMetastoreOrc=true`.
{code}
scala> spark.version
res0: String = 2.2.0-SNAPSHOT

scala> sql(""select click_id, search_id from testorc"").show
+--------+---------+
|click_id|search_id|
+--------+---------+
|      12|        2|
+--------+---------+

scala> sql(""set spark.sql.hive.convertMetastoreOrc=true"").show(false)
+----------------------------------+-----+
|key                               |value|
+----------------------------------+-----+
|spark.sql.hive.convertMetastoreOrc|true |
+----------------------------------+-----+

scala> sql(""select click_id, search_id from testorc"").show
17/03/08 12:15:43 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.IllegalArgumentException: Field ""click_id"" does not exist.
{code};;;","24/Aug/17 19:23;dongjoon;This will be resolved via Apache ORC 1.4.0.;;;","11/Oct/17 03:54;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19470;;;","13/Oct/17 15:25;cloud_fan;Issue resolved by pull request 19470
[https://github.com/apache/spark/pull/19470];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark.rpc.askTimeout defalut value is not 120s,SPARK-18353,13019129,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,JasonPan,JasonPan,08/Nov/16 07:24,19/Nov/16 11:29,14/Jul/23 06:29,19/Nov/16 11:28,1.6.1,2.0.1,,,,,,,2.1.0,,,,Spark Core,,,,,,,,0,,,,,,"in http://spark.apache.org/docs/latest/configuration.html 
spark.rpc.askTimeout  120s	Duration for an RPC ask operation to wait before timing out
the defalut value is 120s as documented.

However when I run ""spark-summit"" with standalone cluster mode:
the cmd is:
Launch Command: ""/opt/jdk1.8.0_102/bin/java"" ""-cp"" ""/opt/spark-2.0.1-bin-hadoop2.7/conf/:/opt/spark-2.0.1-bin-hadoop2.7/jars/*"" ""-Xmx1024M"" ""-Dspark.eventLog.enabled=true"" ""-Dspark.master=spark://9.111.159.127:7101"" ""-Dspark.driver.supervise=false"" ""-Dspark.app.name=org.apache.spark.examples.SparkPi"" ""-Dspark.submit.deployMode=cluster"" ""-Dspark.jars=file:/opt/spark-1.6.1-bin-hadoop2.6/lib/spark-examples-1.6.1-hadoop2.6.0.jar"" ""-Dspark.history.ui.port=18087"" ""-Dspark.rpc.askTimeout=10"" ""-Dspark.history.fs.logDirectory=file:/opt/tmp/spark-event"" ""-Dspark.eventLog.dir=file:///opt/tmp/spark-event"" ""org.apache.spark.deploy.worker.DriverWrapper"" ""spark://Worker@9.111.159.127:7103"" ""/opt/spark-2.0.1-bin-hadoop2.7/work/driver-20161109031939-0002/spark-examples-1.6.1-hadoop2.6.0.jar"" ""org.apache.spark.examples.SparkPi"" ""1000""

Dspark.rpc.askTimeout=10

the value is 10, it is not the same as document.

Note: when I summit to REST URL, it has no this issue.

",Linux zzz 3.10.0-327.el7.x86_64 #1 SMP Thu Oct 29 17:29:29 EDT 2015 x86_64 x86_64 x86_64 GNU/Linux,apachespark,JasonPan,xq2005,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 19 11:28:52 UTC 2016,,,,,,,,,,"0|i361zj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/16 07:39;JasonPan;in org.apache.spark.deploy.Client,
there is one line:
conf.set(""spark.rpc.askTimeout"", ""10"")

should we remove this line?

when use rest:
in org.apache.spark.deploy.rest.RestSubmissionClient , there is no this line.;;;","08/Nov/16 12:54;srowen;It does seem like Client should not set this value to 10, given the docs and the rest of the code. That could be removed. Also, spark.rpc.askTimeout doesn't actually default to 120s, but defaults to spark.network.timeout, which defaults to 120s. It could be worth updating the docs for the several properties that also have this default.;;;","08/Nov/16 13:53;JasonPan;Thanks.

Yes, if spark.rpc.askTimeout is not configured, will use the value of spark.network.timeout. 
The hard code in Client make behavior inconsistent with document, in addition, when we use ""--conf spark.rpc.askTimeout="" in spark-summit command or set it in spark-default.conf, it will not take effect due to it is configured in org.apache.spark.deploy.Client, when the network is busy, we can have no way to increase the rpc ascTimeout.;;;","09/Nov/16 17:57;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15833;;;","09/Nov/16 19:21;srowen;BTW [~JasonPan] it looks like you're setting JVM props, and not using --conf ? does --conf make it work?;;;","10/Nov/16 00:48;JasonPan;Hi Sean.

I was using ""--conf"" to set the parameter when summit. It didn't work.;;;","10/Nov/16 01:06;JasonPan;append the summit command:

spark-submit --class org.apache.spark.examples.SparkPi --master spark://9.111.159.127:7101 --conf  spark.rpc.askTimeout=150 --deploy-mode cluster /opt/spark-1.6.1-bin-hadoop2.6/lib/spark-examples-1.6.1-hadoop2.6.0.jar 100000
the parameter doesn't work.

Use rest: (6068 port is for rest)
spark-submit --class org.apache.spark.examples.SparkPi --master spark://9.111.159.127:6068 --conf  spark.rpc.askTimeout=150 --deploy-mode cluster /opt/spark-1.6.1-bin-hadoop2.6/lib/spark-examples-1.6.1-hadoop2.6.0.jar 100000
the parameter works.
 
We now summit to rest url for a workaround, otherwise,  the ""spark.rpc.askTimeout"" is always 10 due to the hardcode.

Thanks.;;;","10/Nov/16 01:11;JasonPan;No matter what the default value is at last,  I think we need a way to configure it.;;;","10/Nov/16 11:15;srowen;Let me know what you think of the pull requests at https://github.com/apache/spark/pull/15833 -- does it reflect your understanding?;;;","11/Nov/16 03:46;JasonPan;Yes, The pull requests make it can be set at least.

Thanks.

;;;","11/Nov/16 11:30;srowen;Any chance you can actually try a build with the patch? or comment on the PR? there's a question about whether this actually addresses your issue.;;;","17/Nov/16 01:13;JasonPan;Thanks sean. It works. 

Just for the doc: ""<td><code>spark.network.timeout</code>, or 10s in standalone clusters</td>""
Actually the default is not 10s in standalone when using rest.;;;","19/Nov/16 11:28;srowen;Issue resolved by pull request 15833
[https://github.com/apache/spark/pull/15833];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Infra for R - need qpdf on Jenkins,SPARK-18347,13019110,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shaneknapp,felixcheung,felixcheung,08/Nov/16 05:50,08/Nov/16 18:18,14/Jul/23 06:29,08/Nov/16 18:15,2.1.0,,,,,,,,,,,,SparkR,,,,,,,,0,,,,,,"As a part of working on building R package (https://issues.apache.org/jira/browse/SPARK-18264) we discover that building the package and vignettes require a tool call qpdf (for compressing PDFs)

In R, it is looking for qpdf as such:
{code}Sys.which(Sys.getenv(""R_QPDF"", ""qpdf"")){code}

ie. which qpdf or whatever the export R_QPDF is pointing to.

Otherwise it raises a warning as such:

{code}
* checking for unstated dependencies in examples ... OK
 WARNING
‘qpdf’ is needed for checks on size reduction of PDFs
{code}

cc 
[~shaneknapp]",,felixcheung,shaneknapp,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 08 18:18:59 UTC 2016,,,,,,,,,,"0|i361vb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/16 17:04;shaneknapp;ok, qpdf is installed:

{noformat}
$ pssh -h jenkins_workers.txt -t 0 ""yum -q -y install qpdf""
[1] 09:00:13 [SUCCESS] amp-jenkins-worker-01
[2] 09:00:14 [SUCCESS] amp-jenkins-worker-08
[3] 09:00:14 [SUCCESS] amp-jenkins-worker-06
[4] 09:00:14 [SUCCESS] amp-jenkins-worker-03
[5] 09:00:15 [SUCCESS] amp-jenkins-worker-02
[6] 09:00:15 [SUCCESS] amp-jenkins-worker-04
[7] 09:00:15 [SUCCESS] amp-jenkins-worker-05
[8] 09:00:16 [SUCCESS] amp-jenkins-worker-07
{noformat}

and for clarification:  does R *really* need an environment variable set to know where qpdf is?  it's currently installed in /usr/bin on all eight jenkins workers and i'm loathe to have to create an env var for a binary in a common dir.;;;","08/Nov/16 17:12;shivaram;Thanks [~shaneknapp] - We'll see the test run but looking at the code I dont think the env variable is required.;;;","08/Nov/16 18:15;felixcheung;Thanks [~shaneknapp] it is able to find it alright from the test run. Thanks for the quick help!;;;","08/Nov/16 18:18;shaneknapp;glad to help!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TRUNCATE TABLE should fail if no partition is matched for the given non-partial partition spec,SPARK-18346,13019106,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,08/Nov/16 05:29,08/Nov/16 14:31,14/Jul/23 06:29,08/Nov/16 14:30,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 08 05:36:05 UTC 2016,,,,,,,,,,"0|i361uf:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"08/Nov/16 05:36;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/15805;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HDFSBackedStateStore can fail to rename files causing snapshotting and recovery to fail,SPARK-18342,13019095,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,brkyvz,brkyvz,brkyvz,08/Nov/16 04:04,10/Nov/16 13:29,14/Jul/23 06:29,08/Nov/16 23:09,2.0.1,,,,,,,,2.0.2,2.1.0,,,Structured Streaming,,,,,,,,0,,,,,,"The HDFSBackedStateStore renames temporary files to delta files as it commits new versions. It however doesn't check whether the rename succeeded. If the rename fails, then recovery will not be possible. It should fail during the rename stage.",,apachespark,brkyvz,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 08 23:09:07 UTC 2016,,,,,,,,,,"0|i361rz:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"08/Nov/16 04:45;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/15804;;;","08/Nov/16 23:09;tdas;Issue resolved by pull request 15804
[https://github.com/apache/spark/pull/15804];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't push down current_timestamp for filters in StructuredStreaming,SPARK-18339,13019064,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tcondie,brkyvz,brkyvz,08/Nov/16 01:30,29/Nov/16 07:09,14/Jul/23 06:29,29/Nov/16 07:09,2.0.1,,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,"For the following workflow:
1. I have a column called time which is at minute level precision in a Streaming DataFrame
2. I want to perform groupBy time, count
3. Then I want my MemorySink to only have the last 30 minutes of counts and I perform this by
{code}
.where('time >= current_timestamp().cast(""long"") - 30 * 60)
{code}

what happens is that the `filter` gets pushed down before the aggregation, and the filter happens on the source data for the aggregation instead of the result of the aggregation (where I actually want to filter).
I guess the main issue here is that `current_timestamp` is non-deterministic in the streaming context and shouldn't be pushed down the filter.

Does this require us to store the `current_timestamp` for each trigger of the streaming job, that is something to discuss.",,apachespark,brkyvz,codingcat,lwlin,marmbrus,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 20 20:51:06 UTC 2016,,,,,,,,,,"0|i361l3:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"20/Nov/16 20:51;apachespark;User 'tcondie' has created a pull request for this issue:
https://github.com/apache/spark/pull/15949;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ObjectHashAggregateSuite fails under Maven builds,SPARK-18338,13019043,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,07/Nov/16 23:54,09/Nov/16 17:50,14/Jul/23 06:29,09/Nov/16 17:50,2.2.0,,,,,,,,2.2.0,,,,SQL,,,,,,,,0,flaky-test,,,,,"Test case initialization order under Maven and SBT are different. Maven always creates instances of all test cases and then run them all together.

This fails {{ObjectHashAggregateSuite}} because the randomized test cases there register a temporary Hive function right before creating a test case, and can be cleared while initializing other successive test cases.

In SBT, this is fine since the created test case is executed immediately after creating the temporary function. 

To fix this issue, we should put initialization/destruction code into {{beforeAll()}} and {{afterAll()}}.
",,apachespark,lian cheng,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 09 17:50:21 UTC 2016,,,,,,,,,,"0|i361gf:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"07/Nov/16 23:58;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/15802;;;","09/Nov/16 17:50;yhuai;Issue resolved by pull request 15802
[https://github.com/apache/spark/pull/15802];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassCastException during count distinct,SPARK-18300,13018842,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,emlyn,emlyn,07/Nov/16 14:19,15/Mar/17 22:32,14/Jul/23 06:29,15/Nov/16 15:00,2.0.1,,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,correctness,,,,,"While trying to reproduce SPARK-18172 in SQL, I found the following SQL query fails in Spark 2.0.1 (via spark-beeline) with a CassCastException:
{code}
select count(distinct b), count(distinct b, c) from (select 1 as a, 2 as b, 3 as c) group by a;
{code}
Selecting either of the two counts on their own runs fine, it is only when both are selected in the same query that it fails. I also tried the same query in pyspark:
{code}
spark.sql('select count(distinct b) as x, count(distinct b, c) as y from (select 1 as a, 2 as b, 3 as c) group by a').show()
{code}
And get the same error, with a full stack trace:
{code}
: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.Literal cannot be cast to org.apache.spark.sql.catalyst.expressions.Attribute
	at org.apache.spark.sql.types.StructType$$anonfun$fromAttributes$1.apply(StructType.scala:388)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.types.StructType$.fromAttributes(StructType.scala:388)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.schema$lzycompute(QueryPlan.scala:242)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.schema(QueryPlan.scala:242)
	at org.apache.spark.sql.execution.CollapseCodegenStages$$anonfun$10.apply(WholeStageCodegenExec.scala:454)
	at org.apache.spark.sql.execution.CollapseCodegenStages$$anonfun$10.apply(WholeStageCodegenExec.scala:454)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.execution.CollapseCodegenStages.supportCodegen(WholeStageCodegenExec.scala:454)
	at org.apache.spark.sql.execution.CollapseCodegenStages.org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen(WholeStageCodegenExec.scala:482)
	at org.apache.spark.sql.execution.CollapseCodegenStages$$anonfun$org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen$2.apply(WholeStageCodegenExec.scala:485)
	at org.apache.spark.sql.execution.CollapseCodegenStages$$anonfun$org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen$2.apply(WholeStageCodegenExec.scala:485)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.execution.CollapseCodegenStages.org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen(WholeStageCodegenExec.scala:485)
	at org.apache.spark.sql.execution.CollapseCodegenStages.org$apache$spark$sql$execution$CollapseCodegenStages$$insertInputAdapter(WholeStageCodegenExec.scala:469)
	at org.apache.spark.sql.execution.CollapseCodegenStages$$anonfun$org$apache$spark$sql$execution$CollapseCodegenStages$$insertInputAdapter$1.apply(WholeStageCodegenExec.scala:471)
	at org.apache.spark.sql.execution.CollapseCodegenStages$$anonfun$org$apache$spark$sql$execution$CollapseCodegenStages$$insertInputAdapter$1.apply(WholeStageCodegenExec.scala:471)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.execution.CollapseCodegenStages.org$apache$spark$sql$execution$CollapseCodegenStages$$insertInputAdapter(WholeStageCodegenExec.scala:471)
	at org.apache.spark.sql.execution.CollapseCodegenStages$$anonfun$org$apache$spark$sql$execution$CollapseCodegenStages$$insertInputAdapter$1.apply(WholeStageCodegenExec.scala:471)
	at org.apache.spark.sql.execution.CollapseCodegenStages$$anonfun$org$apache$spark$sql$execution$CollapseCodegenStages$$insertInputAdapter$1.apply(WholeStageCodegenExec.scala:471)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.execution.CollapseCodegenStages.org$apache$spark$sql$execution$CollapseCodegenStages$$insertInputAdapter(WholeStageCodegenExec.scala:471)
	at org.apache.spark.sql.execution.CollapseCodegenStages.org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen(WholeStageCodegenExec.scala:483)
	at org.apache.spark.sql.execution.CollapseCodegenStages$$anonfun$org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen$2.apply(WholeStageCodegenExec.scala:485)
	at org.apache.spark.sql.execution.CollapseCodegenStages$$anonfun$org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen$2.apply(WholeStageCodegenExec.scala:485)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.execution.CollapseCodegenStages.org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen(WholeStageCodegenExec.scala:485)
	at org.apache.spark.sql.execution.CollapseCodegenStages.org$apache$spark$sql$execution$CollapseCodegenStages$$insertInputAdapter(WholeStageCodegenExec.scala:469)
	at org.apache.spark.sql.execution.CollapseCodegenStages$$anonfun$org$apache$spark$sql$execution$CollapseCodegenStages$$insertInputAdapter$1.apply(WholeStageCodegenExec.scala:471)
	at org.apache.spark.sql.execution.CollapseCodegenStages$$anonfun$org$apache$spark$sql$execution$CollapseCodegenStages$$insertInputAdapter$1.apply(WholeStageCodegenExec.scala:471)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.execution.CollapseCodegenStages.org$apache$spark$sql$execution$CollapseCodegenStages$$insertInputAdapter(WholeStageCodegenExec.scala:471)
	at org.apache.spark.sql.execution.CollapseCodegenStages.org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen(WholeStageCodegenExec.scala:483)
	at org.apache.spark.sql.execution.CollapseCodegenStages$$anonfun$org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen$2.apply(WholeStageCodegenExec.scala:485)
	at org.apache.spark.sql.execution.CollapseCodegenStages$$anonfun$org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen$2.apply(WholeStageCodegenExec.scala:485)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.execution.CollapseCodegenStages.org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen(WholeStageCodegenExec.scala:485)
	at org.apache.spark.sql.execution.CollapseCodegenStages.apply(WholeStageCodegenExec.scala:490)
	at org.apache.spark.sql.execution.CollapseCodegenStages.apply(WholeStageCodegenExec.scala:430)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:93)
	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at org.apache.spark.sql.execution.QueryExecution.prepareForExecution(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:83)
	at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2572)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:1934)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2149)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,codingcat,emlyn,hvanhovell,smilegator,Tagar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19954,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 15 22:12:07 UTC 2016,,,,,,,,,,"0|i3607j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/16 21:03;hvanhovell;Hmmm... The {{org.apache.spark.sql.catalyst.optimizer.FoldablePropagation}} optimization rule replaces the group by key {{a}} by a literal. A literal is not a NamedExpression.;;;","11/Nov/16 23:49;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/15857;;;","15/Nov/16 15:00;smilegator;Issue resolved by pull request 15857
[https://github.com/apache/spark/pull/15857];;;","15/Nov/16 21:43;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/15891;;;","15/Nov/16 22:12;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/15892;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogicalPlanToSQLSuite should not use resource dependent path for golden file generation,SPARK-18292,13018668,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,06/Nov/16 07:58,09/Nov/16 17:49,14/Jul/23 06:29,09/Nov/16 17:49,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"`LogicalPlanToSQLSuite` uses the following command to update the existing answer files.
{code}
SPARK_GENERATE_GOLDEN_FILES=1 build/sbt ""hive/test-only *LogicalPlanToSQLSuite""
{code}

However, after introducing `getTestResourcePath`, it fails to update the previous golden answer files in the predefined directory. This issue aims to fix that.",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 09 17:49:13 UTC 2016,,,,,,,,,,"0|i35z4v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/16 08:05;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/15789;;;","09/Nov/16 17:49;srowen;Resolved by https://github.com/apache/spark/pull/15789;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scheme of DataFrame generated from RDD is different between master and 2.0,SPARK-18284,13018338,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kiszk,kiszk,kiszk,05/Nov/16 02:47,25/May/17 07:37,14/Jul/23 06:29,02/Dec/16 04:33,2.1.0,2.2.0,,,,,,,2.2.0,,,,SQL,,,,,,,,0,,,,,,"When the following program is executed, a schema of dataframe is different among master, branch 2.0, and branch 2.1. The result should be false.

{code:java}
val df = sparkContext.parallelize(1 to 8, 1).toDF()
df.printSchema
df.filter(""value > 4"").count

=== master ===
root
 |-- value: integer (nullable = true)

=== branch 2.1 ===
root
 |-- value: integer (nullable = true)

=== branch 2.0 ===
root
 |-- value: integer (nullable = false)
{code}",,apachespark,cloud_fan,kiszk,maropu,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20866,,,,,,,,,,SPARK-18623,,,,,,,,,,,SPARK-14584,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 05 18:50:24 UTC 2016,,,,,,,,,,"0|i35x3j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/16 06:17;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/15780;;;","02/Dec/16 04:33;cloud_fan;Issue resolved by pull request 15780
[https://github.com/apache/spark/pull/15780];;;","05/Dec/16 18:50;yhuai;[~kiszk] btw, do we know what caused the nullable setting change in 2.1?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
toLocalIterator yields time out error on pyspark2,SPARK-18281,13018312,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,lminer,lminer,04/Nov/16 23:38,27/Sep/17 13:46,14/Jul/23 06:29,20/Dec/16 21:13,2.0.1,,,,,,,,2.0.3,2.1.1,,,PySpark,,,,,,,,1,,,,,,"I run the example straight out of the api docs for toLocalIterator and it gives a time out exception:

{code}
from pyspark import SparkContext
sc = SparkContext()
rdd = sc.parallelize(range(10))
[x for x in rdd.toLocalIterator()]
{code}

conf file:
spark.driver.maxResultSize 6G
spark.executor.extraJavaOptions -XX:+UseG1GC -XX:MaxPermSize=1G -XX:+HeapDumpOnOutOfMemoryError
spark.executor.memory   16G
spark.executor.uri  foo/spark-2.0.1-bin-hadoop2.7.tgz
spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.buffer.dir  /raid0/spark
spark.hadoop.fs.s3n.buffer.dir  /raid0/spark
spark.hadoop.fs.s3a.connection.timeout 500000
spark.hadoop.fs.s3n.multipart.uploads.enabled   true
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 2
spark.hadoop.parquet.block.size 2147483648
spark.hadoop.parquet.enable.summary-metadata    false
spark.jars.packages com.databricks:spark-avro_2.11:3.0.1,com.amazonaws:aws-java-sdk-pom:1.10.34
spark.local.dir /raid0/spark
spark.mesos.coarse  false
spark.mesos.constraints  priority:1
spark.network.timeout   600
spark.rpc.message.maxSize    500
spark.speculation   false
spark.sql.parquet.mergeSchema   false
spark.sql.planner.externalSort  true
spark.submit.deployMode client
spark.task.cpus 1

Exception here:
{code}
---------------------------------------------------------------------------
timeout                                   Traceback (most recent call last)
<ipython-input-1-6319dd276401> in <module>()
      2 sc = SparkContext()
      3 rdd = sc.parallelize(range(10))
----> 4 [x for x in rdd.toLocalIterator()]

/foo/spark-2.0.1-bin-hadoop2.7/python/pyspark/rdd.pyc in _load_from_socket(port, serializer)
    140     try:
    141         rf = sock.makefile(""rb"", 65536)
--> 142         for item in serializer.load_stream(rf):
    143             yield item
    144     finally:

/foo/spark-2.0.1-bin-hadoop2.7/python/pyspark/serializers.pyc in load_stream(self, stream)
    137         while True:
    138             try:
--> 139                 yield self._read_with_length(stream)
    140             except EOFError:
    141                 return

/foo/spark-2.0.1-bin-hadoop2.7/python/pyspark/serializers.pyc in _read_with_length(self, stream)
    154 
    155     def _read_with_length(self, stream):
--> 156         length = read_int(stream)
    157         if length == SpecialLengths.END_OF_DATA_SECTION:
    158             raise EOFError

/foo/spark-2.0.1-bin-hadoop2.7/python/pyspark/serializers.pyc in read_int(stream)
    541 
    542 def read_int(stream):
--> 543     length = stream.read(4)
    544     if not length:
    545         raise EOFError

/usr/lib/python2.7/socket.pyc in read(self, size)
    378                 # fragmentation issues on many platforms.
    379                 try:
--> 380                     data = self._sock.recv(left)
    381                 except error, e:
    382                     if e.args[0] == EINTR:

timeout: timed out
{code}

","Ubuntu 14.04.5 LTS
Driver: AWS M4.XLARGE
Slaves: AWS M4.4.XLARGE
mesos 1.0.1
spark 2.0.1
pyspark",apachespark,davies,dusenberrymw,holden,lebigot,lminer,mylesbaker,realno,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18649,,,,SPARK-14334,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 13 01:54:04 UTC 2017,,,,,,,,,,"0|i35wxr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Dec/16 23:34;dusenberrymw;I'm also seeing the same error with both Python 2.7 and Python 3.5 on Spark 2.0.2 and the Git master when using {{rdd.toLocalIterator()}} or {{df.toLocalIterator()}} for a PySpark RDD and DataFrame, respectively.

On Spark 1.6.x, {{rdd.toLocalIterator()}} worked correctly.

Here's another example using DataFrames:
{code}
df = spark.createDataFrame([[1],[2],[3]])
it = df.toLocalIterator()   # should timeout here with an ""java.net.SocketTimeoutException: Accept timed out"" error
row = next(it)   # throws an ""Exception: could not open socket"" error
{code}

Result:
{code}
ERROR PythonRDD: Error while sending iterator
java.net.SocketTimeoutException: Accept timed out
	at java.net.PlainSocketImpl.socketAccept(Native Method)
	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)
	at java.net.ServerSocket.implAccept(ServerSocket.java:545)
	at java.net.ServerSocket.accept(ServerSocket.java:513)
	at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:697)
{code}

[~davies] I see that [SPARK-14334 | https://issues.apache.org/jira/browse/SPARK-14334] re-engineered and expanded the {{toLocalIterator}} functionality to DataSets/DataFrames for both Scala/Java & Python.  Do you have any thoughts on the issue that is arising now?;;;","13/Dec/16 03:46;viirya;[~mwdusenb@us.ibm.com] I can reproduce your issue. I already have the fixing too. If you are not working on this, I will submit a PR for it.

BTW, I can't exactly reproduce the issue reported by [~lminer]:

{code}
from pyspark import SparkContext
sc = SparkContext()
rdd = sc.parallelize(range(10))
[x for x in rdd.toLocalIterator()]
{code}

But the following one will be failed:
{code}
from pyspark import SparkContext
sc = SparkContext()
rdd = sc.parallelize(range(10))
it = rdd.toLocalIterator()
next(it)
{code}

They are caused by the same bug. I'd fix them together.
;;;","13/Dec/16 05:57;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/16263;;;","13/Dec/16 19:20;dusenberrymw;[~viirya] Thanks for taking on this bug!  I tried out PR, and I'm still running into a socket timeout error for the example I gave above:

{code}
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/mwdusenb/spark/python/pyspark/sql/dataframe.py"", line 416, in toLocalIterator
    peek = next(iter)
  File ""/home/mwdusenb/spark/python/pyspark/rdd.py"", line 140, in _load_from_socket
    for item in serializer.load_stream(rf):
  File ""/home/mwdusenb/spark/python/pyspark/serializers.py"", line 144, in load_stream
    yield self._read_with_length(stream)
  File ""/home/mwdusenb/spark/python/pyspark/serializers.py"", line 161, in _read_with_length
    length = read_int(stream)
  File ""/home/mwdusenb/spark/python/pyspark/serializers.py"", line 555, in read_int
    length = stream.read(4)
  File ""/opt/anaconda3/lib/python3.5/socket.py"", line 575, in readinto
    return self._sock.recv_into(b)
socket.timeout: timed out
{code}

Interestingly, it looks like the {{it = df.toLocalIterator()}} line launches a very large number of {{toLocalIterator}} jobs, and then the Python socket times out while those jobs are running.;;;","13/Dec/16 19:29;dusenberrymw;Here's another interesting finding.  The first (original) example fails with the timeout.  However, if you create the DataFrame, do something with it, and then create the iterator, it will work.

{code}
df = spark.createDataFrame([[1],[2],[3]])
it = df.toLocalIterator()  # FAILS HERE
row = next(it)
{code}

{code}
df = spark.createDataFrame([[1],[2],[3]])
df.count()
it = df.toLocalIterator()  # No longer fails
row = next(it)
{code}

This leads me to believe there may be something wrong with the creation of the DataFrame.;;;","14/Dec/16 03:42;viirya;[~mwdusenb@us.ibm.com] Thanks for reporting this again! I can't reproduce this after applying the PR. However, I think the remaining issue is similar to the change in the PR.

In JVM side, we just get an iterator of the RDD partitioned results. Once the connection is established, we begin to write elements through the socket. However, if the RDD is not materialized before, the materialization time + network cost might exceed the timeout setting before serving the first element to Python.

That is why when you materialize the RDD by running {{df.count}}, it will not fail.

I'd change the PR accordingly. May you try it again and see if it solves your tests? Thanks.;;;","14/Dec/16 22:30;dusenberrymw;[~viirya] Thanks for continuing to work on this!  With the latest PR update, the original example now runs correctly when running PySpark in local mode ({{./bin/pyspark}}).  However, I then tried the same example  on Yarn again ({{./bin/pyspark --master yarn --deploy-mode client}}) and ran into the same socket timeout issues.

I looked into it further and found some interesting findings.  In local mode execution, the number of partitions for the DataFrame was {{48}}, while in Yarn execution mode, the same DataFrame started at {{665}} partitions.  Looking at the UI, as soon as {{it = df.toLocalIterator()}} is called, it will launch a number of {{toLocalIterator}} jobs equal to the number of partitions, so {{48}} jobs in local mode, and {{665}} in Yarn mode.  Currently, the {{it = df.toLocalIterator()}} line blocks until all of those jobs finish.  So, if the number of partitions is high enough, the socket timeout will still be triggered.

Below is a reproducible example (I hope!) in local mode ({{./bin/pyspark}}):

{code}
df = spark.createDataFrame([[1],[2],[3]])
it = df.toLocalIterator()
row = next(it)   # this should work
df.rdd.getNumPartitions()  # returns `48`

# Now let's break it
df2 = df.repartition(700)  # increase number of partitions
it2 = df2.toLocalIterator()  # THIS FAILS -> `socket.timeout: timed out`
{code};;;","14/Dec/16 23:31;holden;For what its worth I can repro on top of the PR with [~mwdusenb@us.ibm.com]'s example. I think we might be going about fixing this in somewhat of an odd way - the current Python behaviour of toLocalIterator is pretty different than that of Scala, we immediately do a foreach on the Scala iterator which is somewhat strange.

Maybe we could change this to behave more like the Scala toLocalIterator and get rid of these timeouts/hacks around the timeouts. What do people think?;;;","15/Dec/16 02:31;viirya;Hi [~holdenk], what you meant for ""we immediately do a foreach on the Scala iterator which is somewhat strange.""?;;;","15/Dec/16 03:20;viirya;[~mwdusenb@us.ibm.com] Thanks for this test case! It is useful to me. However I need to increase the partition number to 1000 to reproduce this issue.

The additional partitions will increase the time to materialize RDD elements and so cause timeout.

I think we can't set a timeout to the socket reading operation like currently doing as the RDD materialization time is unpredictable. I will keep the connection timeout untouched but unset timeout for socket reading. ;;;","15/Dec/16 03:43;viirya;[~mwdusenb@us.ibm.com] BTW, I updated the fixing and if you have time to test it again, that would be great. Thank you.;;;","20/Dec/16 21:13;davies;Issue resolved by pull request 16263
[https://github.com/apache/spark/pull/16263];;;","25/Feb/17 04:13;realno;Is this bug really resolved? I am using the latest 2.1.0 release and having the same timeout behaviour as [~mwdusenb@us.ibm.com] described. When using the iterator in local mode it works fine but as soon as moving to cluster it will timeout. I also tested with Mike's example and was able to validate it. Can someone point me to a fix or an alternative with similar functionality?

Thanks!;;;","12/Mar/17 11:31;lebigot;Same here: I see the problem with the latest version too (2.1.0)! The problem appears randomly, for me (I haven't built any minimal example, but the problem looks very much like the one reported here: toLocalIterator used, etc.).;;;","12/Mar/17 11:41;viirya;Can you provide some info about your environment? Few reproducible examples we used before are:

{code}
df = spark.createDataFrame([[1],[2],[3]])
it = df.toLocalIterator()
row = next(it)   # this should work
df.rdd.getNumPartitions()  # returns `48`

# Now let's break it
df2 = df.repartition(700)  # increase number of partitions
it2 = df2.toLocalIterator()  # THIS FAILS -> `socket.timeout: timed out`
{code}

{code}
df = spark.createDataFrame([[1],[2],[3]])
it = df.toLocalIterator()  # FAILS HERE
row = next(it)
{code}

Can you run this examples without failure?
;;;","12/Mar/17 11:42;viirya;Or you have other reproducible examples to test?;;;","12/Mar/17 11:50;lebigot;Thanks Liang-Chi. Now I do have a minimal example: the small example which is marked above as working is not working on my machine:
{code}
~/Downloads/spark-2.1.0-bin-hadoop2.7/bin % PYSPARK_DRIVER_PYTHON=ipython2 ./pyspark

Python 2.7.13 (default, Dec 23 2016, 05:05:58)
Type ""copyright"", ""credits"" or ""license"" for more information.

IPython 5.3.0 -- An enhanced Interactive Python.
?         -> Introduction and overview of IPython's features.
%quickref -> Quick reference.
help      -> Python's own help system.
object?   -> Details about 'object', use 'object??' for extra details.
17/03/12 12:46:29 WARN SparkContext: Support for Java 7 is deprecated as of Spark 2.0.0
2017-03-12 12:46:30.538 java[75598:10832148] Unable to load realm info from SCDynamicStore
17/03/12 12:46:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/03/12 12:46:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
17/03/12 12:46:48 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.1.0
      /_/

Using Python version 2.7.13 (default, Dec 23 2016 05:05:58)
SparkSession available as 'spark'.
In [1]: df = spark.createDataFrame([[1],[2],[3]])
   ...: it = df.toLocalIterator()
   ...: row = next(it)   # this should work
   ...: df.rdd.getNumPartitions()  # returns `48`
   ...: 

---------------------------------------------------------------------------
timeout                                   Traceback (most recent call last)
<ipython-input-1-828d0f5b5ce8> in <module>()
      1 df = spark.createDataFrame([[1],[2],[3]])
      2 it = df.toLocalIterator()
----> 3 row = next(it)   # this should work
      4 df.rdd.getNumPartitions()  # returns `48`

/Users/lebigot/Downloads/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.pyc in _load_from_socket(port, serializer)
    138     try:
    139         rf = sock.makefile(""rb"", 65536)
--> 140         for item in serializer.load_stream(rf):
    141             yield item
    142     finally:

/Users/lebigot/Downloads/spark-2.1.0-bin-hadoop2.7/python/pyspark/serializers.pyc in load_stream(self, stream)
    142         while True:
    143             try:
--> 144                 yield self._read_with_length(stream)
    145             except EOFError:
    146                 return

/Users/lebigot/Downloads/spark-2.1.0-bin-hadoop2.7/python/pyspark/serializers.pyc in _read_with_length(self, stream)
    159
    160     def _read_with_length(self, stream):
--> 161         length = read_int(stream)
    162         if length == SpecialLengths.END_OF_DATA_SECTION:
    163             raise EOFError

/Users/lebigot/Downloads/spark-2.1.0-bin-hadoop2.7/python/pyspark/serializers.pyc in read_int(stream)
    553
    554 def read_int(stream):
--> 555     length = stream.read(4)
    556     if not length:
    557         raise EOFError

/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/socket.pyc in read(self, size)
    382                 # fragmentation issues on many platforms.
    383                 try:
--> 384                     data = self._sock.recv(left)
    385                 except error, e:
    386                     if e.args[0] == EINTR:

timeout: timed out
{code}
The number of partitions is 4.

Configuration:
- latest macOS Sierra (10.12.3),
- IPython, etc. provided through MacPorts,
- no special Spark configuration except for the verbosity level,
- nothing else running on my machine (MacBook early 2015).;;;","12/Mar/17 12:54;lebigot;The second example also fails, but differently:
{code}
~/Downloads/spark-2.1.0-bin-hadoop2.7/bin % PYSPARK_DRIVER_PYTHON=ipython2 ./pyspark

Python 2.7.13 (default, Dec 23 2016, 05:05:58)
Type ""copyright"", ""credits"" or ""license"" for more information.

IPython 5.3.0 -- An enhanced Interactive Python.
?         -> Introduction and overview of IPython's features.
%quickref -> Quick reference.
help      -> Python's own help system.
object?   -> Details about 'object', use 'object??' for extra details.
17/03/12 13:52:32 WARN SparkContext: Support for Java 7 is deprecated as of Spark 2.0.0
2017-03-12 13:52:32.493 java[79268:10882992] Unable to load realm info from SCDynamicStore
17/03/12 13:52:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/03/12 13:52:42 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.1.0
      /_/

Using Python version 2.7.13 (default, Dec 23 2016 05:05:58)
SparkSession available as 'spark'.
(…)
In [2]: df = spark.createDataFrame([[1],[2],[3]])
   ...: 
In [3]: df.rdd.getNumPartitions()
Out[3]: 4
In [4]: df2 = df.repartition(700)  # increase number of partitions
   ...: it2 = df2.toLocalIterator()  # THIS FAILS -> `socket.timeout: timed out`
   ...: 
   ...: 
In [5]: 17/03/12 13:53:20 ERROR PythonRDD: Error while sending iterator
java.net.SocketTimeoutException: Accept timed out
        at java.net.PlainSocketImpl.socketAccept(Native Method)
        at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java)
        at java.net.ServerSocket.implAccept(ServerSocket.java:530)
        at java.net.ServerSocket.accept(ServerSocket.java:498)
        at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:69)
{code};;;","12/Mar/17 13:15;viirya;Besides, can you also provide the error log?;;;","12/Mar/17 13:25;viirya;[~lebigot] Thanks for the error log! It is weird because looks like you run the following old code which is fixed by the PR submitted for this issue: https://github.com/apache/spark/pull/16263

{code}
/Users/lebigot/Downloads/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.pyc in _load_from_socket(port, serializer)
    138     try:
    139         rf = sock.makefile(""rb"", 65536)
--> 140         for item in serializer.load_stream(rf):
    141             yield item
    142     finally:
{code}

I go to check the detailed changes in Spark 2.1.0: https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315420&version=12335644

I don't find this issue in the JIRA list. So I think this fixing is not included in Spark 2.1.0 release. 

;;;","12/Mar/17 13:27;viirya;Oh. Btw, you can see the Fix Version/s of this JIRA is 2.0.3, 2.1.1.;;;","12/Mar/17 15:56;lebigot;Thanks Liang-Chi. I naively thought that if version 2.0.3 was listed in the fixed version it implied that 2.1 had the fix. I'm looking forward to using the fixed version, then (not sure yet how to do this right now without compiling anything, though).;;;","12/Mar/17 16:05;srowen;2.1.1 means 2.1.1 is the first 2.1.x version that has the fix, so, not 2.1.0. 2.0.3 comes after 2.1.0 chronologically.;;;","13/Mar/17 01:54;viirya;That is right. So you can try 2.1.1 or latest codebase to test it. Please let me know if this issue happens still. Thanks.;;;",,,,,,,,,,,,,,,,,,,
Potential deadlock in `StandaloneSchedulerBackend.dead`,SPARK-18280,13018289,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,04/Nov/16 22:12,08/Dec/16 23:02,14/Jul/23 06:29,08/Nov/16 21:19,1.6.2,2.0.0,2.0.1,,,,,,2.0.3,2.1.0,,,Spark Core,,,,,,,,0,,,,,,"""StandaloneSchedulerBackend.dead"" is called in a RPC thread, so it should not call ""SparkContext.stop"" in the same thread. ""SparkContext.stop"" will block until all RPC threads exit, if it's called inside a RPC thread, it will be dead-lock.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 04 22:16:06 UTC 2016,,,,,,,,,,"0|i35wsn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/16 22:16;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/15775;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory leak in PySpark StringIndexer,SPARK-18274,13018145,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,techaddict,jonasamrich,jonasamrich,04/Nov/16 15:31,08/Dec/16 07:29,14/Jul/23 06:29,01/Dec/16 21:23,1.5.2,1.6.3,2.0.1,2.0.2,2.1.0,,,,2.0.3,2.1.0,2.2.0,,ML,PySpark,,,,,,,0,,,,,,"StringIndexerModel won't get collected by GC in Java even when deleted in Python. It can be reproduced by this code, which fails after couple of iterations (around 7 if you set driver memory to 600MB): 

{code}
import random, string
from pyspark.ml.feature import StringIndexer

l = [(''.join(random.choice(string.ascii_uppercase) for _ in range(10)), ) for _ in range(int(7e5))]  # 700000 random strings of 10 characters
df = spark.createDataFrame(l, ['string'])

for i in range(50):
    indexer = StringIndexer(inputCol='string', outputCol='index')
    indexer.fit(df)
{code}

Explicit call to Python GC fixes the issue - following code runs fine:

{code}
for i in range(50):
    indexer = StringIndexer(inputCol='string', outputCol='index')
    indexer.fit(df)
    gc.collect()
{code}

The issue is similar to SPARK-6194 and can be probably fixed by calling jvm detach in model's destructor. This is implemented in pyspark.mlib.common.JavaModelWrapper but missing in pyspark.ml.wrapper.JavaWrapper. Other models in ml package may also be affected by this memory leak. ",,apachespark,jonasamrich,josephkb,mlnick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18630,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 08 07:29:12 UTC 2016,,,,,,,,,,"0|i35vwv:",9223372036854775807,,,,,josephkb,,,,,,,,2.0.3,2.1.0,,,,,,,,,,"10/Nov/16 16:25;apachespark;User 'techaddict' has created a pull request for this issue:
https://github.com/apache/spark/pull/15843;;;","11/Nov/16 07:48;josephkb;Adding one more TODO for this task:
The current fix for this is to put a {{__del__}} method in JavaWrapper which releases the Java object.  But that exposes another bug: copy should be implemented within JavaParams, not JavaModel.  Otherwise, JavaEvaluator (which inherits from JavaParams) can be copied to produce multiple Python instances (which should be treated independently) all of which link to the same Java object.  Changing one instance will then change others.;;;","01/Dec/16 21:23;josephkb;Issue resolved by pull request 15843
[https://github.com/apache/spark/pull/15843];;;","08/Dec/16 07:29;mlnick;Went ahead and re-marked fix version to {{2.1.0}} since RC2 has been cut.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NumberFormatException when reading csv for a nullable column,SPARK-18269,13018026,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,jzijlstra,jzijlstra,04/Nov/16 10:44,12/Dec/22 18:10,14/Jul/23 06:29,07/Nov/16 02:51,2.0.1,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Having a schema with a nullable column thrown an java.lang.NumberFormatException: null when the data + delimeter isn't specified in the csv.

Specifying the schema:

{code}
StructType(Array(
  StructField(""id"", IntegerType, nullable = false),
  StructField(""underlyingId"", IntegerType, true)
))
{code}

Data (without trailing delimeter to specify the second column):
{code}
1
{code}

Read the data:
{code}
sparkSession.read
    .schema(sourceSchema)
    .option(""header"", ""false"")
    .option(""delimiter"", """"""\t"""""")
    .csv(files(dates): _*)
    .rdd
{code}

Actual Result: 
{code}
java.lang.NumberFormatException: null
	at java.lang.Integer.parseInt(Integer.java:542)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at org.apache.spark.sql.execution.datasources.csv.CSVTypeCast$.castTo(CSVInferSchema.scala:244)
{code}

Reason:
The csv line is parsed into a Map (indexSafeTokens), which is short of one value. So indexSafeTokens(index) throws a NullpointerException reading the optional value which isn't in the Map.

The NullpointerException is then given to the CSVTypeCast.castTo(datum: String, .....) as the datum value.
The subsequent NumberFormatException is thrown due to the fact that a NullpointerException cannot be cast into the Type.

Possible fix:
- Use the provided schema to parse the line with the correct number of columns
- Since its nullable implement a try catch on CSVRelation.csvParser indexSafeTokens(index)",,apachespark,jzijlstra,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 01 16:40:39 UTC 2016,,,,,,,,,,"0|i35v6f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/16 11:18;gurwls223;Could you try set {{nullValue}} to {{""null""}}?;;;","04/Nov/16 11:24;gurwls223;BTW, if I remember this correctly, it seems always forcing the schema to nullable for reading data with file-based data sources (except structured streaming) as you described in SPARK-18270.;;;","04/Nov/16 12:22;jzijlstra;The error that is thrown is java.lang.NumberFormatException: null. In this case null is a NullPointerException and not the value ""null"".
I did try this before submitting this issue but having the value ""null"" as nullValue doesn't work since ""null"" != NullPointerException.

Apparently putting a NullpointerException in a parameter of type String works.;;;","04/Nov/16 13:39;gurwls223;Ah, yes. it seems a bug. I can reproduce this. Thanks for correcting me. It will be a quicky fix. Please let me submit a PR.;;;","04/Nov/16 13:59;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/15767;;;","01/Dec/16 16:40;jzijlstra;Thanks for the quick response. Eagerly awaiting the spark 2.1 release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JSON.org license is now CatX,SPARK-18262,13017930,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,srowen,busbey,busbey,04/Nov/16 03:40,27/Mar/19 13:03,14/Jul/23 06:29,10/Nov/16 17:34,,,,,,,,,2.1.0,,,,,,,,,,,,0,,,,,,"per [update resolved legal|http://www.apache.org/legal/resolved.html#json]:

{quote}
CAN APACHE PRODUCTS INCLUDE WORKS LICENSED UNDER THE JSON LICENSE?

No. As of 2016-11-03 this has been moved to the 'Category X' license list. Prior to this, use of the JSON Java library was allowed. See Debian's page for a list of alternatives.
{quote}

I'm not actually clear if Spark is using one of the JSON.org licensed libraries. As of current master (dc4c6009) the java library gets called out in the [NOTICE file for our source repo|https://github.com/apache/spark/blob/dc4c60098641cf64007e2f0e36378f000ad5f6b1/NOTICE#L424] but:

1) It doesn't say where in the source
2) the given url is 404 (http://www.json.org/java/index.html)
3) It doesn't actually say in the NOTICE what license the inclusion is under
4) the JSON.org license for the java {{org.json:json}} artifact (what the blurb in #2 is usually referring to) doesn't show up in our LICENSE file, nor in the {{licenses/}} directory
5) I don't see a direct reference to the {{org.json:json}} artifact in our poms.

So maybe it's just coming in transitively and we can exclude it / ping whoever is bringing it in?",,apachespark,busbey,rxin,stevel@apache.org,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 27 13:03:03 UTC 2019,,,,,,,,,,"0|i35ul3:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"04/Nov/16 09:11;srowen;I'll investigate. Yes it comes from the shaded Hive dependency. I think we'll have to find a way to get rid of it or re-shade a newer release of Hive once it fixes the issue. It's not used by Spark itself. I'll figure out whether we even distribute this code in a release.;;;","07/Nov/16 15:35;srowen;{{org.json:json}} is brought in from the {{hive-exec}} dependency. It isn't used directly by Spark, but, the releases do end up releasing a copy of this JAR file as a result.

I suppose that means that one ideal way forward is to wait for Hive to fix this dependency issue too, then publish a new {{org.spark-project}} version of it, and depend on that. This could take a while, and may not help older versions anyway.

I looked into usages of this library in Hive itself and it's referenced in only a handful of places, half of which are elements that Spark probably doesn't ever touch, like Tez support. I tried simply excluding the dependency. While I got test failures, I can't seem to determine whether they're related, because the error changes from run to run and the one semi-persistent one does not seem related. 

I will try to open a PR with this change as a WIP to see what Jenkins says as a second opinion.;;;","07/Nov/16 16:06;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15798;;;","28/Nov/16 16:09;stevel@apache.org;~tdunning has done a mostly-compatible org.json replacement JAR, see HADOOP-13794. If org.json can't be pulled, and there's a lib using a non-shaded dependency, this should be able to act as a replacement;;;","27/Mar/19 12:34;yumwang;[~srowen] I'm not sure do we need exclude {{org.json#json}} from [https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/package.scala|https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/package.scala#L20]

Run {{HiveSparkSubmitSuite}} to reproduce::
{code:java}
HiveSparkSubmitSuite.test(""SPARK-8020: set sql conf in spark conf"") {
  val unusedJar = TestUtils.createJarWithClasses(Seq.empty)
  val args = Seq(
    ""--class"", SparkSQLConfTest.getClass.getName.stripSuffix(""$""),
    ""--name"", ""SparkSQLConfTest"",
    ""--master"", ""local-cluster[2,1,1024]"",
    ""--conf"", ""spark.ui.enabled=false"",
    ""--conf"", ""spark.master.rest.enabled=false"",
    ""--conf"", ""spark.sql.hive.metastore.version=0.12"",
    ""--conf"", ""spark.sql.hive.metastore.jars=maven"",
    ""--driver-java-options"", ""-Dderby.system.durability=test"",
    unusedJar.toString)
  runSparkSubmit(args)
}
{code}
{noformat}
...
2019-03-04 11:51:02.184 - stderr>       found com.google.protobuf#protobuf-java;2.4.1 in central
2019-03-04 11:51:02.207 - stderr>       found org.iq80.snappy#snappy;0.2 in user-list
2019-03-04 11:51:02.212 - stderr>       found org.json#json;20090211 in user-list
2019-03-04 11:51:02.234 - stderr>       found commons-configuration#commons-configuration;1.6 in user-list
...
{noformat};;;","27/Mar/19 12:48;srowen;I don't see org.json:json in the output of mvn dependency:tree right now. What is the output you're quoting here, is it just from what's resolved in the test when running the Hive tests? I don't think that means it's actually bundled with Spark. [~yumwang];;;","27/Mar/19 13:03;yumwang;Yes. It only happens when running the Hive tests. Thank you [~srowen]
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QueryExecution should not catch Throwable,SPARK-18259,13017846,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,hvanhovell,hvanhovell,hvanhovell,03/Nov/16 21:49,04/Nov/16 04:59,14/Jul/23 06:29,04/Nov/16 04:59,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,QueryExecution currently captures Throwable. This is far from a best practice. It would be better if we'd catch AnalysisExceptions.,,apachespark,hvanhovell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 03 22:48:05 UTC 2016,,,,,,,,,,"0|i35u2n:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"03/Nov/16 22:48;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/15760;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve error reporting for FileStressSuite in streaming,SPARK-18257,13017792,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,03/Nov/16 19:20,03/Nov/16 22:30,14/Jul/23 06:29,03/Nov/16 22:30,,,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,FileStressSuite doesn't report errors when they occur.,,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 03 19:24:06 UTC 2016,,,,,,,,,,"0|i35tq7:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"03/Nov/16 19:24;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/15757;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDFs don't see aliased column names,SPARK-18254,13017698,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eyalfa,nchammas,nchammas,03/Nov/16 15:35,25/Nov/16 21:28,14/Jul/23 06:29,03/Nov/16 23:24,2.0.1,,,,,,,,,,,,SQL,,,,,,,,0,correctness,,,,,"Dunno if I'm misinterpreting something here, but this seems like a bug in how UDFs work, or in how they interface with the optimizer.

Here's a basic reproduction. I'm using {{length_udf()}} just for illustration; it could be any UDF that accesses fields that have been aliased.

{code}
import pyspark
from pyspark.sql import Row
from pyspark.sql.functions import udf, col, struct


def length(full_name):
    # The non-aliased names, FIRST and LAST, show up here, instead of
    # first_name and last_name.
    # print(full_name)
    return len(full_name.first_name) + len(full_name.last_name)


if __name__ == '__main__':
    spark = (
        pyspark.sql.SparkSession.builder
        .getOrCreate())

    length_udf = udf(length)

    names = spark.createDataFrame([
        Row(FIRST='Nick', LAST='Chammas'),
        Row(FIRST='Walter', LAST='Williams'),
    ])

    names_cleaned = (
        names
        .select(
            col('FIRST').alias('first_name'),
            col('LAST').alias('last_name'),
        )
        .withColumn('full_name', struct('first_name', 'last_name'))
        .select('full_name'))

    # We see the schema we expect here.
    names_cleaned.printSchema()

    # However, here we get an AttributeError. length_udf() cannot
    # find first_name or last_name.
    (names_cleaned
    .withColumn('length', length_udf('full_name'))
    .show())
{code}

When I run this I get a long stack trace, but the relevant portions seem to be:

{code}
  File "".../udf-alias.py"", line 10, in length
    return len(full_name.first_name) + len(full_name.last_name)
  File ""/usr/local/Cellar/apache-spark/2.0.1/libexec/python/lib/pyspark.zip/pyspark/sql/types.py"", line 1502, in __getattr__
    raise AttributeError(item)
AttributeError: first_name
{code}

{code}
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/usr/local/Cellar/apache-spark/2.0.1/libexec/python/lib/pyspark.zip/pyspark/sql/types.py"", line 1497, in __getattr__
    idx = self.__fields__.index(item)
ValueError: 'first_name' is not in list
{code}

Here are the relevant execution plans:

{code}
names_cleaned.explain()

== Physical Plan ==
*Project [struct(FIRST#0 AS first_name#5, LAST#1 AS last_name#6) AS full_name#10]
+- Scan ExistingRDD[FIRST#0,LAST#1]
{code}

{code}
(names_cleaned
.withColumn('length', length_udf('full_name'))
.explain())

== Physical Plan ==
*Project [struct(FIRST#0 AS first_name#5, LAST#1 AS last_name#6) AS full_name#10, pythonUDF0#21 AS length#17]
+- BatchEvalPython [length(struct(FIRST#0, LAST#1))], [FIRST#0, LAST#1, pythonUDF0#21]
   +- Scan ExistingRDD[FIRST#0,LAST#1]
{code}

It looks like from the second execution plan that {{BatchEvalPython}} somehow gets the unaliased column names, whereas the {{Project}} right above it gets the aliased names.","Python 3.5, Java 8",davies,hvanhovell,marmbrus,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18589,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 03 22:08:00 UTC 2016,,,,,,,,,,"0|i35t5j:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"03/Nov/16 15:49;nchammas;[~marmbrus] / [~hvanhovell]: Is there a workaround for this issue?;;;","03/Nov/16 16:45;hvanhovell;Hmmmm... I am not python expert. Is it normal that we pass a row with schema info to UDF?

[~davies] any idea?;;;","03/Nov/16 16:48;nchammas;Yep, it works fine if the column names haven't been aliased.;;;","03/Nov/16 16:50;hvanhovell;So using FIRST and LAST as column names work?;;;","03/Nov/16 16:51;hvanhovell;Oh, BatchEvalPython is pushed below the Projection. That is not supposed to happen.;;;","03/Nov/16 16:55;nchammas;Yes, if I don't alias the columns and/or update {{length()}} to use {{FIRST}} and {{LAST}}, everything works.;;;","03/Nov/16 18:28;nchammas;Interestingly, if I add {{names_cleaned.persist()}} right before the failing call, it makes it work. So, like this:

{code}
    ...
    names_cleaned.persist()

    # Now it works.
    (names_cleaned
    .withColumn('length', length_udf('full_name'))
    .show())
{code}

And the physical plan now looks like this:

{code}
(names_cleaned
.withColumn('length', length_udf('full_name'))
.explain())

== Physical Plan ==
*Project [full_name#10, pythonUDF0#36 AS length#27]
+- BatchEvalPython [length(full_name#10)], [full_name#10, pythonUDF0#36]
   +- InMemoryTableScan [full_name#10]
      :  +- InMemoryRelation [full_name#10], true, 10000, StorageLevel(memory, 1 replicas)
      :     :  +- *Project [struct(FIRST#0 AS first_name#5, LAST#1 AS last_name#6) AS full_name#10]
      :     :     +- Scan ExistingRDD[FIRST#0,LAST#1]
{code}

Now {{BatchEvalPython}} gets the correct, aliased column names.

{{persist()}} is an expensive workaround. Is there a simpler way?;;;","03/Nov/16 20:22;marmbrus;Is this yet another bug caused by the generic operator push down?  Can we turn that off?;;;","03/Nov/16 20:49;davies;I doubt it's a bug in ExtractPythonUDFs, not operator push down, not verified yet.;;;","03/Nov/16 21:21;davies;I tried the following in master (2.1), it works

{code}
        from pyspark.sql.functions import udf, col, struct
        myadd = udf(lambda s: s.a + s.b, IntegerType())
        df = self.spark.range(10).selectExpr(""id as a"", ""id as b"")\
            .select(struct(col(""a""), col(""b"")).alias('s'))
        df = df.select(df.s, myadd(df.s).alias(""a""))
        df.explain(True)
        rs = df.collect()
{code}

[~nchammas] Could you also try yours on master?;;;","03/Nov/16 21:45;nchammas;If I try branch-2.1 on commit {{569f77a11819523bdf5dc2c6429fc3399cbb6519}}, the original repro case works. 👍

(And for the record, this is still broken on the latest commit in branch-2.0, {{dae1581d9461346511098dc83938939a0f930048}}, so the fix is 2.1+.)

So while we're waiting for 2.1 to be released, is there a workaround you'd recommend, apart from {{persist()}}?;;;","03/Nov/16 21:56;davies;Could you also try 2.0.2?;;;","03/Nov/16 21:59;nchammas;Just tried it. Seems like the fix is only available in 2.1+.;;;","03/Nov/16 22:00;hvanhovell;We 'accidentally' fixed this yesterday with commit: https://github.com/apache/spark/commit/f151bd1af8a05d4b6c901ebe6ac0b51a4a1a20df
;;;","03/Nov/16 22:08;nchammas;👏 👍 😀

So it was specifically some broken interaction between structs and aliases, I guess? Anyway, glad it's been fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"DataSet API | RuntimeException: Null value appeared in non-nullable field when holding Option Case Class",SPARK-18251,13017588,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,aniket,aniket,03/Nov/16 10:49,05/Dec/16 04:52,14/Jul/23 06:29,30/Nov/16 21:36,2.0.1,,,,,,,,2.2.0,,,,Spark Core,,,,,,,,0,,,,,,"I am running into a runtime exception when a DataSet is holding an Empty object instance for an Option type that is holding non-nullable field. For instance, if we have the following case class:

case class DataRow(id: Int, value: String)

Then, DataSet[Option[DataRow]] can only hold Some(DataRow) objects and cannot hold Empty. If it does so, the following exception is thrown:

{noformat}
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 0.0 failed 1 times, most recent failure: Lost task 6.0 in stage 0.0 (TID 6, localhost): java.lang.RuntimeException: Null value appeared in non-nullable field:
- field (class: ""scala.Int"", name: ""id"")
- option value class: ""DataSetOptBug.DataRow""
- root class: ""scala.Option""
If the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

The bug can be reproduce by using the program: https://gist.github.com/aniketbhatnagar/2ed74613f70d2defe999c18afaa4816e",OS X,aniket,apachespark,cloud_fan,hvanhovell,jayadevan.m,koert,lian cheng,uncleGen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 05 04:52:55 UTC 2016,,,,,,,,,,"0|i35sh3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/16 15:26;jayadevan.m;Hi [~aniket]

I tried to run your code. I am able to run successfully

data.map(i => if (i < 500) { Some(DataRow(i.toInt, s""i$i"")) } else { Option.empty }).filter(_.isEmpty).count().

I don't think .map(_.get) is required. Just count will give the result.

;;;","11/Nov/16 23:22;aniket;Hi [~jayadevan.m]

Which version of scala and spark did you use? I can reproduce this on spark 2.0.1 and scala 2.11.8. I have created a sample project with all the dependencies to easily reproduce this:
https://github.com/aniketbhatnagar/SPARK-18251-data-set-option-bug
To reproduce the bug, simple checkout the project and run the command sbt run.

Thanks,
Aniket;;;","16/Nov/16 16:19;hvanhovell;I cannot reproduce this on master or branch 2.1. This is still an issue on 2.0, see: https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2728434780191932/4444281805930326/6987336228780374/latest.html

[~cloud_fan] Any idea which patch fixed this?;;;","17/Nov/16 13:21;cloud_fan;This is a tricky problem.

First, using `Option[T]` as the type of Dataset is not well supported, you can try `Seq(Some(1 -> ""a""), None).toDS.collect`, it will return `Array(Some(1 -> a), Some(0 -> null))` instead of `Array(Some((1,a)), None)`. The reason is, we explicitly forbid users to use null as top-level non-flat objects, but we forget to handle top level None. We should create a ticket for it.

In Spark 2.0, typed filter is not optimized well, so the example code will first deserialize row to object, then apply the map function, then serialize the mapped object to row, then the filter operator will deserialize the row to object, and apply the filter function. In Spark 2.1, typed filter will be pushed down through `SerializeFromObject`, which means the process get optimized to: deserialize row to object, then apply the map function, then apply the filter function, then serialize object to row. So this bug is kind of hidden in Spark 2.1(the null check exception is thrown during serialization)

We have 2 choices here:
1. forbid top-level None, as well as null.
2. treat `Option` like a normal product, which is a single field struct.

Any ideas? cc [~yhuai] [~lian cheng] [~marmbrus];;;","18/Nov/16 18:36;lian cheng;I'd prefer option 1 because of consistency of the semantics, and I don't think this is really a bug since {{Option\[T\]}} shouldn't be used as top level {{Dataset}} types anyway.

While doing schema inference, Catalyst always translates {{Option\[T\]}} to the nullable version of {{T'}}, where {{T'}} is the inferred data type of {{T}}. Take {{case class A(i: Option\[Int\])}} as an example, if we go for option 2, then what should the inferred schema of {{A}} be? To keep the original semantics, it should be
{noformat}
new StructType()
  .add(""i"", IntegerType, nullable = true)
{noformat}
while option 2 requires
{noformat}
new StructType()
  .add(""i"", new StructType()
    .add(""value"", IntegerType, nullable = true), nullable = true)
{noformat}
since now {{Option\[T\]}} is treated as a single field struct.

Option 1 keeps the current semantics, which is pretty clear and easy to reason about, while option 2 either introduces inconsistency or requires us to further special case schema inference for top level {{Dataset}} types.
;;;","22/Nov/16 14:17;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/15979;;;","30/Nov/16 21:36;lian cheng;Issue resolved by pull request 15979
[https://github.com/apache/spark/pull/15979];;;","30/Nov/16 21:41;lian cheng;One more comment about why we shouldn't allow a {{Option\[T <: Product\]}} to be used as top-level Dataset type: one way to think about this more intuitively is to make an analogy to databases. In a database table, you cannot mark a row itself as null. Instead, you are only allowed to mark a field of a row to be null.

Instead of using {{Option\[T <: Product\]}}, the user should resort to {{Tuple1\[T <: Product\]}}. Thus, you have a row consisting of a single field, which can be filled with either a null or a struct.;;;","05/Dec/16 04:52;koert;i think all of these arguments are very valid, however to me they argue for a constraint on the top-level type of the Dataset, not the Encoder. the difference matters because Encoders are also used for types that are not the top-level type of the Dataset, such as the key type in groupByKey, or the buffer in an Aggregator.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hive.exec.stagingdir have no effect in spark2.0.1,SPARK-18237,13017478,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ClassNotFoundExp,ClassNotFoundExp,ClassNotFoundExp,03/Nov/16 02:06,26/Dec/16 07:01,14/Jul/23 06:29,03/Nov/16 19:01,2.0.1,,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,,,,,,"hive.exec.stagingdir have no effect in spark2.0.1,
this relevant to https://issues.apache.org/jira/browse/SPARK-11021",,apachespark,ClassNotFoundExp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-11021,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 26 01:18:04 UTC 2016,,,,,,,,,,"0|i35rsn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/16 02:42;apachespark;User 'ClassNotFoundExp' has created a pull request for this issue:
https://github.com/apache/spark/pull/15744;;;","26/Dec/16 01:18;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16399;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassCastException occurs when using select query on ORC file,SPARK-18220,13017115,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,cloud_fan,jerryjung,jerryjung,02/Nov/16 07:56,30/Nov/16 17:47,14/Jul/23 06:29,30/Nov/16 17:47,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,orcfile,sql,,,,"Error message is below.
{noformat}
==========================================================
16/11/02 16:38:09 INFO ReaderImpl: Reading ORC rows from hdfs://xxx/part-00022 with {include: [true], offset: 0, length: 9223372036854775807}
16/11/02 16:38:09 INFO Executor: Finished task 17.0 in stage 22.0 (TID 42). 1220 bytes result sent to driver
16/11/02 16:38:09 INFO TaskSetManager: Finished task 17.0 in stage 22.0 (TID 42) in 116 ms on localhost (executor driver) (19/20)
16/11/02 16:38:09 ERROR Executor: Exception in task 10.0 in stage 22.0 (TID 35)
java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.HiveVarcharWritable cannot be cast to org.apache.hadoop.io.Text
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)
	at org.apache.spark.sql.hive.HiveInspectors$$anonfun$unwrapperFor$23.apply(HiveInspectors.scala:526)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$14$$anonfun$apply$15.apply(TableReader.scala:419)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$14$$anonfun$apply$15.apply(TableReader.scala:419)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:435)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:426)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:232)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:804)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:804)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

ORC dump info.
==========================================================
File Version: 0.12 with HIVE_8732
16/11/02 16:39:21 INFO orc.ReaderImpl: Reading ORC rows from hdfs://XXX/part-00000 with {include: null, offset: 0, length: 9223372036854775807}
16/11/02 16:39:21 INFO orc.RecordReaderFactory: Schema is not specified on read. Using file schema.
Rows: 7
Compression: ZLIB
Compression size: 262144
Type: struct<a:varchar(2),b(50),c:varchar(6),d:varchar(50),e:varchar(4),f:varchar(50),g:timestamp>
{noformat}",,apachespark,cloud_fan,hvanhovell,jerryjung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 29 13:42:06 UTC 2016,,,,,,,,,,"0|i35pjz:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"10/Nov/16 06:59;cloud_fan;Is it an external orc file or written by Spark SQL? ;;;","11/Nov/16 09:32;jerryjung;It is written by Spark SQL. ;;;","15/Nov/16 00:51;cloud_fan;do you have the code snippet to reproduce this bug? i.e. how you write and read the orc file;;;","25/Nov/16 19:32;hvanhovell;I tried reproducing this but to no avail. [~jerryjung] could you give us a reproducible example?;;;","27/Nov/16 00:46;cloud_fan;I'm closing this ticket, please reopen it if there is a reproducible example;;;","28/Nov/16 06:42;jerryjung;[~cloud_fan]
For further information, tables were created in spark 1.X version.
The original data's orc dump is below.
===========================================================================================================
File Version: 0.12 with HIVE_8732
16/11/28 15:11:51 INFO orc.ReaderImpl: Reading ORC rows from hdfs://xxx:8020/xxx/part-00000 with {include: null, offset: 0, length: 9223372036854775807}
16/11/28 15:11:51 INFO orc.RecordReaderFactory: Schema is not specified on read. Using file schema.
Rows: 7
Compression: ZLIB
Compression size: 262144
Type: struct<aaa:varchar(2),bbb:varchar(50),ccc:varchar(6),ddd:varchar(50),eee:timestamp>

Log message is below.
===========================================================================================================
16/11/28 15:35:31 INFO CatalystSqlParser: Parsing command: varchar(6)
16/11/28 15:35:31 INFO CatalystSqlParser: Parsing command: varchar(50)
16/11/28 15:35:31 INFO CatalystSqlParser: Parsing command: varchar(4)
16/11/28 15:35:31 INFO CatalystSqlParser: Parsing command: varchar(50)
16/11/28 15:35:31 INFO CatalystSqlParser: Parsing command: timestamp
16/11/28 15:35:31 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 258.9 KB, free 398.7 MB)
16/11/28 15:35:31 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.1 KB, free 398.7 MB)
16/11/28 15:35:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 80.80.11.97:47902 (size: 23.1 KB, free: 399.0 MB)
16/11/28 15:35:31 INFO SparkContext: Created broadcast 2 from processCmd at CliDriver.java:376
16/11/28 15:35:31 INFO PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
16/11/28 15:35:31 INFO OrcInputFormat: FooterCacheHitRatio: 0/0
16/11/28 15:35:31 INFO PerfLogger: </PERFLOG method=OrcGetSplits start=1480314931501 end=1480314931511 duration=10 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
16/11/28 15:35:31 INFO SparkContext: Starting job: processCmd at CliDriver.java:376
16/11/28 15:35:31 INFO DAGScheduler: Got job 1 (processCmd at CliDriver.java:376) with 2 output partitions
16/11/28 15:35:31 INFO DAGScheduler: Final stage: ResultStage 1 (processCmd at CliDriver.java:376)
16/11/28 15:35:31 INFO DAGScheduler: Parents of final stage: List()
16/11/28 15:35:31 INFO DAGScheduler: Missing parents: List()
16/11/28 15:35:31 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at processCmd at CliDriver.java:376), which has no missing parents
16/11/28 15:35:31 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.5 KB, free 398.7 MB)
16/11/28 15:35:31 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.5 KB, free 398.7 MB)
16/11/28 15:35:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 80.80.11.97:47902 (size: 4.5 KB, free: 399.0 MB)
16/11/28 15:35:31 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
16/11/28 15:35:31 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at processCmd at CliDriver.java:376)
16/11/28 15:35:31 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
16/11/28 15:35:31 INFO FairSchedulableBuilder: Added task set TaskSet_1.0 tasks to pool default
16/11/28 15:35:31 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 5984 bytes)
16/11/28 15:35:31 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 5984 bytes)
16/11/28 15:35:31 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)
16/11/28 15:35:31 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)
....
16/11/28 15:35:31 INFO OrcRawRecordMerger: min key = null, max key = null
16/11/28 15:35:31 INFO OrcRawRecordMerger: min key = null, max key = null
16/11/28 15:35:31 INFO ReaderImpl: Reading ORC rows from 
...
16/11/28 15:35:31 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 2)
java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.HiveVarcharWritable cannot be cast to org.apache.hadoop.io.Text
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)
	at org.apache.spark.sql.hive.HiveInspectors$$anonfun$unwrapperFor$23.apply(HiveInspectors.scala:529)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$14$$anonfun$apply$15.apply(TableReader.scala:419)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$14$$anonfun$apply$15.apply(TableReader.scala:419)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:435)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:426)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:232)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:819)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:819)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

I created a separate table by querying an existing table in hive for testing purposes.
The table is normally queried by spark-sql.
The orc dump information is as follows:
===========================================================================================================
Type: struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:timestamp>

Log message is below.
===========================================================================================================
16/11/28 15:37:51 INFO CatalystSqlParser: Parsing command: string
16/11/28 15:37:51 INFO CatalystSqlParser: Parsing command: string
16/11/28 15:37:51 INFO CatalystSqlParser: Parsing command: string
16/11/28 15:37:51 INFO CatalystSqlParser: Parsing command: string
16/11/28 15:37:51 INFO CatalystSqlParser: Parsing command: timestamp
16/11/28 15:37:51 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 258.9 KB, free 398.5 MB)
16/11/28 15:37:51 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 23.1 KB, free 398.4 MB)
16/11/28 15:37:51 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 80.80.11.97:47902 (size: 23.1 KB, free: 399.0 MB)
16/11/28 15:37:51 INFO SparkContext: Created broadcast 4 from processCmd at CliDriver.java:376
16/11/28 15:37:51 INFO PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
16/11/28 15:37:51 INFO OrcInputFormat: FooterCacheHitRatio: 0/0
16/11/28 15:37:51 INFO PerfLogger: </PERFLOG method=OrcGetSplits start=1480315071479 end=1480315071489 duration=10 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
16/11/28 15:37:51 INFO SparkContext: Starting job: processCmd at CliDriver.java:376
16/11/28 15:37:51 INFO DAGScheduler: Got job 2 (processCmd at CliDriver.java:376) with 2 output partitions
16/11/28 15:37:51 INFO DAGScheduler: Final stage: ResultStage 2 (processCmd at CliDriver.java:376)
16/11/28 15:37:51 INFO DAGScheduler: Parents of final stage: List()
16/11/28 15:37:51 INFO DAGScheduler: Missing parents: List()
16/11/28 15:37:51 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[14] at processCmd at CliDriver.java:376), which has no missing parents
16/11/28 15:37:51 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 8.4 KB, free 398.4 MB)
16/11/28 15:37:51 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.5 KB, free 398.4 MB)
16/11/28 15:37:51 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 80.80.11.97:47902 (size: 4.5 KB, free: 398.9 MB)
16/11/28 15:37:51 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:996
16/11/28 15:37:51 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at processCmd at CliDriver.java:376)
16/11/28 15:37:51 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
16/11/28 15:37:51 INFO FairSchedulableBuilder: Added task set TaskSet_2.0 tasks to pool default
16/11/28 15:37:51 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 5941 bytes)
16/11/28 15:37:51 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, ANY, 5941 bytes)
16/11/28 15:37:51 INFO Executor: Running task 0.0 in stage 2.0 (TID 4)
16/11/28 15:37:51 INFO Executor: Running task 1.0 in stage 2.0 (TID 5)
...
16/11/28 15:37:51 INFO OrcRawRecordMerger: min key = null, max key = null
16/11/28 15:37:51 INFO ReaderImpl: Reading ORC rows from 
...
16/11/28 15:37:51 INFO Executor: Finished task 0.0 in stage 2.0 (TID 4). 1563 bytes result sent to driver
16/11/28 15:37:51 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 39 ms on localhost (executor driver) (1/2)
16/11/28 15:37:51 INFO Executor: Finished task 1.0 in stage 2.0 (TID 5). 1464 bytes result sent to driver
16/11/28 15:37:51 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 39 ms on localhost (executor driver) (2/2)
16/11/28 15:37:51 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool default
16/11/28 15:37:51 INFO DAGScheduler: ResultStage 2 (processCmd at CliDriver.java:376) finished in 0.044 s
16/11/28 15:37:51 INFO DAGScheduler: Job 2 finished: processCmd at CliDriver.java:376, took 0.057510 s

The only difference between the two tables in my opinion is a struct.
Is there anything else I should check?;;;","28/Nov/16 13:58;cloud_fan;> I created a separate table by querying an existing table in hive for testing purposes.

can you provide the SQL statement that created this table? and it would be good if you can run `DESC TABLE` for the hive table and post the results. Thanks!;;;","28/Nov/16 14:38;jerryjung;CREATE EXTERNAL TABLE `d_c`.`dcoc_ircs_op_brch`(`ircs_op_brch_cd` string, `ircs_op_brch_nm` string, `cms_brch_cd` string, `cms_brch_nm` string, `etl_job_dtm` timestamp)
ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'
WITH SERDEPROPERTIES (
  'serialization.format' = '1'
)
STORED AS
  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'
LOCATION 'hdfs://xxx:8020/xxx'
TBLPROPERTIES (
  'rawDataSize' = '2426',
  'numFiles' = '0',
  'transient_lastDdlTime' = '1480313167',
  'totalSize' = '0',
  'COLUMN_STATS_ACCURATE' = 'true',
  'numRows' = '6'
)

;;;","29/Nov/16 01:49;cloud_fan;The orc file is written by Hive, not by Spark SQL, can you use `CREATE TABLE ... AS SELECT ... FROM hive_table` to make Spark SQL to write out the orc file and try again?;;;","29/Nov/16 01:51;cloud_fan;BTW, the table created by Spark 1.X, are you able to read it with Spark 1.X?;;;","29/Nov/16 02:12;jerryjung;Same error occurred!
spark-sql> CREATE  TABLE zz  as select * from d_c.dcoc_ircs_op_brch;
16/11/29 11:09:28 INFO SparkSqlParser: Parsing command: CREATE  TABLE zz  as select * from d_c.dcoc_ircs_op_brch
16/11/29 11:09:28 INFO HiveMetaStore: 0: get_database: d_c
16/11/29 11:09:28 INFO audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_database: d_c
16/11/29 11:09:28 INFO HiveMetaStore: 0: get_table : db=d_c tbl=dcoc_ircs_op_brch
16/11/29 11:09:28 INFO audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_table : db=d_c tbl=dcoc_ircs_op_brch
16/11/29 11:09:28 INFO HiveMetaStore: 0: get_table : db=d_c tbl=dcoc_ircs_op_brch
16/11/29 11:09:28 INFO audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_table : db=d_c tbl=dcoc_ircs_op_brch
16/11/29 11:09:28 INFO CatalystSqlParser: Parsing command: varchar(6)
16/11/29 11:09:28 INFO CatalystSqlParser: Parsing command: varchar(50)
16/11/29 11:09:28 INFO CatalystSqlParser: Parsing command: varchar(4)
16/11/29 11:09:28 INFO CatalystSqlParser: Parsing command: varchar(50)
16/11/29 11:09:28 INFO CatalystSqlParser: Parsing command: timestamp
16/11/29 11:09:30 INFO HiveMetaStore: 0: get_table : db=default tbl=zz
16/11/29 11:09:30 INFO audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_table : db=default tbl=zz
16/11/29 11:09:30 INFO HiveMetaStore: 0: get_database: default
16/11/29 11:09:30 INFO audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_database: default
16/11/29 11:09:30 INFO HiveMetaStore: 0: get_database: default
16/11/29 11:09:30 INFO audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_database: default
16/11/29 11:09:30 INFO HiveMetaStore: 0: get_table : db=default tbl=zz
16/11/29 11:09:30 INFO audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_table : db=default tbl=zz
16/11/29 11:09:30 INFO HiveMetaStore: 0: get_database: default
16/11/29 11:09:30 INFO audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_database: default
16/11/29 11:09:30 INFO HiveMetaStore: 0: create_table: Table(tableName:zz, dbName:default, owner:hadoop, createTime:1480385368, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:ircs_op_brch_cd, type:string, comment:null), FieldSchema(name:ircs_op_brch_nm, type:string, comment:null), FieldSchema(name:cms_brch_cd, type:string, comment:null), FieldSchema(name:cms_brch_nm, type:string, comment:null), FieldSchema(name:etl_job_dtm, type:timestamp, comment:null)], location:hdfs://xxx/user/hive/warehouse/zz, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={""type"":""struct"",""fields"":[{""name"":""ircs_op_brch_cd"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""ircs_op_brch_nm"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""cms_brch_cd"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""cms_brch_nm"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""etl_job_dtm"",""type"":""timestamp"",""nullable"":true,""metadata"":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=hive}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
... parameters:{spark.sql.sources.schema.part.0={""type"":""struct"",""fields"":[{""name"":""ircs_op_brch_cd"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""ircs_op_brch_nm"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""cms_brch_cd"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""cms_brch_nm"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""etl_job_dtm"",""type"":""timestamp"",""nullable"":true,""metadata"":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=hive}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
...
16/11/29 11:09:30 INFO FileUtils: Creating directory if it doesn't exist: hdfs://xxx/user/hive/warehouse/zz
16/11/29 11:09:31 INFO HiveMetaStore: 0: get_table : db=default tbl=zz
16/11/29 11:09:31 INFO audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_table : db=default tbl=zz
16/11/29 11:09:31 INFO CatalystSqlParser: Parsing command: string
16/11/29 11:09:31 INFO CatalystSqlParser: Parsing command: string
16/11/29 11:09:31 INFO CatalystSqlParser: Parsing command: string
16/11/29 11:09:31 INFO CatalystSqlParser: Parsing command: string
16/11/29 11:09:31 INFO CatalystSqlParser: Parsing command: timestamp
16/11/29 11:09:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 258.9 KB, free 398.7 MB)
16/11/29 11:09:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.1 KB, free 398.7 MB)
16/11/29 11:09:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 80.80.11.97:39889 (size: 23.1 KB, free: 399.0 MB)
16/11/29 11:09:31 INFO SparkContext: Created broadcast 0 from processCmd at CliDriver.java:376
16/11/29 11:09:31 INFO FileUtils: Creating directory if it doesn't exist: hdfs://xxx/user/hive/warehouse/zz/.hive-staging_hive_2016-11-29_11-09-31_532_3872774957419759303-1
16/11/29 11:09:31 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
16/11/29 11:09:31 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
16/11/29 11:09:31 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
16/11/29 11:09:31 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
16/11/29 11:09:31 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
16/11/29 11:09:31 INFO PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
16/11/29 11:09:31 INFO deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir
16/11/29 11:09:31 INFO OrcInputFormat: FooterCacheHitRatio: 0/0
16/11/29 11:09:31 INFO PerfLogger: </PERFLOG method=OrcGetSplits start=1480385371818 end=1480385371865 duration=47 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
16/11/29 11:09:31 INFO SparkContext: Starting job: processCmd at CliDriver.java:376
16/11/29 11:09:31 INFO DAGScheduler: Got job 0 (processCmd at CliDriver.java:376) with 2 output partitions
16/11/29 11:09:31 INFO DAGScheduler: Final stage: ResultStage 0 (processCmd at CliDriver.java:376)
16/11/29 11:09:31 INFO DAGScheduler: Parents of final stage: List()
16/11/29 11:09:31 INFO DAGScheduler: Missing parents: List()
16/11/29 11:09:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at processCmd at CliDriver.java:376), which has no missing parents
16/11/29 11:09:32 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 75.8 KB, free 398.7 MB)
16/11/29 11:09:32 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 29.7 KB, free 398.6 MB)
16/11/29 11:09:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 80.80.11.97:39889 (size: 29.7 KB, free: 398.9 MB)
16/11/29 11:09:32 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:996
16/11/29 11:09:32 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at processCmd at CliDriver.java:376)
16/11/29 11:09:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/11/29 11:09:32 INFO FairSchedulableBuilder: Added task set TaskSet_0.0 tasks to pool default
16/11/29 11:09:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5987 bytes)
16/11/29 11:09:32 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5987 bytes)
16/11/29 11:09:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/11/29 11:09:32 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/11/29 11:09:32 INFO HadoopRDD: Input split: hdfs://xxx/edw/warehouse/dw/db=d_c/tb=dcoc_ircs_op_brch/part-00000:0+815
16/11/29 11:09:32 INFO HadoopRDD: Input split: hdfs://xxx/edw/warehouse/dw/db=d_c/tb=dcoc_ircs_op_brch/part-00001:0+749
16/11/29 11:09:32 INFO OrcRawRecordMerger: min key = null, max key = null
16/11/29 11:09:32 INFO OrcRawRecordMerger: min key = null, max key = null
16/11/29 11:09:32 INFO ReaderImpl: Reading ORC rows from hdfs://xxx/edw/warehouse/dw/db=d_c/tb=dcoc_ircs_op_brch/part-00001 with {include: [true, true, true, true, true, true], offset: 0, length: 9223372036854775807}
16/11/29 11:09:32 INFO ReaderImpl: Reading ORC rows from hdfs://xxx/edw/warehouse/dw/db=d_c/tb=dcoc_ircs_op_brch/part-00000 with {include: [true, true, true, true, true, true], offset: 0, length: 9223372036854775807}
16/11/29 11:09:32 INFO CodeGenerator: Code generated in 337.683679 ms
16/11/29 11:09:32 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.HiveVarcharWritable cannot be cast to org.apache.hadoop.io.Text
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)
	at org.apache.spark.sql.hive.HiveInspectors$$anonfun$unwrapperFor$23.apply(HiveInspectors.scala:529)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$14$$anonfun$apply$15.apply(TableReader.scala:419)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$14$$anonfun$apply$15.apply(TableReader.scala:419)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:435)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:426)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.sql.hive.SparkHiveWriterContainer.writeToFile(hiveWriterContainers.scala:185)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:151)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:151)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
16/11/29 11:09:32 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.HiveVarcharWritable cannot be cast to org.apache.hadoop.io.Text
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)
	at org.apache.spark.sql.hive.HiveInspectors$$anonfun$unwrapperFor$23.apply(HiveInspectors.scala:529)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$14$$anonfun$apply$15.apply(TableReader.scala:419)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$14$$anonfun$apply$15.apply(TableReader.scala:419)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:435)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:426)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.sql.hive.SparkHiveWriterContainer.writeToFile(hiveWriterContainers.scala:185)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:151)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:151)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
16/11/29 11:09:32 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.HiveVarcharWritable cannot be cast to org.apache.hadoop.io.Text
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)
	at org.apache.spark.sql.hive.HiveInspectors$$anonfun$unwrapperFor$23.apply(HiveInspectors.scala:529)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$14$$anonfun$apply$15.apply(TableReader.scala:419)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$14$$anonfun$apply$15.apply(TableReader.scala:419)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:435)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:426)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.sql.hive.SparkHiveWriterContainer.writeToFile(hiveWriterContainers.scala:185)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:151)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:151)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745);;;","29/Nov/16 02:18;jerryjung;Sure, It was working in Spark 1.X.
Even the 2.0.3 version works fine.
As mentioned above, errors only occur in version 2.1.0.;;;","29/Nov/16 13:42;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16060;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: org.apache.spark.sql.kafka010.KafkaSourceSuite.assign from specific offsets,SPARK-18212,13017029,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,koeninger,davies,davies,01/Nov/16 22:44,03/Nov/16 21:44,14/Jul/23 06:29,03/Nov/16 21:43,,,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.3/1968/testReport/junit/org.apache.spark.sql.kafka010/KafkaSourceSuite/assign_from_specific_offsets/,,apachespark,davies,koeninger,lwlin,marmbrus,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 03 21:43:55 UTC 2016,,,,,,,,,,"0|i35p1b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/16 22:50;davies;cc [~zsxwing];;;","01/Nov/16 23:32;marmbrus;/cc [~cody@koeninger.org];;;","02/Nov/16 05:49;koeninger;So here's a heavily excerpted version of what I see happening in that log:

{code}
16/11/01 14:08:46.593 pool-1-thread-1-ScalaTest-running-KafkaSourceSuite INFO KafkaTestUtils:   Sent 34 to partition 2, offset 3
16/11/01 14:08:46.593 pool-1-thread-1-ScalaTest-running-KafkaSourceSuite INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
16/11/01 14:08:46.596 pool-1-thread-1-ScalaTest-running-KafkaSourceSuite INFO KafkaTestUtils: Created consumer to get latest offsets


16/11/01 14:08:47.833 Executor task launch worker-2 ERROR Executor: Exception in task 1.0 in stage 29.0 (TID 142)
java.lang.AssertionError: assertion failed: Failed to get records for spark-kafka-source-a9485cc4-c83d-4e97-a20e-3960565b3fdb-335403166-execut\
or topic-5-2 3 after polling for 512


16/11/01 14:08:49.252 pool-1-thread-1-ScalaTest-running-KafkaSourceSuite INFO KafkaTestUtils: Closed consumer to get latest offsets
16/11/01 14:08:49.252 pool-1-thread-1-ScalaTest-running-KafkaSourceSuite INFO KafkaSourceSuite: Added data, expected offset [(topic-5-0,4), (topic-5-1,4), (topic-5-2,4), (topic-5-3,4), (topic-5-4,4)]
{code}


We're waiting on the producer's send future for up to 10 seconds; it takes almost 3 seconds between when the producer send finishes and the consumer that's being used to verify the post-send offsets finishes; but in the meantime we're only waiting half a second for executor fetches.

It's really ugly, but probably the easiest way to make this less flaky is to increase the value of kafkaConsumer.pollTimeoutMs to the same order of magnitude being used for the other test waits.

[~zsxwing] unless you see anything else wrong in the log or have a better idea, I can put in a pr tomorrow to increase that poll timeout in tests.
;;;","02/Nov/16 08:16;marmbrus;+1 to upping the timeout. We run with {{.option(""kafkaConsumer.pollTimeoutMs"", 10 * 1000)}};;;","02/Nov/16 15:14;apachespark;User 'koeninger' has created a pull request for this issue:
https://github.com/apache/spark/pull/15737;;;","02/Nov/16 21:19;zsxwing;[~cody@koeninger.org] Sounds good to me.;;;","03/Nov/16 21:43;marmbrus;Issue resolved by pull request 15737
[https://github.com/apache/spark/pull/15737];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pipeline.copy does not create an instance with the same UID,SPARK-18210,13017023,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,wojtek-szymanski,wojtek-szymanski,wojtek-szymanski,01/Nov/16 22:28,06/Nov/16 15:48,14/Jul/23 06:29,06/Nov/16 15:48,2.0.1,,,,,,,,2.1.0,,,,ML,,,,,,,,0,,,,,,"org.apache.spark.ml.Pipeline.copy(extra: ParamMap) does not create an instance with the same UID.
It does not conform to the method specification from its base class org.apache.spark.ml.param.Params.copy(extra: ParamMap)

The following commit contains:
- fix for Pipeline UID
- missing tests for Pipeline.copy
- minor improvements in tests for PipelineModel.copy

https://github.com/apache/spark/commit/8764e36f9764f89343a3e0fe6eff05cb41bb36cf

Let me know if you are fine with these changes, so I will open a new PR.",,apachespark,wojtek-szymanski,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 03 20:54:05 UTC 2016,,,,,,,,,,"0|i35ozz:",9223372036854775807,,,,,yanboliang,,,,,,,,,,,,,,,,,,,"03/Nov/16 14:59;yanboliang;This make sense, please feel free to send a PR. Thanks.;;;","03/Nov/16 20:54;apachespark;User 'wojtek-szymanski' has created a pull request for this issue:
https://github.com/apache/spark/pull/15759;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executor OOM due to a memory leak in BytesToBytesMap,SPARK-18208,13017006,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiexiong,jiexiong,jiexiong,01/Nov/16 21:19,08/Apr/22 07:09,14/Jul/23 06:29,07/Dec/16 12:34,2.0.0,,,,,,,,2.0.3,2.1.0,,,Shuffle,Spark Core,,,,,,,0,,,,,,"While running a Spark job, we see that the job fails because of executor OOM with following stack trace -

{noformat}
         java.lang.OutOfMemoryError: No enough memory for aggregation
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:161)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

The code is trying to reuse the BytesToBytesMap after spilling by calling the reset function (see - https://github.com/facebook/FB-Spark/blob/fb-2.0/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java#L897). The reset function is releasing all memory pages, but its not reseting the pointer array. If the pointer array size has grown beyond the fair share, the BytesToBytes map is not being allocated any memory page further and hence the OOM",,apachespark,connectsachit,dongjoon,emlyn,jiexiong,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 08 07:09:28 UTC 2022,,,,,,,,,,"0|i35ow7:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"01/Nov/16 22:58;apachespark;User 'jiexiong' has created a pull request for this issue:
https://github.com/apache/spark/pull/15722;;;","07/Apr/22 17:39;connectsachit;[~jiexiong]  [~dongjoon] : Getting same issue in Spark 2.4.4 . Could you please suggest what could be done? ;;;","08/Apr/22 07:09;dongjoon;[~connectsachit]. Sorry but there is nothing for us to help you in that case because all Apache Spark 3.0- are EOL (End-Of-Life) according to the Apache Spark community versioning policy and release cadence.

- https://spark.apache.org/versioning-policy.html

The general community recommendation is to use the latest Apache Spark version which is currently Apache Spark 3.2.1 . Note that we are going to release Apache Spark 3.3.0 soon (before the end of June).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"class ""org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection"" grows beyond 64 KB",SPARK-18207,13017004,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kiszk,dondrake,dondrake,01/Nov/16 21:15,12/Nov/16 17:31,14/Jul/23 06:29,08/Nov/16 11:05,2.0.1,2.1.0,,,,,,,2.1.0,,,,Optimizer,SQL,,,,,,,0,,,,,,"I have 2 wide dataframes that contain nested data structures, when I explode one of the dataframes, it doesn't include records with an empty nested structure (outer explode not supported).  So, I create a similar dataframe with null values and union them together.  See SPARK-13721 for more details as to why I have to do this.

I was hoping that SPARK-16845 was going to address my issue, but it does not.  I was asked by [~lwlin] to open this JIRA.  

I will attach a code snippet that can be pasted into spark-shell that duplicates my code and the exception.  This worked just fine in Spark 1.6.x.

{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 35 in stage 5.0 failed 4 times, most recent failure: Lost task 35.3 in stage 5.0 (TID 812, somehost.mydomain.com, executor 8): java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: org.codehaus.janino.JaninoRuntimeException: Code of method ""apply(Lorg/apache/spark/sql/catalyst/InternalRow;)Lorg/apache/spark/sql/catalyst/expressions/UnsafeRow;"" of class ""org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection"" grows beyond 64 KB
{code}
",,apachespark,dondrake,kiszk,lwlin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16845,SPARK-13721,,,,,,,,,"01/Nov/16 21:21;dondrake;spark-18207.txt;https://issues.apache.org/jira/secure/attachment/12836440/spark-18207.txt",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 12 17:31:09 UTC 2016,,,,,,,,,,"0|i35ovr:",9223372036854775807,,,,,marmbrus,,,,,,,,,,,,,,,,,,,"01/Nov/16 21:21;dondrake;Please read the comments at the top of the attachment, you need to :paste portions of this into spark-shell.

;;;","02/Nov/16 09:26;srowen;SPARK-16845 is unresolved and this seems to be exactly the same issue. Why open another jIRA?;;;","02/Nov/16 14:33;dondrake;I opened it based on [~lwlin]'s suggestion in the comments of SPARK-16845.  ;;;","02/Nov/16 14:37;srowen;Can you note the difference here? if he's just saying that one fix X doesn't fix the issue I don't know if that means it's a different issue.;;;","02/Nov/16 14:45;dondrake;The difference with my case versus the other test cases is that my scenario involves a wide dataframe (800+ columns) that also have multiple nested structures (arrays of classes) involved in a SQL query (union).  

I have verified that [~lwlin]'s fix does not work for my case, but it does work for wide dataframes without nested structures.

I agree it's similar to the others, but more complicated to reproduce.
;;;","02/Nov/16 16:41;kiszk;I created a smaller program to reproduce this problem, and understand why this error occurs.
This program tries to calculate a hash value for a row that includes 1000 String fields. During a code generation, {{HashExpression.doGenCode}} generates a lot of Java statements to calculate a hash value for the row. The generated code exceeds 64KB. 

I have just created a fix. Can I use this JIRA entry to submit a PR? Or, should I use SPARK-16845?;;;","03/Nov/16 02:47;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/15745;;;","12/Nov/16 17:31;dondrake;Hi, I was able to download a nightly SNAPSHOT release and verify that this resolves the issue for my project.  Thanks to everyone who contributed to this fix and getting it merged in a timely manner.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GraphX Invalid initial capacity when running triangleCount,SPARK-18200,13016926,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dennyglee,dennyglee,01/Nov/16 15:35,04/Dec/16 02:48,14/Jul/23 06:29,03/Nov/16 06:49,2.0.0,2.0.1,2.0.2,,,,,,2.0.3,2.1.0,,,GraphX,,,,,,,,0,graph,graphx,,,,"Running GraphX triangle count on large-ish file results in the ""Invalid initial capacity"" error when running on Spark 2.0 (tested on Spark 2.0, 2.0.1, and 2.0.2).  You can see the results at: http://bit.ly/2eQKWDN

Running the same code on Spark 1.6 and the query completes without any problems: http://bit.ly/2fATO1M

As well, running the GraphFrames version of this code runs as well (Spark 2.0, GraphFrames 0.2): http://bit.ly/2fAS8W8

Reference Stackoverflow question:
Spark GraphX: requirement failed: Invalid initial capacity (http://stackoverflow.com/questions/40337366/spark-graphx-requirement-failed-invalid-initial-capacity)","Databricks, Ubuntu 16.04, macOS Sierra",apachespark,cybertaurean,dennyglee,dongjoon,sathyasrini,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 04 02:48:14 UTC 2016,,,,,,,,,,"0|i35oef:",9223372036854775807,,,,,,,,,,,,,2.0.3,2.1.0,,,,,,,,,,"02/Nov/16 22:50;dongjoon;Hi, [~dennyglee].
It's due to `OpenHashSet`. I'll make a PR for this.;;;","02/Nov/16 22:57;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/15741;;;","02/Nov/16 22:58;dongjoon;Actually, there is a node who doesn't have any neighbors. So, it requested to create `VertexSet` with zero initial capacity.;;;","02/Nov/16 23:19;dongjoon;The described scenario is also tested.
{code}
scala> import org.apache.spark.graphx.{GraphLoader, PartitionStrategy}

scala> val filepath = ""/tmp/ca-HepTh.txt""

scala> val graph = GraphLoader.edgeListFile(sc, filepath, true).partitionBy(PartitionStrategy.RandomVertexCut)

scala> val triCounts = graph.triangleCount().vertices

scala> triCounts.toDF().show()
+-----+---+
|   _1| _2|
+-----+---+
|50130|  2|
|20484| 11|
|10598|190|
|31760| 29|
{code};;;","03/Nov/16 01:54;sathyasrini;Thank you  Dongjoon Hyun and  Denny Lee for taking a very serious consideration to my question in Stack Overflow (http://stackoverflow.com/questions/40337366/spark-graphx-requirement-failed-invalid-initial-capacity).
I am in the process of implementing and testing the proposed solution and the reported answers work fine. Thanks;;;","03/Nov/16 02:15;dongjoon;Good for you. :);;;","03/Nov/16 17:23;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/15754;;;","04/Dec/16 01:37;cybertaurean;Does this issue exist currently in version 2.0.1?. I just ran a test and it's throwing the following exception.

User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 10.0 failed 4 times, most recent failure: Lost task 3.3 in stage 10.0 (TID 196, BD-S2F13): java.lang.IllegalArgumentException: requirement failed: Invalid initial capacity
at scala.Predef$.require(Predef.scala:224)
at org.apache.spark.util.collection.OpenHashSet$mcJ$sp.<init>(OpenHashSet.scala:51)
at org.apache.spark.util.collection.OpenHashSet$mcJ$sp.<init>(OpenHashSet.scala:57)
at org.apache.spark.graphx.lib.TriangleCount$$anonfun$5.apply(TriangleCount.scala:70)
at org.apache.spark.graphx.lib.TriangleCount$$anonfun$5.apply(TriangleCount.scala:69)
at org.apache.spark.graphx.impl.VertexPartitionBaseOps.map(VertexPartitionBaseOps.scala:61)
at org.apache.spark.graphx.impl.VertexRDDImpl$$anonfun$mapValues$2.apply(VertexRDDImpl.scala:102)
at org.apache.spark.graphx.impl.VertexRDDImpl$$anonfun$mapValues$2.apply(VertexRDDImpl.scala:102)
at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
at org.apache.spark.graphx.impl.VertexRDDImpl$$anonfun$3.apply(VertexRDDImpl.scala:156)
at org.apache.spark.graphx.impl.VertexRDDImpl$$anonfun$3.apply(VertexRDDImpl.scala:154)
at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332)
at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330)
at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935)
at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926)
at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)
at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926)
at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670)
at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:281)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
at org.apache.spark.scheduler.Task.run(Task.scala:86)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745);;;","04/Dec/16 02:12;dongjoon;Hi,
Yes, the bugs are there in 2.0.1.
The fix will be in upcoming Apache Spark 2.0.3 and 2.1.0.
We cannot backport into 2.0.1 because it's already released.;;;","04/Dec/16 02:48;cybertaurean;Thanks much [~dongjoon];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
task not serializable with groupByKey() + mapGroups() + map,SPARK-18189,13016769,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eseyfe,yangyangyyy,yangyangyyy,31/Oct/16 23:56,04/Nov/16 21:54,14/Jul/23 06:29,01/Nov/16 18:18,,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,,,,,,"just run the following code 

{code}
val a = spark.createDataFrame(sc.parallelize(Seq((1,2),(3,4)))).as[(Int,Int)]
val grouped = a.groupByKey({x:(Int,Int)=>x._1})
val mappedGroups = grouped.mapGroups((k,x)=>{(k,1)})
val yyy = sc.broadcast(1)
val last = mappedGroups.rdd.map(xx=>{
  val simpley = yyy.value
  
  1
})

{code}



spark says Task not serializable



",,apachespark,yangyangyyy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 04 21:54:05 UTC 2016,,,,,,,,,,"0|i35nfj:",9223372036854775807,,,,,,,,,,,,,2.0.3,2.1.0,,,,,,,,,,"01/Nov/16 00:12;apachespark;User 'seyfe' has created a pull request for this issue:
https://github.com/apache/spark/pull/15706;;;","04/Nov/16 21:54;apachespark;User 'seyfe' has created a pull request for this issue:
https://github.com/apache/spark/pull/15774;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CompactibleFileStreamLog should not rely on ""compactInterval"" to detect a compaction batch",SPARK-18187,13016761,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tcondie,zsxwing,zsxwing,31/Oct/16 23:30,18/Nov/16 19:12,14/Jul/23 06:29,18/Nov/16 19:12,2.0.1,,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,"Right now CompactibleFileStreamLog uses compactInterval to check if a batch is a compaction batch. However, since this conf is controlled by the user, they may just change it, and CompactibleFileStreamLog will just silently return the wrong answer.",,apachespark,lwlin,marmbrus,tcondie,uncleGen,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 18 19:12:53 UTC 2016,,,,,,,,,,"0|i35ndr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/16 03:59;lwlin;hi [~zsxwing] how are we planning to fix this? thanks;;;","01/Nov/16 23:53;zsxwing;[~lwlin] I don't have a sold idea. Feel free to propose your thoughts!;;;","02/Nov/16 00:06;marmbrus;I think the configuration should only be used when deciding if we should perform a new compaction.  The identification of a compaction vs a delta should be done based on the file itself.  Today this could be done by looking for the {{compact}} suffix.  However, I think this mechanism also has issues, as two streams writing to the same log but with different configurations would fail to conflict.

That said, I think fixing the latter issue is going to require us rev-ing the log version.  Since thats not free, we would probably want to see if there are other changes we should lump into the new version.  Given that, I'd be okay keeping the existing format, looking at file names instead of modular arithmetic, and revisiting moving the compaction identifier into the log itself (rather than the filename) in a follow up.;;;","08/Nov/16 07:41;uncleGen;[~marmbrus] +1 to your 

+“I think the configuration should only be used when deciding if we should perform a new compaction. The identification of a compaction vs a delta should be done based on the file itself.“+

h4. how to set ""compactInterval""?

{{compactInterval}} can be set by user in the first time. In case it is changed by user, we should check-update and use it. If {{isDeletingExpiredLog=false}}, we can get original {{compactInterval}} by computing the interval of {{.compact}} suffix, and then check it against user setting; if {{isDeletingExpiredLog=true}}, we can just use user setting , because this no expired meta log.
;;;","09/Nov/16 08:54;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15827;;;","09/Nov/16 16:27;apachespark;User 'lw-lin' has created a pull request for this issue:
https://github.com/apache/spark/pull/15828;;;","11/Nov/16 21:52;apachespark;User 'tcondie' has created a pull request for this issue:
https://github.com/apache/spark/pull/15852;;;","14/Nov/16 22:28;tcondie;Hi [~zsxwing], I have added some new logic to the the determination of a compact interval in [CompactibleFileStreamLog.scala]. This new logic is described in the comments. Let me know what you think.;;;","18/Nov/16 19:12;zsxwing;Resolved by https://github.com/apache/spark/pull/15852;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AnalysisException in first/last during aggregation,SPARK-18172,13016421,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,windpiger,emlyn,emlyn,30/Oct/16 10:06,19/Nov/16 09:03,14/Jul/23 06:29,16/Nov/16 19:27,2.0.1,,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,,,,,,"Since Spark 2.0.1, the following pyspark snippet fails with {{AnalysisException: The second argument of First should be a boolean literal}} (but it's not restricted to Python, similar code with in Java fails in the same way).
It worked in Spark 2.0.0, so I believe it may be related to the fix for SPARK-16648.
{code}
from pyspark.sql import functions as F
ds = spark.createDataFrame(sc.parallelize([[1, 1, 2], [1, 2, 3], [1, 3, 4]]))
ds.groupBy(ds._1).agg(F.first(ds._2), F.countDistinct(ds._2), F.countDistinct(ds._2, ds._3)).show()
{code}
It works if any of the three arguments to {{.agg}} is removed.

The stack trace is:
{code}
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-3-73596fd1f689> in <module>()
----> 1 ds.groupBy(ds._1).agg(F.first(ds._2),F.countDistinct(ds._2),F.countDistinct(ds._2, ds._3)).show()

/usr/local/Cellar/apache-spark/2.0.1/libexec/python/pyspark/sql/dataframe.py in show(self, n, truncate)
    285         +---+-----+
    286         """"""
--> 287         print(self._jdf.showString(n, truncate))
    288
    289     def __repr__(self):

/usr/local/Cellar/apache-spark/2.0.1/libexec/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1131         answer = self.gateway_client.send_command(command)
   1132         return_value = get_return_value(
-> 1133             answer, self.gateway_client, self.target_id, self.name)
   1134
   1135         for temp_arg in temp_args:

/usr/local/Cellar/apache-spark/2.0.1/libexec/python/pyspark/sql/utils.py in deco(*a, **kw)
     61     def deco(*a, **kw):
     62         try:
---> 63             return f(*a, **kw)
     64         except py4j.protocol.Py4JJavaError as e:
     65             s = e.java_exception.toString()

/usr/local/Cellar/apache-spark/2.0.1/libexec/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    317                 raise Py4JJavaError(
    318                     ""An error occurred while calling {0}{1}{2}.\n"".
--> 319                     format(target_id, ""."", name), value)
    320             else:
    321                 raise Py4JError(

Py4JJavaError: An error occurred while calling o76.showString.
: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: makeCopy, tree: first(_2#1L)()
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.makeCopy(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren(TreeNode.scala:256)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$.org$apache$spark$sql$catalyst$optimizer$RewriteDistinctAggregates$$patchAggregateFunctionChildren$1(RewriteDistinctAggregates.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$$anonfun$16.apply(RewriteDistinctAggregates.scala:182)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$$anonfun$16.apply(RewriteDistinctAggregates.scala:180)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$.rewrite(RewriteDistinctAggregates.scala:180)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$$anonfun$apply$1.applyOrElse(RewriteDistinctAggregates.scala:105)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$$anonfun$apply$1.applyOrElse(RewriteDistinctAggregates.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:301)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:301)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$.apply(RewriteDistinctAggregates.scala:104)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$.apply(RewriteDistinctAggregates.scala:102)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
	at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)
	at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:83)
	at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2572)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:1934)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2149)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1$$anonfun$apply$13.apply(TreeNode.scala:413)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1$$anonfun$apply$13.apply(TreeNode.scala:413)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1.apply(TreeNode.scala:412)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1.apply(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	... 62 more
Caused by: org.apache.spark.sql.AnalysisException: The second argument of First should be a boolean literal.;
	at org.apache.spark.sql.catalyst.expressions.aggregate.First.<init>(First.scala:43)
	... 72 more
{code}",,cloud_fan,codingcat,emlyn,hvanhovell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 17 15:02:33 UTC 2016,,,,,,,,,,"0|i35laf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Nov/16 14:41;emlyn;I can also reproduce it in Spark SQL with the following query:
{code:sql}
create table if not exists tmp as select 1 as a, 2 as b, 3 as c;
select first(b), count(distinct b), count(distinct b, c) from tmp group by a;
{code}
As above, removing any of the three expressions from the select allows it to succeed.
While investigating this, I ran into another issue that I've reported separately in SPARK-18300.;;;","16/Nov/16 17:05;hvanhovell;I cannot reproduce this on branch-2.0 or master. I have tried both your examples: https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2728434780191932/708008531802106/6987336228780374/latest.html;;;","16/Nov/16 17:07;hvanhovell;[~emlyn] can you still reproduce this?;;;","16/Nov/16 18:31;emlyn;It occurs on 2.0.1 and 2.0.2 (on Mac, installed via Homebrew), but I'd need more time to compile a more recent version and test that.
Since it was literally just the snippet above that triggered it, I think it's probably fixed if you can't reproduce it.
Maybe it is the same underlying problem as SPARK-18300, and the fix for that fixed this too?
;;;","16/Nov/16 19:27;hvanhovell;This is different from SPARK-18300, this is not a case where foldable propagation is an issue. The thing is that we fixed a lot of these issues recently, so I am not sure which one fixed this one.

I am closing this one, please re-open if you hit this again.;;;","17/Nov/16 13:55;cloud_fan;I believe it's already fixed by https://issues.apache.org/jira/browse/SPARK-18137, and I can pass your test in master branch. Can you double check?;;;","17/Nov/16 15:02;emlyn;I'm not sure I've got the time to build from source at the moment to verify this, I think if it now works for you and [~hvanhovell] it's most likely fixed now. If it reoccurs for me with 2.0.3 or 2.1.0 once they're released I'll reopen this.

Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Suppress warnings when dropping views on a dropped table,SPARK-18169,13016383,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,dongjoon,dongjoon,29/Oct/16 23:08,22/Nov/16 23:35,14/Jul/23 06:29,22/Nov/16 23:33,2.0.0,2.0.1,,,,,,,,,,,SQL,,,,,,,,0,,,,,,"Apache Spark 2.0.0 ~ 2.0.2-rc1 shows an inconsistent *AnalysisException* warning message when dropping a *view* on a dropped table. This does not happen on dropping *temporary views*. Also, Spark 1.6.x does not show warnings. We had better suppress this to be more consistent in Spark 2.x and with Spark 1.6.x.

{code}
scala> sql(""create table t(a int)"")

scala> sql(""create view v as select * from t"")

scala> sql(""create temporary view tv as select * from t"")

scala> sql(""drop table t"")

scala> sql(""drop view tv"")

scala> sql(""drop view v"")
16/10/29 15:50:03 WARN DropTableCommand: org.apache.spark.sql.AnalysisException: Table or view not found: `default`.`t`; line 1 pos 91;
'SubqueryAlias v, `default`.`v`
+- 'Project ['gen_attr_0 AS a#19]
   +- 'SubqueryAlias t
      +- 'Project ['gen_attr_0]
         +- 'SubqueryAlias gen_subquery_0
            +- 'Project ['a AS gen_attr_0#18]
               +- 'UnresolvedRelation `default`.`t`

org.apache.spark.sql.AnalysisException: Table or view not found: `default`.`t`; line 1 pos 91;
'SubqueryAlias v, `default`.`v`
+- 'Project ['gen_attr_0 AS a#19]
   +- 'SubqueryAlias t
      +- 'Project ['gen_attr_0]
         +- 'SubqueryAlias gen_subquery_0
            +- 'Project ['a AS gen_attr_0#18]
               +- 'UnresolvedRelation `default`.`t`
...
res5: org.apache.spark.sql.DataFrame = []
{code}

Note that this is different case of dropping non-exist view. For the non-exist view, Spark raises NoSuchTableException.",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18549,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 22 23:35:29 UTC 2016,,,,,,,,,,"0|i35l1z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/16 23:15;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/15682;;;","22/Nov/16 23:35;dongjoon;This issue is superceded by SPARK-18549 .;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GeneralizedLinearRegression Wrong Value Range for Poisson Distribution  ,SPARK-18166,13016283,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,actuaryzhang,actuaryzhang,actuaryzhang,28/Oct/16 22:01,14/Nov/16 11:14,14/Jul/23 06:29,14/Nov/16 11:14,2.0.0,,,,,,,,2.1.0,,,,ML,,,,,,,,0,,,,,,"The current implementation of Poisson GLM seems to allow only positive values (See below). This is not correct since the support of Poisson includes the origin. 

    override def initialize(y: Double, weight: Double): Double = {
      require(y {color:red} > {color} 0.0, ""The response variable of Poisson family "" +
        s""should be positive, but got $y"")
      y
    }

The fix is easy, just change it to 
 require(y {color:red} >= {color} 0.0, ""The response variable of Poisson family "" +",,actuaryzhang,apachespark,,,,,,,,,,,,,,,,,,,,,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/regression/GeneralizedLinearRegression.scala,,Important,,,,,,,,9223372036854775807,,,Mon Nov 14 11:14:37 UTC 2016,,,,,,,,,,"0|i35kfr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/16 11:45;srowen;Agree, feel free to open a PR to fix that.;;;","30/Oct/16 03:16;apachespark;User 'actuaryzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/15683;;;","14/Nov/16 11:14;srowen;Resolved by https://github.com/apache/spark/pull/15683;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark.files & spark.jars should not be passed to driver in yarn mode,SPARK-18160,13016067,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,zjffdu,zjffdu,zjffdu,28/Oct/16 07:45,02/Nov/16 18:49,14/Jul/23 06:29,02/Nov/16 18:49,1.6.2,2.0.1,,,,,,,2.0.3,2.1.0,,,Spark Core,YARN,,,,,,,0,,,,,,"The following command will fails for spark 2.0
{noformat}
bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-cluster --conf spark.files=/usr/spark-client/conf/hive-site.xml examples/target/original-spark-examples_2.11.jar
{noformat}


The above command can reproduce the error as following in a multiple node cluster. To be noticed, this issue only happens in multiple node cluster. As in the single node cluster, AM use the same local filesystem as the the driver.
{noformat}
16/10/28 07:21:42 ERROR SparkContext: Error initializing SparkContext.
java.io.FileNotFoundException: File file:/usr/spark-client/conf/hive-site.xml does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:537)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:750)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:527)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)
	at org.apache.spark.SparkContext.addFile(SparkContext.scala:1443)
	at org.apache.spark.SparkContext.addFile(SparkContext.scala:1415)
	at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:462)
	at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:462)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:462)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2296)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:843)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:835)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:835)
	at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:31)
	at org.apache.spark.examples.SparkPi.main(SparkPi.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:637)
{noformat}",,apachespark,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 28 07:53:06 UTC 2016,,,,,,,,,,"0|i35j3r:",9223372036854775807,,,,,,,,,,,,,1.6.3,2.0.2,,,,,,,,,,"28/Oct/16 07:53;apachespark;User 'zjffdu' has created a pull request for this issue:
https://github.com/apache/spark/pull/15669;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Misleading Error Message for Aggregation Without Window/GroupBy,SPARK-18148,13016017,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiangxb1987,cheffpj,cheffpj,28/Oct/16 01:43,01/Nov/16 20:25,14/Jul/23 06:29,01/Nov/16 18:25,2.0.0,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,,,,,,"The following error message points to a random column I'm not actually using in my query, making it hard to diagnose.

{code}
org.apache.spark.sql.AnalysisException: expression '`randomColumn`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;
{code}

Note in the code below, I forgot to add {{.over(weeklyWindow)}} in the line for {{withColumn(""user_count""...}}

{code}
spark.read.load(""/some-data"")
  .withColumn(""date_dt"", to_date($""date""))
  .withColumn(""year"", year($""date_dt""))
  .withColumn(""week"", weekofyear($""date_dt""))
  .withColumn(""user_count"", count($""userId""))
  .withColumn(""daily_max_in_week"", max($""user_count"").over(weeklyWindow))
)
{code}

CC: [~marmbrus]",Databricks,apachespark,cheffpj,jiangxb1987,rxin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 28 11:15:04 UTC 2016,,,,,,,,,,"0|i35isn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/16 08:39;jiangxb1987;[~pat.mcdonough@databricks.com] I've reproduced this bug, will submit a PR to resolve it. Thanks!;;;","28/Oct/16 11:15;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/15672;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken Spark SQL Codegen,SPARK-18147,13015989,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,viirya,koert,koert,27/Oct/16 22:21,06/Dec/18 11:21,14/Jul/23 06:29,10/Nov/16 05:05,2.0.1,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"this is me on purpose trying to break spark sql codegen to uncover potential issues, by creating arbitrately complex data structures using primitives, strings, basic collections (map, seq, option), tuples, and case classes.

first example: nested case classes
code:
{noformat}
class ComplexResultAgg[B: TypeTag, C: TypeTag](val zero: B, result: C) extends Aggregator[Row, B, C] {
  override def reduce(b: B, input: Row): B = b

  override def merge(b1: B, b2: B): B = b1

  override def finish(reduction: B): C = result

  override def bufferEncoder: Encoder[B] = ExpressionEncoder[B]()
  override def outputEncoder: Encoder[C] = ExpressionEncoder[C]()
}

case class Struct2(d: Double = 0.0, s1: Seq[Double] = Seq.empty, s2: Seq[Long] = Seq.empty)

case class Struct3(a: Struct2 = Struct2(), b: Struct2 = Struct2())

val df1 = Seq((""a"", ""aa""), (""a"", ""aa""), (""b"", ""b""), (""b"", null)).toDF(""x"", ""y"").groupBy(""x"").agg(
  new ComplexResultAgg(""boo"", Struct3()).toColumn
)
df1.printSchema
df1.show
{noformat}

the result is:
{noformat}
[info]   Cause: java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 33, Column 12: Expression ""isNull1"" is not an rvalue
[info] /* 001 */ public java.lang.Object generate(Object[] references) {
[info] /* 002 */   return new SpecificMutableProjection(references);
[info] /* 003 */ }
[info] /* 004 */
[info] /* 005 */ class SpecificMutableProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {
[info] /* 006 */
[info] /* 007 */   private Object[] references;
[info] /* 008 */   private MutableRow mutableRow;
[info] /* 009 */   private Object[] values;
[info] /* 010 */   private java.lang.String errMsg;
[info] /* 011 */   private Object[] values1;
[info] /* 012 */   private java.lang.String errMsg1;
[info] /* 013 */   private boolean[] argIsNulls;
[info] /* 014 */   private scala.collection.Seq argValue;
[info] /* 015 */   private java.lang.String errMsg2;
[info] /* 016 */   private boolean[] argIsNulls1;
[info] /* 017 */   private scala.collection.Seq argValue1;
[info] /* 018 */   private java.lang.String errMsg3;
[info] /* 019 */   private java.lang.String errMsg4;
[info] /* 020 */   private Object[] values2;
[info] /* 021 */   private java.lang.String errMsg5;
[info] /* 022 */   private boolean[] argIsNulls2;
[info] /* 023 */   private scala.collection.Seq argValue2;
[info] /* 024 */   private java.lang.String errMsg6;
[info] /* 025 */   private boolean[] argIsNulls3;
[info] /* 026 */   private scala.collection.Seq argValue3;
[info] /* 027 */   private java.lang.String errMsg7;
[info] /* 028 */   private boolean isNull_0;
[info] /* 029 */   private InternalRow value_0;
[info] /* 030 */
[info] /* 031 */   private void apply_1(InternalRow i) {
[info] /* 032 */
[info] /* 033 */     if (isNull1) {
[info] /* 034 */       throw new RuntimeException(errMsg3);
[info] /* 035 */     }
[info] /* 036 */
[info] /* 037 */     boolean isNull24 = false;
[info] /* 038 */     final com.tresata.spark.sql.Struct2 value24 = isNull24 ? null : (com.tresata.spark.sql.Struct2) value1.a();
[info] /* 039 */     isNull24 = value24 == null;
[info] /* 040 */
[info] /* 041 */     boolean isNull23 = isNull24;
[info] /* 042 */     final scala.collection.Seq value23 = isNull23 ? null : (scala.collection.Seq) value24.s2();
[info] /* 043 */     isNull23 = value23 == null;
[info] /* 044 */     argIsNulls1[0] = isNull23;
[info] /* 045 */     argValue1 = value23;
[info] /* 046 */
[info] /* 047 */
[info] /* 048 */
[info] /* 049 */     boolean isNull22 = false;
[info] /* 050 */     for (int idx = 0; idx < 1; idx++) {
[info] /* 051 */       if (argIsNulls1[idx]) { isNull22 = true; break; }
[info] /* 052 */     }
[info] /* 053 */
[info] /* 054 */     final ArrayData value22 = isNull22 ? null : new org.apache.spark.sql.catalyst.util.GenericArrayData(argValue1);
[info] /* 055 */     if (isNull22) {
[info] /* 056 */       values1[2] = null;
[info] /* 057 */     } else {
[info] /* 058 */       values1[2] = value22;
[info] /* 059 */     }
[info] /* 060 */   }
[info] /* 061 */
[info] /* 062 */
[info] /* 063 */   private void apply1_1(InternalRow i) {
[info] /* 064 */
[info] /* 065 */     if (isNull1) {
[info] /* 066 */       throw new RuntimeException(errMsg7);
[info] /* 067 */     }
[info] /* 068 */
[info] /* 069 */     boolean isNull41 = false;
[info] /* 070 */     final com.tresata.spark.sql.Struct2 value41 = isNull41 ? null : (com.tresata.spark.sql.Struct2) value1.b();
[info] /* 071 */     isNull41 = value41 == null;
[info] /* 072 */
[info] /* 073 */     boolean isNull40 = isNull41;
[info] /* 074 */     final scala.collection.Seq value40 = isNull40 ? null : (scala.collection.Seq) value41.s2();
[info] /* 075 */     isNull40 = value40 == null;
[info] /* 076 */     argIsNulls3[0] = isNull40;
[info] /* 077 */     argValue3 = value40;
[info] /* 078 */
[info] /* 079 */
[info] /* 080 */
[info] /* 081 */     boolean isNull39 = false;
[info] /* 082 */     for (int idx = 0; idx < 1; idx++) {
[info] /* 083 */       if (argIsNulls3[idx]) { isNull39 = true; break; }
[info] /* 084 */     }
[info] /* 085 */
[info] /* 086 */     final ArrayData value39 = isNull39 ? null : new org.apache.spark.sql.catalyst.util.GenericArrayData(argValue3);
[info] /* 087 */     if (isNull39) {
[info] /* 088 */       values2[2] = null;
[info] /* 089 */     } else {
[info] /* 090 */       values2[2] = value39;
[info] /* 091 */     }
[info] /* 092 */   }
[info] /* 093 */
[info] /* 094 */
[info] /* 095 */   private void apply_0(InternalRow i) {
[info] /* 096 */
[info] /* 097 */     if (isNull1) {
[info] /* 098 */       throw new RuntimeException(errMsg1);
[info] /* 099 */     }
[info] /* 100 */
[info] /* 101 */     boolean isNull16 = false;
[info] /* 102 */     final com.tresata.spark.sql.Struct2 value16 = isNull16 ? null : (com.tresata.spark.sql.Struct2) value1.a();
[info] /* 103 */     isNull16 = value16 == null;
[info] /* 104 */
[info] /* 105 */     boolean isNull15 = isNull16;
[info] /* 106 */     final double value15 = isNull15 ? -1.0 : value16.d();
[info] /* 107 */     if (isNull15) {
[info] /* 108 */       values1[0] = null;
[info] /* 109 */     } else {
[info] /* 110 */       values1[0] = value15;
[info] /* 111 */     }
[info] /* 112 */     if (isNull1) {
[info] /* 113 */       throw new RuntimeException(errMsg2);
[info] /* 114 */     }
[info] /* 115 */
[info] /* 116 */     boolean isNull20 = false;
[info] /* 117 */     final com.tresata.spark.sql.Struct2 value20 = isNull20 ? null : (com.tresata.spark.sql.Struct2) value1.a();
[info] /* 118 */     isNull20 = value20 == null;
[info] /* 119 */
[info] /* 120 */     boolean isNull19 = isNull20;
[info] /* 121 */     final scala.collection.Seq value19 = isNull19 ? null : (scala.collection.Seq) value20.s1();
[info] /* 122 */     isNull19 = value19 == null;
[info] /* 123 */     argIsNulls[0] = isNull19;
[info] /* 124 */     argValue = value19;
[info] /* 125 */
[info] /* 126 */
[info] /* 127 */
[info] /* 128 */     boolean isNull18 = false;
[info] /* 129 */     for (int idx = 0; idx < 1; idx++) {
[info] /* 130 */       if (argIsNulls[idx]) { isNull18 = true; break; }
[info] /* 131 */     }
[info] /* 132 */
[info] /* 133 */     final ArrayData value18 = isNull18 ? null : new org.apache.spark.sql.catalyst.util.GenericArrayData(argValue);
[info] /* 134 */     if (isNull18) {
[info] /* 135 */       values1[1] = null;
[info] /* 136 */     } else {
[info] /* 137 */       values1[1] = value18;
[info] /* 138 */     }
[info] /* 139 */   }
[info] /* 140 */
[info] /* 141 */
[info] /* 142 */   private void apply1_0(InternalRow i) {
[info] /* 143 */
[info] /* 144 */     if (isNull1) {
[info] /* 145 */       throw new RuntimeException(errMsg5);
[info] /* 146 */     }
[info] /* 147 */
[info] /* 148 */     boolean isNull33 = false;
[info] /* 149 */     final com.tresata.spark.sql.Struct2 value33 = isNull33 ? null : (com.tresata.spark.sql.Struct2) value1.b();
[info] /* 150 */     isNull33 = value33 == null;
[info] /* 151 */
[info] /* 152 */     boolean isNull32 = isNull33;
[info] /* 153 */     final double value32 = isNull32 ? -1.0 : value33.d();
[info] /* 154 */     if (isNull32) {
[info] /* 155 */       values2[0] = null;
[info] /* 156 */     } else {
[info] /* 157 */       values2[0] = value32;
[info] /* 158 */     }
[info] /* 159 */     if (isNull1) {
[info] /* 160 */       throw new RuntimeException(errMsg6);
[info] /* 161 */     }
[info] /* 162 */
[info] /* 163 */     boolean isNull37 = false;
[info] /* 164 */     final com.tresata.spark.sql.Struct2 value37 = isNull37 ? null : (com.tresata.spark.sql.Struct2) value1.b();
[info] /* 165 */     isNull37 = value37 == null;
[info] /* 166 */
[info] /* 167 */     boolean isNull36 = isNull37;
[info] /* 168 */     final scala.collection.Seq value36 = isNull36 ? null : (scala.collection.Seq) value37.s1();
[info] /* 169 */     isNull36 = value36 == null;
[info] /* 170 */     argIsNulls2[0] = isNull36;
[info] /* 171 */     argValue2 = value36;
[info] /* 172 */
[info] /* 173 */
[info] /* 174 */
[info] /* 175 */     boolean isNull35 = false;
[info] /* 176 */     for (int idx = 0; idx < 1; idx++) {
[info] /* 177 */       if (argIsNulls2[idx]) { isNull35 = true; break; }
[info] /* 178 */     }
[info] /* 179 */
[info] /* 180 */     final ArrayData value35 = isNull35 ? null : new org.apache.spark.sql.catalyst.util.GenericArrayData(argValue2);
[info] /* 181 */     if (isNull35) {
[info] /* 182 */       values2[1] = null;
[info] /* 183 */     } else {
[info] /* 184 */       values2[1] = value35;
[info] /* 185 */     }
[info] /* 186 */   }
[info] /* 187 */
[info] /* 188 */
[info] /* 189 */   public SpecificMutableProjection(Object[] references) {
[info] /* 190 */     this.references = references;
[info] /* 191 */     mutableRow = new org.apache.spark.sql.catalyst.expressions.GenericMutableRow(1);
[info] /* 192 */     this.values = null;
[info] /* 193 */     this.errMsg = (java.lang.String) references[1];
[info] /* 194 */     this.values1 = null;
[info] /* 195 */     this.errMsg1 = (java.lang.String) references[2];
[info] /* 196 */     argIsNulls = new boolean[1];
[info] /* 197 */
[info] /* 198 */     this.errMsg2 = (java.lang.String) references[3];
[info] /* 199 */     argIsNulls1 = new boolean[1];
[info] /* 200 */
[info] /* 201 */     this.errMsg3 = (java.lang.String) references[4];
[info] /* 202 */     this.errMsg4 = (java.lang.String) references[5];
[info] /* 203 */     this.values2 = null;
[info] /* 204 */     this.errMsg5 = (java.lang.String) references[6];
[info] /* 205 */     argIsNulls2 = new boolean[1];
[info] /* 206 */
[info] /* 207 */     this.errMsg6 = (java.lang.String) references[7];
[info] /* 208 */     argIsNulls3 = new boolean[1];
[info] /* 209 */
[info] /* 210 */     this.errMsg7 = (java.lang.String) references[8];
[info] /* 211 */     this.isNull_0 = true;
[info] /* 212 */     this.value_0 = null;
[info] /* 213 */   }
[info] /* 214 */
[info] /* 215 */   public org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection target(MutableRow row) {
[info] /* 216 */     mutableRow = row;
[info] /* 217 */     return this;
[info] /* 218 */   }
[info] /* 219 */
[info] /* 220 */   /* Provide immutable access to the last projected row. */
[info] /* 221 */   public InternalRow currentValue() {
[info] /* 222 */     return (InternalRow) mutableRow;
[info] /* 223 */   }
[info] /* 224 */
[info] /* 225 */   public java.lang.Object apply(java.lang.Object _i) {
[info] /* 226 */     InternalRow i = (InternalRow) _i;
[info] /* 227 */
[info] /* 228 */
[info] /* 229 */
[info] /* 230 */     Object obj = ((Expression) references[0]).eval(null);
[info] /* 231 */     org.apache.spark.sql.expressions.Aggregator value2 = (org.apache.spark.sql.expressions.Aggregator) obj;
[info] /* 232 */
[info] /* 233 */     boolean isNull4 = i.isNullAt(0);
[info] /* 234 */     UTF8String value4 = isNull4 ? null : (i.getUTF8String(0));
[info] /* 235 */
[info] /* 236 */     boolean isNull3 = isNull4;
[info] /* 237 */     final java.lang.String value3 = isNull3 ? null : (java.lang.String) value4.toString();
[info] /* 238 */     isNull3 = value3 == null;
[info] /* 239 */     boolean isNull1 = false || isNull3;
[info] /* 240 */     final com.tresata.spark.sql.Struct3 value1 = isNull1 ? null : (com.tresata.spark.sql.Struct3) value2.finish(value3);
[info] /* 241 */     isNull1 = value1 == null;
[info] /* 242 */
[info] /* 243 */     boolean isNull5 = false;
[info] /* 244 */     InternalRow value5 = null;
[info] /* 245 */     if (!false && isNull1) {
[info] /* 246 */
[info] /* 247 */       final InternalRow value7 = null;
[info] /* 248 */       isNull5 = true;
[info] /* 249 */       value5 = value7;
[info] /* 250 */     } else {
[info] /* 251 */
[info] /* 252 */       boolean isNull8 = false;
[info] /* 253 */       this.values = new Object[2];
[info] /* 254 */       if (isNull1) {
[info] /* 255 */         throw new RuntimeException(errMsg);
[info] /* 256 */       }
[info] /* 257 */
[info] /* 258 */       boolean isNull11 = false;
[info] /* 259 */       final com.tresata.spark.sql.Struct2 value11 = isNull11 ? null : (com.tresata.spark.sql.Struct2) value1.a();
[info] /* 260 */       isNull11 = value11 == null;
[info] /* 261 */       boolean isNull9 = false;
[info] /* 262 */       InternalRow value9 = null;
[info] /* 263 */       if (!false && isNull11) {
[info] /* 264 */
[info] /* 265 */         final InternalRow value13 = null;
[info] /* 266 */         isNull9 = true;
[info] /* 267 */         value9 = value13;
[info] /* 268 */       } else {
[info] /* 269 */
[info] /* 270 */         boolean isNull14 = false;
[info] /* 271 */         values1 = new Object[3];apply_0(i);
[info] /* 272 */         apply_1(i);
[info] /* 273 */         final InternalRow value14 = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(values1);
[info] /* 274 */         this.values1 = null;
[info] /* 275 */         isNull9 = isNull14;
[info] /* 276 */         value9 = value14;
[info] /* 277 */       }
[info] /* 278 */       if (isNull9) {
[info] /* 279 */         values[0] = null;
[info] /* 280 */       } else {
[info] /* 281 */         values[0] = value9;
[info] /* 282 */       }
[info] /* 283 */       if (isNull1) {
[info] /* 284 */         throw new RuntimeException(errMsg4);
[info] /* 285 */       }
[info] /* 286 */
[info] /* 287 */       boolean isNull28 = false;
[info] /* 288 */       final com.tresata.spark.sql.Struct2 value28 = isNull28 ? null : (com.tresata.spark.sql.Struct2) value1.b();
[info] /* 289 */       isNull28 = value28 == null;
[info] /* 290 */       boolean isNull26 = false;
[info] /* 291 */       InternalRow value26 = null;
[info] /* 292 */       if (!false && isNull28) {
[info] /* 293 */
[info] /* 294 */         final InternalRow value30 = null;
[info] /* 295 */         isNull26 = true;
[info] /* 296 */         value26 = value30;
[info] /* 297 */       } else {
[info] /* 298 */
[info] /* 299 */         boolean isNull31 = false;
[info] /* 300 */         values2 = new Object[3];apply1_0(i);
[info] /* 301 */         apply1_1(i);
[info] /* 302 */         final InternalRow value31 = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(values2);
[info] /* 303 */         this.values2 = null;
[info] /* 304 */         isNull26 = isNull31;
[info] /* 305 */         value26 = value31;
[info] /* 306 */       }
[info] /* 307 */       if (isNull26) {
[info] /* 308 */         values[1] = null;
[info] /* 309 */       } else {
[info] /* 310 */         values[1] = value26;
[info] /* 311 */       }
[info] /* 312 */       final InternalRow value8 = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(values);
[info] /* 313 */       this.values = null;
[info] /* 314 */       isNull5 = isNull8;
[info] /* 315 */       value5 = value8;
[info] /* 316 */     }
[info] /* 317 */     this.isNull_0 = isNull5;
[info] /* 318 */     this.value_0 = value5;
[info] /* 319 */
[info] /* 320 */     // copy all the results into MutableRow
[info] /* 321 */
[info] /* 322 */     if (!this.isNull_0) {
[info] /* 323 */       mutableRow.update(0, this.value_0);
[info] /* 324 */     } else {
[info] /* 325 */       mutableRow.setNullAt(0);
[info] /* 326 */     }
[info] /* 327 */
[info] /* 328 */     return mutableRow;
[info] /* 329 */   }
[info] /* 330 */ }
{noformat}",,apachespark,chamcyl,chermenin,cloud_fan,Gna Phetsarath,kiszk,koert,lwlin,marmbrus,mgaido,nsyca,tsuresh,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 06 11:21:36 UTC 2018,,,,,,,,,,"0|i35imf:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"27/Oct/16 22:28;koert;it also breaks with an option of a case class. like this:
{noformat}
 val df1 = Seq((""a"", ""aa""), (""a"", ""aa""), (""b"", ""b""), (""b"", null)).toDF(""x"", ""y"").groupBy(""x"").agg(
   new ComplexResultAgg(""boo"", Option(Struct2())).toColumn
 )
{noformat};;;","27/Oct/16 22:56;marmbrus;/cc [~cloud_fan];;;","28/Oct/16 09:55;kiszk;This also cause the same exception.

{code:java}
class ComplexResultAgg[B: TypeTag, C: TypeTag](val zero: B, result: C) extends Aggregator[Row, B, C] {
  override def reduce(b: B, input: Row): B = b
  override def merge(b1: B, b2: B): B = b1
  override def finish(reduction: B): C = result
  override def bufferEncoder: Encoder[B] = ExpressionEncoder[B]()
  override def outputEncoder: Encoder[C] = ExpressionEncoder[C]()
}
case class Struct2(i: Int)
case class Struct3(a: Struct2, b: Struct2)
Seq(1).toDF(""x"").groupBy(""x"")
  .agg(new ComplexResultAgg(""a"", Struct3(null, null)).toColumn).show
{code}

In this case, there is an issue in codegen for {{AssertNotNull}}. {{isNull1}} ({{${childGen.isNull}}) is out of the scope when {{ctx.splitExpression}} is called at caller of {{AssertNotNull.doGenCode}}. Here, {{CreateStruct.doGenCode}} calls {{ctx.splitExpression}}.
;;;","08/Nov/16 08:34;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/15807;;;","10/Nov/16 05:05;cloud_fan;Issue resolved by pull request 15807
[https://github.com/apache/spark/pull/15807];;;","10/Jan/18 12:23;chermenin;Is it resolved for the 2.2.1 version? I have a similar problem with PySpark and Structured Streaming:

{noformat}
18/01/10 15:12:37 ERROR CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 174, Column 113: Expression ""isNull3"" is not an rvalue
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UTF8String lastRegex;
/* 009 */   private java.util.regex.Pattern pattern;
/* 010 */   private String lastReplacement;
/* 011 */   private UTF8String lastReplacementInUTF8;
/* 012 */   private java.lang.StringBuffer result;
/* 013 */   private UTF8String lastRegex1;
/* 014 */   private java.util.regex.Pattern pattern1;
/* 015 */   private String lastReplacement1;
/* 016 */   private UTF8String lastReplacementInUTF81;
/* 017 */   private java.lang.StringBuffer result1;
/* 018 */   private UnsafeRow result2;
/* 019 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 020 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 021 */
/* 022 */   public SpecificUnsafeProjection(Object[] references) {
/* 023 */     this.references = references;
/* 024 */     lastRegex = null;
/* 025 */     pattern = null;
/* 026 */     lastReplacement = null;
/* 027 */     lastReplacementInUTF8 = null;
/* 028 */     result = new java.lang.StringBuffer();
/* 029 */     lastRegex1 = null;
/* 030 */     pattern1 = null;
/* 031 */     lastReplacement1 = null;
/* 032 */     lastReplacementInUTF81 = null;
/* 033 */     result1 = new java.lang.StringBuffer();
/* 034 */     result2 = new UnsafeRow(3);
/* 035 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result2, 64);
/* 036 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 037 */
/* 038 */   }
/* 039 */
/* 040 */   public void initialize(int partitionIndex) {
/* 041 */
/* 042 */   }
/* 043 */
/* 044 */
/* 045 */   private void apply_1(InternalRow i) {
/* 046 */
/* 047 */     UTF8String[] args1 = new UTF8String[3];
/* 048 */
/* 049 */
/* 050 */     if (!false) {
/* 051 */       args1[0] = ((UTF8String) references[6]);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     if (!false) {
/* 056 */       args1[1] = ((UTF8String) references[7]);
/* 057 */     }
/* 058 */
/* 059 */
/* 060 */     boolean isNull13 = false;
/* 061 */
/* 062 */     UTF8String value14 = i.getUTF8String(2);
/* 063 */
/* 064 */
/* 065 */     UTF8String value13 = null;
/* 066 */
/* 067 */     if (!((UTF8String) references[8]).equals(lastRegex1)) {
/* 068 */       // regex value changed
/* 069 */       lastRegex1 = ((UTF8String) references[8]).clone();
/* 070 */       pattern1 = java.util.regex.Pattern.compile(lastRegex1.toString());
/* 071 */     }
/* 072 */     if (!((UTF8String) references[9]).equals(lastReplacementInUTF81)) {
/* 073 */       // replacement string changed
/* 074 */       lastReplacementInUTF81 = ((UTF8String) references[9]).clone();
/* 075 */       lastReplacement1 = lastReplacementInUTF81.toString();
/* 076 */     }
/* 077 */     result1.delete(0, result1.length());
/* 078 */     java.util.regex.Matcher matcher1 = pattern1.matcher(value14.toString());
/* 079 */
/* 080 */     while (matcher1.find()) {
/* 081 */       matcher1.appendReplacement(result1, lastReplacement1);
/* 082 */     }
/* 083 */     matcher1.appendTail(result1);
/* 084 */     value13 = UTF8String.fromString(result1.toString());
/* 085 */     if (!false) {
/* 086 */       args1[2] = value13;
/* 087 */     }
/* 088 */
/* 089 */     UTF8String value10 = UTF8String.concat(args1);
/* 090 */     boolean isNull10 = value10 == null;
/* 091 */   }
/* 092 */
/* 093 */
/* 094 */   private void apply1_1(InternalRow i) {
/* 095 */
/* 096 */
/* 097 */     final Object value20 = null;
/* 098 */     if (true) {
/* 099 */       rowWriter.setNullAt(2);
/* 100 */     } else {
/* 101 */
/* 102 */     }
/* 103 */
/* 104 */   }
/* 105 */
/* 106 */
/* 107 */   private void apply_0(InternalRow i) {
/* 108 */
/* 109 */     UTF8String[] args = new UTF8String[3];
/* 110 */
/* 111 */
/* 112 */     if (!false) {
/* 113 */       args[0] = ((UTF8String) references[2]);
/* 114 */     }
/* 115 */
/* 116 */
/* 117 */     if (!false) {
/* 118 */       args[1] = ((UTF8String) references[3]);
/* 119 */     }
/* 120 */
/* 121 */
/* 122 */     boolean isNull6 = true;
/* 123 */     UTF8String value6 = null;
/* 124 */
/* 125 */     boolean isNull7 = i.isNullAt(1);
/* 126 */     UTF8String value7 = isNull7 ? null : (i.getUTF8String(1));
/* 127 */     if (!isNull7) {
/* 128 */
/* 129 */
/* 130 */
/* 131 */       isNull6 = false; // resultCode could change nullability.
/* 132 */
/* 133 */       if (!((UTF8String) references[4]).equals(lastRegex)) {
/* 134 */         // regex value changed
/* 135 */         lastRegex = ((UTF8String) references[4]).clone();
/* 136 */         pattern = java.util.regex.Pattern.compile(lastRegex.toString());
/* 137 */       }
/* 138 */       if (!((UTF8String) references[5]).equals(lastReplacementInUTF8)) {
/* 139 */         // replacement string changed
/* 140 */         lastReplacementInUTF8 = ((UTF8String) references[5]).clone();
/* 141 */         lastReplacement = lastReplacementInUTF8.toString();
/* 142 */       }
/* 143 */       result.delete(0, result.length());
/* 144 */       java.util.regex.Matcher matcher = pattern.matcher(value7.toString());
/* 145 */
/* 146 */       while (matcher.find()) {
/* 147 */         matcher.appendReplacement(result, lastReplacement);
/* 148 */       }
/* 149 */       matcher.appendTail(result);
/* 150 */       value6 = UTF8String.fromString(result.toString());
/* 151 */       isNull6 = false;
/* 152 */
/* 153 */
/* 154 */     }
/* 155 */     if (!isNull6) {
/* 156 */       args[2] = value6;
/* 157 */     }
/* 158 */
/* 159 */     UTF8String value3 = UTF8String.concat(args);
/* 160 */     boolean isNull3 = value3 == null;
/* 161 */   }
/* 162 */
/* 163 */
/* 164 */   private void apply1_0(InternalRow i) {
/* 165 */
/* 166 */
/* 167 */     apply_0(i);
/* 168 */     apply_1(i);
/* 169 */     apply_2(i);
/* 170 */     int varargNum = 4;
/* 171 */     int idxInVararg = 0;
/* 172 */
/* 173 */     UTF8String[] array = new UTF8String[varargNum];
/* 174 */     array[idxInVararg ++] = false ? (UTF8String) null : ((UTF8String) references[1]);array[idxInVararg ++] = isNull3 ? (UTF8String) null : value3;array[idxInVararg ++] = isNull10 ? (UTF8String) null : value10;array[idxInVararg ++] = false ? (UTF8String) null : ((UTF8String) references[10]);
/* 175 */     UTF8String value = UTF8String.concatWs(((UTF8String) references[0]), array);
/* 176 */     boolean isNull = value == null;
/* 177 */     if (isNull) {
/* 178 */       rowWriter.setNullAt(0);
/* 179 */     } else {
/* 180 */       rowWriter.write(0, value);
/* 181 */     }
/* 182 */
/* 183 */
/* 184 */     Object obj = ((Expression) references[11]).eval(i);
/* 185 */     boolean isNull19 = obj == null;
/* 186 */     UTF8String value19 = null;
/* 187 */     if (!isNull19) {
/* 188 */       value19 = (UTF8String) obj;
/* 189 */     }
/* 190 */     if (isNull19) {
/* 191 */       rowWriter.setNullAt(1);
/* 192 */     } else {
/* 193 */       rowWriter.write(1, value19);
/* 194 */     }
/* 195 */
/* 196 */   }
/* 197 */
/* 198 */
/* 199 */   private void apply_2(InternalRow i) {
/* 200 */
/* 201 */     final ArrayData value17 = null;
/* 202 */   }
/* 203 */
/* 204 */
/* 205 */   // Scala.Function1 need this
/* 206 */   public java.lang.Object apply(java.lang.Object row) {
/* 207 */     return apply((InternalRow) row);
/* 208 */   }
/* 209 */
/* 210 */   public UnsafeRow apply(InternalRow i) {
/* 211 */     holder.reset();
/* 212 */
/* 213 */     rowWriter.zeroOutNullBytes();
/* 214 */     apply1_0(i);
/* 215 */     apply1_1(i);
/* 216 */     result2.setTotalSize(holder.totalSize());
/* 217 */     return result2;
/* 218 */   }
/* 219 */ }

org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 174, Column 113: Expression ""isNull3"" is not an rvalue
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:11004)
	at org.codehaus.janino.UnitCompiler.toRvalueOrCompileException(UnitCompiler.java:6639)
	at org.codehaus.janino.UnitCompiler.getConstantValue2(UnitCompiler.java:5001)
	at org.codehaus.janino.UnitCompiler.access$10500(UnitCompiler.java:206)
	at org.codehaus.janino.UnitCompiler$13.visitAmbiguousName(UnitCompiler.java:4984)
	at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:3633)
	at org.codehaus.janino.Java$Lvalue.accept(Java.java:3563)
	at org.codehaus.janino.UnitCompiler.getConstantValue(UnitCompiler.java:4956)
	at org.codehaus.janino.UnitCompiler.getConstantValue2(UnitCompiler.java:5027)
	at org.codehaus.janino.UnitCompiler.access$9700(UnitCompiler.java:206)
	at org.codehaus.janino.UnitCompiler$13.visitConditionalExpression(UnitCompiler.java:4964)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3877)
	at org.codehaus.janino.UnitCompiler.getConstantValue(UnitCompiler.java:4956)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4925)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3189)
	at org.codehaus.janino.UnitCompiler.access$5100(UnitCompiler.java:206)
	at org.codehaus.janino.UnitCompiler$9.visitAssignment(UnitCompiler.java:3143)
	at org.codehaus.janino.UnitCompiler$9.visitAssignment(UnitCompiler.java:3139)
	at org.codehaus.janino.Java$Assignment.accept(Java.java:3847)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3139)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2112)
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:206)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1377)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1370)
	at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2558)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1370)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1450)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2811)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1262)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1234)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:538)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:890)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:894)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:206)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:377)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:369)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1128)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:369)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:1209)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:564)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:420)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:206)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:374)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:369)
	at org.codehaus.janino.Java$AbstractPackageMemberClassDeclaration.accept(Java.java:1309)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:369)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:345)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:396)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:311)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:229)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:196)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:91)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1000)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1067)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1064)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:946)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:412)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:366)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:32)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:930)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:130)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:140)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.generateResultProjection(AggregationIterator.scala:219)
	at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.generateResultProjection(TungstenAggregationIterator.scala:144)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.<init>(AggregationIterator.scala:266)
	at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:92)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1$$anonfun$4.apply(HashAggregateExec.scala:106)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1$$anonfun$4.apply(HashAggregateExec.scala:97)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
18/01/10 15:12:37 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)
{noformat}
;;;","06/Dec/18 10:50;chamcyl;I hava a similar problem at version 2.3.0 . but it work well in 2.0.1 and 2.1.0 ..
{code:java}
2018-12-06 18:45:45,945 ERROR (org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:91) - failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 250, Column 21: Expression ""project_isNull22"" is not an rvalue
org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 250, Column 21: Expression ""project_isNull22"" is not an rvalue
at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:11821)
at org.codehaus.janino.UnitCompiler.toRvalueOrCompileException(UnitCompiler.java:7170)
at org.codehaus.janino.UnitCompiler.getConstantValue2(UnitCompiler.java:5332)
at org.codehaus.janino.UnitCompiler.access$9400(UnitCompiler.java:212)
at org.codehaus.janino.UnitCompiler$13$1.visitAmbiguousName(UnitCompiler.java:5287)
at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4053)
at org.codehaus.janino.UnitCompiler$13.visitLvalue(UnitCompiler.java:5284)
at org.codehaus.janino.Java$Lvalue.accept(Java.java:3977)
at org.codehaus.janino.UnitCompiler.getConstantValue(UnitCompiler.java:5280)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2391)
at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:212)
at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1474)
at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1466)
at org.codehaus.janino.Java$IfStatement.accept(Java.java:2926)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1466)
at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1546)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1532)
at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:212)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1472)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1466)
at org.codehaus.janino.Java$Block.accept(Java.java:2756)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1466)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2444)
at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:212)
at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1474)
at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1466)
at org.codehaus.janino.Java$IfStatement.accept(Java.java:2926)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1466)
at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1546)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1532)
at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:212)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1472)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1466)
at org.codehaus.janino.Java$Block.accept(Java.java:2756)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1466)
at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1546)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1532)
at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:212)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1472)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1466)
at org.codehaus.janino.Java$Block.accept(Java.java:2756)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1466)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2455)
at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:212)
at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1474)
at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1466)
at org.codehaus.janino.Java$IfStatement.accept(Java.java:2926)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1466)
at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1546)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1532)
at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:212)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1472)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1466)
at org.codehaus.janino.Java$Block.accept(Java.java:2756)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1466)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1571)
at org.codehaus.janino.UnitCompiler.access$2600(UnitCompiler.java:212)
at org.codehaus.janino.UnitCompiler$6.visitDoStatement(UnitCompiler.java:1481)
at org.codehaus.janino.UnitCompiler$6.visitDoStatement(UnitCompiler.java:1466)
at org.codehaus.janino.Java$DoStatement.accept(Java.java:3304)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1466)
at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1546)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1532)
at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:212)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1472)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1466)
at org.codehaus.janino.Java$Block.accept(Java.java:2756)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1466)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1634)
at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:212)
at org.codehaus.janino.UnitCompiler$6.visitForStatement(UnitCompiler.java:1475)
at org.codehaus.janino.UnitCompiler$6.visitForStatement(UnitCompiler.java:1466)
at org.codehaus.janino.Java$ForStatement.accept(Java.java:2973)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1466)
at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1546)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1532)
at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:212)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1472)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1466)
at org.codehaus.janino.Java$Block.accept(Java.java:2756)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1466)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1821)
at org.codehaus.janino.UnitCompiler.access$2200(UnitCompiler.java:212)
at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1477)
at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1466)
at org.codehaus.janino.Java$WhileStatement.accept(Java.java:3031)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1466)
at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1546)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3075)
at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1336)
at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1309)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:799)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:958)
at org.codehaus.janino.UnitCompiler.access$700(UnitCompiler.java:212)
at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:393)
at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:385)
at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1286)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:385)
at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:1285)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:825)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:411)
at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:212)
at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:390)
at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:385)
at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1405)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:385)
at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:357)
at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:234)
at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:446)
at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:313)
at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:235)
at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:204)
at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1421)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1497)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1494)
at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1369)
at org.apache.spark.sql.execution.WholeStageCodegenExec.liftedTree1$1(WholeStageCodegenExec.scala:579)
at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:578)
at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)
at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:337)
at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3272)
at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)
at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)
at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253)
at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252)
at org.apache.spark.sql.Dataset.head(Dataset.scala:2484)
at org.apache.spark.sql.Dataset.take(Dataset.scala:2698)
at org.apache.spark.sql.Dataset.showString(Dataset.scala:254)
at org.apache.spark.sql.Dataset.show(Dataset.scala:725)
at org.apache.spark.sql.Dataset.show(Dataset.scala:702)
at com.netentsec.uba.sparksql.SparkSqlRunner.main(SparkSqlRunner.java:211)
2018-12-06 18:45:45,950 INFO (org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54) - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */ return new GeneratedIteratorForCodegenStage3(references);
/* 003 */ }
/* 004 */
/* 005 */ final class GeneratedIteratorForCodegenStage3 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 006 */ private Object[] references;
/* 007 */ private scala.collection.Iterator[] inputs;
/* 008 */ private long scan_scanTime;
/* 009 */ private int scan_batchIdx;
/* 010 */ private org.apache.spark.sql.execution.joins.UnsafeHashedRelation bhj_relation;
/* 011 */ private boolean locallimit_stopEarly;
/* 012 */ private int locallimit_count;
/* 013 */ private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder[] scan_mutableStateArray4 = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder[7];
/* 014 */ private org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[] scan_mutableStateArray2 = new org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[3];
/* 015 */ private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray5 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[7];
/* 016 */ private UnsafeRow[] scan_mutableStateArray3 = new UnsafeRow[7];
/* 017 */ private org.apache.spark.sql.vectorized.ColumnarBatch[] scan_mutableStateArray1 = new org.apache.spark.sql.vectorized.ColumnarBatch[1];
/* 018 */ private scala.collection.Iterator[] scan_mutableStateArray = new scala.collection.Iterator[1];
/* 019 */
/* 020 */ public GeneratedIteratorForCodegenStage3(Object[] references) {
/* 021 */ this.references = references;
/* 022 */ }
/* 023 */
/* 024 */ public void init(int index, scala.collection.Iterator[] inputs) {
/* 025 */ partitionIndex = index;
/* 026 */ this.inputs = inputs;
/* 027 */ wholestagecodegen_init_0();
/* 028 */ wholestagecodegen_init_1();
/* 029 */ wholestagecodegen_init_2();
/* 030 */
/* 031 */ }
/* 032 */
/* 033 */ private void wholestagecodegen_init_0() {
/* 034 */ scan_mutableStateArray[0] = inputs[0];
/* 035 */
/* 036 */ scan_mutableStateArray3[0] = new UnsafeRow(3);
/* 037 */ scan_mutableStateArray4[0] = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(scan_mutableStateArray3[0], 0);
/* 038 */ scan_mutableStateArray5[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(scan_mutableStateArray4[0], 3);
/* 039 */ scan_mutableStateArray3[1] = new UnsafeRow(3);
/* 040 */ scan_mutableStateArray4[1] = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(scan_mutableStateArray3[1], 0);
/* 041 */ scan_mutableStateArray5[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(scan_mutableStateArray4[1], 3);
/* 042 */ scan_mutableStateArray3[2] = new UnsafeRow(5);
/* 043 */ scan_mutableStateArray4[2] = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(scan_mutableStateArray3[2], 32);
/* 044 */ scan_mutableStateArray5[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(scan_mutableStateArray4[2], 5);
/* 045 */
/* 046 */ bhj_relation = ((org.apache.spark.sql.execution.joins.UnsafeHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[5] /* broadcast */).value()).asReadOnlyCopy();
/* 047 */ incPeakExecutionMemory(bhj_relation.estimatedSize());
/* 048 */
/* 049 */ org.apache.spark.TaskContext$.MODULE$.get().addTaskCompletionListener(new org.apache.spark.util.TaskCompletionListener() {
/* 050 */ @Override
/* 051 */ public void onTaskCompletion(org.apache.spark.TaskContext context) {
/* 052 */ ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* avgHashProbe */).set(bhj_relation.getAverageProbesPerLookup());
/* 053 */ }
/* 054 */ });
/* 055 */
/* 056 */ }
/* 057 */
/* 058 */ private void scan_nextBatch() throws java.io.IOException {
/* 059 */ long getBatchStart = System.nanoTime();
/* 060 */ if (scan_mutableStateArray[0].hasNext()) {
/* 061 */ scan_mutableStateArray1[0] = (org.apache.spark.sql.vectorized.ColumnarBatch)scan_mutableStateArray[0].next();
/* 062 */ ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(scan_mutableStateArray1[0].numRows());
/* 063 */ scan_batchIdx = 0;
/* 064 */ scan_mutableStateArray2[0] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) scan_mutableStateArray1[0].column(0);
/* 065 */ scan_mutableStateArray2[1] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) scan_mutableStateArray1[0].column(1);
/* 066 */ scan_mutableStateArray2[2] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) scan_mutableStateArray1[0].column(2);
/* 067 */
/* 068 */ }
/* 069 */ scan_scanTime += System.nanoTime() - getBatchStart;
/* 070 */ }
/* 071 */
/* 072 */ private void wholestagecodegen_init_2() {
/* 073 */ scan_mutableStateArray5[6] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(scan_mutableStateArray4[6], 6);
/* 074 */
/* 075 */ }
/* 076 */
/* 077 */ private void wholestagecodegen_init_1() {
/* 078 */ scan_mutableStateArray3[3] = new UnsafeRow(2);
/* 079 */ scan_mutableStateArray4[3] = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(scan_mutableStateArray3[3], 32);
/* 080 */ scan_mutableStateArray5[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(scan_mutableStateArray4[3], 2);
/* 081 */ scan_mutableStateArray3[4] = new UnsafeRow(8);
/* 082 */ scan_mutableStateArray4[4] = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(scan_mutableStateArray3[4], 64);
/* 083 */ scan_mutableStateArray5[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(scan_mutableStateArray4[4], 8);
/* 084 */ scan_mutableStateArray3[5] = new UnsafeRow(6);
/* 085 */ scan_mutableStateArray4[5] = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(scan_mutableStateArray3[5], 192);
/* 086 */ scan_mutableStateArray5[5] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(scan_mutableStateArray4[5], 6);
/* 087 */ scan_mutableStateArray3[6] = new UnsafeRow(6);
/* 088 */ scan_mutableStateArray4[6] = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(scan_mutableStateArray3[6], 192);
/* 089 */
/* 090 */ }
/* 091 */
/* 092 */ protected void processNext() throws java.io.IOException {
/* 093 */ if (scan_mutableStateArray1[0] == null) {
/* 094 */ scan_nextBatch();
/* 095 */ }
/* 096 */ while (scan_mutableStateArray1[0] != null) {
/* 097 */ int scan_numRows = scan_mutableStateArray1[0].numRows();
/* 098 */ int scan_localEnd = scan_numRows - scan_batchIdx;
/* 099 */ for (int scan_localIdx = 0; scan_localIdx < scan_localEnd; scan_localIdx++) {
/* 100 */ int scan_rowIdx = scan_batchIdx + scan_localIdx;
/* 101 */ do {
/* 102 */ boolean scan_isNull = scan_mutableStateArray2[0].isNullAt(scan_rowIdx);
/* 103 */ long scan_value = scan_isNull ? -1L : (scan_mutableStateArray2[0].getLong(scan_rowIdx));
/* 104 */
/* 105 */ boolean filter_isNull2 = true;
/* 106 */ long filter_value2 = -1L;
/* 107 */
/* 108 */ boolean filter_isNull4 = false;
/* 109 */ double filter_value4 = -1.0;
/* 110 */ if (false || 600.0D == 0) {
/* 111 */ filter_isNull4 = true;
/* 112 */ } else {
/* 113 */ boolean filter_isNull6 = scan_isNull;
/* 114 */ long filter_value6 = -1L;
/* 115 */ if (!filter_isNull6) {
/* 116 */ filter_value6 = scan_value / 1000000L;
/* 117 */ }
/* 118 */ boolean filter_isNull5 = filter_isNull6;
/* 119 */ double filter_value5 = -1.0;
/* 120 */ if (!filter_isNull6) {
/* 121 */ filter_value5 = (double) filter_value6;
/* 122 */ }
/* 123 */ if (filter_isNull5) {
/* 124 */ filter_isNull4 = true;
/* 125 */ } else {
/* 126 */ filter_value4 = (double)(filter_value5 / 600.0D);
/* 127 */ }
/* 128 */ }
/* 129 */ boolean filter_isNull3 = filter_isNull4;
/* 130 */ long filter_value3 = -1L;
/* 131 */
/* 132 */ if (!filter_isNull4) {
/* 133 */ filter_value3 = (long)(java.lang.Math.floor(filter_value4));
/* 134 */ }
/* 135 */ if (!filter_isNull3) {
/* 136 */ filter_isNull2 = false; // resultCode could change nullability.
/* 137 */ filter_value2 = filter_value3 * 600L;
/* 138 */
/* 139 */ }
/* 140 */ boolean filter_isNull1 = filter_isNull2;
/* 141 */ UTF8String filter_value1 = null;
/* 142 */ if (!filter_isNull1) {
/* 143 */ try {
/* 144 */ filter_value1 = UTF8String.fromString(((java.text.DateFormat) references[3] /* formatter */).format(
/* 145 */ new java.util.Date(filter_value2 * 1000L)));
/* 146 */ } catch (java.lang.IllegalArgumentException e) {
/* 147 */ filter_isNull1 = true;
/* 148 */ }
/* 149 */ }
/* 150 */ if (!(!(filter_isNull1))) continue;
/* 151 */
/* 152 */ boolean scan_isNull2 = scan_mutableStateArray2[2].isNullAt(scan_rowIdx);
/* 153 */ long scan_value2 = scan_isNull2 ? -1L : (scan_mutableStateArray2[2].getLong(scan_rowIdx));
/* 154 */
/* 155 */ if (!(!(scan_isNull2))) continue;
/* 156 */
/* 157 */ ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* numOutputRows */).add(1);
/* 158 */
/* 159 */ boolean project_isNull6 = true;
/* 160 */ long project_value6 = -1L;
/* 161 */
/* 162 */ boolean project_isNull8 = false;
/* 163 */ double project_value8 = -1.0;
/* 164 */ if (false || 600.0D == 0) {
/* 165 */ project_isNull8 = true;
/* 166 */ } else {
/* 167 */ boolean project_isNull10 = scan_isNull;
/* 168 */ long project_value10 = -1L;
/* 169 */ if (!project_isNull10) {
/* 170 */ project_value10 = scan_value / 1000000L;
/* 171 */ }
/* 172 */ boolean project_isNull9 = project_isNull10;
/* 173 */ double project_value9 = -1.0;
/* 174 */ if (!project_isNull10) {
/* 175 */ project_value9 = (double) project_value10;
/* 176 */ }
/* 177 */ if (project_isNull9) {
/* 178 */ project_isNull8 = true;
/* 179 */ } else {
/* 180 */ project_value8 = (double)(project_value9 / 600.0D);
/* 181 */ }
/* 182 */ }
/* 183 */ boolean project_isNull7 = project_isNull8;
/* 184 */ long project_value7 = -1L;
/* 185 */
/* 186 */ if (!project_isNull8) {
/* 187 */ project_value7 = (long)(java.lang.Math.floor(project_value8));
/* 188 */ }
/* 189 */ if (!project_isNull7) {
/* 190 */ project_isNull6 = false; // resultCode could change nullability.
/* 191 */ project_value6 = project_value7 * 600L;
/* 192 */
/* 193 */ }
/* 194 */ boolean project_isNull5 = project_isNull6;
/* 195 */ UTF8String project_value5 = null;
/* 196 */ if (!project_isNull5) {
/* 197 */ try {
/* 198 */ project_value5 = UTF8String.fromString(((java.text.DateFormat) references[4] /* formatter */).format(
/* 199 */ new java.util.Date(project_value6 * 1000L)));
/* 200 */ } catch (java.lang.IllegalArgumentException e) {
/* 201 */ project_isNull5 = true;
/* 202 */ }
/* 203 */ }
/* 204 */
/* 205 */ // generate join key for stream side
/* 206 */
/* 207 */ scan_mutableStateArray4[3].reset();
/* 208 */
/* 209 */ scan_mutableStateArray5[3].zeroOutNullBytes();
/* 210 */
/* 211 */ scan_mutableStateArray5[3].write(0, scan_value2);
/* 212 */
/* 213 */ if (project_isNull5) {
/* 214 */ scan_mutableStateArray5[3].setNullAt(1);
/* 215 */ } else {
/* 216 */ scan_mutableStateArray5[3].write(1, project_value5);
/* 217 */ }
/* 218 */ scan_mutableStateArray3[3].setTotalSize(scan_mutableStateArray4[3].totalSize());
/* 219 */
/* 220 */ // find matches from HashedRelation
/* 221 */ UnsafeRow bhj_matched = scan_mutableStateArray3[3].anyNull() ? null: (UnsafeRow)bhj_relation.getValue(scan_mutableStateArray3[3]);
/* 222 */ if (bhj_matched != null) {
/* 223 */ {
/* 224 */ ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* numOutputRows */).add(1);
/* 225 */
/* 226 */ if (locallimit_count < 21) {
/* 227 */ locallimit_count += 1;
/* 228 */
/* 229 */ boolean project_isNull19 = false;
/* 230 */ UTF8String project_value19 = null;
/* 231 */ if (!false) {
/* 232 */ project_value19 = UTF8String.fromString(String.valueOf(scan_value2));
/* 233 */ }
/* 234 */ scan_mutableStateArray4[6].reset();
/* 235 */
/* 236 */ scan_mutableStateArray5[6].zeroOutNullBytes();
/* 237 */
/* 238 */ if (project_isNull19) {
/* 239 */ scan_mutableStateArray5[6].setNullAt(0);
/* 240 */ } else {
/* 241 */ scan_mutableStateArray5[6].write(0, project_value19);
/* 242 */ }
/* 243 */
/* 244 */ if (project_isNull5) {
/* 245 */ scan_mutableStateArray5[6].setNullAt(1);
/* 246 */ } else {
/* 247 */ scan_mutableStateArray5[6].write(1, project_value5);
/* 248 */ }
/* 249 */
/* 250 */ if (project_isNull22) {
/* 251 */ scan_mutableStateArray5[6].setNullAt(2);
/* 252 */ } else {
/* 253 */ scan_mutableStateArray5[6].write(2, project_value22);
/* 254 */ }
/* 255 */
/* 256 */ if (project_isNull24) {
/* 257 */ scan_mutableStateArray5[6].setNullAt(3);
/* 258 */ } else {
/* 259 */ scan_mutableStateArray5[6].write(3, project_value24);
/* 260 */ }
/* 261 */
/* 262 */ if (project_isNull26) {
/* 263 */ scan_mutableStateArray5[6].setNullAt(4);
/* 264 */ } else {
/* 265 */ scan_mutableStateArray5[6].write(4, project_value26);
/* 266 */ }
/* 267 */
/* 268 */ if (project_isNull28) {
/* 269 */ scan_mutableStateArray5[6].setNullAt(5);
/* 270 */ } else {
/* 271 */ scan_mutableStateArray5[6].write(5, project_value28);
/* 272 */ }
/* 273 */ scan_mutableStateArray3[6].setTotalSize(scan_mutableStateArray4[6].totalSize());
/* 274 */ append(scan_mutableStateArray3[6]);
/* 275 */
/* 276 */ } else {
/* 277 */ locallimit_stopEarly = true;
/* 278 */ }
/* 279 */
/* 280 */ }
/* 281 */ }
/* 282 */
/* 283 */ } while(false);
/* 284 */ if (shouldStop()) { scan_batchIdx = scan_rowIdx + 1; return; }
/* 285 */ }
/* 286 */ scan_batchIdx = scan_numRows;
/* 287 */ scan_mutableStateArray1[0] = null;
/* 288 */ scan_nextBatch();
/* 289 */ }
/* 290 */ ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* scanTime */).add(scan_scanTime / (1000 * 1000));
/* 291 */ scan_scanTime = 0;
/* 292 */ }
/* 293 */
/* 294 */ @Override
/* 295 */ protected boolean stopEarly() {
/* 296 */ return locallimit_stopEarly;
/* 297 */ }
/* 298 */
/* 299 */ }
{code}
 

 

 ;;;","06/Dec/18 10:56;mgaido;[~chamcyl] could you try 2.3.2 please? If it fails on 2.3.2, can you please create a new JIRA and provide a reproducer? Thanks.;;;","06/Dec/18 11:21;chamcyl;[~mgaido]  it work well on 2.3.2 .    It should be a good idea to upgrade to this stable version.Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingQueryListener.QueryStartedEvent is not written to event log,SPARK-18144,13015950,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,codingcat,zsxwing,zsxwing,27/Oct/16 19:53,02/Nov/16 06:41,14/Jul/23 06:29,02/Nov/16 06:41,2.0.0,2.0.1,2.0.2,,,,,,2.0.3,2.1.0,,,Structured Streaming,,,,,,,,0,,,,,,,,apachespark,codingcat,lwlin,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 28 20:43:10 UTC 2016,,,,,,,,,,"0|i35idr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/16 20:43;apachespark;User 'CodingCat' has created a pull request for this issue:
https://github.com/apache/spark/pull/15675;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"jdbc datasource read fails when  quoted  columns (eg:mixed case, reserved words) in source table are used  in the filter.",SPARK-18141,13015920,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tsuresh,tsuresh,tsuresh,27/Oct/16 18:16,07/Jun/17 08:20,14/Jul/23 06:29,02/Dec/16 03:14,2.0.0,2.0.1,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"create table t1(""Name"" text, ""Id"" integer)
insert into t1 values('Mike', 1)

val df = sqlContext.read.jdbc(jdbcUrl, ""t1"", new Properties) 
df.filter(""Id = 1"").show() 
df.filter(""`Id` = 1"").show()


Error :
Cause: org.postgresql.util.PSQLException: ERROR: column ""id"" does not exist
  Position: 35
  at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2182)
  at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1911)
  at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:173)
  at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:622)
  at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:472)
  at org.postgresql.jdbc.PgStatement.executeQuery(PgStatement.java:386)
  at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:295)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)

I am working on fix for this issue, will submit PR soon.",,apachespark,smilegator,tsuresh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-14460,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 02 03:14:25 UTC 2016,,,,,,,,,,"0|i35i73:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/16 18:43;apachespark;User 'sureshthalamati' has created a pull request for this issue:
https://github.com/apache/spark/pull/15662;;;","02/Dec/16 03:14;smilegator;Issue resolved by pull request 15662
[https://github.com/apache/spark/pull/15662];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RewriteDistinctAggregates UnresolvedException when a UDAF has a foldable TypeCheck,SPARK-18137,13015742,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,windpiger,windpiger,windpiger,27/Oct/16 09:00,19/Nov/16 09:04,14/Jul/23 06:29,08/Nov/16 11:11,2.1.0,,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,,,,,,"when run a sql with distinct(on spark github master branch), it throw UnresolvedException.

For example:
run a test case on spark(branch master)  with sql:
{noformat}
SELECT percentile_approx(key, 0.99999), count(distinct key),sum(distinct key) FROM src LIMIT 1
{noformat}
and it throw exception:
{noformat}
org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to dataType on unresolved object, tree: 'percentile_approx(CAST(src.`key` AS DOUBLE), CAST(0.99999BD AS DOUBLE), 10000)
	at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.dataType(unresolved.scala:92)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$.org$apache$spark$sql$catalyst$optimizer$RewriteDistinctAggregates$$nullify(RewriteDistinctAggregates.scala:261)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$.org$apache$spark$sql$catalyst$optimizer$RewriteDistinctAggregates$$evalWithinGroup$1(RewriteDistinctAggregates.scala:136)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$$anonfun$16.apply(RewriteDistinctAggregates.scala:187)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$$anonfun$16.apply(RewriteDistinctAggregates.scala:180)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$.rewrite(RewriteDistinctAggregates.scala:180)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$$anonfun$apply$1.applyOrElse(RewriteDistinctAggregates.scala:105)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$$anonfun$apply$1.applyOrElse(RewriteDistinctAggregates.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:301)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:301)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$.apply(RewriteDistinctAggregates.scala:104)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$.apply(RewriteDistinctAggregates.scala:102)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
	at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)
	at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:83)
	at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2572)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:1934)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2149)
	at com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:76)
	at com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:42)
	at com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:100)
	at com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:128)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$1.apply(DriverLocal.scala:202)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$1.apply(DriverLocal.scala:191)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:145)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:140)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:34)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:178)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:34)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:191)
	at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:584)
	at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:584)
	at scala.util.Try$.apply(Try.scala:192)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:579)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:491)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:394)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:351)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:217)
	at java.lang.Thread.run(Thread.java:745)
{noformat}",,apachespark,codingcat,hvanhovell,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-14171,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 08 18:26:09 UTC 2016,,,,,,,,,,"0|i35h3r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/16 03:41;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/15668;;;","08/Nov/16 18:03;hvanhovell;[~srowen] How did you set the Assignee? I spend a while on this, but could not figure out how to do it.;;;","08/Nov/16 18:07;srowen;Oh just had to add the person to the ""Contributor"" role at https://issues.apache.org/jira/plugins/servlet/project-config/SPARK/roles
... and so I made you a JIRA admin so you can edit that too. ;;;","08/Nov/16 18:26;hvanhovell;Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python ML Pipeline Example has syntax errors,SPARK-18133,13015687,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,as2,nirmal,nirmal,27/Oct/16 06:29,02/Nov/16 09:24,14/Jul/23 06:29,28/Oct/16 09:35,2.0.1,,,,,,,,2.0.3,2.1.0,,,Examples,ML,,,,,,,0,easyfix,,,,,"$ ./bin/spark-submit examples/src/main/python/ml/pipeline_example.py
  File ""/spark-2.0.0-bin-hadoop2.7/examples/src/main/python/ml/pipeline_example.py"", line 38
    (0L, ""a b c d e spark"", 1.0),
      ^
SyntaxError: invalid syntax

Removing 'L' from all occurrences resolves the issue.",OS X,apachespark,as2,nirmal,wm624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 02 05:55:06 UTC 2016,,,,,,,,,,"0|i35grj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/16 07:06;wm624;Python 2.7.11 |Anaconda 2.4.0 (x86_64)| (default, Dec  6 2015, 18:57:58)

MacBook Pro (Retina, 15-inch, Mid 2015)

It works fine.;;;","27/Oct/16 07:09;wm624;Use Pyspark:

>>> training = spark.createDataFrame([
...         (0L, ""a b c d e spark"", 1.0),
...         (1L, ""b d"", 0.0),
...         (2L, ""spark f g h"", 1.0),
...         (3L, ""hadoop mapreduce"", 0.0)
...     ], [""id"", ""text"", ""label""])
>>> training
DataFrame[id: bigint, text: string, label: double]
There is no such error.;;;","27/Oct/16 07:11;srowen;He must be using Python 3, which no longer has long vs int, and which won't accept ""0L"". I think we just need to remove ""L"" in all examples and it will work on 2 or 3.;;;","27/Oct/16 08:04;as2;Started working on this.;;;","27/Oct/16 09:21;apachespark;User 'jagadeesanas2' has created a pull request for this issue:
https://github.com/apache/spark/pull/15660;;;","02/Nov/16 04:16;apachespark;User 'jagadeesanas2' has created a pull request for this issue:
https://github.com/apache/spark/pull/15728;;;","02/Nov/16 04:23;nirmal;Thanks All.;;;","02/Nov/16 05:55;apachespark;User 'jagadeesanas2' has created a pull request for this issue:
https://github.com/apache/spark/pull/15729;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark 2.0 branch's spark-release-publish failed because style check failed.,SPARK-18132,13015675,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,yhuai,yhuai,27/Oct/16 05:10,01/Nov/16 20:25,14/Jul/23 06:29,27/Oct/16 05:24,,,,,,,,,2.0.2,2.1.0,,,Build,,,,,,,,0,,,,,,"[ERROR] src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java:[147] (sizes) LineLength: Line is longer than 100 characters (found 110).
[WARNING] checkstyle:check violations detected but failOnViolation set to false",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 27 05:24:16 UTC 2016,,,,,,,,,,"0|i35gov:",9223372036854775807,,,,,,,,,,,,,2.0.3,2.1.0,,,,,,,,,,"27/Oct/16 05:12;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/15656;;;","27/Oct/16 05:24;yhuai;Issue resolved by pull request 15656
[https://github.com/apache/spark/pull/15656];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spark generated code causes CompileException when groupByKey, reduceGroups and map(_._2) are used",SPARK-18125,13015605,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,viirya,rayqiu,rayqiu,26/Oct/16 22:16,08/Dec/16 23:00,14/Jul/23 06:29,07/Nov/16 11:19,2.0.1,,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,1,,,,,,"Code logic looks like this:
{noformat}
            .groupByKey
            .reduceGroups
            .map(_._2)
{noformat}
Works fine with 2.0.0.

2.0.1 error Message: 
{noformat}
Caused by: java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 206, Column 123: Unknown variable or type ""value4""
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificMutableProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificMutableProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private java.lang.String errMsg;
/* 011 */   private java.lang.String errMsg1;
/* 012 */   private boolean MapObjects_loopIsNull1;
/* 013 */   private io.mistnet.analytics.lib.ConnLog MapObjects_loopValue0;
/* 014 */   private java.lang.String errMsg2;
/* 015 */   private Object[] values1;
/* 016 */   private boolean MapObjects_loopIsNull3;
/* 017 */   private java.lang.String MapObjects_loopValue2;
/* 018 */   private boolean isNull_0;
/* 019 */   private boolean value_0;
/* 020 */   private boolean isNull_1;
/* 021 */   private InternalRow value_1;
/* 022 */
/* 023 */   private void apply_4(InternalRow i) {
/* 024 */
/* 025 */     boolean isNull52 = MapObjects_loopIsNull1;
/* 026 */     final double value52 = isNull52 ? -1.0 : MapObjects_loopValue0.ts();
/* 027 */     if (isNull52) {
/* 028 */       values1[8] = null;
/* 029 */     } else {
/* 030 */       values1[8] = value52;
/* 031 */     }
/* 032 */     boolean isNull54 = MapObjects_loopIsNull1;
/* 033 */     final java.lang.String value54 = isNull54 ? null : (java.lang.String) MapObjects_loopValue0.uid();
/* 034 */     isNull54 = value54 == null;
/* 035 */     boolean isNull53 = isNull54;
/* 036 */     final UTF8String value53 = isNull53 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value54);
/* 037 */     isNull53 = value53 == null;
/* 038 */     if (isNull53) {
/* 039 */       values1[9] = null;
/* 040 */     } else {
/* 041 */       values1[9] = value53;
/* 042 */     }
/* 043 */     boolean isNull56 = MapObjects_loopIsNull1;
/* 044 */     final java.lang.String value56 = isNull56 ? null : (java.lang.String) MapObjects_loopValue0.src();
/* 045 */     isNull56 = value56 == null;
/* 046 */     boolean isNull55 = isNull56;
/* 047 */     final UTF8String value55 = isNull55 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value56);
/* 048 */     isNull55 = value55 == null;
/* 049 */     if (isNull55) {
/* 050 */       values1[10] = null;
/* 051 */     } else {
/* 052 */       values1[10] = value55;
/* 053 */     }
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */   private void apply_7(InternalRow i) {
/* 058 */
/* 059 */     boolean isNull69 = MapObjects_loopIsNull1;
/* 060 */     final scala.Option value69 = isNull69 ? null : (scala.Option) MapObjects_loopValue0.orig_bytes();
/* 061 */     isNull69 = value69 == null;
/* 062 */
/* 063 */     final boolean isNull68 = isNull69 || value69.isEmpty();
/* 064 */     long value68 = isNull68 ?
/* 065 */     -1L : (Long) value69.get();
/* 066 */     if (isNull68) {
/* 067 */       values1[17] = null;
/* 068 */     } else {
/* 069 */       values1[17] = value68;
/* 070 */     }
/* 071 */     boolean isNull71 = MapObjects_loopIsNull1;
/* 072 */     final scala.Option value71 = isNull71 ? null : (scala.Option) MapObjects_loopValue0.resp_bytes();
/* 073 */     isNull71 = value71 == null;
/* 074 */
/* 075 */     final boolean isNull70 = isNull71 || value71.isEmpty();
/* 076 */     long value70 = isNull70 ?
/* 077 */     -1L : (Long) value71.get();
/* 078 */     if (isNull70) {
/* 079 */       values1[18] = null;
/* 080 */     } else {
/* 081 */       values1[18] = value70;
/* 082 */     }
/* 083 */     boolean isNull74 = MapObjects_loopIsNull1;
/* 084 */     final scala.Option value74 = isNull74 ? null : (scala.Option) MapObjects_loopValue0.conn_state();
/* 085 */     isNull74 = value74 == null;
/* 086 */
/* 087 */     final boolean isNull73 = isNull74 || value74.isEmpty();
/* 088 */     java.lang.String value73 = isNull73 ?
/* 089 */     null : (java.lang.String) value74.get();
/* 090 */     boolean isNull72 = isNull73;
/* 091 */     final UTF8String value72 = isNull72 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value73);
/* 092 */     isNull72 = value72 == null;
/* 093 */     if (isNull72) {
/* 094 */       values1[19] = null;
/* 095 */     } else {
/* 096 */       values1[19] = value72;
/* 097 */     }
/* 098 */   }
/* 099 */
/* 100 */
/* 101 */   private void apply_1(InternalRow i) {
/* 102 */
/* 103 */     boolean isNull37 = MapObjects_loopIsNull1;
/* 104 */     final scala.Option value37 = isNull37 ? null : (scala.Option) MapObjects_loopValue0.sensor_name();
/* 105 */     isNull37 = value37 == null;
/* 106 */
/* 107 */     final boolean isNull36 = isNull37 || value37.isEmpty();
/* 108 */     java.lang.String value36 = isNull36 ?
/* 109 */     null : (java.lang.String) value37.get();
/* 110 */     boolean isNull35 = isNull36;
/* 111 */     final UTF8String value35 = isNull35 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value36);
/* 112 */     isNull35 = value35 == null;
/* 113 */     if (isNull35) {
/* 114 */       values1[2] = null;
/* 115 */     } else {
/* 116 */       values1[2] = value35;
/* 117 */     }
/* 118 */     boolean isNull40 = MapObjects_loopIsNull1;
/* 119 */     final scala.Option value40 = isNull40 ? null : (scala.Option) MapObjects_loopValue0.ioa_uuid();
/* 120 */     isNull40 = value40 == null;
/* 121 */
/* 122 */     final boolean isNull39 = isNull40 || value40.isEmpty();
/* 123 */     java.lang.String value39 = isNull39 ?
/* 124 */     null : (java.lang.String) value40.get();
/* 125 */     boolean isNull38 = isNull39;
/* 126 */     final UTF8String value38 = isNull38 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value39);
/* 127 */     isNull38 = value38 == null;
/* 128 */     if (isNull38) {
/* 129 */       values1[3] = null;
/* 130 */     } else {
/* 131 */       values1[3] = value38;
/* 132 */     }
/* 133 */   }
/* 134 */
/* 135 */
/* 136 */   private void apply_12(InternalRow i) {
/* 137 */
/* 138 */     boolean isNull98 = MapObjects_loopIsNull1;
/* 139 */     final scala.Option value98 = isNull98 ? null : (scala.Option) MapObjects_loopValue0.cc();
/* 140 */     isNull98 = value98 == null;
/* 141 */
/* 142 */     final boolean isNull97 = isNull98 || value98.isEmpty();
/* 143 */     java.lang.String value97 = isNull97 ?
/* 144 */     null : (java.lang.String) value98.get();
/* 145 */     boolean isNull96 = isNull97;
/* 146 */     final UTF8String value96 = isNull96 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value97);
/* 147 */     isNull96 = value96 == null;
/* 148 */     if (isNull96) {
/* 149 */       values1[29] = null;
/* 150 */     } else {
/* 151 */       values1[29] = value96;
/* 152 */     }
/* 153 */     boolean isNull101 = MapObjects_loopIsNull1;
/* 154 */     final scala.Option value101 = isNull101 ? null : (scala.Option) MapObjects_loopValue0.location();
/* 155 */     isNull101 = value101 == null;
/* 156 */
/* 157 */     final boolean isNull100 = isNull101 || value101.isEmpty();
/* 158 */     java.lang.String value100 = isNull100 ?
/* 159 */     null : (java.lang.String) value101.get();
/* 160 */     boolean isNull99 = isNull100;
/* 161 */     final UTF8String value99 = isNull99 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value100);
/* 162 */     isNull99 = value99 == null;
/* 163 */     if (isNull99) {
/* 164 */       values1[30] = null;
/* 165 */     } else {
/* 166 */       values1[30] = value99;
/* 167 */     }
/* 168 */   }
/* 169 */
/* 170 */
/* 171 */   private void apply_9(InternalRow i) {
/* 172 */
/* 173 */     boolean isNull83 = MapObjects_loopIsNull1;
/* 174 */     final scala.Option value83 = isNull83 ? null : (scala.Option) MapObjects_loopValue0.history();
/* 175 */     isNull83 = value83 == null;
/* 176 */
/* 177 */     final boolean isNull82 = isNull83 || value83.isEmpty();
/* 178 */     java.lang.String value82 = isNull82 ?
/* 179 */     null : (java.lang.String) value83.get();
/* 180 */     boolean isNull81 = isNull82;
/* 181 */     final UTF8String value81 = isNull81 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value82);
/* 182 */     isNull81 = value81 == null;
/* 183 */     if (isNull81) {
/* 184 */       values1[23] = null;
/* 185 */     } else {
/* 186 */       values1[23] = value81;
/* 187 */     }
/* 188 */     boolean isNull85 = MapObjects_loopIsNull1;
/* 189 */     final scala.Option value85 = isNull85 ? null : (scala.Option) MapObjects_loopValue0.orig_pkts();
/* 190 */     isNull85 = value85 == null;
/* 191 */
/* 192 */     final boolean isNull84 = isNull85 || value85.isEmpty();
/* 193 */     long value84 = isNull84 ?
/* 194 */     -1L : (Long) value85.get();
/* 195 */     if (isNull84) {
/* 196 */       values1[24] = null;
/* 197 */     } else {
/* 198 */       values1[24] = value84;
/* 199 */     }
/* 200 */   }
/* 201 */
/* 202 */
/* 203 */   private void apply1_1(InternalRow i) {
/* 204 */
/* 205 */     boolean isNull25 = false;
/* 206 */     final io.mistnet.analytics.scan.SrcDstGrouped value25 = isNull25 ? null : (io.mistnet.analytics.scan.SrcDstGrouped) value4._2();
/* 207 */     isNull25 = value25 == null;
/* 208 */
/* 209 */     if (isNull25) {
/* 210 */       throw new RuntimeException(errMsg2);
/* 211 */     }
/* 212 */
/* 213 */     boolean isNull23 = false;
/* 214 */     final scala.collection.Seq value23 = isNull23 ? null : (scala.collection.Seq) value25.cs();
/* 215 */     isNull23 = value23 == null;
/* 216 */     ArrayData value22 = null;
/* 217 */
/* 218 */     if (!isNull23) {
/* 219 */
/* 220 */       InternalRow[] convertedArray1 = null;
/* 221 */       int dataLength1 = value23.size();
/* 222 */       convertedArray1 = new InternalRow[dataLength1];
/* 223 */
/* 224 */       int loopIndex1 = 0;
/* 225 */       while (loopIndex1 < dataLength1) {
/* 226 */         MapObjects_loopValue0 = (io.mistnet.analytics.lib.ConnLog) (value23.apply(loopIndex1));
/* 227 */         MapObjects_loopIsNull1 = MapObjects_loopValue0 == null;
/* 228 */
/* 229 */
/* 230 */         boolean isNull26 = false;
/* 231 */         InternalRow value26 = null;
/* 232 */         if (!false && MapObjects_loopIsNull1) {
/* 233 */
/* 234 */           final InternalRow value28 = null;
/* 235 */           isNull26 = true;
/* 236 */           value26 = value28;
/* 237 */         } else {
/* 238 */
/* 239 */           boolean isNull29 = false;
/* 240 */           values1 = new Object[31];apply_0(i);
/* 241 */           apply_1(i);
/* 242 */           apply_2(i);
/* 243 */           apply_3(i);
/* 244 */           apply_4(i);
/* 245 */           apply_5(i);
/* 246 */           apply_6(i);
/* 247 */           apply_7(i);
/* 248 */           apply_8(i);
/* 249 */           apply_9(i);
/* 250 */           apply_10(i);
/* 251 */           apply_11(i);
/* 252 */           apply_12(i);
/* 253 */           final InternalRow value29 = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(values1);
/* 254 */           this.values1 = null;
/* 255 */           isNull26 = isNull29;
/* 256 */           value26 = value29;
/* 257 */         }
/* 258 */         if (isNull26) {
/* 259 */           convertedArray1[loopIndex1] = null;
/* 260 */         } else {
/* 261 */           convertedArray1[loopIndex1] = value26 instanceof UnsafeRow? value26.copy() : value26;
/* 262 */         }
/* 263 */
/* 264 */         loopIndex1 += 1;
/* 265 */       }
/* 266 */
/* 267 */       value22 = new org.apache.spark.sql.catalyst.util.GenericArrayData(convertedArray1);
/* 268 */     }
/* 269 */     if (isNull23) {
/* 270 */       values[2] = null;
/* 271 */     } else {
/* 272 */       values[2] = value22;
/* 273 */     }
/* 274 */   }
/* 275 */
/* 276 */
/* 277 */   private void apply_3(InternalRow i) {
/* 278 */
/* 279 */     boolean isNull49 = MapObjects_loopIsNull1;
/* 280 */     final scala.Option value49 = isNull49 ? null : (scala.Option) MapObjects_loopValue0.date();
/* 281 */     isNull49 = value49 == null;
/* 282 */
/* 283 */     final boolean isNull48 = isNull49 || value49.isEmpty();
/* 284 */     java.lang.String value48 = isNull48 ?
/* 285 */     null : (java.lang.String) value49.get();
/* 286 */     boolean isNull47 = isNull48;
/* 287 */     final UTF8String value47 = isNull47 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value48);
/* 288 */     isNull47 = value47 == null;
/* 289 */     if (isNull47) {
/* 290 */       values1[6] = null;
/* 291 */     } else {
/* 292 */       values1[6] = value47;
/* 293 */     }
/* 294 */     boolean isNull51 = MapObjects_loopIsNull1;
/* 295 */     final scala.Option value51 = isNull51 ? null : (scala.Option) MapObjects_loopValue0.hour();
/* 296 */     isNull51 = value51 == null;
/* 297 */
/* 298 */     final boolean isNull50 = isNull51 || value51.isEmpty();
/* 299 */     int value50 = isNull50 ?
/* 300 */     -1 : (Integer) value51.get();
/* 301 */     if (isNull50) {
/* 302 */       values1[7] = null;
/* 303 */     } else {
/* 304 */       values1[7] = value50;
/* 305 */     }
/* 306 */   }
/* 307 */
/* 308 */
/* 309 */   private void apply_6(InternalRow i) {
/* 310 */
/* 311 */     boolean isNull65 = MapObjects_loopIsNull1;
/* 312 */     final scala.Option value65 = isNull65 ? null : (scala.Option) MapObjects_loopValue0.service();
/* 313 */     isNull65 = value65 == null;
/* 314 */
/* 315 */     final boolean isNull64 = isNull65 || value65.isEmpty();
/* 316 */     java.lang.String value64 = isNull64 ?
/* 317 */     null : (java.lang.String) value65.get();
/* 318 */     boolean isNull63 = isNull64;
/* 319 */     final UTF8String value63 = isNull63 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value64);
/* 320 */     isNull63 = value63 == null;
/* 321 */     if (isNull63) {
/* 322 */       values1[15] = null;
/* 323 */     } else {
/* 324 */       values1[15] = value63;
/* 325 */     }
/* 326 */     boolean isNull67 = MapObjects_loopIsNull1;
/* 327 */     final scala.Option value67 = isNull67 ? null : (scala.Option) MapObjects_loopValue0.duration();
/* 328 */     isNull67 = value67 == null;
/* 329 */
/* 330 */     final boolean isNull66 = isNull67 || value67.isEmpty();
/* 331 */     double value66 = isNull66 ?
/* 332 */     -1.0 : (Double) value67.get();
/* 333 */     if (isNull66) {
/* 334 */       values1[16] = null;
/* 335 */     } else {
/* 336 */       values1[16] = value66;
/* 337 */     }
/* 338 */   }
/* 339 */
/* 340 */
/* 341 */   private void apply_0(InternalRow i) {
/* 342 */
/* 343 */     boolean isNull32 = MapObjects_loopIsNull1;
/* 344 */     final scala.Option value32 = isNull32 ? null : (scala.Option) MapObjects_loopValue0.log_type();
/* 345 */     isNull32 = value32 == null;
/* 346 */
/* 347 */     final boolean isNull31 = isNull32 || value32.isEmpty();
/* 348 */     java.lang.String value31 = isNull31 ?
/* 349 */     null : (java.lang.String) value32.get();
/* 350 */     boolean isNull30 = isNull31;
/* 351 */     final UTF8String value30 = isNull30 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value31);
/* 352 */     isNull30 = value30 == null;
/* 353 */     if (isNull30) {
/* 354 */       values1[0] = null;
/* 355 */     } else {
/* 356 */       values1[0] = value30;
/* 357 */     }
/* 358 */     boolean isNull34 = MapObjects_loopIsNull1;
/* 359 */     final scala.Option value34 = isNull34 ? null : (scala.Option) MapObjects_loopValue0.timestamp();
/* 360 */     isNull34 = value34 == null;
/* 361 */
/* 362 */     final boolean isNull33 = isNull34 || value34.isEmpty();
/* 363 */     long value33 = isNull33 ?
/* 364 */     -1L : (Long) value34.get();
/* 365 */     if (isNull33) {
/* 366 */       values1[1] = null;
/* 367 */     } else {
/* 368 */       values1[1] = value33;
/* 369 */     }
/* 370 */   }
/* 371 */
/* 372 */
/* 373 */   private void apply_11(InternalRow i) {
/* 374 */
/* 375 */     boolean isNull94 = MapObjects_loopIsNull1;
/* 376 */     final scala.Option value94 = isNull94 ? null : (scala.Option) MapObjects_loopValue0.tunnel_parents();
/* 377 */     isNull94 = value94 == null;
/* 378 */
/* 379 */     final boolean isNull93 = isNull94 || value94.isEmpty();
/* 380 */     scala.collection.Seq value93 = isNull93 ?
/* 381 */     null : (scala.collection.Seq) value94.get();
/* 382 */     ArrayData value92 = null;
/* 383 */
/* 384 */     if (!isNull93) {
/* 385 */
/* 386 */       UTF8String[] convertedArray = null;
/* 387 */       int dataLength = value93.size();
/* 388 */       convertedArray = new UTF8String[dataLength];
/* 389 */
/* 390 */       int loopIndex = 0;
/* 391 */       while (loopIndex < dataLength) {
/* 392 */         MapObjects_loopValue2 = (java.lang.String) (value93.apply(loopIndex));
/* 393 */         MapObjects_loopIsNull3 = MapObjects_loopValue2 == null;
/* 394 */
/* 395 */
/* 396 */         boolean isNull95 = MapObjects_loopIsNull3;
/* 397 */         final UTF8String value95 = isNull95 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(MapObjects_loopValue2);
/* 398 */         isNull95 = value95 == null;
/* 399 */         if (isNull95) {
/* 400 */           convertedArray[loopIndex] = null;
/* 401 */         } else {
/* 402 */           convertedArray[loopIndex] = value95;
/* 403 */         }
/* 404 */
/* 405 */         loopIndex += 1;
/* 406 */       }
/* 407 */
/* 408 */       value92 = new org.apache.spark.sql.catalyst.util.GenericArrayData(convertedArray);
/* 409 */     }
/* 410 */     if (isNull93) {
/* 411 */       values1[28] = null;
/* 412 */     } else {
/* 413 */       values1[28] = value92;
/* 414 */     }
/* 415 */   }
/* 416 */
/* 417 */
/* 418 */   private void apply_8(InternalRow i) {
/* 419 */
/* 420 */     boolean isNull76 = MapObjects_loopIsNull1;
/* 421 */     final scala.Option value76 = isNull76 ? null : (scala.Option) MapObjects_loopValue0.local_orig();
/* 422 */     isNull76 = value76 == null;
/* 423 */
/* 424 */     final boolean isNull75 = isNull76 || value76.isEmpty();
/* 425 */     boolean value75 = isNull75 ?
/* 426 */     false : (Boolean) value76.get();
/* 427 */     if (isNull75) {
/* 428 */       values1[20] = null;
/* 429 */     } else {
/* 430 */       values1[20] = value75;
/* 431 */     }
/* 432 */     boolean isNull78 = MapObjects_loopIsNull1;
/* 433 */     final scala.Option value78 = isNull78 ? null : (scala.Option) MapObjects_loopValue0.local_resp();
/* 434 */     isNull78 = value78 == null;
/* 435 */
/* 436 */     final boolean isNull77 = isNull78 || value78.isEmpty();
/* 437 */     boolean value77 = isNull77 ?
/* 438 */     false : (Boolean) value78.get();
/* 439 */     if (isNull77) {
/* 440 */       values1[21] = null;
/* 441 */     } else {
/* 442 */       values1[21] = value77;
/* 443 */     }
/* 444 */     boolean isNull80 = MapObjects_loopIsNull1;
/* 445 */     final scala.Option value80 = isNull80 ? null : (scala.Option) MapObjects_loopValue0.missed_bytes();
/* 446 */     isNull80 = value80 == null;
/* 447 */
/* 448 */     final boolean isNull79 = isNull80 || value80.isEmpty();
/* 449 */     long value79 = isNull79 ?
/* 450 */     -1L : (Long) value80.get();
/* 451 */     if (isNull79) {
/* 452 */       values1[22] = null;
/* 453 */     } else {
/* 454 */       values1[22] = value79;
/* 455 */     }
/* 456 */   }
/* 457 */
/* 458 */
/* 459 */   private void apply1_0(InternalRow i) {
/* 460 */
/* 461 */     boolean isNull17 = false;
/* 462 */     final io.mistnet.analytics.scan.SrcDstGrouped value17 = isNull17 ? null : (io.mistnet.analytics.scan.SrcDstGrouped) value4._2();
/* 463 */     isNull17 = value17 == null;
/* 464 */
/* 465 */     if (isNull17) {
/* 466 */       throw new RuntimeException(errMsg);
/* 467 */     }
/* 468 */
/* 469 */     boolean isNull15 = false;
/* 470 */     final java.lang.String value15 = isNull15 ? null : (java.lang.String) value17.src();
/* 471 */     isNull15 = value15 == null;
/* 472 */     boolean isNull14 = isNull15;
/* 473 */     final UTF8String value14 = isNull14 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value15);
/* 474 */     isNull14 = value14 == null;
/* 475 */     if (isNull14) {
/* 476 */       values[0] = null;
/* 477 */     } else {
/* 478 */       values[0] = value14;
/* 479 */     }
/* 480 */     boolean isNull21 = false;
/* 481 */     final io.mistnet.analytics.scan.SrcDstGrouped value21 = isNull21 ? null : (io.mistnet.analytics.scan.SrcDstGrouped) value4._2();
/* 482 */     isNull21 = value21 == null;
/* 483 */
/* 484 */     if (isNull21) {
/* 485 */       throw new RuntimeException(errMsg1);
/* 486 */     }
/* 487 */
/* 488 */     boolean isNull19 = false;
/* 489 */     final java.lang.String value19 = isNull19 ? null : (java.lang.String) value21.dest();
/* 490 */     isNull19 = value19 == null;
/* 491 */     boolean isNull18 = isNull19;
/* 492 */     final UTF8String value18 = isNull18 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value19);
/* 493 */     isNull18 = value18 == null;
/* 494 */     if (isNull18) {
/* 495 */       values[1] = null;
/* 496 */     } else {
/* 497 */       values[1] = value18;
/* 498 */     }
/* 499 */   }
/* 500 */
/* 501 */
/* 502 */   private void apply_2(InternalRow i) {
/* 503 */
/* 504 */     boolean isNull43 = MapObjects_loopIsNull1;
/* 505 */     final scala.Option value43 = isNull43 ? null : (scala.Option) MapObjects_loopValue0.user_uuid();
/* 506 */     isNull43 = value43 == null;
/* 507 */
/* 508 */     final boolean isNull42 = isNull43 || value43.isEmpty();
/* 509 */     java.lang.String value42 = isNull42 ?
/* 510 */     null : (java.lang.String) value43.get();
/* 511 */     boolean isNull41 = isNull42;
/* 512 */     final UTF8String value41 = isNull41 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value42);
/* 513 */     isNull41 = value41 == null;
/* 514 */     if (isNull41) {
/* 515 */       values1[4] = null;
/* 516 */     } else {
/* 517 */       values1[4] = value41;
/* 518 */     }
/* 519 */     boolean isNull46 = MapObjects_loopIsNull1;
/* 520 */     final scala.Option value46 = isNull46 ? null : (scala.Option) MapObjects_loopValue0.host_uuid();
/* 521 */     isNull46 = value46 == null;
/* 522 */
/* 523 */     final boolean isNull45 = isNull46 || value46.isEmpty();
/* 524 */     java.lang.String value45 = isNull45 ?
/* 525 */     null : (java.lang.String) value46.get();
/* 526 */     boolean isNull44 = isNull45;
/* 527 */     final UTF8String value44 = isNull44 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value45);
/* 528 */     isNull44 = value44 == null;
/* 529 */     if (isNull44) {
/* 530 */       values1[5] = null;
/* 531 */     } else {
/* 532 */       values1[5] = value44;
/* 533 */     }
/* 534 */   }
/* 535 */
/* 536 */
/* 537 */   private void apply_5(InternalRow i) {
/* 538 */
/* 539 */     boolean isNull57 = MapObjects_loopIsNull1;
/* 540 */     final int value57 = isNull57 ? -1 : MapObjects_loopValue0.src_port();
/* 541 */     if (isNull57) {
/* 542 */       values1[11] = null;
/* 543 */     } else {
/* 544 */       values1[11] = value57;
/* 545 */     }
/* 546 */     boolean isNull59 = MapObjects_loopIsNull1;
/* 547 */     final java.lang.String value59 = isNull59 ? null : (java.lang.String) MapObjects_loopValue0.dest();
/* 548 */     isNull59 = value59 == null;
/* 549 */     boolean isNull58 = isNull59;
/* 550 */     final UTF8String value58 = isNull58 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value59);
/* 551 */     isNull58 = value58 == null;
/* 552 */     if (isNull58) {
/* 553 */       values1[12] = null;
/* 554 */     } else {
/* 555 */       values1[12] = value58;
/* 556 */     }
/* 557 */     boolean isNull60 = MapObjects_loopIsNull1;
/* 558 */     final int value60 = isNull60 ? -1 : MapObjects_loopValue0.dest_port();
/* 559 */     if (isNull60) {
/* 560 */       values1[13] = null;
/* 561 */     } else {
/* 562 */       values1[13] = value60;
/* 563 */     }
/* 564 */     boolean isNull62 = MapObjects_loopIsNull1;
/* 565 */     final java.lang.String value62 = isNull62 ? null : (java.lang.String) MapObjects_loopValue0.proto();
/* 566 */     isNull62 = value62 == null;
/* 567 */     boolean isNull61 = isNull62;
/* 568 */     final UTF8String value61 = isNull61 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value62);
/* 569 */     isNull61 = value61 == null;
/* 570 */     if (isNull61) {
/* 571 */       values1[14] = null;
/* 572 */     } else {
/* 573 */       values1[14] = value61;
/* 574 */     }
/* 575 */   }
/* 576 */
/* 577 */
/* 578 */   private void apply_10(InternalRow i) {
/* 579 */
/* 580 */     boolean isNull87 = MapObjects_loopIsNull1;
/* 581 */     final scala.Option value87 = isNull87 ? null : (scala.Option) MapObjects_loopValue0.orig_ip_bytes();
/* 582 */     isNull87 = value87 == null;
/* 583 */
/* 584 */     final boolean isNull86 = isNull87 || value87.isEmpty();
/* 585 */     long value86 = isNull86 ?
/* 586 */     -1L : (Long) value87.get();
/* 587 */     if (isNull86) {
/* 588 */       values1[25] = null;
/* 589 */     } else {
/* 590 */       values1[25] = value86;
/* 591 */     }
/* 592 */     boolean isNull89 = MapObjects_loopIsNull1;
/* 593 */     final scala.Option value89 = isNull89 ? null : (scala.Option) MapObjects_loopValue0.resp_pkts();
/* 594 */     isNull89 = value89 == null;
/* 595 */
/* 596 */     final boolean isNull88 = isNull89 || value89.isEmpty();
/* 597 */     long value88 = isNull88 ?
/* 598 */     -1L : (Long) value89.get();
/* 599 */     if (isNull88) {
/* 600 */       values1[26] = null;
/* 601 */     } else {
/* 602 */       values1[26] = value88;
/* 603 */     }
/* 604 */     boolean isNull91 = MapObjects_loopIsNull1;
/* 605 */     final scala.Option value91 = isNull91 ? null : (scala.Option) MapObjects_loopValue0.resp_ip_bytes();
/* 606 */     isNull91 = value91 == null;
/* 607 */
/* 608 */     final boolean isNull90 = isNull91 || value91.isEmpty();
/* 609 */     long value90 = isNull90 ?
/* 610 */     -1L : (Long) value91.get();
/* 611 */     if (isNull90) {
/* 612 */       values1[27] = null;
/* 613 */     } else {
/* 614 */       values1[27] = value90;
/* 615 */     }
/* 616 */   }
/* 617 */
/* 618 */
/* 619 */   public SpecificMutableProjection(Object[] references) {
/* 620 */     this.references = references;
/* 621 */     mutableRow = new org.apache.spark.sql.catalyst.expressions.GenericMutableRow(2);
/* 622 */     this.values = null;
/* 623 */     this.errMsg = (java.lang.String) references[3];
/* 624 */     this.errMsg1 = (java.lang.String) references[4];
/* 625 */
/* 626 */
/* 627 */     this.errMsg2 = (java.lang.String) references[5];
/* 628 */     this.values1 = null;
/* 629 */
/* 630 */
/* 631 */     this.isNull_0 = true;
/* 632 */     this.value_0 = false;
/* 633 */     this.isNull_1 = true;
/* 634 */     this.value_1 = null;
/* 635 */   }
/* 636 */
/* 637 */   public org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection target(MutableRow row) {
/* 638 */     mutableRow = row;
/* 639 */     return this;
/* 640 */   }
/* 641 */
/* 642 */   /* Provide immutable access to the last projected row. */
/* 643 */   public InternalRow currentValue() {
/* 644 */     return (InternalRow) mutableRow;
/* 645 */   }
/* 646 */
/* 647 */   public java.lang.Object apply(java.lang.Object _i) {
/* 648 */     InternalRow i = (InternalRow) _i;
/* 649 */
/* 650 */
/* 651 */
/* 652 */     Object obj = ((Expression) references[0]).eval(null);
/* 653 */     scala.Tuple2 value1 = (scala.Tuple2) obj;
/* 654 */
/* 655 */     boolean isNull2 = false;
/* 656 */     final boolean value2 = isNull2 ? false : (Boolean) value1._1();
/* 657 */     this.isNull_0 = isNull2;
/* 658 */     this.value_0 = value2;
/* 659 */
/* 660 */
/* 661 */     Object obj1 = ((Expression) references[1]).eval(null);
/* 662 */     scala.Tuple2 value4 = (scala.Tuple2) obj1;
/* 663 */
/* 664 */     boolean isNull8 = false;
/* 665 */     final io.mistnet.analytics.scan.SrcDstGrouped value8 = isNull8 ? null : (io.mistnet.analytics.scan.SrcDstGrouped) value4._2();
/* 666 */     isNull8 = value8 == null;
/* 667 */     boolean isNull6 = false;
/* 668 */     boolean value6 = true;
/* 669 */
/* 670 */     if (!false && isNull8) {
/* 671 */     } else {
/* 672 */
/* 673 */       Object obj2 = ((Expression) references[2]).eval(null);
/* 674 */       scala.None$ value10 = (scala.None$) obj2;
/* 675 */
/* 676 */       boolean isNull11 = false;
/* 677 */       final io.mistnet.analytics.scan.SrcDstGrouped value11 = isNull11 ? null : (io.mistnet.analytics.scan.SrcDstGrouped) value4._2();
/* 678 */       isNull11 = value11 == null;
/* 679 */       boolean isNull9 = false || isNull11;
/* 680 */       final boolean value9 = isNull9 ? false : value10.equals(value11);
/* 681 */       if (!isNull9 && value9) {
/* 682 */       } else if (!false && !isNull9) {
/* 683 */         value6 = false;
/* 684 */       } else {
/* 685 */         isNull6 = true;
/* 686 */       }
/* 687 */     }
/* 688 */     boolean isNull5 = false;
/* 689 */     InternalRow value5 = null;
/* 690 */     if (!isNull6 && value6) {
/* 691 */
/* 692 */       final InternalRow value12 = null;
/* 693 */       isNull5 = true;
/* 694 */       value5 = value12;
/* 695 */     } else {
/* 696 */
/* 697 */       boolean isNull13 = false;
/* 698 */       this.values = new Object[3];apply1_0(i);
/* 699 */       apply1_1(i);
/* 700 */       final InternalRow value13 = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(values);
/* 701 */       this.values = null;
/* 702 */       isNull5 = isNull13;
/* 703 */       value5 = value13;
/* 704 */     }
/* 705 */     this.isNull_1 = isNull5;
/* 706 */     this.value_1 = value5;
/* 707 */
/* 708 */     // copy all the results into MutableRow
/* 709 */
/* 710 */     if (!this.isNull_0) {
/* 711 */       mutableRow.setBoolean(0, this.value_0);
/* 712 */     } else {
/* 713 */       mutableRow.setNullAt(0);
/* 714 */     }
/* 715 */
/* 716 */     if (!this.isNull_1) {
/* 717 */       mutableRow.update(1, this.value_1);
/* 718 */     } else {
/* 719 */       mutableRow.setNullAt(1);
/* 720 */     }
/* 721 */
/* 722 */     return mutableRow;
/* 723 */   }
/* 724 */ }

	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
	at org.spark_project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at org.spark_project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at org.spark_project.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:841)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:140)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.generate(GenerateMutableProjection.scala:44)
	at org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:369)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3$$anonfun$4.apply(SortAggregateExec.scala:93)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3$$anonfun$4.apply(SortAggregateExec.scala:92)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.<init>(AggregationIterator.scala:143)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.<init>(SortBasedAggregationIterator.scala:39)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:84)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:75)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{noformat}",,apachespark,hvanhovell,kiszk,rayqiu,zkull,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Mon Nov 07 15:21:05 UTC 2016,,,,,,,,,,"0|i35g9b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/16 08:20;zkull;Same here:

After groupByKey( _._1).reduceGroups((a,b) => (a._1, a._2 ++ a._2)).map(_._2):

java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 31, Column 69: Unknown variable or type ""value4""

Spark Version 2.0.1;;;","27/Oct/16 20:22;rayqiu;Move to Priority Critical unless a workaround is identified.  This is a very basic functionality.;;;","27/Oct/16 22:37;hvanhovell;Could one of you provide a reproducible example.;;;","27/Oct/16 23:06;hvanhovell;I tried something like this on master and on branch-2.0:
{noformat}
val ds = spark.range(10000).select($""id"" % 100 as ""grp_id"", array($""id"")).as[(Long, Seq[Long])] 
ds.groupByKey(_._1).reduceGroups((a, b) => (a._1, a._2 ++ b._2)).map(_._2)
{noformat};;;","27/Oct/16 23:42;rayqiu;Try this in spark-shell:

case class Route(src: String, dest: String, cost: Int)
case class GroupedRoutes(src: String, dest: String, routes: Seq[Route])

val ds = sc.parallelize(Array(
    Route(""a"", ""b"", 1),
    Route(""a"", ""b"", 2),
    Route(""a"", ""c"", 2),
    Route(""a"", ""d"", 10),
    Route(""b"", ""a"", 1),
    Route(""b"", ""a"", 5),
    Route(""b"", ""c"", 6))
  ).toDF.as[Route]

val grped = ds.map(r => GroupedRoutes(r.src, r.dest, Seq(r)))
  .groupByKey(r => (r.src, r.dest))
  .reduceGroups { (g1: GroupedRoutes, g2:  GroupedRoutes) =>
    GroupedRoutes(g1.src, g1.dest, g1.routes ++ g2.routes)
  }.map(_._2)

Same thing works fine in 2.0.0

On Thu, Oct 27, 2016 at 4:06 PM, Herman van Hovell (JIRA) <jira@apache.org>




-- 
Regards,
Ray
;;;","28/Oct/16 10:26;kiszk;I confirmed this code can reproduce on 2.0.1.
This problem occurs due to the similar reason in SPARK-18147

To call {{ctx.splitExpression}} in {{createStruct.doGenCode}} make *a variable* inaccesssible by splitting the original one function into multiple functions.
SPARK-14793 seems to have introduced {{ctx.splitExpression}} here to fix other issues on April. Since Ray says this code works in 2.0.0, other changes may introduce this issue.;;;","31/Oct/16 09:09;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/15693;;;","07/Nov/16 15:21;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/15796;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Unable to query global temp views when hive support is enabled. ,SPARK-18121,13015572,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ksunitha,ksunitha,ksunitha,26/Oct/16 20:30,28/Oct/16 00:42,14/Jul/23 06:29,28/Oct/16 00:39,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Querying on a global temp view throws Table or view not found exception when Hive support is enabled. 

Testcase to reproduce the problem: 
The test needs to run when hive support is enabled.  
{code}
  test(""query global temp view"") {
    val df = Seq(1).toDF(""i1"")
    df.createGlobalTempView(""tbl1"")
    checkAnswer(spark.sql(""select * from global_temp.tbl1""), Row(1))
    spark.sql(""drop view global_temp.tbl1"")
  }
{code}
Cause:
HiveSessionCatalog.lookupRelation does not check for the global temp views. 
",,apachespark,cloud_fan,ksunitha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 28 00:39:36 UTC 2016,,,,,,,,,,"0|i35g1z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/16 20:31;ksunitha;I have a fix for this, will submit a PR soon. ;;;","26/Oct/16 20:39;apachespark;User 'skambha' has created a pull request for this issue:
https://github.com/apache/spark/pull/15649;;;","28/Oct/16 00:39;cloud_fan;Issue resolved by pull request 15649
[https://github.com/apache/spark/pull/15649];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QueryExecutionListener method doesnt' get executed for DataFrameWriter methods,SPARK-18120,13015567,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,salilsurendran,salilsurendran,26/Oct/16 20:24,17/Feb/17 05:09,14/Jul/23 06:29,17/Feb/17 05:09,2.0.1,,,,,,,,2.2.0,,,,SQL,,,,,,,,0,,,,,,"QueryExecutionListener is a class that has methods named onSuccess() and onFailure() that gets called when a query is executed. Each of those methods takes a QueryExecution object as a parameter which can be used for metrics analysis. It gets called for several of the DataSet methods like take, head, first, collect etc. but doesn't get called for any of the DataFrameWriter methods like saveAsTable, save etc. ",,apachespark,cloud_fan,codingcat,irashid,jayadevan.m,rxin,salilsurendran,thomastechs,tsuresh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 17 05:09:28 UTC 2017,,,,,,,,,,"0|i35g0v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/16 11:21;thomastechs;[~salilsurendran] [~rxin] [~jayadevan.m]
Is it really expected to have the listener similar implementation in the DataFrameWriter? 
Just would like to clarify, Or it was not included in the DataFrameWriter  due to any particular reason?;;;","27/Nov/16 18:26;rxin;They should get triggered. Are you sure they don't?
;;;","29/Nov/16 16:22;jayadevan.m;[~thomastechs],[~salilsurendran][~rxin]

Hi All, 
I am working on fix for this issue
Regards
Jay;;;","29/Nov/16 16:28;salilsurendran;[~rxin@databricks.com]. No it doesn't get triggered. [~jayadevan.m] I already have the code written and ready. Will be making a PR soon.;;;","19/Jan/17 17:36;thomastechs;Hi [~salilsurendran]
Are you working on this, please let us know ?
I can submit a PR.

;;;","19/Jan/17 21:21;salilsurendran;[~thomastechs]I will be making a PR today.;;;","20/Jan/17 13:08;salilsurendran;Sorry for the delay but ran into some unexpected unit test failures. Rerunning the whole test suite to make sure nothing is broken.;;;","20/Jan/17 20:35;apachespark;User 'salilsurendran' has created a pull request for this issue:
https://github.com/apache/spark/pull/16664;;;","16/Feb/17 19:44;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16962;;;","17/Feb/17 05:09;cloud_fan;Issue resolved by pull request 16962
[https://github.com/apache/spark/pull/16962];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Namenode safemode check is only performed on one namenode which can stuck the startup of SparkHistory server,SPARK-18119,13015540,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nfraison.criteo,nfraison.criteo,nfraison.criteo,26/Oct/16 19:09,25/Nov/16 09:47,14/Jul/23 06:29,25/Nov/16 09:46,1.6.2,2.0.1,,,,,,,2.1.0,,,,Spark Core,,,,,,,,0,,,,,,"SparkHistory server startup is stuck when one of the 2 HA namenode is in safemode displaying this log message: HDFS is still in safe mode. Waiting... 

This happens even if one of the 2 namenode is in active mode because it only request the first one of 2 available namenode in an HA configuration",HDFS cdh 5.5.0 with HA namenode,apachespark,nfraison.criteo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 25 09:46:18 UTC 2016,,,,,,,,,,"0|i35fuv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/16 19:30;apachespark;User 'ashangit' has created a pull request for this issue:
https://github.com/apache/spark/pull/15648;;;","25/Nov/16 09:46;srowen;Issue resolved by pull request 15648
[https://github.com/apache/spark/pull/15648];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SpecificSafeProjection.apply of Java Object from Dataset to JavaRDD Grows Beyond 64 KB,SPARK-18118,13015518,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kiszk,aeskilson,aeskilson,26/Oct/16 18:09,28/Nov/16 12:20,14/Jul/23 06:29,28/Nov/16 12:20,2.1.0,,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,,,,,,"For sufficiently wide or nested Java Objects, when SpecificSafeProjection attempts to recreate the object from an InternalRow, the generated SpecificSafeProjection.apply method is larger than allowed: 

{code}
Caused by: org.codehaus.janino.JaninoRuntimeException: Code of method ""apply(Ljava/lang/Object;)Ljava/lang/Object;"" of class ""org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection"" grows beyond 64 KB
{code}

Although related, this issue appears not to have been resolved by SPARK-15285. Since there is only one top-level object when projecting, splitExpressions finds no additional Expressions to split. The result is a single large, nested Expression that forms the apply code.

See the reproducer for an example [1].

[1] - https://github.com/bdrillard/specific-safe-projection-error",,aeskilson,apachespark,codingcat,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-15285,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 28 09:40:06 UTC 2016,,,,,,,,,,"0|i35fpz:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"28/Nov/16 09:40;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/16032;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MesosClusterScheduler generate bad command options,SPARK-18114,13015373,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wlsailor,wlsailor,wlsailor,26/Oct/16 10:51,01/Nov/16 20:25,14/Jul/23 06:29,01/Nov/16 13:46,2.0.1,,,,,,,,2.0.2,2.1.0,,,Mesos,,,,,,,,0,easyfix,newbie,,,,"{code:title=MesosClusterScheduler.java|borderStyle=solid}
desc.schedulerProperties
      .filter { case (key, _) => !replicatedOptionsBlacklist.contains(key) }
      .foreach { case (key, value) => options ++= Seq(""--conf"", s""$key=${shellEscape(value)}"") }
    options
{code}
As above, --conf option value is not enclosed by """", for example, if we have a multi value config like  
{quote}
spark.driver.extraJavaOptions= -Dlog4j.configuration=file:./conf/log4j-server.properties -Dalluxio.user.file.writetype.default=THROUGH
{quote}
Without """" the next alluxio config will be treated as spark-submit options and cause error.","CentOS 7, Jdk8, scala 2.11.8",apachespark,wlsailor,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 01 19:14:04 UTC 2016,,,,,,,,,,"0|i35etr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/16 11:00;apachespark;User 'LeightonWong' has created a pull request for this issue:
https://github.com/apache/spark/pull/15643;;;","01/Nov/16 13:46;srowen;Resolved by https://github.com/apache/spark/pull/15643;;;","01/Nov/16 19:14;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15719;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Sending AskPermissionToCommitOutput failed, driver enter into task deadloop",SPARK-18113,13015355,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jinxing6042@126.com,xq2005,xq2005,26/Oct/16 09:41,17/May/20 17:48,14/Jul/23 06:29,18/Jan/17 18:48,2.0.1,,,,,,,,2.2.0,,,,Scheduler,Spark Core,,,,,,,0,,,,,,"Executor sends *AskPermissionToCommitOutput* to driver failed, and retry another sending. Driver receives 2 AskPermissionToCommitOutput messages and handles them. But executor ignores the first response(true) and receives the second response(false). The TaskAttemptNumber for this partition in authorizedCommittersByStage is locked forever. Driver enters into infinite loop.

h4. Driver Log:

{noformat}
16/10/25 05:38:28 INFO TaskSetManager: Starting task 24.0 in stage 2.0 (TID 110, cwss04.sh01.com, partition 24, PROCESS_LOCAL, 5248 bytes)
...
16/10/25 05:39:00 WARN TaskSetManager: Lost task 24.0 in stage 2.0 (TID 110, cwss04.sh01.com): TaskCommitDenied (Driver denied task commit) for job: 2, partition: 24, attemptNumber: 0
...
16/10/25 05:39:00 INFO OutputCommitCoordinator: Task was denied committing, stage: 2, partition: 24, attempt: 0
...
16/10/26 15:53:03 INFO TaskSetManager: Starting task 24.1 in stage 2.0 (TID 119, cwss04.sh01.com, partition 24, PROCESS_LOCAL, 5248 bytes)
...
16/10/26 15:53:05 WARN TaskSetManager: Lost task 24.1 in stage 2.0 (TID 119, cwss04.sh01.com): TaskCommitDenied (Driver denied task commit) for job: 2, partition: 24, attemptNumber: 1
16/10/26 15:53:05 INFO OutputCommitCoordinator: Task was denied committing, stage: 2, partition: 24, attempt: 1
...
16/10/26 15:53:05 INFO TaskSetManager: Starting task 24.28654 in stage 2.0 (TID 28733, cwss04.sh01.com, partition 24, PROCESS_LOCAL, 5248 bytes)
...
{noformat}

h4. Executor Log:

{noformat}
...
16/10/25 05:38:42 INFO Executor: Running task 24.0 in stage 2.0 (TID 110)
...
16/10/25 05:39:10 WARN NettyRpcEndpointRef: Error sending message [message = AskPermissionToCommitOutput(2,24,0)] in 1 attempts
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.rpc.askTimeout
        at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
        at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
        at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
        at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
        at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
        at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
        at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:78)
        at org.apache.spark.scheduler.OutputCommitCoordinator.canCommit(OutputCommitCoordinator.scala:95)
        at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:73)
        at org.apache.spark.SparkHadoopWriter.commit(SparkHadoopWriter.scala:106)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1212)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
        at org.apache.spark.scheduler.Task.run(Task.scala:86)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:279)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.lang.Thread.run(Thread.java:785)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
        at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
        at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)
        at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
        at scala.concurrent.Await$.result(package.scala:190)
        at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:81)
        ... 13 more
...
16/10/25 05:39:16 INFO Executor: Running task 24.1 in stage 2.0 (TID 119)
...
16/10/25 05:39:24 INFO SparkHadoopMapRedUtil: attempt_201610250536_0002_m_000024_119: Not committed because the driver did not authorize commit
...
{noformat}
","# cat /etc/redhat-release 
Red Hat Enterprise Linux Server release 7.2 (Maipo)",514793425@qq.com,aash,apachespark,jinxing6042@126.com,rajeshhadoop,xq2005,xukun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8029,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 02 23:15:49 UTC 2017,,,,,,,,,,"0|i35epr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/16 09:48;xq2005;Fix is easy, in handleAskPermissionToCommit

when (existingCommitter && attemptNumber == authorizedCommitters(partition)) ,  return true too.;;;","05/Jan/17 18:24;aash;[~xq2005] can you please send a PR to https://github.com/apache/spark with your proposed change?

I think I'm seeing the same issue and would like to see the code diff you're suggesting so I can test the fix.

Thanks!;;;","07/Jan/17 06:10;jinxing6042@126.com;[~xq2005], [~aash]
I am seeing this issue in my cluster some times. If *OutputCommittCoordinatorEndpoint* receive *AskPermissionToCommitOutput* for the first time, *OutputCommitCoordinatoryEndpoint* will mark the task attempt as a committer in *authorizedCommittersByStage* and send back the response. But if the worker failed to get the response in *spark.rpc.timeout*, it will retry sending *AskPermissionToCommitOutput*. However it will be denied by *OutputCommitCoordinatorEndpoint*, because it has already registered a committer for the partition, even though the registered committer and the worker are the same. 
Reproducing is easy:
{code:title=OutputCommitCoordinator.scala|borderStyle=solid}
......
  // Marked private[scheduler] instead of private so this can be mocked in tests
  private[scheduler] def handleAskPermissionToCommit(
      stage: StageId,
      partition: PartitionId,
      attemptNumber: TaskAttemptNumber): Boolean = synchronized {
    authorizedCommittersByStage.get(stage) match {
      case Some(authorizedCommitters) =>
        authorizedCommitters(partition) match {
          case NO_AUTHORIZED_COMMITTER =>
            logDebug(s""Authorizing attemptNumber=$attemptNumber to commit for stage=$stage, "" +
              s""partition=$partition"")
            authorizedCommitters(partition) = attemptNumber
            Thread.sleep(150000)
            true
          case existingCommitter =>
            logDebug(s""Denying attemptNumber=$attemptNumber to commit for stage=$stage, "" +
              s""partition=$partition; existingCommitter = $existingCommitter"")
            false
        }
      case None =>
        logDebug(s""Stage $stage has completed, so not allowing attempt number $attemptNumber of"" +
          s""partition $partition to commit"")
        false
    }
  }
......
{code}
When worker asks to be registered as a committer for the first time, sleep 150 seconds, which is bigger than *spark.rpc.timeout=120 seconds*. when worker retries *AskPermissionToCommitOutput* it will get *CommitDeniedException*, then the task will fail with reason *TaskCommitDenied*, which is not regarded as a task failure(SPARK-11178), so TaskScheduler will schedule this task infinitely.

[~xq2005]
If you don't have time, could I make a pr for this?;;;","08/Jan/17 16:27;apachespark;User 'jinxing64' has created a pull request for this issue:
https://github.com/apache/spark/pull/16503;;;","09/Jan/17 08:25;aash;Thanks for sending in that PR [~jinxing6042@126.com]!  It's very similar to the one I've also been testing -- see https://github.com/palantir/spark/pull/79 for that diff (I haven't sent a PR into Apache yet).

Has that fixed the problem for you?;;;","09/Jan/17 15:12;jinxing6042@126.com;[~aash]
Thanks a lot for your reply!
I think the ideas we have for this issue are the same; I think both can solve my problem;
And how do you think about the unit test raised in my pr ? Could you leave some comments if possible ?
It's my first PR for spark. It's great if you can give some advice ~~Thanks a lot ! : );;;","09/Jan/17 20:54;aash;I've done some more diagnosis on an example I saw, and think there's a failure mode that https://issues.apache.org/jira/browse/SPARK-8029 didn't consider.  Here's the sequence of steps I think causes this issue:

- executor requests a task commit
- coordinator approves commit and sends response message
- executor is preempted by YARN!
- response message goes nowhere
- OCC has attempt=0 fixed for that stage/partition now so no other attempt will succeed :(

Are you running in YARN with preemption enabled? Is there preemption activity around the time of the task deadloop?;;;","11/Jan/17 03:49;jinxing6042@126.com;Thanks a lot for your reply~ [~aash]. Your comment is very helpful;

In my cluster preemption is enabled, but I didn't find preemption activity around the deadloop.
I think anything cause the *AskPermissionToCommitOutput's* response timeout can lead to this deadloop.
Maybe there are two things need to do:
1. It's suitable to use *ask* to replace *askWithRetry*, which is too expensive and bug proven. If timeout, just fail the task and let Spark reschedule it;
2. Coordinator receiver should be idempotent for *AskPermissionToCommitOutput*.;;;","07/Feb/17 08:52;xukun;[~jinxing6042@126.com] [~aash]
 I've met the same question. Here's the sequence of steps:
* executor requests a task commit
* coordinator approves commit and sends response message
* executor receive response message and goes susscess and send StatusUpdate to driver
* executor is preempted by YARN before receive StatusUpdate
* task retry because of ExecutorLost and no other attempt will succeed

I think this pr can not resolve this scene. can you give me some advices?;;;","09/Feb/17 04:45;jinxing6042@126.com;[~xukun]

Can you reproduce the bug with steps above?

When ExecutorLost, *TaskSetManager* will call:
{code}
sched.dagScheduler.taskEnded(
            tasks(index), Resubmitted, null, Seq.empty, info)
{code}

Then lock in *OutputCommitCoordinator* will be cleared. So I don't think the scenario you raised exists.
Let me know if you have different ideas :);;;","14/Feb/17 08:33;xukun;[~jinxing6042@126.com]
yes. reproduce steps:

1. executor is running task x.0
2. appmaster get completed container statuses and tell driver to remove executor (now executor is not killed,it is still running.)
3. driver process CompletionEvent and TaskSetManager will call:
{code}
sched.dagScheduler.taskEnded(
            tasks(index), Resubmitted, null, Seq.empty, info)
{code}
4.task x.0 request a task commit and coordinator approves commit and sends response message
5.drvier receive AskPermissionToCommitOutput and authorize attemptNumber=0 for partition x
6.driver starting task x.1 because of ExecutorLost and enter into task deadloop

When appmaster  get completed container statuses , executor is not preempted immediately. executor will be killed after a while. After TaskSetManager call
{code}
sched.dagScheduler.taskEnded(
            tasks(index), Resubmitted, null, Seq.empty, info)
{code}
, task x.0 will send AskPermissionToCommitOutput. So driver enter into task deadloop.
;;;","14/Feb/17 10:21;aash;Thanks for the updates you both.  I've been working with a coworker who's seeing this on a cluster of his and we think the deadloop is caused by an interaction between commit authorization and preemption (his cluster has lots of preemption occurring).

You can see the in-progress patch we've been working on at https://github.com/palantir/spark/pull/94 which adds a two-phase commit sequence to the OutputCommitCoordinator.  We're not done testing it yet but it might provide a useful example for discussion here.

Maybe also we should file another ticket for the deadloop in preemption scenario, which is a little different from the deadloop in lost message scenario (what I think the already-merged PR fixes).;;;","14/Feb/17 11:49;xukun;[~aash]

Thanks for your reply. I think [https://github.com/palantir/spark/pull/94] can not resolve this scenario.
{code}
driver:  starting task 678.0 --- Lost executor --- retry 678.1 --- Authorizing attemptNumber=0,partition=678 --- Denying attemptNumber=1,partition=678 -- deadloop

executor:  Running task 678.0 -------------------------------------outputCommitCoordinator.canCommit ---- finished task 678.0 ---- RECEIVED SIGNAL 15: 
{code}

After driver retry task 678.1,  executor execute outputCommitCoordinator.canCommit. Because It is a gap between appmaster get completed container statuses and executor is killed.;;;","14/Feb/17 13:38;aash;[~xukun] the scenario you describe should be accommodated by the newly-added {{spark.scheduler.outputcommitcoordinator.maxwaittime}} in that PR, which sets a timeout for how long an executor can hold the commit lock for a task.  If the executor fails while holding the lock, then after that period of time the lock will be released and a subsequent executor will be able to commit the task.  By default right now it is 2min.;;;","15/Feb/17 02:46;xukun;[~aash]

According my scenario and [https://github.com/palantir/spark/pull/94] code

task 678.0
outputCommitCoordinator.canCommit will match CommitState(NO_AUTHORIZED_COMMITTER, _, Uncommitted) =>  CommitState(attemptNumber, System.nanoTime(), MidCommit)

outputCommitCoordinator.commitDone match CommitState(existingCommitter, startTime, MidCommit) if attemptNumber == existingCommitter =>
 CommitState(attemptNumber, startTime, Committed)

task 678.1
outputCommitCoordinator.canCommit match CommitState(existingCommitter, _, Committed) 

If executor is preempted after outputCommitCoordinator.commitDone, driver still enter into task deadloop;;;","21/Feb/17 08:40;jinxing6042@126.com;[~xukun]

I got the scenario you described. 

??When appmaster get completed container statuses , executor is not preempted immediately. executor will be killed after a while.??
Yes, I think this is the root cause of this bug, which can be fixed by https://github.com/palantir/spark/pull/94.

??If executor is preempted after outputCommitCoordinator.commitDone, driver still enter into task deadloop??
I hardly agree with above. Since when *commitDone* is called, no more *AskPermissionToCommit* will be sent again. Thus I believe 
{code}
sched.dagScheduler.taskEnded(
            tasks(index), Resubmitted, null, Seq.empty, info)
{code}
can clean the commit lock clearly, following *AskPermissionToCommit* from other task attempt will be authorized.
Thoughts ?;;;","23/Feb/17 11:28;xukun;[~jinxing6042[~jinxing6042@126.com]
When appmaster get completed container statuses , executor is not preempted immediately. executor will be killed after a while.
Yes, I think this is the root cause of this bug, which can be fixed by https://github.com/palantir/spark/pull/94.

Sorry, I don't understand. 
my steps is :
1.task 678.0 -> AskPermissionToCommitOutput
2.task 678.0 -> InformCommitDone
3.task 678.1 -> AskPermissionToCommitOutput
4.task 678.1 -> InformCommitDone
I think it is not fixed by https://github.com/palantir/spark/pull/94. Can u show me the step? 
Thanks!;;;","02/Mar/17 23:15;aash;We discovered another bug related to committing that causes task deadloop and have work being done in SPARK-19631 to fix it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Spark2.x does not support read data from Hive 2.x metastore,SPARK-18112,13015350,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,smilegator,KaiXu,KaiXu,26/Oct/16 08:49,12/Dec/22 18:11,14/Jul/23 06:29,15/Mar/17 02:54,2.0.0,2.0.1,,,,,,,2.2.0,,,,SQL,,,,,,,,0,,,,,,"Hive2.0 has been released in February 2016, after that Hive2.0.1 and Hive2.1.0 have also been released for a long time, but till now spark only support to read hive metastore data from Hive1.2.1 and older version, since Hive2.x has many bugs fixed and performance improvement it's better and urgent to upgrade to support Hive2.x

failed to load data from hive2.x metastore:
Exception in thread ""main"" java.lang.NoSuchFieldError: HIVE_STATS_JDBC_TIMEOUT
        at org.apache.spark.sql.hive.HiveUtils$.hiveClientConfigurations(HiveUtils.scala:197)
        at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:262)
        at org.apache.spark.sql.hive.HiveSharedState.metadataHive$lzycompute(HiveSharedState.scala:39)
        at org.apache.spark.sql.hive.HiveSharedState.metadataHive(HiveSharedState.scala:38)
        at org.apache.spark.sql.hive.HiveSharedState.externalCatalog$lzycompute(HiveSharedState.scala:4
        at org.apache.spark.sql.hive.HiveSharedState.externalCatalog(HiveSharedState.scala:45)
        at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:50)
        at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
        at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:31)
        at org.apache.spark.sql.SparkSession.table(SparkSession.scala:568)
        at org.apache.spark.sql.SparkSession.table(SparkSession.scala:564)",,apachespark,cloud_fan,dapengsun,dongjoon,dougb,elgalu,EugeniuZ,honglun,JPMoresmau,KaiXu,rahulvkulkarni,smilegator,Tavis,tgraves,toopt4,txhsj,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-13446,,,,SPARK-19076,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 23 11:22:32 UTC 2019,,,,,,,,,,"0|i35eon:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/16 01:27;KaiXu;Spark 2.0 removes JavaSparkListener and change SparkListener from a trait to an abstract class: https://issues.apache.org/jira/browse/SPARK-14358, but Hive2.x still use JavaSparkListener: https://issues.apache.org/jira/browse/SPARK-17563, this Hive patch solved this issue: https://issues.apache.org/jira/browse/HIVE-14029, but from spark side it still does not support read data from Hive2.x metastore.;;;","31/Oct/16 09:07;srowen;Yes, this may be an instance where Hive 2 will have to shim to be compatible with Spark 1 vs 2 simultaneously.;;;","10/Jan/17 09:33;gurwls223;Could we resolve this as a duplicate of SPARK-13446?;;;","10/Mar/17 03:39;smilegator;Let me resolve it for supporting Hive 2.1.0 metastore.;;;","10/Mar/17 04:41;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17232;;;","15/Mar/17 02:54;cloud_fan;Issue resolved by pull request 17232
[https://github.com/apache/spark/pull/17232];;;","15/Jan/18 14:49;JPMoresmau;I'm using Hive 2.3.2 and Spark 2.2.1, but I still run into this issue. Is there any specific configuration setting I should look for?;;;","23/Apr/18 17:57;Tavis;It looks to me like this issue has actually not been fixed.  As seen in the stack trace, the offending code is in 

/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala

line 205, where the method attempts to fetch the value of configuration parameter HIVE_STATS_JDBC_TIMEOUT

which was a configuration parameter defined in import org.apache.hadoop.hive.conf.HiveConf, which is part of hive-common.  However, this configuration parameter was removed in Hive 2, therefore the above code will throw an exception when run with hive-common versions 2.x.  It is possible there are other configuration parameters requested in HiveUtils.scala that have been removed as well; I haven't checked.  In any event, the above line 205 is still present in the Master branch as of today, so Spark still does not work with Hive 2.x.;;;","23/Apr/18 18:21;Tavis;Sorry, following myself up... the next parameter, HIVE_STATS_RETRIES_WAIT, also needs to be removed.  A quick search of the code does not find these two parameters used anywhere else in Spark, so I think the two lines can just be deleted without causing any downstream issues.;;;","23/Aug/18 05:46;elgalu;Same issue. It only gets resolved if I remove spark-hive_2.11-2.3.1.jar but then pyspark and sparklyr stop working.;;;","05/Sep/18 09:17;toopt4;[~hyukjin.kwon] [~srowen] can this ticket be re-opened? This code is still in master as mentioned in comments above;;;","05/Sep/18 09:26;gurwls223;Can you post reproducer step by step? did you set {{spark.sql.hive.metastore.version}} and jar properly?;;;","05/Sep/18 13:17;srowen;I don't know much about this part, but do we need Hive 2.x on the Spark (client) side in order to read from Hive 2.x metastore? Are you including Hive 2.x in your app? I don't know if that works.;;;","06/Sep/18 02:32;gurwls223;We need the metastore jar if I understood correctly. FWIW, I am seeing few tests internally running with different metastore support. I doubt if there's an issue about its supportability itself.;;;","26/Sep/18 14:22;EugeniuZ;This issue should be reopened.

As already commented by [~Tavis] https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala#L204 is referenced but it is not present in HiveConf since branch 2.0

https://github.com/apache/hive/blob/branch-1.2/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java#L1290
https://github.com/apache/hive/blob/branch-2.0/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
;;;","26/Sep/18 14:33;gurwls223;Can you post reproducer steps please before we open this?;;;","26/Sep/18 16:16;EugeniuZ;I can only describe my situation. I am using AWS EMR 5.17.0 with Hive, Spark, Zeppelin, Hue installed. In Zeppelin the configuration variable for spark interpretter points to /usr/lib/spark. There I found jars/ folder. In jars folder I have the following hive related libraries. 

{code}
-rw-r--r-- 1 root root   139044 Aug 15 01:06 hive-beeline-1.2.1-spark2-amzn-0.jar
-rw-r--r-- 1 root root    40850 Aug 15 01:06 hive-cli-1.2.1-spark2-amzn-0.jar
-rw-r--r-- 1 root root 11497847 Aug 15 01:06 hive-exec-1.2.1-spark2-amzn-0.jar
-rw-r--r-- 1 root root   101113 Aug 15 01:06 hive-jdbc-1.2.1-spark2-amzn-0.jar
-rw-r--r-- 1 root root  5472179 Aug 15 01:06 hive-metastore-1.2.1-spark2-amzn-0.jar
{code}

If I replace them with their 2.3.3 equivalents, e.g. hive-exec-1.2.1-spark2-amzn-0.jar -> hive-exec-2.3.3-amzn-1.jar I get the following error when running SQL query in spark:

{code}
java.lang.NoSuchFieldError: HIVE_STATS_JDBC_TIMEOUT
	at org.apache.spark.sql.hive.HiveUtils$.formatTimeVarsForHiveClient(HiveUtils.scala:205)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:286)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)
	at org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)
	at org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:641)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.zeppelin.spark.SparkSqlInterpreter.interpret(SparkSqlInterpreter.java:116)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:97)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:498)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:175)
	at org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}

;;;","26/Sep/18 17:35;elgalu;And to get things worse Hive is already in version 3. Same with Hadoop, the default Spark+Hadoop distribution comes with Hadoop 2.7 while Hadoop is already 3.1. Is really hard to understand how such a popular open source project like Spark keeps dependencies years old, some are 7 years old or more.;;;","26/Sep/18 21:13;toopt4;here, here!;;;","27/Sep/18 04:30;gurwls223;Hadoop 3 profile. See https://github.com/apache/spark/pull/21588 and please provide some input at https://issues.apache.org/jira/browse/SPARK-20202;;;","27/Sep/18 04:33;gurwls223;Re: https://issues.apache.org/jira/browse/SPARK-18112?focusedCommentId=16629000&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16629000

Did you set {{spark.sql.hive.metastore.version}}?;;;","27/Sep/18 04:42;gurwls223;Hive 3 support. See https://github.com/apache/spark/pull/21404;;;","27/Sep/18 08:40;EugeniuZ;RE: https://issues.apache.org/jira/browse/SPARK-18112?focusedCommentId=16629743&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16629743 

[~hyukjin.kwon] I set the config value to 2.3.3, didn't help.;;;","27/Sep/18 09:11;gurwls223;Have you provided the jars into {{spark.sql.hive.metastore.jars}} exclusively?;;;","27/Sep/18 09:16;EugeniuZ;Tried setting it to ""maven"" and then to ""/usr/lib/hive/lib"" from where I copied the 2.3.3 version of hive-*.jar libraries. That didn't help.

In any case, how setting hive-* libraries would help with [https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala#L204] referencing a field which doesn't exist anymore ? The problem is in spark library, isn't it ?;;;","27/Sep/18 11:23;gurwls223;It uses Hive fork (1.2.1) jar at that point. When it creates a client, it uses specified metastore jars. ;;;","27/Sep/18 11:26;gurwls223;Such things should be asked to mailing list really. It still sounds like misconfiguration.;;;","27/Sep/18 14:01;Tavis;Why are you even asking these questions?  I have already pointed to the offending lines of code in /src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala that are causing this error and explained why the error is happening (see my comments from April 23rd).  All you have to do is remove those two parameters and push.;;;","27/Sep/18 14:09;gurwls223;At that code you pointed out, Spark's Hive fork should be used. Different jar provided in {{spark.sql.hive.metastore.jars}} is used to create the correspending client to access different version of Hive via isolated classloader.
The problem here is, you guys removed hive-1.2.1 in the jars and didn't provide newer Hive jars in {{spark.sql.hive.metastore.jars}} properly.
;;;","27/Sep/18 14:09;gurwls223;I asked questions because i'm pretty sure it's misconfiguration. That's why I am asking reproducible steps.;;;","27/Sep/18 14:46;gurwls223;To be more specific, the code you guys pointed out is executed by Spark's Hive fork 1.2.1 which contains that configuration (https://github.com/apache/hive/blob/branch-1.2/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java#L1290) 

That's meant to be executed with Spark's Hive fork. So you should leave the jar as is. And then, the higher jars for Hive to create Hive client should be provided to {{spark.sql.hive.metastore.jars}} and {{spark.sql.hive.metastore.version}} should be set accordingly.

The problem here looks, you guys completely replaced the jars into higher Hive jars. Therefore, it throws {{NoSuchFieldError}}

I recently manually tested 1.2.1, 2.3.0 and 3.0.0 (against https://github.com/apache/spark/pull/21404) in few months ago against Apache Spark. I am pretty sure that it works for now.

If I am mistaken or misunderstood at some points, please provide a reproducible step, or at least why it fails. Let me take a look.;;;","27/Sep/18 14:47;gurwls223;Also, take a look for the codes, open JIRAs and PRs before complaining next time.;;;","27/Sep/18 16:23;EugeniuZ;_""The problem here looks, you guys completely replaced the jars into higher Hive jars. Therefore, it throws {{NoSuchFieldError}}_"" - yes you are right. That was my intent. I wanted to be able to connect to metastore database created by a Hive client 2.x. If I use that 1.2.1 fork I was getting some query errors due to me using bloom filters on multiple columns of the table. My understanding is that Hive client 1.2.1 is not seeing that information that is why I was trying to replace the jars for a higher version.;;;","27/Sep/18 23:06;gurwls223;{quote}
 If I use that 1.2.1 fork I was getting some query errors due to me using bloom filters on multiple columns of the table
{quote}

Can you file another JIRA then? Some tests were added for that (SPARK-25427). If that's not respected, it sounds another problem.;;;","27/Sep/18 23:29;gurwls223;To avoid Hive fork 1.2.1 in Spark itself at all, please provide some input at https://issues.apache.org/jira/browse/SPARK-20202 to upgrade it to Hive 2.3.2. Now it's somehow blocked and I need some input there.;;;","11/Jun/19 04:01;honglun;Why do my issue still exist，I use the Spark-2.4.3 and Hive-2.3.3.;;;","11/Jun/19 06:28;dongjoon;What is your issue, [~honglun]? In this issue, `HonglunChen` seems to appear once (two hour ago).
bq. Why do my issue still exist;;;","11/Jun/19 06:57;honglun;[~dongjoon],I replaced the hive-1.x jars(hive-beeline-1.2.1,hive-cli-1.2.1,hive-exec-1.2.1,hive-jdbc-1.2.1,hive-metastore-1.2.1) with the hive-2.3.3 jars,and set ""start-thriftserver.sh --conf spark.sql.hive.metastore.jars=/app/spark/jars/* , --conf spark.sql.hive.metastore.version=2.3.3"",but it didn't work.The error still appears: _Exception in thread ""main"" java.lang.NoSuchFieldError: HIVE_STATS_JDBC_TIMEOUT._

Did  I misunderstand at some something? I found that HiveConf.java in the hive-exec-1.2.1 has the field _HIVE_STATS_JDBC_TIMEOUT, but HiveConf.java is  in the hive-common-2.3.3 and it has no field_ _HIVE_STATS_JDBC_TIMEOUT._;;;","11/Jun/19 09:08;gurwls223;you should place 2.3.3 jars if you want do use 2.3.3.;;;","11/Jun/19 09:26;honglun;[~hyukjin.kwon] Thanks, I did not remove the 1.2.1 jars, and put all the 2.3.3 jars in a separate directory, then set ""--conf spark.sql.hive.metastore.jars=/path of dir/*"". It works now！;;;","12/Jun/19 20:18;dongjoon;[~honglun]. You should not replace Apache Spark jars in `jars` directory. Those files are used for internal Hive-related *execution* inside Spark.
`--conf spark.sql.hive.metastore.version` is only loading new Hive jars additionally for *metastore* access in a isolated class loader separately.;;;","14/Jun/19 01:33;honglun;[~dongjoon] Thank you, I get it.;;;","23/Aug/19 11:22;rahulvkulkarni;Getting this issue on Spark 2.4.3 with Hive 2.3.3. Also tried with Hive 2.3.5. Tried using explicit config variables spark.sql.hive.metastore.version and spark.sql.hive.metastore.jars, but no luck.

 

cdh235m1:/spark-2.4.3-bin-without-hadoop/conf # spark-shell \
> --jars /opt/teradata/tdqg/connector/tdqg-spark-connector/02.05.00.04-159/lib/spark-loaderfactory-02.05.00.04-159.jar, \
> /opt/teradata/tdqg/connector/tdqgspark-connector/02.05.00.04-159/lib/log4j-api-2.7.jar, \
> /opt/teradata/tdqg/connector/tdqgspark-connector/02.05.00.04-159/lib/log4j-core-2.7.jar, \
> /opt/teradata/tdqg/connector/tdqgspark-connector/02.05.00.04-159/lib/qgc-spark-02.05.00.04-159.jar, \
> /opt/teradata/tdqg/connector/tdqgspark-connector/02.05.00.04-159/lib/spark-loader-02.05.00.04-159.jar, \
> /opt/teradata/tdqg/connector/tdqgspark-connector/02.05.00.04-159/lib/json-simple-1.1.1.jar \
> --conf spark.sql.hive.metastore.version=2.3.3 \
> --conf spark.sql.hive.metastore.jars=/apache-hive-2.3.3-bin/lib/* \
> --master spark://cdh235m1.labs.teradata.com:7077
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
19/08/23 07:15:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] Unable to bind key for unsupported operation: backward-delete-word
[INFO] Unable to bind key for unsupported operation: backward-delete-word
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
Spark context Web UI available at http://cdh235m1.labs.teradata.com:4040
Spark context available as 'sc' (master = spark://cdh235m1.labs.teradata.com:7077, app id = app-20190823071550-0000).
Spark session available as 'spark'.
Welcome to
 ____ __
 / __/__ ___ _____/ /__
 _\ \/ _ \/ _ `/ __/ '_/
 /___/ .__/\_,_/_/ /_/\_\ version 2.4.3
 /_/

Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_202)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import tdqg.ForeignServer
import tdqg.ForeignServer

scala> ForeignServer.getDatasetFromSql(""SELECT * FROM GDCData.Telco_Churn_Anal_Train_V"")
19/08/23 07:16:03 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
java.lang.NoSuchFieldError: HIVE_STATS_JDBC_TIMEOUT
 at org.apache.spark.sql.hive.HiveUtils$.formatTimeVarsForHiveClient(HiveUtils.scala:204)
 at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:285)
 at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
 at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
 at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:215)
 at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)
 at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)
 at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
 at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:214)
 at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)
 at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)
 at org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:141)
 at org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:136)
 at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:55)
 at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:55)
 at org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:91)
 at org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:91)
 at org.apache.spark.sql.catalyst.catalog.SessionCatalog.isTemporaryTable(SessionCatalog.scala:736)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.isRunningDirectlyOnFiles(Analyzer.scala:749)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:683)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:715)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:708)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)
 at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:708)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:654)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)
 at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
 at scala.collection.immutable.List.foldLeft(List.scala:84)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)
 at scala.collection.immutable.List.foreach(List.scala:392)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)
 at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)
 at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
 at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
 at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
 at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
 at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
 at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)
 at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
 at tdqg.ForeignServer.getDatasetFromSql(ForeignServer.java:344)
 ... 49 elided

scala>;;;"
Wrong ApproximatePercentile answer when multiple records have the minimum value,SPARK-18111,13015326,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ZenWzh,ZenWzh,ZenWzh,26/Oct/16 06:57,02/Nov/16 18:47,14/Jul/23 06:29,01/Nov/16 13:13,2.0.1,,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,,,,,,"When multiple records have the minimum value, the answer of ApproximatePercentile is wrong.

Suppose we have a table with 12 records and 4 partitions, values of column ""col"" in these partitions are:
1, 1, 2
1, 1, 3
1, 1, 4
1, 1, 5
If we query percentile_approx(col, array(0.5)), the current answer is ""5"", which is far from the correct answer ""1"".

The test case is as below:
{code}
  test(""percentile_approx, multiple records with the minimum value in a partition"") {
    withTempView(table) {
      spark.sparkContext.makeRDD(Seq(1, 1, 2, 1, 1, 3, 1, 1, 4, 1, 1, 5), 4).toDF(""col"")
        .createOrReplaceTempView(table)
      checkAnswer(
        spark.sql(s""SELECT percentile_approx(col, array(0.5)) FROM $table""),
        Row(Seq(1.0D))
      )
    }
  }
{code}",,apachespark,ZenWzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18221,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 02 09:15:04 UTC 2016,,,,,,,,,,"0|i35ejb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/16 07:05;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/15641;;;","26/Oct/16 10:09;srowen;The percentile at 0.5 is the median, and 2 is as good an answer as 1 here. Does this show a bug? If I understand your test case then don't you want to test on a case like ""1 2 2"" and show that it returns 1 perhaps? because the median is obviously 2. It is an approximate algorithm, note.;;;","27/Oct/16 00:42;ZenWzh;[~srowen] The minimum is not only skipped once in the whole data, but *skipped per partition*.
For example, we have two partitions of data: (1, 1, 3, 3) and (5, 5, 7, 7), then when we do global merging, the samples in QuantileSummaries is (1, 3, 3, 5, 7, 7), and the percentiles result returned for query percentile_approx(0.25, 0.5, 0.75) is (3.0, 5.0, 7.0), but the correct answer should be (1.0, 3.0, 5.0). Of course we can say it's an approximate algorithm, but this error is already *beyond the error bound which the algo provides*. And also, we can make the error even larger if we construct more such partitions and thus more skipped minimum elements.;;;","27/Oct/16 07:01;srowen;You may have a point but this example still doesn't seem to show it. 3, 5, 7 are as correct as 1, 3, 5, even when computing the 'exact' percentiles. In fact the conventional definition of percentiles would say 3, 5, 7 are correct because 25% of the data falls below 3, not 1.

The PR you show suggests a different problem, that an element gets added twice when it's the only element in the partition. ;;;","27/Oct/16 08:28;ZenWzh;[~srowen] Sorry for the ambiguous example. I've updated the description and used a more obvious example.
BTW, my pr does address the problem in this jira, with a small change from ""currHead.value < head.value"" to ""currHead.value <= head.value"";;;","27/Oct/16 08:38;srowen;Yes that's clear, thank you. Just checking. It really says '5', not '2'? but both are wrong, yes.;;;","27/Oct/16 08:54;ZenWzh;Yes, it says '5'. Because the samples in QuantileSummaries after the final merging is (1, 2, 3, 4, 5), and the rank of 0.5 percentile is quantile * count, i.e. 0.5 * 12 = 6. The rank is larger than its length, in which case it just returns the last sample.;;;","27/Oct/16 08:56;ZenWzh;Everytime it calls compress(), it will lose one duplicated minimum value.;;;","01/Nov/16 13:13;srowen;Resolved by https://github.com/apache/spark/pull/15641;;;","02/Nov/16 09:15;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/15732;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing parameter in Python for RandomForest regression and classification,SPARK-18110,13015309,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,26/Oct/16 05:02,30/Oct/16 23:22,14/Jul/23 06:29,30/Oct/16 23:22,2.0.1,,,,,,,,2.1.0,,,,PySpark,,,,,,,,0,,,,,,,,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 26 05:05:05 UTC 2016,,,,,,,,,,"0|i35efj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/16 05:05;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/15638;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partition discovery fails with explicitly written long partitions,SPARK-18108,13015294,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maropu,richard.moorhead,richard.moorhead,26/Oct/16 02:28,16/Dec/16 14:48,14/Jul/23 06:29,16/Dec/16 14:48,2.0.1,,,,,,,,2.1.1,2.2.0,,,SQL,,,,,,,,4,,,,,,"We have parquet data written from Spark1.6 that, when read from 2.0.1, produces errors.
{code}
case class A(a: Long, b: Int)
val as = Seq(A(1,2))
//partition explicitly written
spark.createDataFrame(as).write.parquet(""/data/a=1/"")
spark.read.parquet(""/data/"").collect
{code}
The above code fails; stack trace attached. 

If an integer used, explicit partition discovery succeeds.
{code}
case class A(a: Int, b: Int)
val as = Seq(A(1,2))
//partition explicitly written
spark.createDataFrame(as).write.parquet(""/data/a=1/"")
spark.read.parquet(""/data/"").collect
{code}
The action succeeds. Additionally, if 'partitionBy' is used instead of explicit writes, partition discovery succeeds. 

Question: Is the first example a reasonable use case? [PartitioningUtils|https://github.com/apache/spark/blob/branch-2.0/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala#L319] seems to default to Integer types unless the partition value exceeds the integer type's length.",,apachespark,holden,maropu,richard.moorhead,shipman05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/16 02:28;richard.moorhead;stacktrace.out;https://issues.apache.org/jira/secure/attachment/12835242/stacktrace.out",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 28 08:09:06 UTC 2016,,,,,,,,,,"0|i35ec7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/16 08:09;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/16030;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Insert overwrite statement runs much slower in spark-sql than it does in hive-client,SPARK-18107,13015286,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,snodawn,snodawn,26/Oct/16 01:53,17/Jun/21 18:28,14/Jul/23 06:29,01/Nov/16 07:23,2.0.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"I find insert overwrite statement running in spark-sql or spark-shell spends much more time than it does in  hive-client (i start it in apache-hive-2.0.1-bin/bin/hive ), where spark costs about ten minutes but hive-client just costs less than 20 seconds.

These are the steps I took.

Test sql is :

insert overwrite table login4game partition(pt='mix_en',dt='2016-10-21')    select distinct account_name,role_id,server,'1476979200' as recdate, 'mix' as platform, 'mix' as pid, 'mix' as dev from tbllog_login  where pt='mix_en' and  dt='2016-10-21' ;

there are 257128 lines of data in tbllog_login with partition(pt='mix_en',dt='2016-10-21')


ps:

I'm sure it must be ""insert overwrite"" costing a lot of time in spark, may be when doing overwrite, it need to spend a lot of time in io or in something else.

I also compare the executing time between insert overwrite statement and insert into statement.

1. insert overwrite statement and insert into statement in spark:

insert overwrite statement costs about 10 minutes
insert into statement costs about 30 seconds


2. insert into statement in spark and insert into statement in hive-client:

spark costs about 30 seconds
hive-client costs about 20 seconds
the difference is little that we can ignore





 
","spark 2.0.0
hive 2.0.1",apachespark,dkbiswal,hemanthbm12,KaiXu,KevinZwx,snodawn,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 17 18:28:55 UTC 2021,,,,,,,,,,"0|i35eaf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/16 08:52;viirya;Can you provide more information about this? Like is it a Hive table or data source table you insert overwrite into?;;;","27/Oct/16 02:56;snodawn;sure, the source table and the target table are both Hive table, like this:

source table:

CREATE TABLE `tbllog_login`(`server` string,`role_id` bigint, `account_name` string, `happened_time` int)
PARTITIONED BY (`pt` string, `dt` string)
stored as orc;

target table:

CREATE TABLE `login4game`(`account_name` string, `role_id` string, `server_id` string, `recdate` string)
PARTITIONED BY (`pt` string, `dt` string) stored as orc;

to confirm my guess, I also do some tests between insert overwrite and insert into statement, where insert overwrite sometimes can be separated into drop selected partition and then insert into table , so there are my steps:

insert overwrite statement is :

insert overwrite table login4game partition(pt='mix_en',dt='2016-10-21') select distinct account_name,role_id,server,'1476979200' as recdate, 'mix' as platform, 'mix' as pid, 'mix' as dev from tbllog_login where pt='mix_en' and dt='2016-10-21'

I separate it into two steps in spark:

1. ALTER TABLE login4game DROP IF EXISTS PARTITION (pt='mix_en', dt='2016-10-21') 

2. insert into table login4game partition(pt='mix_en',dt='2016-10-21') select distinct account_name,role_id,server,'1476979200' as recdate, 'mix' as platform, 'mix' as pid, 'mix' as dev from tbllog_login where pt='mix_en' and dt='2016-10-21'

where I can get the same result as it does in insert overwrite statement in spark, but it only costs me about 1 minutes, but if I runs insert overwrite statement, it will cost me about 10 minutes.


if it's necessary to provide the execution logs, I can upload it later.

;;;","27/Oct/16 07:07;viirya;I checked the current codes for inserting into Hive table in Spark. It delegates the operation to Hive library.

Because you use a much new Hive version (2.0.1), I guess the performance difference is between your Hive version and older Hive version Spark uses.;;;","27/Oct/16 07:12;viirya;I found a PR at Hive which should be the one to largely improve insert overwrite on Hive.

https://github.com/apache/hive/commit/ba21806b77287e237e1aa68fa169d2a81e07346d
;;;","27/Oct/16 07:54;viirya;I can create a PR for this. But it may require [~snodawn] to verify the performance since I don't have Hive 2.0.1 environment to compare them.;;;","27/Oct/16 14:21;snodawn;Thanks for your reply. I have tested the performance between hive 2.0.1 and hive 1.2.1, which verifies that you may be true !

The version 1.2.1 is downloaded from here, http://mirror.bit.edu.cn/apache/hive/hive-1.2.1/

where the patch [HIVE-11940] is available after hive 2.0.0

I run the same sql between version Hive 1.2.1 and Hive 2.0.1 with the same data.

In Hive 1.2.1, it costs 520.037 seconds to complete the insert overwrite statement which is similar to what it does in spark (with 1.2.1 hive version), but costs 32.183 seconds in completing the insert into statement.

In Hive 2.0.1, it costs 35.975 seconds to complete the insert overwrite statement.


I will paste the execution logs in a new comment to show the running process between Hive 1.2.1 and Hive 2.0.1 



;;;","27/Oct/16 14:25;snodawn;Here is the execution logs of Hive 1.2.1,  [Insert overwrite]


0: jdbc:hive2://master.mydata.com:23250> insert overwrite table login4game partition(pt='mix_en',dt='2016-10-21')    select distinct account_name,role_id,server,'1476979200' as recdate, 'mix' as platform, 'mix' as pid, 'mix' as dev from tbllog_login  where pt='mix_en' and  dt='2016-10-21' ;
INFO  : Number of reduce tasks not specified. Estimated from input data size: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:3
INFO  : Submitting tokens for job: job_1472611548204_72607
INFO  : The url to track the job: http://master.mydata.com:9378/proxy/application_1472611548204_72607/
INFO  : Starting Job = job_1472611548204_72607, Tracking URL = http://master.mydata.com:9378/proxy/application_1472611548204_72607/
INFO  : Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1472611548204_72607
INFO  : Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
INFO  : 2016-10-27 21:40:59,879 Stage-1 map = 0%,  reduce = 0%
INFO  : 2016-10-27 21:41:08,391 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 2.82 sec
INFO  : 2016-10-27 21:41:09,452 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 9.79 sec
INFO  : 2016-10-27 21:41:11,610 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 17.62 sec
INFO  : 2016-10-27 21:41:21,171 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 25.56 sec
INFO  : MapReduce Total cumulative CPU time: 25 seconds 560 msec
INFO  : Ended Job = job_1472611548204_72607
INFO  : Loading data to table my_log.login4game partition (pt=mix_en, dt=2016-10-21) from hdfs://master.mydata.com:45660/data/warehouse/staging/.hive-staging_hive_2016-10-27_21-40-48_927_5041661215303190236-1/-ext-10000
INFO  : Partition my_log.login4game{pt=mix_en, dt=2016-10-21} stats: [numFiles=1, numRows=66138, totalSize=485769, rawDataSize=41402388]
No rows affected (520.037 seconds);;;","27/Oct/16 14:26;viirya;Looks like HIVE-11940 largely improves insert overwrite performance. I have the patch ready and I will submit a PR tomorrow. It will be good if you can test it to see it improves the performance then.;;;","27/Oct/16 14:26;snodawn;Here is the execution logs of Hive 1.2.1, [Insert into]:

0: jdbc:hive2://master.mydata.com:23250> insert into table login4game partition(pt='mix_en',dt='2016-10-21')    select distinct account_name,role_id,server,'1476979200' as recdate, 'mix' as platform, 'mix' as pid, 'mix' as dev from tbllog_login  where pt='mix_en' and  dt='2016-10-21' ;
INFO  : Number of reduce tasks not specified. Estimated from input data size: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:3
INFO  : Submitting tokens for job: job_1472611548204_72608
INFO  : The url to track the job: http://master.mydata.com:9378/proxy/application_1472611548204_72608/
INFO  : Starting Job = job_1472611548204_72608, Tracking URL = http://master.mydata.com:9378/proxy/application_1472611548204_72608/
INFO  : Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1472611548204_72608
INFO  : Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
INFO  : 2016-10-27 21:51:37,717 Stage-1 map = 0%,  reduce = 0%
INFO  : 2016-10-27 21:51:46,455 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 3.17 sec
INFO  : 2016-10-27 21:51:48,576 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 16.16 sec
INFO  : 2016-10-27 21:51:56,945 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 22.7 sec
INFO  : MapReduce Total cumulative CPU time: 22 seconds 700 msec
INFO  : Ended Job = job_1472611548204_72608
INFO  : Loading data to table my_log.login4game partition (pt=mix_en, dt=2016-10-21) from hdfs://master.mydata.com:45660/data/warehouse/staging/.hive-staging_hive_2016-10-27_21-51-26_264_2085348807080462789-1/-ext-10000
INFO  : Partition my_log.login4game{pt=mix_en, dt=2016-10-21} stats: [numFiles=2, numRows=132276, totalSize=971551, rawDataSize=82804776]

No rows affected (32.183 seconds)
;;;","27/Oct/16 14:28;snodawn;Here is the execution logs of Hive 2.0.1, [Insert overwrite]:

0: jdbc:hive2://master.mydata.com:23250> insert overwrite table login4game partition(pt='mix_en',dt='2016-10-21')    select distinct account_name,role_id,server,'1476979200' as recdate, 'mix' as platform, 'mix' as pid, 'mix' as dev from tbllog_login  where pt='mix_en' and  dt='2016-10-21' ;
INFO  : Compiling command(queryId=hadoop_20161027215659_8a71044b-364c-4e41-a168-10213add0d5b): insert overwrite table login4game partition(pt='mix_en',dt='2016-10-21')    select distinct account_name,role_id,server,'1476979200' as recdate, 'mix' as platform, 'mix' as pid, 'mix' as dev from tbllog_login  where pt='mix_en' and  dt='2016-10-21'
INFO  : Semantic Analysis Completed
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_col0, type:string, comment:null), FieldSchema(name:_col1, type:string, comment:null), FieldSchema(name:_col2, type:string, comment:null), FieldSchema(name:_col3, type:string, comment:null), FieldSchema(name:_col4, type:string, comment:null), FieldSchema(name:_col5, type:string, comment:null), FieldSchema(name:_col6, type:string, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hadoop_20161027215659_8a71044b-364c-4e41-a168-10213add0d5b); Time taken: 1.142 seconds
DEBUG : Encoding valid txns info 109561:56926:56928:56930:56932:56934:56936:56938:56940:56942:56944:56946:56948:56950:56952:56954:56956:56958:56960:56962:56964:56966:56968:56970:56972:56978:56980:56982:56984:57076:57077:57078:57079:57080:57081:57096:57102:57106:57119:57121:57123:57124:57125:57126:57127:57128:57129:57130:57131:57132:57133:57134:57135:57136:57137:57138:57139:57140:57141:80435:80465:80715:80785:93705 txnid:0
INFO  : Executing command(queryId=hadoop_20161027215659_8a71044b-364c-4e41-a168-10213add0d5b): insert overwrite table login4game partition(pt='mix_en',dt='2016-10-21')    select distinct account_name,role_id,server,'1476979200' as recdate, 'mix' as platform, 'mix' as pid, 'mix' as dev from tbllog_login  where pt='mix_en' and  dt='2016-10-21'
WARN  : Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.
INFO  : WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.
INFO  : Query ID = hadoop_20161027215659_8a71044b-364c-4e41-a168-10213add0d5b
INFO  : Total jobs = 1
INFO  : Launching Job 1 out of 1
INFO  : Starting task [Stage-1:MAPRED] in serial mode
INFO  : Number of reduce tasks not specified. Estimated from input data size: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
DEBUG : Configuring job job_1472611548204_72609 with /tmp/hadoop-yarn/staging/hadoop/.staging/job_1472611548204_72609 as the submit dir
DEBUG : adding the following namenodes' delegation tokens:[hdfs://master.mydata.com:45660]
DEBUG : Creating splits at hdfs://master.mydata.com:45660/tmp/hadoop-yarn/staging/hadoop/.staging/job_1472611548204_72609
INFO  : number of splits:3
INFO  : Submitting tokens for job: job_1472611548204_72609
INFO  : The url to track the job: http://master.mydata.com:9378/proxy/application_1472611548204_72609/
INFO  : Starting Job = job_1472611548204_72609, Tracking URL = http://master.mydata.com:9378/proxy/application_1472611548204_72609/
INFO  : Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1472611548204_72609
INFO  : Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
INFO  : 2016-10-27 21:57:14,183 Stage-1 map = 0%,  reduce = 0%
INFO  : 2016-10-27 21:57:22,745 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 3.19 sec
INFO  : 2016-10-27 21:57:24,910 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 11.16 sec
INFO  : 2016-10-27 21:57:25,991 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 19.65 sec
INFO  : 2016-10-27 21:57:32,455 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 26.72 sec
INFO  : MapReduce Total cumulative CPU time: 26 seconds 720 msec
INFO  : Ended Job = job_1472611548204_72609
INFO  : Starting task [Stage-7:CONDITIONAL] in serial mode
INFO  : Stage-4 is selected by condition resolver.
INFO  : Stage-3 is filtered out by condition resolver.
INFO  : Stage-5 is filtered out by condition resolver.
INFO  : Starting task [Stage-4:MOVE] in serial mode
INFO  : Moving data to: hdfs://master.mydata.com:45660/data/warehouse/staging/.hive-staging_hive_2016-10-27_21-56-59_013_3776500562235975275-1/-ext-10000 from hdfs://master.mydata.com:45660/data/warehouse/staging/.hive-staging_hive_2016-10-27_21-56-59_013_3776500562235975275-1/-ext-10002
INFO  : Starting task [Stage-0:MOVE] in serial mode
INFO  : Loading data to table my_log.login4game partition (pt=mix_en, dt=2016-10-21) from hdfs://master.mydata.com:45660/data/warehouse/staging/.hive-staging_hive_2016-10-27_21-56-59_013_3776500562235975275-1/-ext-10000
INFO  : Starting task [Stage-2:STATS] in serial mode
INFO  : MapReduce Jobs Launched: 
INFO  : Stage-Stage-1: Map: 3  Reduce: 1   Cumulative CPU: 26.72 sec   HDFS Read: 53287090 HDFS Write: 485485 SUCCESS
INFO  : Total MapReduce CPU Time Spent: 26 seconds 720 msec
INFO  : Completed executing command(queryId=hadoop_20161027215659_8a71044b-364c-4e41-a168-10213add0d5b); Time taken: 34.541 seconds
INFO  : OK
DEBUG : Shutting down query insert overwrite table login4game partition(pt='mix_en',dt='2016-10-21')    select distinct account_name,role_id,server,'1476979200' as recdate, 'mix' as platform, 'mix' as pid, 'mix' as dev from tbllog_login  where pt='mix_en' and  dt='2016-10-21'
No rows affected (35.975 seconds);;;","28/Oct/16 01:38;snodawn;Ok, it sounds good, thanks! I would have a try later.;;;","28/Oct/16 02:08;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/15667;;;","29/Oct/16 07:33;snodawn; I have tested the performance before and after the patch [https://github.com/apache/spark/pull/15667] . But it seems to improve a few after patching, where it costs 531 seconds before patching, and costs 518 seconds after patching. 

I will add the execution logs in a new comment later.;;;","29/Oct/16 07:40;snodawn;insert overwrite in spark 2.0.0， without patch

scala> val befores =System.currentTimeMillis();spark.sql(""insert overwrite table login4game partition(pt='mix_en',dt='2016-10-21')    select distinct account_name,role_id,server,'1476979200' as recdate, 'mix' as platform, 'mix' as pid, 'mix' as dev from tbllog_login  where pt='mix_en' and  dt='2016-10-21' "");val interval=System.currentTimeMillis()-befores;println(s""insertval is => ${interval/1000} seconds"")
3549.075: [GC [PSYoungGen: 435328K->77744K(566784K)] 826575K->469112K(1232384K), 0.0463220 secs] [Times: user=0.27 sys=0.00, real=0.04 secs] 
3549.394: [GC [PSYoungGen: 511920K->26018K(566784K)] 903288K->459991K(1232384K), 0.0804300 secs] [Times: user=0.44 sys=0.00, real=0.08 secs] 
[Stage 4:========>                                                (11 + 8) / 72]3549.698: [GC [PSYoungGen: 463266K->55486K(493056K)] 897239K->490323K(1158656K), 0.0359570 secs] [Times: user=0.17 sys=0.00, real=0.04 secs] 
[Stage 4:===============>                                         (19 + 8) / 72]3549.929: [GC [PSYoungGen: 492734K->82976K(563712K)] 927571K->522173K(1229312K), 0.0328120 secs] [Times: user=0.16 sys=0.01, real=0.03 secs] 
[Stage 4:===============================>                         (40 + 8) / 72]3550.166: [GC [PSYoungGen: 520736K->29045K(564736K)] 959933K->468617K(1230336K), 0.0244060 secs] [Times: user=0.12 sys=0.00, real=0.02 secs] 
[Stage 4:====================================>                    (46 + 8) / 72]3550.392: [GC [PSYoungGen: 466309K->41380K(567296K)] 905881K->481176K(1232896K), 0.0320150 secs] [Times: user=0.16 sys=0.00, real=0.04 secs] 
[Stage 4:===========================================>             (55 + 8) / 72]3550.603: [GC [PSYoungGen: 486820K->96197K(541696K)] 926616K->536271K(1207296K), 0.0326490 secs] [Times: user=0.17 sys=0.00, real=0.03 secs] 
[Stage 4:=================================================>       (63 + 8) / 72]3550.868: [GC [PSYoungGen: 541637K->21696K(567296K)] 981711K->462242K(1232896K), 0.0259070 secs] [Times: user=0.11 sys=0.00, real=0.03 secs] 
[Stage 5:>                                                        (0 + 0) / 200]3551.127: [GC [PSYoungGen: 457912K->125270K(561664K)] 898458K->565984K(1227264K), 0.0497940 secs] [Times: user=0.25 sys=0.00, real=0.05 secs] 
3551.328: [GC [PSYoungGen: 561494K->104705K(527872K)] 1002208K->552355K(1193472K), 0.0489880 secs] [Times: user=0.28 sys=0.00, real=0.05 secs] 
3551.513: [GC [PSYoungGen: 494819K->94833K(485376K)] 942469K->544947K(1150976K), 0.0472640 secs] [Times: user=0.26 sys=0.00, real=0.05 secs] 
[Stage 5:====>                                                   (17 + 8) / 200]3551.701: [GC [PSYoungGen: 484977K->90004K(545792K)] 935091K->544576K(1211392K), 0.0543700 secs] [Times: user=0.33 sys=0.00, real=0.06 secs] 
[Stage 5:=======>                                                (25 + 8) / 200]3551.878: [GC [PSYoungGen: 480005K->96725K(543744K)] 934833K->565661K(1209344K), 0.0475640 secs] [Times: user=0.24 sys=0.00, real=0.05 secs] 
[Stage 5:=========>                                              (34 + 8) / 200]3552.093: [GC [PSYoungGen: 486869K->86720K(539136K)] 964767K->567333K(1204736K), 0.0383360 secs] [Times: user=0.23 sys=0.00, real=0.04 secs] 
[Stage 5:===========>                                            (40 + 8) / 200]3552.351: [GC [PSYoungGen: 478363K->73147K(541696K)] 958976K->556404K(1207296K), 0.0401180 secs] [Times: user=0.21 sys=0.00, real=0.04 secs] 
[Stage 5:=============>                                          (48 + 8) / 200]3552.519: [GC [PSYoungGen: 464827K->74781K(541184K)] 948084K->560529K(1206784K), 0.0459060 secs] [Times: user=0.26 sys=0.00, real=0.05 secs] 
[Stage 5:================>                                       (60 + 8) / 200]3552.834: [GC [PSYoungGen: 480797K->68184K(543232K)] 966545K->555417K(1208832K), 0.0403320 secs] [Times: user=0.21 sys=0.00, real=0.05 secs] 
[Stage 5:==================>                                     (67 + 8) / 200]3553.031: [GC [PSYoungGen: 474200K->53137K(560640K)] 961433K->541548K(1226240K), 0.0306190 secs] [Times: user=0.15 sys=0.00, real=0.03 secs] 
[Stage 5:=====================>                                  (77 + 8) / 200]3553.219: [GC [PSYoungGen: 481681K->54172K(559616K)] 970092K->543955K(1225216K), 0.0334520 secs] [Times: user=0.17 sys=0.00, real=0.03 secs] 
[Stage 5:========================>                               (88 + 8) / 200]3553.408: [GC [PSYoungGen: 482621K->61091K(564736K)] 972404K->552016K(1230336K), 0.0398400 secs] [Times: user=0.20 sys=0.00, real=0.04 secs] 
[Stage 5:==========================>                             (96 + 8) / 200]3553.596: [GC [PSYoungGen: 500899K->63593K(564736K)] 992091K->556228K(1230336K), 0.0350330 secs] [Times: user=0.20 sys=0.00, real=0.03 secs] 
3553.780: [GC [PSYoungGen: 503401K->63313K(570368K)] 996039K->557297K(1235968K), 0.0344000 secs] [Times: user=0.19 sys=0.00, real=0.04 secs] 
[Stage 5:============================>                          (104 + 8) / 200]3553.962: [GC [PSYoungGen: 516945K->75754K(570368K)] 1010929K->571824K(1235968K), 0.0439400 secs] [Times: user=0.27 sys=0.00, real=0.05 secs] 
[Stage 5:================================>                      (119 + 8) / 200]3554.250: [GC [PSYoungGen: 529386K->51713K(581120K)] 1025456K->550074K(1246720K), 0.0335360 secs] [Times: user=0.19 sys=0.00, real=0.03 secs] 
[Stage 5:===================================>                   (128 + 8) / 200]3554.449: [GC [PSYoungGen: 518657K->82800K(580096K)] 1017018K->582606K(1245696K), 0.0401180 secs] [Times: user=0.24 sys=0.00, real=0.04 secs] 
[Stage 5:=====================================>                 (136 + 8) / 200]3554.662: [GC [PSYoungGen: 549674K->77682K(584192K)] 1049481K->580293K(1249792K), 0.0385940 secs] [Times: user=0.23 sys=0.01, real=0.04 secs] 
[Stage 5:=======================================>               (144 + 8) / 200]3554.863: [GC [PSYoungGen: 549652K->96461K(568832K)] 1052263K->602122K(1234432K), 0.0415910 secs] [Times: user=0.25 sys=0.00, real=0.05 secs] 
[Stage 5:==========================================>            (153 + 8) / 200]3555.071: [GC [PSYoungGen: 568372K->95716K(573952K)] 1074033K->616225K(1239552K), 0.0424680 secs] [Times: user=0.24 sys=0.01, real=0.04 secs] 
[Stage 5:============================================>          (161 + 8) / 200]3555.277: [GC [PSYoungGen: 551396K->83988K(577536K)] 1071905K->606963K(1243136K), 0.0379390 secs] [Times: user=0.22 sys=0.00, real=0.04 secs] 
[Stage 5:==============================================>        (170 + 8) / 200]3555.482: [GC [PSYoungGen: 539448K->99534K(569856K)] 1062423K->624985K(1235456K), 0.0403270 secs] [Times: user=0.23 sys=0.01, real=0.04 secs] 
[Stage 5:================================================>      (178 + 8) / 200]3555.688: [GC [PSYoungGen: 548828K->68376K(574464K)] 1074279K->596302K(1240064K), 0.0304960 secs] [Times: user=0.18 sys=0.00, real=0.03 secs] 
[Stage 5:===================================================>   (186 + 8) / 200]3555.867: [GC [PSYoungGen: 517912K->72104K(565760K)] 1045838K->601477K(1231360K), 0.0294950 secs] [Times: user=0.18 sys=0.00, real=0.03 secs] 
Moved: 'hdfs://master.com:45660/data/warehouse/my_log.db/login4game/pt=mix_en/dt=2016-10-21/part-00000' to trash at: hdfs://master.com:45660/user/hadoop/.Trash/Current
Moved: 'hdfs://master.com:45660/data/warehouse/my_log.db/login4game/pt=mix_en/dt=2016-10-21/part-00000_copy_1' to trash at: hdfs://master.com:45660/user/hadoop/.Trash/Current
Moved: 'hdfs://master.com:45660/data/warehouse/my_log.db/login4game/pt=mix_en/dt=2016-10-21/part-00001' to trash at: hdfs://master.com:45660/user/hadoop/.Trash/Current
Moved: 'hdfs://master.com:45660/data/warehouse/my_log.db/login4game/pt=mix_en/dt=2016-10-21/part-00001_copy_1' to trash at: hdfs://master.com:45660/user/hadoop/.Trash/Current

....

Moved: 'hdfs://master.com:45660/data/warehouse/my_log.db/login4game/pt=mix_en/dt=2016-10-21/part-00199' to trash at: hdfs://master.com:45660/user/hadoop/.Trash/Current
Moved: 'hdfs://master.com:45660/data/warehouse/my_log.db/login4game/pt=mix_en/dt=2016-10-21/part-00199_copy_1' to trash at: hdfs://master.com:45660/user/hadoop/.Trash/Current
3561.575: [GC [PSYoungGen: 524712K->1952K(570880K)] 1054085K->532733K(1236480K), 0.0148940 secs] [Times: user=0.06 sys=0.00, real=0.01 secs] 
3562.795: [GC [PSYoungGen: 454560K->128K(560128K)] 985341K->531885K(1225728K), 0.0088420 secs] [Times: user=0.04 sys=0.00, real=0.01 secs] 
3614.152: [GC [PSYoungGen: 144044K->1440K(434688K)] 675801K->533205K(1100288K), 0.0063920 secs] [Times: user=0.02 sys=0.01, real=0.01 secs] 
3614.159: [Full GC [PSYoungGen: 1440K->0K(434688K)] [ParOldGen: 531765K->191685K(665600K)] 533205K->191685K(1100288K) [PSPermGen: 123513K->123511K(253440K)], 0.5924720 secs] [Times: user=2.71 sys=0.00, real=0.59 secs] 
3820.964: [GC [PSYoungGen: 433152K->1120K(560128K)] 624837K->192805K(1225728K), 0.0057470 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] 
4052.337: [GC [PSYoungGen: 425056K->1088K(561664K)] 616741K->192965K(1227264K), 0.0048500 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] 
16/10/29 14:02:40 WARN log: Updating partition stats fast for: login4game
16/10/29 14:02:40 WARN log: Updated size to 795542
insertval is => 531 seconds
befores: Long = 1477720430016
interval: Long = 531015;;;","29/Oct/16 07:45;snodawn;insert overwrite in spark 2.1.0, with patch

scala> val befores =System.currentTimeMillis();spark.sql(""insert overwrite table login4game partition(pt='mix_en',dt='2016-10-21')    select distinct account_name,role_id,server,'1476979200' as recdate, 'mix' as platform, 'mix' as pid, 'mix' as dev from tbllog_login  where pt='mix_en' and  dt='2016-10-21' "");val interval=System.currentTimeMillis()-befores;println(s""insertval is => ${interval/1000} seconds"")
139.850: [GC [PSYoungGen: 656889K->42484K(656896K)] 789101K->206163K(1343488K), 0.0704840 secs] [Times: user=0.33 sys=0.09, real=0.07 secs] 
16/10/29 14:24:12 WARN HiveConf: HiveConf of name hive.server2.auth.hadoop does not exist
[Stage 0:>                                                         (0 + 8) / 72]144.774: [GC [PSYoungGen: 656884K->42494K(465408K)] 820563K->323651K(1152000K), 0.0995720 secs] [Times: user=0.47 sys=0.09, real=0.10 secs] 
[Stage 0:==========>                                              (13 + 8) / 72]145.697: [GC [PSYoungGen: 465406K->42661K(465920K)] 746563K->359222K(1152512K), 0.1012790 secs] [Times: user=0.54 sys=0.05, real=0.10 secs] 
[Stage 0:==================>                                      (23 + 8) / 72]146.232: [GC [PSYoungGen: 465573K->20917K(547840K)] 782134K->340686K(1234432K), 0.0262040 secs] [Times: user=0.12 sys=0.00, real=0.03 secs] 
[Stage 0:===============================>                         (40 + 8) / 72]146.725: [GC [PSYoungGen: 427445K->8807K(415744K)] 747214K->332722K(1102336K), 0.0243020 secs] [Times: user=0.08 sys=0.01, real=0.02 secs] 
[Stage 0:=========================================>               (53 + 8) / 72]147.178: [GC [PSYoungGen: 415335K->9865K(545792K)] 739250K->334857K(1232384K), 0.0186080 secs] [Times: user=0.08 sys=0.00, real=0.02 secs] 
[Stage 1:>                                                        (0 + 8) / 200]148.353: [GC [PSYoungGen: 404916K->38320K(433664K)] 729907K->363760K(1120256K), 0.0260660 secs] [Times: user=0.10 sys=0.00, real=0.03 secs] 
[Stage 1:==>                                                      (8 + 8) / 200]149.077: [GC [PSYoungGen: 433417K->47237K(550912K)] 758856K->376034K(1237504K), 0.0219810 secs] [Times: user=0.09 sys=0.00, real=0.02 secs] 
[Stage 1:====>                                                   (17 + 8) / 200]149.800: [GC [PSYoungGen: 450693K->17621K(544256K)] 779490K->349370K(1230848K), 0.0217030 secs] [Times: user=0.11 sys=0.00, real=0.02 secs] 
[Stage 1:=========>                                              (34 + 8) / 200]150.319: [GC [PSYoungGen: 420875K->25444K(553984K)] 752624K->359975K(1240576K), 0.0196560 secs] [Times: user=0.11 sys=0.01, real=0.02 secs] 
[Stage 1:==========>                                             (38 + 9) / 200]150.590: [GC [PSYoungGen: 440156K->51351K(550400K)] 774687K->388898K(1236992K), 0.0216630 secs] [Times: user=0.10 sys=0.01, real=0.02 secs] 
[Stage 1:================>                                       (58 + 8) / 200]150.883: [GC [PSYoungGen: 466010K->13379K(553472K)] 803557K->353656K(1240064K), 0.0202700 secs] [Times: user=0.09 sys=0.01, real=0.02 secs] 
[Stage 1:==================>                                     (67 + 8) / 200]151.106: [GC [PSYoungGen: 439211K->53013K(551936K)] 779488K->395761K(1238528K), 0.0248140 secs] [Times: user=0.11 sys=0.01, real=0.02 secs] 
[Stage 1:=====================>                                  (75 + 8) / 200]151.401: [GC [PSYoungGen: 478804K->30308K(569344K)] 821551K->375786K(1255936K), 0.0210760 secs] [Times: user=0.08 sys=0.00, real=0.02 secs] 
[Stage 1:=======================>                                (83 + 8) / 200]151.621: [GC [PSYoungGen: 480205K->55779K(566272K)] 825682K->404149K(1252864K), 0.0271460 secs] [Times: user=0.12 sys=0.00, real=0.03 secs] 
[Stage 1:===========================>                            (99 + 8) / 200]151.966: [GC [PSYoungGen: 505776K->44022K(577536K)] 854146K->395148K(1264128K), 0.0235970 secs] [Times: user=0.11 sys=0.00, real=0.02 secs] 
[Stage 1:==============================>                        (112 + 8) / 200]152.211: [GC [PSYoungGen: 513424K->22242K(576512K)] 864550K->376234K(1263104K), 0.0188640 secs] [Times: user=0.07 sys=0.01, real=0.02 secs] 
[Stage 1:=================================>                     (123 + 8) / 200]152.420: [GC [PSYoungGen: 491713K->72455K(584704K)] 845705K->427875K(1271296K), 0.0251320 secs] [Times: user=0.10 sys=0.00, real=0.02 secs] 
[Stage 1:====================================>                  (132 + 8) / 200]152.672: [GC [PSYoungGen: 552126K->35637K(589312K)] 907546K->393850K(1275904K), 0.0223350 secs] [Times: user=0.09 sys=0.01, real=0.02 secs] 
[Stage 1:============================================>          (160 + 8) / 200]152.907: [GC [PSYoungGen: 515366K->41942K(579584K)] 873578K->402911K(1266176K), 0.0223650 secs] [Times: user=0.09 sys=0.01, real=0.02 secs] 
[Stage 1:===============================================>       (171 + 8) / 200]153.141: [GC [PSYoungGen: 526730K->32197K(584704K)] 887699K->395982K(1271296K), 0.0229770 secs] [Times: user=0.11 sys=0.00, real=0.02 secs] 
[Stage 1:==================================================>    (184 + 8) / 200]153.375: [GC [PSYoungGen: 516922K->54805K(595456K)] 880707K->420731K(1282048K), 0.0226350 secs] [Times: user=0.11 sys=0.01, real=0.03 secs] 
16/10/29 14:24:27 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
Moved: 'hdfs://master.com:45660/data/warehouse/qjzs_log.db/login4game/pt=mix_en/dt=2016-10-21' to trash at: hdfs://master.com:45660/user/hadoop/.Trash/Current
157.468: [GC [PSYoungGen: 557589K->3078K(598528K)] 923515K->371833K(1285120K), 0.0185200 secs] [Times: user=0.06 sys=0.01, real=0.02 secs] 
230.673: [GC [PSYoungGen: 505862K->1504K(599552K)] 874617K->371303K(1286144K), 0.0089300 secs] [Times: user=0.04 sys=0.00, real=0.01 secs] 
468.206: [GC [PSYoungGen: 503264K->1216K(600576K)] 873063K->371447K(1287168K), 0.0077540 secs] [Times: user=0.03 sys=0.00, real=0.01 secs] 
16/10/29 14:32:43 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
16/10/29 14:32:47 WARN log: Updating partition stats fast for: login4game
16/10/29 14:32:47 WARN log: Updated size to 795542
insertval is => 518 seconds
befores: Long = 1477722249349
interval: Long = 518026
;;;","02/Nov/16 02:31;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/15726;;;","16/May/19 13:43;KaiXu;it seems this issue have not been fixed? I encountered this issue with spark2.4.3, the query I run is from TPC-DS, [https://github.com/hortonworks/hive-testbench/blob/hdp3/ddl-tpcds/bin_partitioned/store_sales.sql];;;","17/Jun/21 18:28;hemanthbm12;We are seeing this issue still exists on Spark versions 2.3.1 and 2.4.7. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Analyze Table accepts a garbage identifier at the end,SPARK-18106,13015281,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,vssrinath,vssrinath,26/Oct/16 00:52,30/Oct/16 22:25,14/Jul/23 06:29,30/Oct/16 22:25,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"{noformat}
scala> sql(""create table test(a int)"")
res2: org.apache.spark.sql.DataFrame = []

scala> sql(""analyze table test compute statistics blah"")
res3: org.apache.spark.sql.DataFrame = []
{noformat}

An identifier that is not ""noscan"" produces an AnalyzeTableCommand with noscan=false",,apachespark,dongjoon,vssrinath,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 26 05:34:05 UTC 2016,,,,,,,,,,"0|i35e9b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/16 04:25;dongjoon;Thank you for reporting this bug.
I'll make a PR to fix this.
;;;","26/Oct/16 05:34;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/15640;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark distributed cache should throw exception if same file is specified to dropped in --files --archives,SPARK-18099,13015169,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kishorvpatil,kishorvpatil,kishorvpatil,25/Oct/16 18:10,17/May/20 18:13,14/Jul/23 06:29,03/Nov/16 21:13,2.0.0,2.0.1,,,,,,,2.1.0,2.2.0,,,Spark Core,YARN,,,,,,,0,,,,,,"Recently, for the changes to [SPARK-14423] Handle jar conflict issue when uploading to distributed cache
If by default yarn#client will upload all the --files and --archives in assembly to HDFS staging folder. It should throw if file appears in both --files and --archives exception to know whether uncompress or leave the file compressed.",,apachespark,kishorvpatil,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18357,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 25 18:25:06 UTC 2016,,,,,,,,,,"0|i35dkf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/16 18:25;apachespark;User 'kishorvpatil' has created a pull request for this issue:
https://github.com/apache/spark/pull/15627;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix default value test in SQLConfSuite to work regardless of warehouse dir's existence,SPARK-18093,13015066,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mgrover,mgrover,mgrover,25/Oct/16 13:36,26/Oct/16 16:08,14/Jul/23 06:29,26/Oct/16 16:08,2.0.2,2.1.0,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,,,,,,"At least on my mac (with JDK 1.7.0_67), {{default value of WAREHOUSE_PATH}} in SQLConfSuite fails because left side of the assert doesn't have a trailing slash while the right does.

As [~srowen] mentions [here|https://github.com/apache/spark/pull/15382#discussion_r84240197], the JVM adds a trailing slash if the directory exists and doesn't if it doesn't. I think it'd be good for the test to work regardless of the directory's existence.",,apachespark,mgrover,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 25 13:44:28 UTC 2016,,,,,,,,,,"0|i35cxj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/16 13:40;srowen;Hm, I thought it would definitely exist at that point in the test. It passes on my Mac and on Jenkins builds. Still there's no harm in making it a non-issue in this test, and matching regardless of trailing slash.;;;","25/Oct/16 13:44;apachespark;User 'markgrover' has created a pull request for this issue:
https://github.com/apache/spark/pull/15623;;;","25/Oct/16 13:44;mgrover;Yeah, I thought so too - but it failed on two different environments for me - an internal Jenkins job and my mac. Perhaps, it's related to some of the profiles/properties I am setting?

Anyways, filed a PR. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deep if expressions cause Generated SpecificUnsafeProjection code to exceed JVM code size limit,SPARK-18091,13015002,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,kapilsingh5050,kapilsingh5050,kapilsingh5050,25/Oct/16 08:02,23/Feb/17 00:58,14/Jul/23 06:29,04/Dec/16 09:20,1.6.1,,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,,,,,,"*Problem Description:*
I have an application in which a lot of if-else decisioning is involved to generate output. I'm getting following exception:
Caused by: org.codehaus.janino.JaninoRuntimeException: Code of method ""(Lorg/apache/spark/sql/catalyst/expressions/GeneratedClass$SpecificUnsafeProjection;Lorg/apache/spark/sql/catalyst/InternalRow;)V"" of class ""org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection"" grows beyond 64 KB
	at org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:941)
	at org.codehaus.janino.CodeContext.write(CodeContext.java:874)
	at org.codehaus.janino.CodeContext.writeBranch(CodeContext.java:965)
	at org.codehaus.janino.UnitCompiler.writeBranch(UnitCompiler.java:10261)

*Steps to Reproduce:*
I've come up with a unit test which I was able to run in CodeGenerationSuite.scala:
{code}
test(""split large if expressions into blocks due to JVM code size limit"") {
    val row = create_row(""afafFAFFsqcategory2dadDADcategory8sasasadscategory24"", 0)
    val inputStr = 'a.string.at(0)
    val inputIdx = 'a.int.at(1)

    val length = 10
    val valuesToCompareTo = for (i <- 1 to (length + 1)) yield (""category"" + i)

    val initCondition = EqualTo(RegExpExtract(inputStr, Literal(""category1""), inputIdx), valuesToCompareTo(0))
    var res: Expression = If(initCondition, Literal(""category1""), Literal(""NULL""))
    var cummulativeCondition: Expression = Not(initCondition)
    for (index <- 1 to length) {
      val valueExtractedFromInput = RegExpExtract(inputStr, Literal(""category"" + (index + 1).toString), inputIdx)
      val currComparee = If(cummulativeCondition, valueExtractedFromInput, Literal(""NULL""))
      val currCondition = EqualTo(currComparee, valuesToCompareTo(index))
      val combinedCond = And(cummulativeCondition, currCondition)
      res = If(combinedCond, If(combinedCond, valueExtractedFromInput, Literal(""NULL"")), res)
      cummulativeCondition = And(Not(currCondition), cummulativeCondition)
    }

    val expressions = Seq(res)
    val plan = GenerateUnsafeProjection.generate(expressions, true)
    val actual = plan(row).toSeq(expressions.map(_.dataType))
    val expected = Seq(UTF8String.fromString(""category2""))

    if (!checkResult(actual, expected)) {
      fail(s""Incorrect Evaluation: expressions: $expressions, actual: $actual, expected: $expected"")
    }
  }
{code}

*Root Cause:*
Current splitting of Projection codes doesn't (and can't) take care of splitting the generated code for individual output column expressions. So it can grow to exceed JVM limit.

*Note:* This issue seems related to SPARK-14887 but I'm not sure whether the root cause is same
 
*Proposed Fix:*
If expression should place it's predicate, true value and false value expressions' generated code in separate methods in context and call these methods instead of putting the whole code directly in its generated code",,apachespark,jsoltren,kapilsingh5050,kiszk,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 17 11:20:12 UTC 2017,,,,,,,,,,"0|i35cjb:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"25/Oct/16 08:06;kapilsingh5050;I've started working on this;;;","25/Oct/16 09:43;apachespark;User 'kapilsingh5050' has created a pull request for this issue:
https://github.com/apache/spark/pull/15620;;;","05/Dec/16 13:41;apachespark;User 'kapilsingh5050' has created a pull request for this issue:
https://github.com/apache/spark/pull/16146;;;","17/Feb/17 11:20;jsoltren;FWIW, anyone pulling this fix in the future will also want https://github.com/apache/spark/pull/16244, or else a bunch of tests will fail.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regression: Hive variables no longer work in Spark 2.0,SPARK-18086,13014946,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rdblue,rdblue,rdblue,25/Oct/16 00:35,07/May/17 04:07,14/Jul/23 06:29,08/Nov/16 01:35,2.0.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"The behavior of variables in the SQL shell has changed from 1.6 to 2.0. Specifically, --hivevar name=value and {{SET hivevar:name=value}} no longer work. Queries that worked correctly in 1.6 will either fail or produce unexpected results in 2.0 so I think this is a regression that should be addressed.

Hive and Spark 1.6 work like this:
1. Command-line args --hiveconf and --hivevar can be used to set session properties. --hiveconf properties are added to the Hadoop Configuration.
2. {{SET}} adds a Hive Configuration property, {{SET hivevar:<name>=<value>}} adds a Hive var.
3. Hive vars can be substituted into queries by name, and Configuration properties can be substituted using {{hiveconf:name}}.

In 2.0, hiveconf, sparkconf, and conf variable prefixes are all removed, then the value in SQLConf for the rest of the key is returned. SET adds properties to the session config and (according to [a comment|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/RuntimeConfig.scala#L28]) the Hadoop configuration ""during I/O"".

{code:title=Hive and Spark 1.6.1 behavior}
[user@host:~]: spark-sql --hiveconf test.conf=1 --hivevar test.var=2
spark-sql> select ""${hiveconf:test.conf}"";
1
spark-sql> select ""${test.conf}"";
${test.conf}
spark-sql> select ""${hivevar:test.var}"";
2
spark-sql> select ""${test.var}"";
2
spark-sql> set test.set=3;
SET test.set=3
spark-sql> select ""${test.set}""
""${test.set}""
spark-sql> select ""${hivevar:test.set}""
""${hivevar:test.set}""
spark-sql> select ""${hiveconf:test.set}""
3
spark-sql> set hivevar:test.setvar=4;
SET hivevar:test.setvar=4
spark-sql> select ""${hivevar:test.setvar}"";
4
spark-sql> select ""${test.setvar}"";
4
{code}

{code:title=Spark 2.0.0 behavior}
[user@host:~]: spark-sql --hiveconf test.conf=1 --hivevar test.var=2
spark-sql> select ""${hiveconf:test.conf}"";
1
spark-sql> select ""${test.conf}"";
1
spark-sql> select ""${hivevar:test.var}"";
${hivevar:test.var}
spark-sql> select ""${test.var}"";
${test.var}
spark-sql> set test.set=3;
test.set        3
spark-sql> select ""${test.set}"";
3
spark-sql> set hivevar:test.setvar=4;
hivevar:test.setvar      4
spark-sql> select ""${hivevar:test.setvar}"";
4
spark-sql> select ""${test.setvar}"";
${test.setvar}
{code}",,apachespark,rdblue,rxin,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-13983,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 03 23:05:54 UTC 2016,,,,,,,,,,"0|i35c6v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/16 18:47;apachespark;User 'rdblue' has created a pull request for this issue:
https://github.com/apache/spark/pull/15738;;;","02/Nov/16 18:50;rdblue;[~rxin], I think the fix for this should go into 2.1.0

The linked PR stores Hive variables in the Hive SessionState and makes them accessable as either {{hivevar:<name>}} or {{<name>}} with higher precedence than configuration variables, to match the behavior of Hive if there is a conflict.;;;","02/Nov/16 18:55;rxin;[~rdblue] There are two separate issues here I believe:

1. Parsing hiveconf in spark-sql shell.
2. Having separate config values.

1 makes sense to me for backward compatibility. How much do you care about 2? I'd argue it's a somewhat esoteric feature to require precedence in hvieconf vs hivevar....;;;","02/Nov/16 19:20;rdblue;Hive variables are set on the Hive SessionState and I think it is a good idea to continue storing them there in case Hive needs to access them. If that is the case, then it is necessary to decide which set of values is checked first: configuration or variables. To match Hive's behavior, I went with variables.;;;","02/Nov/16 22:18;rxin;The thing is that we don't really propagate Hive's session state over to most places, except to the part that connects to the metastore. What we do propagate is to send all the settings from SQLConf over to Hadoop Configuration and HiveConf.

It seems like this would solve almost all your problems by just implementing parsing hiveconf in spark-sql shell.
;;;","02/Nov/16 23:11;rdblue;What is the rationale for propagating configuration but not variables?

This also handles the case where there are collisions, though that is unlikely.;;;","02/Nov/16 23:13;rxin;Because the execution code no longer depends on Hive's internals. Only the catalog (metastore) depends on Hive internals, which doesn't use session variables I believe.
;;;","03/Nov/16 20:37;rxin;[~rdblue] Does my explanation make sense? Can you change the pr (or have a new pr) to just do the command line argument so we can get that into 2.1?
;;;","03/Nov/16 23:05;rdblue;Yeah, I'll update the PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix default Locale used in DateFormat, NumberFormat to Locale.US",SPARK-18076,13014750,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,srowen,srowen,24/Oct/16 14:48,02/Nov/16 09:39,14/Jul/23 06:29,02/Nov/16 09:39,2.0.1,,,,,,,,2.1.0,,,,MLlib,Spark Core,SQL,,,,,,1,releasenotes,,,,,"Many parts of the code use {{DateFormat}} and {{NumberFormat}} instances. Although the behavior of these format is mostly determined by things like format strings, the exact behavior can vary according to the platform's default locale. Although the locale defaults to ""en"", it can be set to something else by env variables. And if it does, it can cause the same code to succeed or fail based just on locale:

{code}
import java.text._
import java.util._

def parse(s: String, l: Locale) = new SimpleDateFormat(""yyyyMMMdd"", l).parse(s)

parse(""1989Dec31"", Locale.US)
Sun Dec 31 00:00:00 GMT 1989

parse(""1989Dec31"", Locale.UK)
Sun Dec 31 00:00:00 GMT 1989

parse(""1989Dec31"", Locale.CHINA)
java.text.ParseException: Unparseable date: ""1989Dec31""
  at java.text.DateFormat.parse(DateFormat.java:366)
  at .parse(<console>:18)
  ... 32 elided

parse(""1989Dec31"", Locale.GERMANY)
java.text.ParseException: Unparseable date: ""1989Dec31""
  at java.text.DateFormat.parse(DateFormat.java:366)
  at .parse(<console>:18)
  ... 32 elided
{code}

Where not otherwise specified, I believe all instances in the code should default to some fixed value, and that should probably be {{Locale.US}}. This matches the JVM's default, and specifies both language (""en"") and region (""US"") to remove ambiguity. This most closely matches what the current code behavior would be (unless default locale was changed), because it will currently default to ""en"".

This affects SQL date/time functions. At the moment, the only SQL function that lets the user specify language/country is ""sentences"", which is consistent with Hive.

It affects dates passed in the JSON API. 

It affects some strings rendered in the UI, potentially. Although this isn't a correctness issue, there may be an argument for not letting that vary (?)

It affects a bunch of instances where dates are formatted into strings for things like IDs or file names, which is far less likely to cause a problem, but worth making consistent.

The other occurrences are in tests.


The downside to this change is also its upside: the behavior doesn't depend on default JVM locale, but, also can't be affected by the default JVM locale. For example, if you wanted to parse some dates in a way that depended on an non-US locale (not just the format string) then it would no longer be possible. There's no means of specifying this, for example, in SQL functions for parsing dates. However, controlling this by globally changing the locale isn't exactly great either.

The purpose of this change is to make the current default behavior deterministic and fixed. PR coming.

CC [~hyukjin.kwon]",,apachespark,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 02 09:39:33 UTC 2016,,,,,,,,,,"0|i35azb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/16 14:58;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15610;;;","02/Nov/16 09:39;srowen;Issue resolved by pull request 15610
[https://github.com/apache/spark/pull/15610];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
binary operator should not consider nullability when comparing input types,SPARK-18070,13014632,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,24/Oct/16 05:29,25/Oct/16 19:08,14/Jul/23 06:29,25/Oct/16 19:08,,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 25 19:08:42 UTC 2016,,,,,,,,,,"0|i35a93:",9223372036854775807,,,,,,,,,,,,,2.0.2,2.1.0,,,,,,,,,,"24/Oct/16 05:38;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/15606;;;","25/Oct/16 19:08;yhuai;Issue resolved by pull request 15606
[https://github.com/apache/spark/pull/15606];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to infer constraints over multiple aliases,SPARK-18063,13014465,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jiangxb1987,jiangxb1987,jiangxb1987,22/Oct/16 06:33,26/Oct/16 18:13,14/Jul/23 06:29,26/Oct/16 18:13,,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,,,,,,"The `UnaryNode.getAliasedConstraints` function fails to replace all expressions by their alias where constraints contains more than one expression to be replaced. For example:
{code}
val tr = LocalRelation('a.int, 'b.string, 'c.int)
val multiAlias = tr.where('a === 'c + 10).select('a.as('x), 'c.as('y))
multiAlias.analyze.constraints
{code}
currently outputs:
{code}
ExpressionSet(Seq(
    IsNotNull(resolveColumn(multiAlias.analyze, ""x"")),
    IsNotNull(resolveColumn(multiAlias.analyze, ""y""))
)
{code}
The constraint {code}resolveColumn(multiAlias.analyze, ""x"") === resolveColumn(multiAlias.analyze, ""y"") + 10){code} is missing.",,apachespark,codingcat,jiangxb1987,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 22 06:40:05 UTC 2016,,,,,,,,,,"0|i3597z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/16 06:40;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/15597;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Thriftserver needs to create SPNego principal,SPARK-18061,13014441,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,cmirashi,cmirashi,22/Oct/16 00:45,06/Sep/17 01:43,14/Jul/23 06:29,06/Sep/17 01:40,1.6.1,2.0.1,,,,,,,2.3.0,,,,SQL,,,,,,,,3,,,,,,"Spark Thriftserver when running in HTTP mode with Kerberos enabled gives a 401 authentication error when receiving beeline HTTP request (with end user as kerberos principal). The similar command works with Hive Thriftserver.

What we find is Hive thriftserver CLI service creates both hive service and SPNego principal when kerberos is enabled whereas Spark Thriftserver
only creates hive service principal.

{code:title=CLIService.java|borderStyle=solid}

if (UserGroupInformation.isSecurityEnabled()) {
      try {
        HiveAuthFactory.loginFromKeytab(hiveConf);
        this.serviceUGI = Utils.getUGI();
      } catch (IOException e) {
        throw new ServiceException(""Unable to login to kerberos with given principal/keytab"", e);
      } catch (LoginException e) {
        throw new ServiceException(""Unable to login to kerberos with given principal/keytab"", e);
      }

      // Also try creating a UGI object for the SPNego principal
      String principal = hiveConf.getVar(ConfVars.HIVE_SERVER2_SPNEGO_PRINCIPAL);
      String keyTabFile = hiveConf.getVar(ConfVars.HIVE_SERVER2_SPNEGO_KEYTAB);
      if (principal.isEmpty() || keyTabFile.isEmpty()) {
        LOG.info(""SPNego httpUGI not created, spNegoPrincipal: "" + principal +
            "", ketabFile: "" + keyTabFile);
      } else {
        try {
          this.httpUGI = HiveAuthFactory.loginFromSpnegoKeytabAndReturnUGI(hiveConf);
          LOG.info(""SPNego httpUGI successfully created."");
        } catch (IOException e) {
          LOG.warn(""SPNego httpUGI creation failed: "", e);
        }
      }
    }

{code}

{code:title=SparkSQLCLIService.scala|borderStyle=solid}

if (UserGroupInformation.isSecurityEnabled) {
      try {
        HiveAuthFactory.loginFromKeytab(hiveConf)
        sparkServiceUGI = Utils.getUGI()
        setSuperField(this, ""serviceUGI"", sparkServiceUGI)
      } catch {
        case e @ (_: IOException | _: LoginException) =>
          throw new ServiceException(""Unable to login to kerberos with given principal/keytab"", e)
      }
    }

{code}

The patch will add missing SPNego principal to Spark Thriftserver.",,apachespark,cmirashi,jeffreyr97,jerryshao,jimvo,rding,rmullapudi,tanejagagan,Wancy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21407,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 06 01:40:55 UTC 2017,,,,,,,,,,"0|i3592n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/16 01:03;apachespark;User 'cmirash' has created a pull request for this issue:
https://github.com/apache/spark/pull/15594;;;","14/Jul/17 21:52;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/18628;;;","06/Sep/17 01:40;jerryshao;Issue resolved by pull request 18628
[https://github.com/apache/spark/pull/18628];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AnalysisException may be thrown when union two DFs whose struct fields have different nullability,SPARK-18058,13014420,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,codingcat,lian cheng,lian cheng,21/Oct/16 22:30,28/Nov/16 16:40,14/Jul/23 06:29,23/Oct/16 17:45,1.6.2,2.0.1,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,,,,,,"The following Spark shell snippet reproduces this issue:
{code}
spark.range(10).createOrReplaceTempView(""t1"")
spark.range(10).map(i => i: java.lang.Long).toDF(""id"").createOrReplaceTempView(""t2"")
sql(""SELECT struct(id) FROM t1 UNION ALL SELECT struct(id) FROM t2"")
{code}

{noformat}
org.apache.spark.sql.AnalysisException: Union can only be performed on tables with the compatible column types. StructType(StructField(id,LongType,true)) <> StructType(StructField(id,LongType,false)) at the first column of the second table;
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:40)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:57)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$11$$anonfun$apply$12.apply(CheckAnalysis.scala:291)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$11$$anonfun$apply$12.apply(CheckAnalysis.scala:289)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$11.apply(CheckAnalysis.scala:289)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$11.apply(CheckAnalysis.scala:278)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:278)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:67)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:132)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:67)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:57)
  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:61)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:573)
  ... 50 elided
{noformat}

The reason is that we treat two {{StructType}} incompatible even if their only differ from each other in field nullability.",,351zyf,agrajm,apachespark,babloo80,codingcat,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 28 16:40:06 UTC 2016,,,,,,,,,,"0|i358xz:",9223372036854775807,,,,,,,,,,,,,2.0.2,2.1.0,,,,,,,,,,"22/Oct/16 01:07;apachespark;User 'CodingCat' has created a pull request for this issue:
https://github.com/apache/spark/pull/15595;;;","22/Oct/16 02:32;babloo80;Wanted to share the test I used while getting this error from Spark 2.0.0

Schema from parquet file.
{code}
scala> d1.printSchema()
root
 |-- task_id: string (nullable = true)
 |-- task_name: string (nullable = true)
 |-- some_histogram: struct (nullable = true)
 |    |-- values: array (nullable = true)
 |    |    |-- element: double (containsNull = true)
 |    |-- freq: array (nullable = true)
 |    |    |-- element: long (containsNull = true)

scala> d2.printSchema() //Data created using dataframe and/or processed before writing to parquet file.
root
 |-- task_id: string (nullable = true)
 |-- task_name: string (nullable = true)
 |-- some_histogram: struct (nullable = true)
 |    |-- values: array (nullable = true)
 |    |    |-- element: double (containsNull = false)
 |    |-- freq: array (nullable = true)
 |    |    |-- element: long (containsNull = false)

scala> d1.union(d2).printSchema()
Exception in thread ""main"" org.apache.spark.sql.AnalysisException: unresolved operator 'Union;
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:40)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:58)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:361)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:67)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:67)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:161)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:167)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:59)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:2594)
	at org.apache.spark.sql.Dataset.union(Dataset.scala:1459)

{code}

But the same union like operation works when I use the following schema
{code}
scala> df1.printSchema()
root
 |-- col1: integer (nullable = false)
 |-- col2: string (nullable = true)
 |-- col3: double (nullable = false)
 |-- col4: string (nullable = false)

scala> df2.printSchema()
root
 |-- col1: integer (nullable = true)
 |-- col2: string (nullable = true)
 |-- col3: double (nullable = true)
 |-- col4: string (nullable = true)

scala> d1.union(d2).printSchema() //this one worked.
root
 |-- col1: integer (nullable = true)
 |-- col2: string (nullable = true)
 |-- col3: double (nullable = true)
 |-- col4: string (nullable = true)
{code};;;","23/Oct/16 18:00;apachespark;User 'CodingCat' has created a pull request for this issue:
https://github.com/apache/spark/pull/15602;;;","28/Nov/16 16:40;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/16041;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dataset.flatMap can't work with types from customized jar,SPARK-18055,13014317,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,davies,davies,21/Oct/16 17:46,12/Apr/17 13:19,14/Jul/23 06:29,08/Mar/17 09:35,2.0.1,,,,,,,,2.0.3,2.1.1,2.2.0,,SQL,,,,,,,,0,,,,,,"Try to apply flatMap() on Dataset column which of of type
com.A.B

Here's a schema of a dataset:
{code}
root
 |-- id: string (nullable = true)
 |-- outputs: array (nullable = true)
 |    |-- element: string
{code}

flatMap works on RDD

{code}
 ds.rdd.flatMap(_.outputs)
{code}

flatMap doesnt work on dataset and gives the following error
{code}
ds.flatMap(_.outputs)
{code}

The exception:
{code}
scala.ScalaReflectionException: class com.A.B in JavaMirror … not found
    at scala.reflect.internal.Mirrors$RootsBase.staticClass(Mirrors.scala:123)
    at scala.reflect.internal.Mirrors$RootsBase.staticClass(Mirrors.scala:22)
    at line189424fbb8cd47b3b62dc41e417841c159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$typecreator3$1.apply(<console>:51)
    at scala.reflect.api.TypeTags$WeakTypeTagImpl.tpe$lzycompute(TypeTags.scala:232)
    at scala.reflect.api.TypeTags$WeakTypeTagImpl.tpe(TypeTags.scala:232)
    at org.apache.spark.sql.SQLImplicits$$typecreator9$1.apply(SQLImplicits.scala:125)
    at scala.reflect.api.TypeTags$WeakTypeTagImpl.tpe$lzycompute(TypeTags.scala:232)
    at scala.reflect.api.TypeTags$WeakTypeTagImpl.tpe(TypeTags.scala:232)
    at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.apply(ExpressionEncoder.scala:49)
    at org.apache.spark.sql.SQLImplicits.newProductSeqEncoder(SQLImplicits.scala:125)
{code}

Spoke to Michael Armbrust and he confirmed it as a Dataset bug.

There is a workaround using explode()
{code}
ds.select(explode(col(""outputs"")))
{code}",,apachespark,cloud_fan,codingcat,davies,pzaczkiewicz,tsuresh,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17890,SPARK-18139,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/16 02:27;cloud_fan;test-jar_2.11-1.0.jar;https://issues.apache.org/jira/secure/attachment/12835722/test-jar_2.11-1.0.jar",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 12 13:19:05 UTC 2017,,,,,,,,,,"0|i358b3:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"28/Oct/16 02:27;cloud_fan;This jar is built with file MyData.scala
{code}
case class MyData(array: Seq[MyData2])
case class MyData2(i: Int)
{code};;;","08/Nov/16 04:41;windpiger;[~davies] I can't reproduce it on the master branch or on databricks(Spark 2.0.1-db1 (Scala 2.11)),my code:

import scala.collection.mutable.ArrayBuffer
case class MyData(id: String,arr: Seq\[String])
val myarr = ArrayBuffer\[MyData]()
for(i <- 20 to 30){
	val arr = ArrayBuffer\[String]()
	for(j <- 1 to 10) {
		arr += (i+j).toString
	}

	val mydata = new MyData(i.toString,arr)
	myarr += mydata
}

val rdd = spark.sparkContext.makeRDD(myarr)
val ds = rdd.toDS

ds.rdd.flatMap(_.arr)
ds.flatMap(_.arr)

there is no exception, it has fixed? or My code is wrong?

I also test with the test-jar_2.11-1.0.jar in spark-shell:
>spark-shell --jars test-jar_2.11-1.0.jar

and there is no exception.
;;;","08/Mar/17 02:00;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/17201;;;","08/Mar/17 09:35;cloud_fan;Issue resolved by pull request 17201
[https://github.com/apache/spark/pull/17201];;;","07/Apr/17 17:43;pzaczkiewicz;[~marmbrus]: I ran into this issue when using a custom org.apache.spark.sql.expressions.Aggregator in Spark 2.0.2.

{code:java}
val aggregator:Aggregator = ....
df.groupByKey(s => CookieId(s.cookie_id)
).agg(aggregator.toColumn)
{code}

I got a very similar {{scala.ScalaReflectionException}}, which is how I found this ticket. Is there an easy way around this short of either converting my brand-new {{Aggregator}} into a {{UserDefinedAggregateFunction}} or custom installing a patched version of Spark onto my cluster?;;;","08/Apr/17 04:57;cloud_fan;I think this is a different issue, can you open a new ticket for it? thanks!;;;","12/Apr/17 13:19;pzaczkiewicz;Actually it seems it was this issue.  I forgot to mention that the Aggregator returns an Array[case class] which I would then like to flatMap.  I get the stack trace on the {{stitcher.toColumn}} line rather than the {{flatMap}} line.  I've since patched Spark with this PR to work around this issue.
{code:java}
case class StitchedVisitor(cookie_id:java.math.BigDecimal, visit_num:Int, ...)
case class CookieId(cookie_id:java.math.BigDecimal)
val aggregator[Aggregator[StitchedVisitor,Array[Option[StitchedVisitor]],Array[StitchedVisitor]]] = ...

df.groupByKey(s => CookieId(s.cookie_id)
).agg(stitcher.toColumn
).flatMap(agg => agg._2)
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ARRAY equality is broken in Spark 2.0,SPARK-18053,13014301,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,lian cheng,lian cheng,21/Oct/16 16:57,23/Nov/16 12:16,14/Jul/23 06:29,23/Nov/16 12:16,2.0.0,2.0.1,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,correctness,,,,,"The following Spark shell reproduces this issue:
{code}
case class Test(a: Seq[Int])
Seq(Test(Seq(1))).toDF().createOrReplaceTempView(""t"")

sql(""SELECT a FROM t WHERE a = array(1)"").show()
// +---+
// |  a|
// +---+
// +---+

sql(""SELECT a FROM (SELECT array(1) AS a) x WHERE x.a = array(1)"").show()
// +---+
// |  a|
// +---+
// |[1]|
// +---+
{code}",,apachespark,cloud_fan,codingcat,jse,kiszk,lian cheng,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 18 09:06:04 UTC 2016,,,,,,,,,,"0|i3587j:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"23/Oct/16 03:48;cloud_fan;[~lian cheng] are you sure this issue exists in 2.0? The new array format is only merged into master branch(2.1);;;","24/Oct/16 19:34;lian cheng;Hm, the user mailing list thread said that it fails under 2.0 https://lists.apache.org/thread.html/%3C1476953644701-27926.post@n3.nabble.com%3E

I haven't verify it under 2.0 yet.;;;","24/Oct/16 19:35;lian cheng;Yea, reproduced using 2.0.;;;","18/Nov/16 09:06;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/15929;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom PartitionCoalescer cause serialization exception,SPARK-18051,13014269,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weichenxu123,weichenxu123,weichenxu123,21/Oct/16 15:24,22/Oct/16 18:59,14/Jul/23 06:29,22/Oct/16 18:59,2.1.0,,,,,,,,2.1.0,,,,Spark Core,,,,,,,,0,,,,,,"for example, the following code cause exception:

{code: title=code}
class MyCoalescer extends PartitionCoalescer{
  override def coalesce(maxPartitions: Int, parent: RDD[_]): Array[PartitionGroup] = {
    val pglist = Array.fill(2)(new PartitionGroup())
    pglist(0).partitions.append(parent.partitions(0), parent.partitions(1), parent.partitions(2))
    pglist(1).partitions.append(parent.partitions(3), parent.partitions(4), parent.partitions(5))
    pglist
  }
}
object Test1 {
  def main(args: Array[String]) = {
    val spark = SparkSession.builder().appName(""test"").getOrCreate()
    val sc = spark.sparkContext
    val rdd = sc.parallelize(1 to 6, 6)
    rdd.coalesce(2, false, Some(new MyCoalescer)).count()
    spark.stop()
  }
}
{code}

it throws exception:
Exception in thread ""dag-scheduler-event-loop"" java.lang.StackOverflowError
at java.lang.Exception.<init>(Exception.java:102)
....
at org.apache.spark.serializer.SerializationDebugger$SerializationDebugger.visitSerializableWithWriteObjectMethod(SerializationDebugger.scala:230)
at org.apache.spark.serializer.SerializationDebugger$SerializationDebugger.visitSerializable(SerializationDebugger.scala:189)
at org.apache.spark.serializer.SerializationDebugger$SerializationDebugger.visit(SerializationDebugger.scala:108)
at org.apache.spark.serializer.SerializationDebugger$SerializationDebugger.visitSerializableWithWriteObjectMethod(SerializationDebugger.scala:243)
at org.apache.spark.serializer.SerializationDebugger$SerializationDebugger.visitSerializable(SerializationDebugger.scala:189)
....",,apachespark,barrybecker4,rxin,weichenxu123,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 21 15:32:06 UTC 2016,,,,,,,,,,"0|i3580f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/16 15:32;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/15587;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark 2.0.1 enable hive throw AlreadyExistsException(message:Database default already exists),SPARK-18050,13014247,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cjuexuan,cjuexuan,21/Oct/16 13:53,23/Nov/16 17:56,14/Jul/23 06:29,23/Nov/16 17:56,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"in spark 2.0.1 ,I enable hive support and when init the sqlContext ,throw a AlreadyExistsException(message:Database default already exists),same as 
https://www.mail-archive.com/dev@spark.apache.org/msg15306.html ,my code is 

{code}
  private val master = ""local[*]""
  private val appName = ""xqlServerSpark""
  val fileSystem = FileSystem.get()
  val sparkConf = new SparkConf().setMaster(master).
    setAppName(appName).set(""spark.sql.warehouse.dir"", s""${fileSystem.getUri.toASCIIString}/user/hive/warehouse"")
  val   hiveContext = SparkSession.builder().config(sparkConf).enableHiveSupport().getOrCreate().sqlContext
    print(sparkConf.get(""spark.sql.warehouse.dir""))
    hiveContext.sql(""show tables"").show()
{code}

the result is correct,but a exception also throwBy the code
","jdk1.8, macOs,spark 2.0.1",alexliu68,apachespark,cjuexuan,cloud_fan,huasanyelao,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 23 13:36:05 UTC 2016,,,,,,,,,,"0|i357vj:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"10/Nov/16 07:42;cloud_fan;can you put the stacktrace here too?;;;","14/Nov/16 12:41;cjuexuan;16/11/14 20:38:03 INFO hive.HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16/11/14 20:38:03 WARN conf.HiveConf: HiveConf of name hive.mapjoin.optimized.keys does not exist
16/11/14 20:38:03 WARN conf.HiveConf: HiveConf of name hive.optimize.multigroupby.common.distincts does not exist
16/11/14 20:38:03 WARN conf.HiveConf: HiveConf of name hive.mapjoin.lazy.hashtable does not exist
16/11/14 20:38:03 WARN conf.HiveConf: HiveConf of name hive.server2.thrift.http.min.worker.threads does not exist
16/11/14 20:38:03 WARN conf.HiveConf: HiveConf of name hive.server2.thrift.http.max.worker.threads does not exist
16/11/14 20:38:03 WARN conf.HiveConf: HiveConf of name hive.server2.logging.operation.verbose does not exist
16/11/14 20:38:04 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16/11/14 20:38:04 INFO metastore.ObjectStore: ObjectStore, initialize called
16/11/14 20:38:04 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
16/11/14 20:38:04 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored
16/11/14 20:38:06 WARN conf.HiveConf: HiveConf of name hive.mapjoin.optimized.keys does not exist
16/11/14 20:38:06 WARN conf.HiveConf: HiveConf of name hive.optimize.multigroupby.common.distincts does not exist
16/11/14 20:38:06 WARN conf.HiveConf: HiveConf of name hive.mapjoin.lazy.hashtable does not exist
16/11/14 20:38:06 WARN conf.HiveConf: HiveConf of name hive.server2.thrift.http.min.worker.threads does not exist
16/11/14 20:38:06 WARN conf.HiveConf: HiveConf of name hive.server2.thrift.http.max.worker.threads does not exist
16/11/14 20:38:06 WARN conf.HiveConf: HiveConf of name hive.server2.logging.operation.verbose does not exist
16/11/14 20:38:06 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=""Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order""
16/11/14 20:38:09 INFO DataNucleus.Datastore: The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
16/11/14 20:38:09 INFO DataNucleus.Datastore: The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
16/11/14 20:38:11 INFO DataNucleus.Datastore: The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
16/11/14 20:38:11 INFO DataNucleus.Datastore: The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
16/11/14 20:38:11 INFO DataNucleus.Query: Reading in results for query ""org.datanucleus.store.rdbms.query.SQLQuery@0"" since the connection used is closing
16/11/14 20:38:12 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL
16/11/14 20:38:12 INFO metastore.ObjectStore: Initialized ObjectStore
16/11/14 20:38:13 INFO metastore.HiveMetaStore: Added admin role in metastore
16/11/14 20:38:13 INFO metastore.HiveMetaStore: Added public role in metastore
16/11/14 20:38:14 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty
16/11/14 20:38:14 INFO metastore.HiveMetaStore: 0: get_all_databases
16/11/14 20:38:14 INFO HiveMetaStore.audit: ugi=cjuexuan	ip=unknown-ip-addr	cmd=get_all_databases	
16/11/14 20:38:14 INFO metastore.HiveMetaStore: 0: get_functions: db=bi pat=*
16/11/14 20:38:14 INFO HiveMetaStore.audit: ugi=cjuexuan	ip=unknown-ip-addr	cmd=get_functions: db=bi pat=*	
16/11/14 20:38:14 INFO DataNucleus.Datastore: The class ""org.apache.hadoop.hive.metastore.model.MResourceUri"" is tagged as ""embedded-only"" so does not have its own datastore table.
16/11/14 20:38:15 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*
16/11/14 20:38:15 INFO HiveMetaStore.audit: ugi=cjuexuan	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16/11/14 20:38:15 INFO metastore.HiveMetaStore: 0: get_functions: db=search pat=*
16/11/14 20:38:15 INFO HiveMetaStore.audit: ugi=cjuexuan	ip=unknown-ip-addr	cmd=get_functions: db=search pat=*	
16/11/14 20:38:15 INFO metastore.HiveMetaStore: 0: get_functions: db=test_randy pat=*
16/11/14 20:38:15 INFO HiveMetaStore.audit: ugi=cjuexuan	ip=unknown-ip-addr	cmd=get_functions: db=test_randy pat=*	
16/11/14 20:38:15 INFO metastore.HiveMetaStore: 0: get_functions: db=testaa pat=*
16/11/14 20:38:15 INFO HiveMetaStore.audit: ugi=cjuexuan	ip=unknown-ip-addr	cmd=get_functions: db=testaa pat=*	
16/11/14 20:38:15 INFO session.SessionState: Created local directory: /var/folders/0h/bdlvyj3j21d3t65dt8thq7500000gp/T/81f6f9b7-5e21-49ce-9dcc-48b5297e8d95_resources
16/11/14 20:38:15 INFO session.SessionState: Created HDFS directory: /tmp/hive-cjuexuan/cjuexuan/81f6f9b7-5e21-49ce-9dcc-48b5297e8d95
16/11/14 20:38:15 INFO session.SessionState: Created local directory: /tmp/cjuexuan/81f6f9b7-5e21-49ce-9dcc-48b5297e8d95
16/11/14 20:38:15 INFO session.SessionState: Created HDFS directory: /tmp/hive-cjuexuan/cjuexuan/81f6f9b7-5e21-49ce-9dcc-48b5297e8d95/_tmp_space.db
16/11/14 20:38:15 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
16/11/14 20:38:15 WARN conf.HiveConf: HiveConf of name hive.mapjoin.optimized.keys does not exist
16/11/14 20:38:15 WARN conf.HiveConf: HiveConf of name hive.optimize.multigroupby.common.distincts does not exist
16/11/14 20:38:15 WARN conf.HiveConf: HiveConf of name hive.mapjoin.lazy.hashtable does not exist
16/11/14 20:38:15 WARN conf.HiveConf: HiveConf of name hive.server2.thrift.http.min.worker.threads does not exist
16/11/14 20:38:15 WARN conf.HiveConf: HiveConf of name hive.server2.thrift.http.max.worker.threads does not exist
16/11/14 20:38:15 WARN conf.HiveConf: HiveConf of name hive.server2.logging.operation.verbose does not exist
16/11/14 20:38:15 INFO session.SessionState: Created local directory: /var/folders/0h/bdlvyj3j21d3t65dt8thq7500000gp/T/59a86193-b20e-4b0e-8c74-ccc43e3f5203_resources
16/11/14 20:38:15 INFO session.SessionState: Created HDFS directory: /tmp/hive-cjuexuan/cjuexuan/59a86193-b20e-4b0e-8c74-ccc43e3f5203
16/11/14 20:38:15 INFO session.SessionState: Created local directory: /tmp/cjuexuan/59a86193-b20e-4b0e-8c74-ccc43e3f5203
16/11/14 20:38:15 INFO session.SessionState: Created HDFS directory: /tmp/hive-cjuexuan/cjuexuan/59a86193-b20e-4b0e-8c74-ccc43e3f5203/_tmp_space.db
16/11/14 20:38:15 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
16/11/14 20:38:16 INFO metastore.HiveMetaStore: 0: create_database: Database(name:default, description:default database, locationUri:file:/user/hive/warehouse, parameters:{})
16/11/14 20:38:16 INFO HiveMetaStore.audit: ugi=cjuexuan	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:file:/user/hive/warehouse, parameters:{})	
16/11/14 20:38:16 ERROR metastore.RetryingHMSHandler: AlreadyExistsException(message:Database default already exists)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy21.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy22.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:309)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:309)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:309)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:280)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:227)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:226)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:269)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:308)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
	at org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:51)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:31)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:471)
	at com.ximalaya.xql.engine.exec.hive.HiveDataFrameReader.load(HiveDataFrameReader.scala:25)
	at com.ximalaya.xql.engine.exec.hive.HiveDataFrameReaderSuite$$anonfun$1.apply$mcV$sp(HiveDataFrameReaderSuite.scala:19)
	at com.ximalaya.xql.engine.exec.hive.HiveDataFrameReaderSuite$$anonfun$1.apply(HiveDataFrameReaderSuite.scala:15)
	at com.ximalaya.xql.engine.exec.hive.HiveDataFrameReaderSuite$$anonfun$1.apply(HiveDataFrameReaderSuite.scala:15)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.scalatest.TestSuite$class.withFixture(TestSuite.scala:196)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at com.ximalaya.xql.engine.exec.hive.HiveDataFrameReaderSuite.org$scalatest$BeforeAndAfterAll$$super$run(HiveDataFrameReaderSuite.scala:12)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at com.ximalaya.xql.engine.exec.hive.HiveDataFrameReaderSuite.run(HiveDataFrameReaderSuite.scala:12)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.run(Runner.scala:850)
	at org.scalatest.tools.Runner.run(Runner.scala)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:138)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)

16/11/14 20:38:16 INFO execution.SparkSqlParser: Parsing command: track_liked
16/11/14 20:38:16 INFO metastore.HiveMetaStore: 0: get_table : db=default tbl=track_liked
;;;","14/Nov/16 12:45;cjuexuan;when I init sparkSession ,throw this error ;;;","15/Nov/16 00:59;cloud_fan;ah, it's not throwing an exception, but logging an error message. When we try to create the default database, we ask hive to do nothing if it already exists. However, Hive will log an error message instead of doing nothing. This should be fine and you can just ignore it.;;;","15/Nov/16 01:22;cjuexuan;thanks;;;","22/Nov/16 14:48;alexliu68;This error logging msg is very annoying. Is there any workaround to not display it at all?;;;","23/Nov/16 13:36;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/15993;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decision Trees do not handle edge cases,SPARK-18036,13014079,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,imatiach,sethah,sethah,20/Oct/16 22:40,24/Jan/17 18:25,14/Jul/23 06:29,24/Jan/17 18:25,,,,,,,,,2.2.0,,,,ML,MLlib,,,,,,,0,,,,,,"Decision trees/GBT/RF do not handle edge cases such as constant features or empty features. For example:

{code}
val dt = new DecisionTreeRegressor()
val data = Seq(LabeledPoint(1.0, Vectors.dense(Array.empty[Double]))).toDF()
dt.fit(data)

java.lang.UnsupportedOperationException: empty.max
  at scala.collection.TraversableOnce$class.max(TraversableOnce.scala:229)
  at scala.collection.mutable.ArrayOps$ofInt.max(ArrayOps.scala:234)
  at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:207)
  at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:105)
  at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:93)
  at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:46)
  at org.apache.spark.ml.Predictor.fit(Predictor.scala:90)
  ... 52 elided

{code}

as well as 

{code}
val dt = new DecisionTreeRegressor()
val data = Seq(LabeledPoint(1.0, Vectors.dense(0.0, 0.0, 0.0))).toDF()
dt.fit(data)

java.lang.UnsupportedOperationException: empty.maxBy
at scala.collection.TraversableOnce$class.maxBy(TraversableOnce.scala:236)
at scala.collection.SeqViewLike$AbstractTransformed.maxBy(SeqViewLike.scala:37)
at org.apache.spark.ml.tree.impl.RandomForest$.binsToBestSplit(RandomForest.scala:846)
{code}",,apachespark,dmcwhorter,imatiach,josephkb,sethah,weichenxu123,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 24 18:25:31 UTC 2017,,,,,,,,,,"0|i356u7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/16 19:29;imatiach;Weichen Xu, are you working on this issue or have you resolved it?  I am interested in investigating this bug.;;;","21/Dec/16 06:10;weichenxu123;Oh, I'm too busy recently to work on it, it would be great if you can resolve it, thanks! ;;;","21/Dec/16 22:30;apachespark;User 'imatiach-msft' has created a pull request for this issue:
https://github.com/apache/spark/pull/16377;;;","21/Dec/16 22:35;imatiach;Thanks, I've sent a pull request to fix this.;;;","24/Jan/17 18:25;josephkb;Issue resolved by pull request 16377
[https://github.com/apache/spark/pull/16377];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce performant and memory efficient APIs to create ArrayBasedMapData,SPARK-18035,13014075,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tejasp,tejasp,tejasp,20/Oct/16 22:07,23/Oct/16 03:44,14/Jul/23 06:29,23/Oct/16 03:44,2.0.1,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"In HiveInspectors, I saw that converting Java map to Spark's `ArrayBasedMapData` spent quite sometime in buffer copying : https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveInspectors.scala#L658

The reason being `map.toSeq` allocates a new buffer and copies the map entries to it: https://github.com/scala/scala/blob/2.11.x/src/library/scala/collection/MapLike.scala#L323

This copy is not needed as we get rid of it once we extract the key and value arrays.

Here is the call trace:

{noformat}
org.apache.spark.sql.hive.HiveInspectors$$anonfun$unwrapperFor$41.apply(HiveInspectors.scala:664)
scala.collection.AbstractMap.toSeq(Map.scala:59)
scala.collection.MapLike$class.toSeq(MapLike.scala:323)
scala.collection.AbstractMap.toBuffer(Map.scala:59)
scala.collection.MapLike$class.toBuffer(MapLike.scala:326)
scala.collection.AbstractTraversable.copyToBuffer(Traversable.scala:104)
scala.collection.TraversableOnce$class.copyToBuffer(TraversableOnce.scala:275)
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
scala.collection.AbstractIterable.foreach(Iterable.scala:54)
scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
scala.collection.Iterator$class.foreach(Iterator.scala:893)
scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:59)
scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:59)
{noformat}",,apachespark,smilegator,tejasp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 23 03:44:22 UTC 2016,,,,,,,,,,"0|i356tb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/16 22:15;apachespark;User 'tejasapatil' has created a pull request for this issue:
https://github.com/apache/spark/pull/15573;;;","23/Oct/16 03:44;smilegator;Issue resolved by pull request 15573
[https://github.com/apache/spark/pull/15573];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade to MiMa 0.1.11,SPARK-18034,13014031,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,20/Oct/16 20:04,21/Oct/16 18:56,14/Jul/23 06:29,21/Oct/16 18:25,,,,,,,,,2.0.2,2.1.0,,,Project Infra,,,,,,,,0,,,,,,We should upgrade to the latest release of MiMa (0.1.11) in order to include my fix for a bug which led to flakiness in the MiMa checks (https://github.com/typesafehub/migration-manager/issues/115),,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 21 18:25:33 UTC 2016,,,,,,,,,,"0|i356jj:",9223372036854775807,,,,,,,,,,,,,2.0.2,2.1.0,,,,,,,,,,"20/Oct/16 20:07;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15571;;;","21/Oct/16 18:25;joshrosen;Issue resolved by pull request 15571
[https://github.com/apache/spark/pull/15571];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: org.apache.spark.streaming.scheduler.ExecutorAllocationManagerSuite basic functionality,SPARK-18031,13013977,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,davies,davies,20/Oct/16 17:39,22/Dec/16 18:33,14/Jul/23 06:29,21/Dec/16 19:18,,,,,,,,,2.0.3,2.1.1,2.2.0,,Spark Core,,,,,,,,0,,,,,,https://spark-tests.appspot.com/test-details?suite_name=org.apache.spark.streaming.scheduler.ExecutorAllocationManagerSuite&test_name=basic+functionality,,apachespark,davies,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 21 19:18:10 UTC 2016,,,,,,,,,,"0|i3567j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/16 23:55;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16321;;;","21/Dec/16 19:18;tdas;Issue resolved by pull request 16321
[https://github.com/apache/spark/pull/16321];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: org.apache.spark.sql.streaming.FileStreamSourceSuite ,SPARK-18030,13013973,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,davies,davies,20/Oct/16 17:35,28/Dec/16 18:11,14/Jul/23 06:29,31/Oct/16 23:05,,,,,,,,,2.0.2,2.1.0,,,Structured Streaming,,,,,,,,0,,,,,,https://spark-tests.appspot.com/test-details?suite_name=org.apache.spark.sql.streaming.FileStreamSourceSuite&test_name=when+schema+inference+is+turned+on%2C+should+read+partition+data,,apachespark,davies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 31 20:13:06 UTC 2016,,,,,,,,,,"0|i3566n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/16 00:31;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/15577;;;","31/Oct/16 20:13;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/15699;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PruneFileSourcePartitions should not change the output of LogicalRelation,SPARK-18029,13013916,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,20/Oct/16 15:00,21/Oct/16 04:41,14/Jul/23 06:29,21/Oct/16 04:41,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 21 04:41:03 UTC 2016,,,,,,,,,,"0|i355tz:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"20/Oct/16 15:06;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/15569;;;","21/Oct/16 04:41;cloud_fan;Issue resolved by pull request 15569
[https://github.com/apache/spark/pull/15569];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
.sparkStaging not clean on RM ApplicationNotFoundException,SPARK-18027,13013822,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,davidshar,davidshar,20/Oct/16 11:05,17/May/20 18:13,14/Jul/23 06:29,26/Oct/16 12:23,1.6.0,,,,,,,,2.1.0,,,,Spark Core,YARN,,,,,,,0,,,,,,"Hi,

It seems that SPARK-7705 didn't fix all issues with .sparkStaging folder cleanup.

in Client.scala:monitorApplication 
{code}
 val report: ApplicationReport =
        try {
          getApplicationReport(appId)
        } catch {
          case e: ApplicationNotFoundException =>
            logError(s""Application $appId not found."")
            return (YarnApplicationState.KILLED, FinalApplicationStatus.KILLED)
          case NonFatal(e) =>
            logError(s""Failed to contact YARN for application $appId."", e)
            return (YarnApplicationState.FAILED, FinalApplicationStatus.FAILED)
        }
....
if (state == YarnApplicationState.FINISHED ||
        state == YarnApplicationState.FAILED ||
        state == YarnApplicationState.KILLED) {
        cleanupStagingDir(appId)
        return (state, report.getFinalApplicationStatus)
 }
{code}

In case of ApplicationNotFoundException, we don't cleanup the sparkStaging folder.

I believe we should call cleanupStagingDir(appId) on the catch clause above.
",,apachespark,davidshar,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18968,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 26 12:23:28 UTC 2016,,,,,,,,,,"0|i35593:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/16 12:37;srowen;Yes, likely both catch clauses.;;;","22/Oct/16 08:54;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15598;;;","22/Oct/16 18:35;davidshar;I believe it there is a major difference between the 2 exceptions above.
1. ApplicationNotFoundException means there is no such running app according to Yarn and it is safe to cleanup.
2. NonFatal, fail to connect to Yarn, we can't be sure that the app is running or not, so we cannot be safe cleaning up.

Therefore, just add cleanup for the first exception.;;;","23/Oct/16 11:30;srowen;OK, though Spark will consider the app failed in this case no matter what. Is it consistent to not clean it up? it won't recover on the Spark side regardless.;;;","24/Oct/16 07:29;davidshar;Yes, I believe it is safer, we cannot be sure what Yarn is doing on connection failure.
;;;","24/Oct/16 08:49;srowen;EDIT: OK I buy into this. https://github.com/apache/spark/pull/15598#discussion_r84643826;;;","26/Oct/16 12:23;srowen;Issue resolved by pull request 15598
[https://github.com/apache/spark/pull/15598];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.NullPointerException instead of real exception when saving DF to MySQL,SPARK-18022,13013759,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,maver1ck,maver1ck,20/Oct/16 06:06,26/Oct/16 12:23,14/Jul/23 06:29,26/Oct/16 12:20,2.0.1,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,,,,,,"Hi,
I have found following issue.
When there is an exception while saving dataframe to MySQL I'm unable to get it.
Instead of I'm getting following stacktrace.
{code}
16/10/20 06:00:35 WARN TaskSetManager: Lost task 56.0 in stage 10.0 (TID 3753, dwh-hn28.adpilot.co): java.lang.NullPointerException: Cannot suppress a null exception.
        at java.lang.Throwable.addSuppressed(Throwable.java:1046)
        at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:256)
        at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:314)
        at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:313)
        at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:902)
        at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:902)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1916)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1916)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
        at org.apache.spark.scheduler.Task.run(Task.scala:86)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code}

The real exception could be for example duplicate on primary key etc.

With this it's very difficult to debugging apps.
",,apachespark,dongjoon,m1lan,maver1ck,tsuresh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 26 12:20:38 UTC 2016,,,,,,,,,,"0|i354v3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/16 21:02;dongjoon;Hi, [~maver1ck].
Could you give us more information to reproduce this?
Is the table created by Spark? Spark does not create INDEX or CONSTRAINT (primary key), does it?;;;","20/Oct/16 21:03;dongjoon;Or, what you want is just general improvement for error handling in `JdbcUtils.scala:256`?;;;","21/Oct/16 03:37;maver1ck;Only improvement in error handling.
Because right now I'm getting only NPE and I have to guess whats the real reason of error.
;;;","21/Oct/16 04:23;maver1ck;I think the problem is in this PR.
https://github.com/apache/spark/commit/811a2cef03647c5be29fef522c423921c79b1bc3

CC: [~davies];;;","21/Oct/16 09:14;srowen;Yes, that code block should handle the case where cause is null. That will help a bit. 
It isn't the cause of the NPE here, which is probably about not handling the case of a null taskMetrics()?;;;","22/Oct/16 08:59;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15599;;;","26/Oct/16 12:20;srowen;Issue resolved by pull request 15599
[https://github.com/apache/spark/pull/15599];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kinesis receiver does not snapshot when shard completes,SPARK-18020,13013718,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maropu,yonran,yonran,20/Oct/16 00:02,26/Jan/17 01:43,14/Jul/23 06:29,26/Jan/17 01:42,2.0.0,,,,,,,,2.2.0,,,,DStreams,,,,,,,,3,kinesis,,,,,"When a kinesis shard is split or combined and the old shard ends, the Amazon Kinesis Client library [calls IRecordProcessor.shutdown|https://github.com/awslabs/amazon-kinesis-client/blob/v1.7.0/src/main/java/com/amazonaws/services/kinesis/clientlibrary/lib/worker/ShutdownTask.java#L100] and expects that {{IRecordProcessor.shutdown}} must checkpoint the sequence number {{ExtendedSequenceNumber.SHARD_END}} before returning. Unfortunately, spark’s [KinesisRecordProcessor|https://github.com/apache/spark/blob/v2.0.1/external/kinesis-asl/src/main/scala/org/apache/spark/streaming/kinesis/KinesisRecordProcessor.scala] sometimes does not checkpoint SHARD_END. This results in an error message, and spark is then blocked indefinitely from processing any items from the child shards.

This issue has also been raised on StackOverflow: [resharding while spark running on kinesis stream|http://stackoverflow.com/questions/38898691/resharding-while-spark-running-on-kinesis-stream]

Exception that is logged:
{code}
16/10/19 19:37:49 ERROR worker.ShutdownTask: Application exception. 
java.lang.IllegalArgumentException: Application didn't checkpoint at end of shard shardId-000000000030
        at com.amazonaws.services.kinesis.clientlibrary.lib.worker.ShutdownTask.call(ShutdownTask.java:106)
        at com.amazonaws.services.kinesis.clientlibrary.lib.worker.MetricsCollectingTaskDecorator.call(MetricsCollectingTaskDecorator.java:49)
        at com.amazonaws.services.kinesis.clientlibrary.lib.worker.MetricsCollectingTaskDecorator.call(MetricsCollectingTaskDecorator.java:24)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{code}

Command used to split shard:

{code}
aws kinesis --region us-west-1 split-shard --stream-name my-stream --shard-to-split shardId-000000000030 --new-starting-hash-key 5316911983139663491615228241121378303
{code}

After the spark-streaming job has hung, examining the DynamoDB table indicates that the parent shard processor has not reached {{ExtendedSequenceNumber.SHARD_END}} and the child shards are still at {{ExtendedSequenceNumber.TRIM_HORIZON}} waiting for the parent to finish:

{code}
aws kinesis --region us-west-1 describe-stream --stream-name my-stream
{
    ""StreamDescription"": {
        ""RetentionPeriodHours"": 24, 
        ""StreamName"": ""my-stream"", 
        ""Shards"": [
            {
                ""ShardId"": ""shardId-000000000030"", 
                ""HashKeyRange"": {
                    ""EndingHashKey"": ""10633823966279326983230456482242756606"", 
                    ""StartingHashKey"": ""0""
                },
                ...
            }, 
            {
                ""ShardId"": ""shardId-000000000062"", 
                ""HashKeyRange"": {
                    ""EndingHashKey"": ""5316911983139663491615228241121378302"", 
                    ""StartingHashKey"": ""0""
                }, 
                ""ParentShardId"": ""shardId-000000000030"", 
                ""SequenceNumberRange"": {
                    ""StartingSequenceNumber"": ""49566806087883755242230188435465744452396445937434624994""
                }
            }, 
            {
                ""ShardId"": ""shardId-000000000063"", 
                ""HashKeyRange"": {
                    ""EndingHashKey"": ""10633823966279326983230456482242756606"", 
                    ""StartingHashKey"": ""5316911983139663491615228241121378303""
                }, 
                ""ParentShardId"": ""shardId-000000000030"", 
                ""SequenceNumberRange"": {
                    ""StartingSequenceNumber"": ""49566806087906055987428719058607280170669094298940605426""
                }
            },
            ...
        ],
        ""StreamStatus"": ""ACTIVE""
    }
}

aws dynamodb --region us-west-1 scan --table-name my-processor
{
    ""Items"": [
        {
            ""leaseOwner"": {
                ""S"": ""localhost:fd385c95-5d19-4678-926f-b6d5f5503cbe""
            }, 
            ""leaseCounter"": {
                ""N"": ""49318""
            }, 
            ""ownerSwitchesSinceCheckpoint"": {
                ""N"": ""62""
            }, 
            ""checkpointSubSequenceNumber"": {
                ""N"": ""0""
            }, 
            ""checkpoint"": {
                ""S"": ""49566573572821264975247582655142547856950135436343247330""
            }, 
            ""parentShardId"": {
                ""SS"": [
                    ""shardId-000000000014""
                ]
            }, 
            ""leaseKey"": {
                ""S"": ""shardId-000000000030""
            }
        }, 
        {
            ""leaseOwner"": {
                ""S"": ""localhost:ca44dc83-2580-4bf3-903f-e7ccc8a3ab02""
            }, 
            ""leaseCounter"": {
                ""N"": ""25439""
            }, 
            ""ownerSwitchesSinceCheckpoint"": {
                ""N"": ""69""
            }, 
            ""checkpointSubSequenceNumber"": {
                ""N"": ""0""
            }, 
            ""checkpoint"": {
                ""S"": ""TRIM_HORIZON""
            }, 
            ""parentShardId"": {
                ""SS"": [
                    ""shardId-000000000030""
                ]
            }, 
            ""leaseKey"": {
                ""S"": ""shardId-000000000062""
            }
        }, 
        {
            ""leaseOwner"": {
                ""S"": ""localhost:94bf603f-780b-4121-87a4-bdf501723f83""
            }, 
            ""leaseCounter"": {
                ""N"": ""25443""
            }, 
            ""ownerSwitchesSinceCheckpoint"": {
                ""N"": ""59""
            }, 
            ""checkpointSubSequenceNumber"": {
                ""N"": ""0""
            }, 
            ""checkpoint"": {
                ""S"": ""TRIM_HORIZON""
            }, 
            ""parentShardId"": {
                ""SS"": [
                    ""shardId-000000000030""
                ]
            }, 
            ""leaseKey"": {
                ""S"": ""shardId-000000000063""
            }
        },
        ...
    ]
}
{code}

Workaround: I manually edited the DynamoDB table to delete the checkpoints for the parent shards. The child shards were then able to begin processing. I’m not sure whether this resulted in a few items being lost though.",,apachespark,baz33,brkyvz,datake914,maropu,yonran,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 26 01:42:24 UTC 2017,,,,,,,,,,"0|i354lz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/16 10:14;datake914;I'm actually experiencing the exact same problem.;;;","03/Dec/16 10:56;maropu;I'm currently looking into this issue.;;;","04/Dec/16 11:16;maropu;I tried to checkpoint with SHARD_END by `IRecordProcessorCheckpointer#checkpoint(ExtendedSequenceNumber.SHARD_END.toString())`, but I got the IllegalArgumentException exception with messages like ""Sequence number must be numeric, but was SHARD_END"". Since I'm not sure this is a expected behaviour, I ask this to aws guys here https://forums.aws.amazon.com/thread.jspa?threadID=244218. ;;;","04/Dec/16 11:31;maropu;I tried to make a workaround patch to fix this issue (https://github.com/apache/spark/compare/master...maropu:SPARK-18020) and I manually checked this issue resolved.
But, I'm not sure this workaround is acceptable, so I continue to look for other better approaches.;;;","08/Dec/16 11:16;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/16213;;;","26/Jan/17 01:42;brkyvz;Resolved by https://github.com/apache/spark/pull/16213;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 2.0.1 SQL Thrift Error,SPARK-18009,13013541,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,dkbiswal,jerryjung,jerryjung,19/Oct/16 13:57,27/Oct/16 05:19,14/Jul/23 06:29,27/Oct/16 05:16,2.0.1,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,1,thrift,,,,,"After deploy spark thrift server on YARN, then I tried to execute from the beeline following command.
> show databases;
I've got this error message. 
{quote}
beeline> !connect jdbc:hive2://localhost:10000 a a
Connecting to jdbc:hive2://localhost:10000
16/10/19 22:50:18 INFO Utils: Supplied authorities: localhost:10000
16/10/19 22:50:18 INFO Utils: Resolved authority: localhost:10000
16/10/19 22:50:18 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://localhost:10000
Connected to: Spark SQL (version 2.0.1)
Driver: Hive JDBC (version 1.2.1.spark2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://localhost:10000> show databases;
java.lang.IllegalStateException: Can't overwrite cause with java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.GenericInternalRow cannot be cast to org.apache.spark.sql.catalyst.expressions.UnsafeRow
	at java.lang.Throwable.initCause(Throwable.java:456)
	at org.apache.hive.service.cli.HiveSQLException.toStackTrace(HiveSQLException.java:236)
	at org.apache.hive.service.cli.HiveSQLException.toStackTrace(HiveSQLException.java:236)
	at org.apache.hive.service.cli.HiveSQLException.toCause(HiveSQLException.java:197)
	at org.apache.hive.service.cli.HiveSQLException.<init>(HiveSQLException.java:108)
	at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:256)
	at org.apache.hive.jdbc.Utils.verifySuccessWithInfo(Utils.java:242)
	at org.apache.hive.jdbc.HiveQueryResultSet.next(HiveQueryResultSet.java:365)
	at org.apache.hive.beeline.BufferedRows.<init>(BufferedRows.java:42)
	at org.apache.hive.beeline.BeeLine.print(BeeLine.java:1794)
	at org.apache.hive.beeline.Commands.execute(Commands.java:860)
	at org.apache.hive.beeline.Commands.sql(Commands.java:713)
	at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:973)
	at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:813)
	at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:771)
	at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:484)
	at org.apache.hive.beeline.BeeLine.main(BeeLine.java:467)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 669.0 failed 4 times, most recent failure: Lost task 0.3 in stage 669.0 (TID 3519, edw-014-22): java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.GenericInternalRow cannot be cast to org.apache.spark.sql.catalyst.expressions.UnsafeRow
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:247)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hive.service.cli.HiveSQLException.newInstance(HiveSQLException.java:244)
	at org.apache.hive.service.cli.HiveSQLException.toStackTrace(HiveSQLException.java:210)
	... 15 more
Error: Error retrieving next row (state=,code=0)
{quote}

""add jar"" command also same error occurred.
{quote}
add jar ~/udf.jar
java.lang.IllegalStateException: Can’t overwrite cause with java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.GenericInternalRow cannot be cast to org.apache.spark.sql.catalyst.expressions.UnsafeRow
at java.lang.Throwable.initCause (Throwable.java:456)
…
{quote}","apache hadoop 2.6.2 
spark 2.0.1",apachespark,cloud_fan,dkbiswal,jerryjung,martha.solarte,sampsa.tuokko@aalto.fi,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 27 05:16:49 UTC 2016,,,,,,,,,,"0|i353in:",9223372036854775807,,,,,,,,,,,,,2.0.2,2.1.0,,,,,,,,,,"25/Oct/16 12:19;martha.solarte;Hi, 
I got the same error with spark 2.0.0 but only if enable spark.sql.thriftServer.incrementalCollect=true.
Do you have this parameter enabled ?
;;;","25/Oct/16 22:41;smilegator;[~dkbiswal] Please fix it tonight. Thanks!;;;","26/Oct/16 01:43;jerryjung;Yes!
In my case, it's necessary option for integration with BI tools.;;;","26/Oct/16 04:08;dkbiswal;[~smilegator][~jerryjung] [~martha.solarte] Thanks. I am testing a fix and should submit a PR for this soon.;;;","26/Oct/16 10:29;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/15642;;;","26/Oct/16 11:19;martha.solarte;Will be the fix available for 2.0.0 ?;;;","26/Oct/16 18:42;dkbiswal;[~martha.solarte] Not sure
[~smilegator] Sean, do we back port to 2.0.0 any more ?;;;","26/Oct/16 23:41;smilegator;We try to fix it in 2.0.2. To get the fix, you have to use the new release.  ;;;","27/Oct/16 05:16;cloud_fan;Issue resolved by pull request 15642
[https://github.com/apache/spark/pull/15642];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataFrame filter Predicate push-down fails for Oracle Timestamp type columns,SPARK-18004,13013426,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,Rui Zha,snalapure@dataken.net,snalapure@dataken.net,19/Oct/16 07:43,03/Jul/17 00:41,14/Jul/23 06:29,03/Jul/17 00:41,1.6.0,,,,,,,,2.3.0,,,,SQL,,,,,,,,1,,,,,,"DataFrame filter Predicate push-down fails for Oracle Timestamp type columns with Exception java.sql.SQLDataException: ORA-01861: literal does not match format string:

Java source code (this code works fine for mysql & mssql databases) :
{noformat}
//DataFrame df = create a DataFrame over an Oracle table
df = df.filter(df.col(""TS"").lt(new java.sql.Timestamp(System.currentTimeMillis())));
		df.explain();
		df.show();
{noformat}

Log statements with the Exception:
{noformat}
Schema: root
 |-- ID: string (nullable = false)
 |-- TS: timestamp (nullable = true)
 |-- DEVICE_ID: string (nullable = true)
 |-- REPLACEMENT: string (nullable = true)
{noformat}

{noformat}
== Physical Plan ==
Filter (TS#1 < 1476861841934000)
+- Scan JDBCRelation(jdbc:oracle:thin:@10.0.0.111:1521:orcl,ORATABLE,[Lorg.apache.spark.Partition;@78c74647,{user=user, password=pwd, url=jdbc:oracle:thin:@10.0.0.111:1521:orcl, dbtable=ORATABLE, driver=oracle.jdbc.driver.OracleDriver})[ID#0,TS#1,DEVICE_ID#2,REPLACEMENT#3] PushedFilters: [LessThan(TS,2016-10-19 12:54:01.934)]
2016-10-19 12:54:04,268 ERROR [Executor task launch worker-0] org.apache.spark.executor.Executor
Exception in task 0.0 in stage 0.0 (TID 0)

java.sql.SQLDataException: ORA-01861: literal does not match format string

	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:461)
	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:402)
	at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1065)
	at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:681)
	at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:256)
	at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:577)
	at oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:239)
	at oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:75)
	at oracle.jdbc.driver.T4CPreparedStatement.executeForDescribe(T4CPreparedStatement.java:1043)
	at oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:1111)
	at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1353)
	at oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:4485)
	at oracle.jdbc.driver.OraclePreparedStatement.executeQuery(OraclePreparedStatement.java:4566)
	at oracle.jdbc.driver.OraclePreparedStatementWrapper.executeQuery(OraclePreparedStatementWrapper.java:5251)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$$anon$1.<init>(JDBCRDD.scala:383)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:359)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{noformat}",,apachespark,grahn,hvanhovell,maropu,phalverson,Rui Zha,snalapure@dataken.net,tenggyut,tsuresh,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20885,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 28 08:56:03 UTC 2017,,,,,,,,,,"0|i352tb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/16 14:41;hvanhovell;So there seems to something going on with the date format here. Could you check what query Spark SQL is sending to Oracle?;;;","17/Nov/16 14:40;snalapure@dataken.net;Date format as per the physical plan logged by Spark Dataframe:   PushedFilters: [LessThan(TS,2016-11-17 19:42:01.057)]

Confirmed the same from Oracle query logs as well: WHERE TS < '2016-11-17 19:42:01.057';;;","17/Nov/16 14:50;hvanhovell;which format should be passed to oracle?;;;","18/Nov/16 08:01;snalapure@dataken.net;The date/timestamp format Oracle expects may vary from instance to instance based on the configuration of parameters NLS_TIMESTAMP_FORMAT and NLS_DATE_FORMAT . So ideal solution would be to use the to_timestamp and to_date Oracle functions. E.g. to_timestamp('12-01-2012 21:24:00', 'dd-mm-yyyy hh24:mi:ss')

http://stackoverflow.com/questions/8855378/oracle-sql-timestamps-in-where-clause
https://docs.oracle.com/cd/B19306_01/server.102/b14225/ch4datetime.htm#i1006312;;;","21/Nov/16 09:30;maropu;The current spark Jdbc interface (JdbcDialect) cannot handle formats for database-dependent timestamps and just puts `Timestamp#toString` in where clauses (https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala#L95). One solution is to transform this `Timestamp` into a database-specific timestamp format like https://github.com/apache/spark/compare/master...maropu:SPARK-18004#diff-5a29ce8f760092fb4a9c1f190cc2f61cR96;;;","09/May/17 08:31;tenggyut;a silly but doable workaround: cast TimestampType row to LongType and compare the inner representation...

{code:java}
src.filter(col(""timestamp_col"").cast(LongType).*(1000l) > condition.asInstanceOf[Timestamp].getTime)
{code};;;","12/May/17 22:51;grahn;The right solution here is to make an explicit cast on the string representation of the timestamp value and not rely on implicit casting by the database
ANSI casting should work in nearly every RDBMS out there if the string is ISO 8601 format.  Eg.
{noformat}
select timestamp '2016-10-19 12:54:01.934';
        timestamp
-------------------------
 2016-10-19 12:54:01.934

{noformat};;;","15/May/17 14:36;phalverson;Here's the culprit, a private function that converts a scala value to a SQL literal. Note use of {{toString}} to format dates and timestamps. Seems like there should be some hook to a {{JDBCDialect}} that can handle vendor-specific syntax etc.

{code:lang=java}
  /**
   * Converts value to SQL expression.
   */
  private def compileValue(value: Any): Any = value match {
    case stringValue: String => s""'${escapeSql(stringValue)}'""
    case timestampValue: Timestamp => ""'"" + timestampValue + ""'""
    case dateValue: Date => ""'"" + dateValue + ""'""
    case arrayValue: Array[Any] => arrayValue.map(compileValue).mkString("", "")
    case _ => value
  }
{code};;;","23/Jun/17 12:13;apachespark;User 'SharpRay' has created a pull request for this issue:
https://github.com/apache/spark/pull/18404;;;","23/Jun/17 14:18;Rui Zha;The PR 18404 is closed. I will resend the PR later.;;;","24/Jun/17 03:11;apachespark;User 'SharpRay' has created a pull request for this issue:
https://github.com/apache/spark/pull/18411;;;","28/Jun/17 08:56;apachespark;User 'SharpRay' has created a pull request for this issue:
https://github.com/apache/spark/pull/18451;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RDD zipWithIndex generate wrong result when one partition contains more than 2147483647 records.,SPARK-18003,13013398,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weichenxu123,weichenxu123,weichenxu123,19/Oct/16 05:08,20/Oct/16 06:41,14/Jul/23 06:29,20/Oct/16 06:41,,,,,,,,,2.0.2,2.1.0,,,Spark Core,,,,,,,,0,correctness,,,,,"RDD zipWithIndex generate wrong result when one partition contains more than Int.MaxValue records.

when RDD contains a partition with more than 2147483647 records, 
error occurs.
for example, if partition-0 has more than 2147483647 records, the index became:
0,1, ..., 2147483647, -2147483648, -2147483647, -2147483646 ....

when we do some operation such as repartition or coalesce, it is possible to generate big partition, so this bug should be fixed.
",,apachespark,kiszk,weichenxu123,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 19 05:11:05 UTC 2016,,,,,,,,,,"0|i352n3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/16 05:11;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/15550;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broke link to R DataFrame In sql-programming-guide ,SPARK-18001,13013383,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,tommy_cug,tommy_cug,tommy_cug,19/Oct/16 03:19,19/Oct/16 04:16,14/Jul/23 06:29,19/Oct/16 04:16,2.0.1,,,,,,,,2.0.2,2.1.0,,,Documentation,,,,,,,,0,,,,,,"In http://spark.apache.org/docs/latest/sql-programming-guide.html, Section ""Untyped Dataset Operations (aka DataFrame Operations)""

Link to R doesn't work that return 
The requested URL /docs/latest/api/R/DataFrame.html was not found on this server.",,apachespark,tommy_cug,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 19 03:27:04 UTC 2016,,,,,,,,,,"0|i352jr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/16 03:27;apachespark;User 'Wenpei' has created a pull request for this issue:
https://github.com/apache/spark/pull/15543;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
catalog.getFunction(name) returns wrong result for a permanent function,SPARK-17996,13013365,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,hvanhovell,hvanhovell,hvanhovell,19/Oct/16 01:47,01/Nov/16 14:43,14/Jul/23 06:29,01/Nov/16 14:43,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"The catalog returns a wrong result, if we lookup a permanent function without specifying the database. For example:
{noformat}
scala> sql(""create function fn1 as 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs'"")
res0: org.apache.spark.sql.DataFrame = []

scala> spark.catalog.getFunction(""fn1"")
res1: org.apache.spark.sql.catalog.Function = Function[name='fn1', className='org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs', isTemporary='true']
{noformat}

It should not return that this function is temporary and define a database.",,apachespark,hvanhovell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 19 02:23:06 UTC 2016,,,,,,,,,,"0|i352fr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/16 02:23;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/15542;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark prints an avalanche of warning messages from Parquet when reading parquet files written by older versions of Parquet-mr,SPARK-17993,13013300,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,michael,michael,michael,18/Oct/16 19:53,04/Jul/17 05:53,14/Jul/23 06:29,10/Nov/16 21:40,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"It looks like https://github.com/apache/spark/pull/14690 broke parquet log output redirection. After that patch, when querying parquet files written by Parquet-mr 1.6.0 Spark prints a torrent of (harmless) warning messages from the Parquet reader:

{code}
Oct 18, 2016 7:42:18 PM WARNING: org.apache.parquet.CorruptStatistics: Ignoring statistics because created_by could not be parsed (see PARQUET-251): parquet-mr version 1.6.0
org.apache.parquet.VersionParser$VersionParseException: Could not parse created_by: parquet-mr version 1.6.0 using format: (.+) version ((.*) )?\(build ?(.*)\)
	at org.apache.parquet.VersionParser.parse(VersionParser.java:112)
	at org.apache.parquet.CorruptStatistics.shouldIgnoreStatistics(CorruptStatistics.java:60)
	at org.apache.parquet.format.converter.ParquetMetadataConverter.fromParquetStatistics(ParquetMetadataConverter.java:263)
	at org.apache.parquet.hadoop.ParquetFileReader$Chunk.readAllPages(ParquetFileReader.java:583)
	at org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:513)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:270)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:225)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:137)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:102)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:162)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:102)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:372)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

This only happens during execution, not planning, and it doesn't matter what log level the {{SparkContext}} is set to.

This is a regression I noted as something we needed to fix as a follow up to PR 14690. I feel responsible, so I'm going to expedite a fix for it. I suspect that PR broke Spark's Parquet log output redirection. That's the premise I'm going by.",,apachespark,emre.colak,jhpoelen,keith.j.kraus,michael,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 04 05:53:11 UTC 2017,,,,,,,,,,"0|i3521b:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"18/Oct/16 22:37;michael;cc [~ekhliang]

I think I have a fix for this. I'm going to submit a PR shortly.;;;","18/Oct/16 23:17;apachespark;User 'mallman' has created a pull request for this issue:
https://github.com/apache/spark/pull/15538;;;","07/Nov/16 20:24;keith.j.kraus;Following the pull request discussion on GitHub, this is currently blocking us from using Spark 2.0 as we have months of data in parquet generated from Spark 1.6 and trying to do any real job in Spark 2.0 causes 100GB+ of log files until our partition is full. A fix prior to the Parquet-1.9 upgrade would be GREATLY appreciated!;;;","07/Nov/16 21:20;michael;Thank you for your input, Keith. I agree this is a major issue, and I'm trying to get this resolved for 2.1.;;;","10/Nov/16 22:23;michael;This patch will be part of Spark 2.1, but it looks like it won't make it into 2.0.2. If you'd like help backporting this patch to 2.0, mail me privately and I can send you a patch.;;;","10/Jan/17 17:44;emre.colak;I'm using spark-shell in version 2.1 and still see this issue although the parquet-mr version is different:

{code}
17/01/10 09:36:35 WARN CorruptStatistics: Ignoring statistics because created_by could not be parsed (see PARQUET-251): parquet-mr (build 32c46643845ea8a705c35d4ec8fc654cc8ff816d)
org.apache.parquet.VersionParser$VersionParseException: Could not parse created_by: parquet-mr (build 32c46643845ea8a705c35d4ec8fc654cc8ff816d) using format: (.+) version ((.*) )?\(build ?(.*)\)
{code}

And here's how I get it:

{code}
val df = spark.read.parquet(""/Users/colake1/genomics/spark/parquet/repo.parquet"")
df.count
{code}

;;;","10/Jan/17 19:42;michael;Hi Emre,

Thanks for reporting this. To clarify, what do you mean by ""the
parquet-mr version"" is different.

;;;","10/Jan/17 19:44;emre.colak;Hi Michael,
Thanks for getting back. I see ""parquet-mr version 1.6.0"" in the original description of this issue. The warning I get says ""parquet-mr (build 32c46643845ea8a705c35d4ec8fc654cc8ff816d)"";;;","11/Jan/17 00:18;michael;Try adding the following line to {{conf/log4j.properties}} and restart your app:

{noformat}
log4j.logger.org.apache.parquet=ERROR
{noformat};;;","11/Jan/17 00:24;michael;Also, if that doesn't work, add the following line as well:

{noformat}
log4j.logger.parquet=ERROR
{noformat};;;","11/Jan/17 01:27;emre.colak;Editing the log4j.properties helped. Thanks Michael.;;;","11/Jan/17 01:32;michael;Cool. I'll work on a simple PR to silence those warnings in the default log configuration.;;;","13/Jan/17 19:46;michael;[~emre.colak] FYI https://github.com/apache/spark/pull/16580;;;","04/Jul/17 05:31;jhpoelen;Please note that this fix is not included in spark 2.1.1 . Suggest to re-open the issue.

https://github.com/apache/spark/blob/v2.1.1/core/src/main/resources/org/apache/spark/log4j-defaults.properties

does not include the fix. However, a recent release candidate does:
https://github.com/apache/spark/blob/v2.2.0-rc6/core/src/main/resources/org/apache/spark/log4j-defaults.properties .;;;","04/Jul/17 05:53;srowen;[~jhpoelen] those are not files that this change touched?
The commit is in branch 2.1: https://github.com/apache/spark/commit/be3933ddfa3b6b6cf458c0fc4865a61fef40e76a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check ascendingOrder type in sort_array function ahead,SPARK-17989,13013227,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,gurwls223,gurwls223,,18/Oct/16 15:33,12/Dec/22 17:50,14/Jul/23 06:29,20/Oct/16 02:37,,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,,,,,,"{code}
spark-sql> SELECT sort_array(array('b', 'd', 'c', 'a'), '1');
16/10/19 00:28:54 ERROR SparkSQLDriver: Failed in [SELECT sort_array(array('b', 'd', 'c', 'a'), '1')]
java.lang.ClassCastException: org.apache.spark.unsafe.types.UTF8String cannot be cast to java.lang.Boolean
	at scala.runtime.BoxesRunTime.unboxToBoolean(BoxesRunTime.java:85)
	at org.apache.spark.sql.catalyst.expressions.SortArray.nullSafeEval(collectionOperations.scala:185)
	at org.apache.spark.sql.catalyst.expressions.BinaryExpression.eval(Expression.scala:416)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1$$anonfun$applyOrElse$1.applyOrElse(expressions.scala:50)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1$$anonfun$applyOrElse$1.applyOrElse(expressions.scala:43)
{code}

I guess we should also check this in `checkInputDataTypes`.",,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 18 16:10:07 UTC 2016,,,,,,,,,,"0|i351l3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/16 16:10;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/15532;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQLTransformer leaks temporary tables,SPARK-17986,13013070,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,drewrobb,drewrobb,drewrobb,18/Oct/16 03:17,22/Oct/16 09:06,14/Jul/23 06:29,22/Oct/16 09:05,,,,,,,,,2.0.2,2.1.0,,,ML,,,,,,,,0,,,,,,"The SQLTransformer creates a temporary table when called, and does not delete this temporary table. When using a SQLTransformer in a long running Spark Streaming task, these temporary tables accumulate.

I believe that the fix would be as simple as calling  `dataset.sparkSession.catalog.dropTempView(tableName)` in the last part of `transform`:
https://github.com/apache/spark/blob/v2.0.1/mllib/src/main/scala/org/apache/spark/ml/feature/SQLTransformer.scala#L65. ",,apachespark,drewrobb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 18 04:57:06 UTC 2016,,,,,,,,,,"0|i350mn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/16 04:57;apachespark;User 'drewrobb' has created a pull request for this issue:
https://github.com/apache/spark/pull/15526;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump commons-lang3 version to 3.5.,SPARK-17985,13013068,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ueshin,ueshin,ueshin,18/Oct/16 02:57,19/Oct/16 09:07,14/Jul/23 06:29,19/Oct/16 09:07,,,,,,,,,2.1.0,,,,Spark Core,,,,,,,,0,,,,,,"{{SerializationUtils.clone()}} of commons-lang3 (<3.5) has a bug that break thread safety, which gets stack sometimes caused by race condition of initializing hash map.
See https://issues.apache.org/jira/browse/LANG-1251.",,apachespark,rxin,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 19 09:07:10 UTC 2016,,,,,,,,,,"0|i350m7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/16 03:03;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/15525;;;","18/Oct/16 20:58;rxin;The patch was reverted due to build failures in Hadoop 2.2:

Actually I'm seeing the following exceptions locally as well as on Jenkins for Hadoop 2.2:

{noformat}
[error] /scratch/rxin/spark/core/src/main/scala/org/apache/spark/util/Utils.scala:1489: value read is not a member of object org.apache.commons.io.IOUtils
[error]       var numBytes = IOUtils.read(gzInputStream, buf)
[error]                              ^
[error] /scratch/rxin/spark/core/src/main/scala/org/apache/spark/util/Utils.scala:1492: value read is not a member of object org.apache.commons.io.IOUtils
[error]         numBytes = IOUtils.read(gzInputStream, buf)
[error]                            ^
{noformat}

I'm going to revert the patch for now.
;;;","19/Oct/16 04:22;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/15548;;;","19/Oct/16 09:07;srowen;Issue resolved by pull request 15548
[https://github.com/apache/spark/pull/15548];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQLBuilder should wrap the generated SQL with parenthesis for LIMIT,SPARK-17982,13013056,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,tafranky@gmail.com,tafranky@gmail.com,18/Oct/16 01:49,12/Nov/16 01:39,14/Jul/23 06:29,11/Nov/16 21:29,2.0.0,2.0.1,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,,,,,,"The following statement fails in the spark shell . 
{noformat}
scala> spark.sql(""CREATE VIEW DEFAULT.sparkshell_2_VIEW__hive_quoted_with_where (WHERE_ID , WHERE_NAME ) AS SELECT `where`.id,`where`.name FROM DEFAULT.`where` limit 2"")

scala> spark.sql(""CREATE VIEW DEFAULT.sparkshell_2_VIEW__hive_quoted_with_where (WHERE_ID , WHERE_NAME ) AS SELECT `where`.id,`where`.name FROM DEFAULT.`where` limit 2"")
java.lang.RuntimeException: Failed to analyze the canonicalized SQL: SELECT `gen_attr_0` AS `WHERE_ID`, `gen_attr_2` AS `WHERE_NAME` FROM (SELECT `gen_attr_1` AS `gen_attr_0`, `gen_attr_3` AS `gen_attr_2` FROM SELECT `gen_attr_1`, `gen_attr_3` FROM (SELECT `id` AS `gen_attr_1`, `name` AS `gen_attr_3` FROM `default`.`where`) AS gen_subquery_0 LIMIT 2) AS gen_subquery_1
  at org.apache.spark.sql.execution.command.CreateViewCommand.prepareTable(views.scala:192)
  at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:122)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)
  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)
  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:186)
  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:167)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:65)
{noformat}
This appears to be a limitation of the create view statement .
",spark 2.0.0,apachespark,dongjoon,jiangxb1987,tafranky@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18209,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 11 23:13:05 UTC 2016,,,,,,,,,,"0|i350jj:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"18/Oct/16 02:22;tafranky@gmail.com;== SQL ==
SELECT `gen_attr_0` AS `WHERE_ID`, `gen_attr_2` AS `WHERE_NAME` FROM (SELECT `gen_attr_1` AS `gen_attr_0`, `gen_attr_3` AS `gen_attr_2` FROM SELECT `gen_attr_1`, `gen_attr_3` FROM (SELECT `id` AS `gen_attr_1`, `name` AS `gen_attr_3` FROM `default`.`where`) AS gen_subquery_0 LIMIT 2) AS gen_subquery_1
----------------------------------------------------------------^^^

  at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:197)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:99)
  at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:45)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:53)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)
  at org.apache.spark.sql.execution.command.CreateViewCommand.prepareTable(views.scala:189)
  ... 64 more
;;;","18/Oct/16 09:58;jiangxb1987;Hi, Seems we don't support specify column list inside CreateView statement, perhaps you may try
{code}
spark.sql(""CREATE VIEW DEFAULT.sparkshell_2_VIEW__hive_quoted_with_where AS SELECT `where`.id,`where`.name FROM DEFAULT.`where` limit 2"")
{code}
Greetings!;;;","18/Oct/16 18:49;tafranky@gmail.com;Hi , 
couple of things that I want to point out here. 

First ,
your workaround will not be acceptable for me because I need to control the name of the view columns . This is due to the fact that I programatically generate the scala code for the spark application . There are some invariants that are required in my framework . 


Second ,
you claim that spark does not support column names in create view statements at all ? 

How come it does work in some cases ?

Is that a limitation that is documented somewhere?
  
;;;","18/Oct/16 22:12;dongjoon;Hi, [~tafranky@gmail.com].
Could you update your JIRA title?
As you see in [~jiangxb]'s example, `limit` is not a problem.;;;","18/Oct/16 22:47;tafranky@gmail.com;Updated the  Title of the  Jira . 

I looked at the views.scala file and I want to know if setting the flag  spark.sql.nativeView.canonical to false is an acceptable  workaround.  

I tested it and it works  but the question is that an acceptable workaround.;;;","19/Oct/16 00:11;dongjoon;I'll investigate it for you, [~tafranky@gmail.com].;;;","19/Oct/16 00:53;dongjoon;Sorry, [~tafranky@gmail.com]. Now, I understand what you meant by `limit`.
The following is the simplified version of your case, isn't it?
{code}
scala> spark.version
res0: String = 2.1.0-SNAPSHOT

scala> sql(""CREATE TABLE tbl(id INT)"")
res1: org.apache.spark.sql.DataFrame = []

scala> sql(""CREATE VIEW v1(id2) AS SELECT id FROM tbl"")
res2: org.apache.spark.sql.DataFrame = []

scala> sql(""CREATE VIEW v2 AS SELECT id FROM tbl limit 2"")
res3: org.apache.spark.sql.DataFrame = []

scala> sql(""CREATE VIEW v3(id2) AS SELECT id FROM tbl limit 2"")
java.lang.RuntimeException: Failed to analyze the canonicalized SQL: ...
{code};;;","19/Oct/16 00:59;dongjoon;I'll make a PR for this today.;;;","19/Oct/16 02:20;jiangxb1987;[~dongjoon] In your examples there is a misleading part:
{code}
scala> sql(""CREATE VIEW v1(id2) AS SELECT id FROM tbl"")
res2: org.apache.spark.sql.DataFrame = []
{code}
The above ""(id2)"" in ""v1(id2)"" is in fact an identifierCommentList instead of colTypeList, so it is not actually creating columns accord.

Perhaps we should listen to [~hvanhovell] whether we should support specify columns in CreateView? ;;;","19/Oct/16 02:27;jiangxb1987;Would you please give a example that it works? Thanks!;;;","19/Oct/16 03:47;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/15546;;;","19/Oct/16 03:50;dongjoon;Hi, [~jiangxb].
I think Spark supports that like MySQL http://dev.mysql.com/doc/refman/5.7/en/create-view.html .
{code}
scala> sql(""SELECT id2 FROM v1"")
res0: org.apache.spark.sql.DataFrame = [id2: int]
{code}

;;;","19/Oct/16 03:54;dongjoon;Hi, [~tafranky@gmail.com].
I made a PR for you. It handles your case, too.
{code}
scala> sql(""create table `where`(id int, name int)"")
res1: org.apache.spark.sql.DataFrame = []

scala> spark.sql(""CREATE VIEW DEFAULT.sparkshell_2_VIEW__hive_quoted_with_where (WHERE_ID , WHERE_NAME ) AS SELECT `where`.id,`where`.name FROM DEFAULT.`where` limit 2"")
res2: org.apache.spark.sql.DataFrame = []
{code}
I just simplify your case because it contains multiple test cases.;;;","19/Oct/16 04:39;jiangxb1987;You are right, just made a mistake. Sorry for that!;;;","19/Oct/16 09:47;dongjoon;Never mind!;;;","02/Nov/16 02:21;tafranky@gmail.com;Wanted to mention that I was able to successfully  verify my cases with the changes made under this request.;;;","10/Nov/16 20:02;dongjoon;Thank you for confirming, [~tafranky@gmail.com].;;;","11/Nov/16 23:13;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/15856;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Incorrectly Set Nullability to False in FilterExec,SPARK-17981,13013035,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,smilegator,smilegator,smilegator,17/Oct/16 23:39,05/Nov/16 10:34,14/Jul/23 06:29,03/Nov/16 15:38,2.0.1,2.1.0,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,correctness,,,,,"When FilterExec contains isNotNull, which could be inferred and pushed down or users specified, we convert the nullability of the involved columns if the top-layer expression is null-intolerant. However, this is not true, if the top-layer expression is not a leaf expression, it could still tolerate the null when it has null-tolerant child expression. 

For example, cast(coalesce(a#5, a#15) as double). Although cast is a null-intolerant expression, but obviously coalesce is a null-tolerant. 

When the nullability is wrong, we could generate incorrect results in different cases.",,apachespark,kiszk,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 05 06:18:05 UTC 2016,,,,,,,,,,"0|i350ev:",9223372036854775807,,,,,,,,,,,,,2.0.2,2.1.0,,,,,,,,,,"17/Oct/16 23:48;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15523;;;","05/Nov/16 06:18;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15781;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EMLDAOptimizer fails with ClassCastException on YARN,SPARK-17975,13012970,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,jvstein,jvstein,17/Oct/16 19:49,09/Feb/17 18:53,14/Jul/23 06:29,09/Feb/17 18:53,2.0.1,,,,,,,,2.0.3,2.1.1,2.2.0,,MLlib,,,,,,,,1,,,,,,"I'm able to reproduce the error consistently with a 2000 record text file with each record having 1-5 terms and checkpointing enabled. It looks like the problem was introduced with the resolution for SPARK-13355.

The EdgeRDD class seems to be lying about it's type in a way that causes RDD.mapPartitionsWithIndex method to be unusable when it's referenced as an RDD of Edge elements.

{code}
val spark = SparkSession.builder.appName(""lda"").getOrCreate()
spark.sparkContext.setCheckpointDir(""hdfs:///tmp/checkpoints"")
val data: RDD[(Long, Vector)] = // snip
data.setName(""data"").cache()
val lda = new LDA
val optimizer = new EMLDAOptimizer
lda.setOptimizer(optimizer)
  .setK(10)
  .setMaxIterations(400)
  .setAlpha(-1)
  .setBeta(-1)
  .setCheckpointInterval(7)
val ldaModel = lda.run(data)
{code}

{noformat}
16/10/16 23:53:54 WARN TaskSetManager: Lost task 3.0 in stage 348.0 (TID 1225, server2.domain): java.lang.ClassCastException: scala.Tuple2 cannot be cast to org.apache.spark.graphx.Edge
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1$$anonfun$apply$1.apply(EdgeRDD.scala:107)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:107)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:105)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
{noformat}","Centos 6, CDH 5.7, Java 1.7u80",apachespark,imatiach,josephkb,jvstein,michaelmalak,peng.meng@intel.com,yuhaoyan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-14804,,,,,,,,,,,,SPARK-17265,,,,SPARK-14804,,,,,,,,,,"04/Jan/17 19:45;jvstein;docs.txt;https://issues.apache.org/jira/secure/attachment/12845603/docs.txt",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 09 18:49:07 UTC 2017,,,,,,,,,,"0|i3500f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/16 19:58;jvstein;This change resolves the issue for me: https://github.com/jvstein/spark/tree/lda-edgerdd;;;","17/Oct/16 20:04;jvstein;Adding a link to another issue that seems to be related to EdgeRDD partition problems.;;;","22/Dec/16 16:32;imatiach;Could you send a link to the repro dataset?  I could work on this issue but it looks like you have a fix already.  For any fixes we need tests to validate them.;;;","04/Jan/17 19:45;jvstein;Attaching vertical bar delimited documents (one per line).

With my quick fix, I'm seeing a lot more persisted RDDs on the ""Storage"" tab. I'm either not cleaning something up or there's another issue related to that.;;;","06/Jan/17 06:00;imatiach;Thank you for sending the dataset, I'm working on this issue.;;;","07/Jan/17 01:06;apachespark;User 'imatiach-msft' has created a pull request for this issue:
https://github.com/apache/spark/pull/16494;;;","07/Jan/17 01:07;imatiach;I was able to reproduce the issue based on your dataset and I've made the suggested fix in the pull request.  I added a test case that had a similar issue to your dataset and could reproduce the error.  Thank you!;;;","26/Jan/17 01:42;josephkb;[SPARK-14804] was just fixed.  [~jvstein], do you have time to test master with your code to see if the bug you hit is fixed?  Thanks!;;;","26/Jan/17 18:09;imatiach;[~josephkb] I was able to verify that this issue is now fixed after rebasing to master - can you please close the bug?  Thank you!;;;","09/Feb/17 18:49;josephkb;Will do, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Query planning slows down dramatically for large query plans even when sub-trees are cached,SPARK-17972,13012940,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,17/Oct/16 18:03,02/Nov/16 21:12,14/Jul/23 06:29,31/Oct/16 20:42,1.6.2,2.0.1,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"The following Spark shell snippet creates a series of query plans that grow exponentially. The {{i}}-th plan is created using 4 *cached* copies of the {{i - 1}}-th plan.

{code}
(0 until 6).foldLeft(Seq(1, 2, 3).toDS) { (plan, iteration) =>
  val start = System.currentTimeMillis()
  val result = plan.join(plan, ""value"").join(plan, ""value"").join(plan, ""value"").join(plan, ""value"")
  result.cache()
  System.out.println(s""Iteration $iteration takes time ${System.currentTimeMillis() - start} ms"")
  result.as[Int]
}
{code}

We can see that although all plans are cached, the query planning time still grows exponentially and quickly becomes unbearable.

{noformat}
Iteration 0 takes time 9 ms
Iteration 1 takes time 19 ms
Iteration 2 takes time 61 ms
Iteration 3 takes time 219 ms
Iteration 4 takes time 830 ms
Iteration 5 takes time 4080 ms
{noformat}

Similar scenarios can be found in iterative ML code and significantly affects usability.

This issue can be fixed by introducing a {{checkpoint()}} method for {{Dataset}} that truncates both the query plan and the lineage of the underlying RDD.",,apachespark,codingcat,lian cheng,rchukh,Tagar,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-11879,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 31 20:41:58 UTC 2016,,,,,,,,,,"0|i34ztr:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"17/Oct/16 18:14;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/15517;;;","27/Oct/16 00:05;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/15651;;;","31/Oct/16 20:41;yhuai;This issue has been resolved by https://github.com/apache/spark/pull/15651.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Calling outer join and na.fill(0) and then inner join will miss rows,SPARK-17957,13012657,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,smilegator,linbojin,linbojin,16/Oct/16 02:17,05/Nov/16 06:18,14/Jul/23 06:29,03/Nov/16 15:38,2.0.1,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,correctness,,,,,"I reported a similar bug two months ago and it's fixed in Spark 2.0.1: https://issues.apache.org/jira/browse/SPARK-17060 But I find a new bug: when I insert a na.fill(0) call between outer join and inner join in the same workflow in SPARK-17060 I get wrong result.

{code:title=spark-shell|borderStyle=solid}
scala> val a = Seq((1, 2), (2, 3)).toDF(""a"", ""b"")
a: org.apache.spark.sql.DataFrame = [a: int, b: int]

scala> val b = Seq((2, 5), (3, 4)).toDF(""a"", ""c"")
b: org.apache.spark.sql.DataFrame = [a: int, c: int]

scala> val ab = a.join(b, Seq(""a""), ""fullouter"").na.fill(0)
ab: org.apache.spark.sql.DataFrame = [a: int, b: int ... 1 more field]

scala> ab.show
+---+---+---+
|  a|  b|  c|
+---+---+---+
|  1|  2|  0|
|  3|  0|  4|
|  2|  3|  5|
+---+---+---+

scala> val c = Seq((3, 1)).toDF(""a"", ""d"")
c: org.apache.spark.sql.DataFrame = [a: int, d: int]

scala> c.show
+---+---+
|  a|  d|
+---+---+
|  3|  1|
+---+---+

scala> ab.join(c, ""a"").show
+---+---+---+---+
|  a|  b|  c|  d|
+---+---+---+---+
+---+---+---+---+
{code}

And again if i use persist, the result is correct. I think the problem is join optimizer similar to this pr: https://github.com/apache/spark/pull/14661

{code:title=spark-shell|borderStyle=solid}
scala> val ab = a.join(b, Seq(""a""), ""outer"").na.fill(0).persist
ab: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [a: int, b: int ... 1 more field]

scala> ab.show
+---+---+---+
|  a|  b|  c|
+---+---+---+
|  1|  2|  0|
|  3|  0|  4|
|  2|  3|  5|
+---+---+---+

scala> ab.join(c, ""a"").show
+---+---+---+---+
|  a|  b|  c|  d|
+---+---+---+---+
|  3|  0|  4|  1|
+---+---+---+---+
{code}
  ","Spark 2.0.1, Mac, Local
",apachespark,dongjoon,linbojin,maropu,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 05 06:18:09 UTC 2016,,,,,,,,,,"0|i34y6v:",9223372036854775807,,,,,,,,,,,,,2.0.2,2.1.0,,,,,,,,,,"16/Oct/16 02:25;linbojin;cc [~smilegator] and [~dongjoon];;;","16/Oct/16 03:51;smilegator;Thank you for reporting it. Let me do a quick check.;;;","16/Oct/16 03:58;smilegator;You can see the plan. The optimized plan is still full outer join. : ) Thus, it should not be caused by outer join elimination. 

{noformat}
    val a = Seq((1, 2), (2, 3)).toDF(""a"", ""b"")
    val b = Seq((2, 5), (3, 4)).toDF(""a"", ""c"")
    val ab = a.join(b, Seq(""a""), ""fullouter"").na.fill(0)
    val c = Seq((3, 1)).toDF(""a"", ""d"")
    ab.join(c, ""a"").explain(true)
{noformat}

{noformat}
== Optimized Logical Plan ==
Project [a#29, b#30, c#31, d#42]
+- Join Inner, (a#29 = a#41)
   :- Project [cast(coalesce(cast(coalesce(a#5, a#15) as double), 0.0) as int) AS a#29, cast(coalesce(cast(b#6 as double), 0.0) as int) AS b#30, cast(coalesce(cast(c#16 as double), 0.0) as int) AS c#31]
   :  +- Filter isnotnull(cast(coalesce(cast(coalesce(a#5, a#15) as double), 0.0) as int))
   :     +- Join FullOuter, (a#5 = a#15)
   :        :- LocalRelation [a#5, b#6]
   :        +- LocalRelation [a#15, c#16]
   +- LocalRelation [a#41, d#42]
{noformat}

Let me find what is the cause. ;;;","16/Oct/16 04:16;smilegator;Found the bug. 
{noformat}
Project [a#29, b#30, c#31, d#48]
+- Join Inner, (a#29 = a#47)
   :- Project [cast(coalesce(cast(coalesce(a#5, a#15) as double), 0.0) as int) AS a#29, cast(coalesce(cast(b#6 as double), 0.0) as int) AS b#30, cast(coalesce(cast(c#16 as double), 0.0) as int) AS c#31]
   :  +- Filter isnotnull(cast(coalesce(cast(coalesce(a#5, a#15) as double), 0.0) as int))
   :     +- Join FullOuter, (a#5 = a#15)
   :        :- LocalRelation [a#5, b#6]
   :        +- LocalRelation [a#15, c#16]
   +- LocalRelation [a#47, d#48]
{noformat}

Will fix it soon. 

;;;","16/Oct/16 04:22;linbojin;Thank you!;;;","17/Oct/16 23:48;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15523;;;","05/Nov/16 06:18;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15781;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSession createDataFrame method throws exception for nested JavaBeans,SPARK-17952,13012544,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,michalsenkyr,baghelamit,baghelamit,15/Oct/16 07:08,12/Dec/22 18:10,14/Jul/23 06:29,05/Oct/18 09:32,2.0.0,2.0.1,2.3.0,,,,,,3.0.0,,,,,,,,,,,,0,,,,,,"As per latest spark documentation for Java at http://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection, 

{quote}
Nested JavaBeans and List or Array fields are supported though.
{quote}

However nested JavaBean is not working. Please see the below code.

SubCategory class

{code}
public class SubCategory implements Serializable{
	private String id;
	private String name;
	
	public String getId() {
		return id;
	}
	public void setId(String id) {
		this.id = id;
	}
	public String getName() {
		return name;
	}
	public void setName(String name) {
		this.name = name;
	}	
}

{code}

Category class

{code}
public class Category implements Serializable{
	private String id;
	private SubCategory subCategory;
	
	public String getId() {
		return id;
	}
	public void setId(String id) {
		this.id = id;
	}
	public SubCategory getSubCategory() {
		return subCategory;
	}
	public void setSubCategory(SubCategory subCategory) {
		this.subCategory = subCategory;
	}
}
{code}

SparkSample class

{code}
public class SparkSample {
	public static void main(String[] args) throws IOException {				
		SparkSession spark = SparkSession
				.builder()
				.appName(""SparkSample"")
				.master(""local"")
				.getOrCreate();
		//SubCategory
		SubCategory sub = new SubCategory();
		sub.setId(""sc-111"");
		sub.setName(""Sub-1"");
		//Category
		Category category = new Category();
		category.setId(""s-111"");
		category.setSubCategory(sub);
		//categoryList
		List<Category> categoryList = new ArrayList<Category>();
		categoryList.add(category);
		 //DF
		Dataset<Row> dframe = spark.createDataFrame(categoryList, Category.class);	
		dframe.show();		
	}
}
{code}


Above code throws below error.

{code}
Exception in thread ""main"" scala.MatchError: com.sample.SubCategory@e7391d (of class com.sample.SubCategory)
	at org.apache.spark.sql.catalyst.CatalystTypeConverters$StructConverter.toCatalystImpl(CatalystTypeConverters.scala:256)
	at org.apache.spark.sql.catalyst.CatalystTypeConverters$StructConverter.toCatalystImpl(CatalystTypeConverters.scala:251)
	at org.apache.spark.sql.catalyst.CatalystTypeConverters$CatalystTypeConverter.toCatalyst(CatalystTypeConverters.scala:103)
	at org.apache.spark.sql.catalyst.CatalystTypeConverters$$anonfun$createToCatalystConverter$2.apply(CatalystTypeConverters.scala:403)
	at org.apache.spark.sql.SQLContext$$anonfun$beansToRows$1$$anonfun$apply$1.apply(SQLContext.scala:1106)
	at org.apache.spark.sql.SQLContext$$anonfun$beansToRows$1$$anonfun$apply$1.apply(SQLContext.scala:1106)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.sql.SQLContext$$anonfun$beansToRows$1.apply(SQLContext.scala:1106)
	at org.apache.spark.sql.SQLContext$$anonfun$beansToRows$1.apply(SQLContext.scala:1104)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.toStream(Iterator.scala:1322)
	at scala.collection.AbstractIterator.toStream(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toSeq(TraversableOnce.scala:298)
	at scala.collection.AbstractIterator.toSeq(Iterator.scala:1336)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:373)
	at com.sample.SparkSample.main(SparkSample.java:33)
{code}


createDataFrame method throws above exception. But I observed that createDataset method works fine with below code.

{code}
Encoder<Category> encoder = Encoders.bean(Category.class); 
Dataset<Category> dframe = spark.createDataset(categoryList, encoder);
dframe.show();
{code}",,apachespark,baghelamit,jmchung,kiszk,michalsenkyr,pashields,tsuresh,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25654,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 05 19:55:55 UTC 2018,,,,,,,,,,"0|i34xhr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/17 11:04;gurwls223;This still happens in the master.;;;","22/Sep/18 19:11;apachespark;User 'michalsenkyr' has created a pull request for this issue:
https://github.com/apache/spark/pull/22527;;;","26/Sep/18 19:47;michalsenkyr;Implemented nested bean support in pull request. Arrays and lists not supported yet. Will add them later if approved to put code in line with docs.;;;","05/Oct/18 09:32;ueshin;Issue resolved by pull request 22527
https://github.com/apache/spark/pull/22527;;;","05/Oct/18 19:55;michalsenkyr;Created a new issue and PR for arrays, lists and maps of beans (SPARK-25654).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sbin/start-* scripts use of `hostname -f` fail with Solaris ,SPARK-17944,13012455,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,erik.oshaughnessy,erik.oshaughnessy,erik.oshaughnessy,14/Oct/16 19:34,22/Oct/16 08:38,14/Jul/23 06:29,22/Oct/16 08:38,2.0.1,,,,,,,,2.1.0,,,,Deploy,,,,,,,,0,,,,,,"{{$SPARK_HOME/sbin/start-master.sh}} fails:

{noformat}
$ ./start-master.sh 
usage: hostname [[-t] system_name]
       hostname [-D]
starting org.apache.spark.deploy.master.Master, logging to /home/eoshaugh/local/spark/logs/spark-eoshaugh-org.apache.spark.deploy.master.Master-1-m7-16-002-ld1.out
failed to launch org.apache.spark.deploy.master.Master:
    --properties-file FILE Path to a custom Spark properties file.
                           Default is conf/spark-defaults.conf.
full log in /home/eoshaugh/local/spark/logs/spark-eoshaugh-org.apache.spark.deploy.master.Master-1-m7-16-002-ld1.out
{noformat}

I found SPARK-17546 which changed the invocation of hostname in sbin/start-master.sh, sbin/start-slaves.sh and sbin/start-mesos-dispatcher.sh to include the flag {{-f}}, which is not a valid command line option for the Solaris hostname implementation. 

As a workaround, Solaris users can substitute:
{noformat}
`/usr/sbin/check-hostname | awk '{print $NF}'`
{noformat}

Admittedly not an obvious fix, but it provides equivalent functionality. ","Solaris 10, Solaris 11",apachespark,erik.oshaughnessy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 22 08:38:12 UTC 2016,,,,,,,,,,"0|i34wxb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/16 19:54;srowen;Yeah, I think Solaris is the odd man out here then. Linux and OS X support this. I'm not sure Solaris is something Spark intends to support in general, for reasons like this. If there's a simple change that makes it work, OK, but otherwise I'd close this.;;;","14/Oct/16 20:16;erik.oshaughnessy;I'm sure there are situations where Linux and OS X disagree with how to get things done, I was hoping for more inclusivity despite Solaris being a less favored development platform.

A simple change might be to add a file similar to {{sbin/spark-config.sh}}, call it {{sbin/spark-funcs.sh}}, and populate it with a set of bash functions which can attempt OS specific invocations until it finds one that is successful or returns an error after exhausting all options. This could provide a framework for resolving future conflicts between OS implementation details in addition to addressing this particular problem.;;;","15/Oct/16 10:25;srowen;Yes, for example `hostname` differs on OS X / Linux but the behavior of `-f` happens to give the same desired output on both. Is `check-hostname` present only on Solaris (or at least, does it do the right thing everywhere we think it exists)? If so I could imagine just checking if that command exists and using it if present.

There are 3 usages, and all could have the same treatment. That's not so bad IMHO. A central script to switch out commands seems appealing but the indirection non-trivially complicates the scripts. I'm not sure how far to go to try to support Solaris, not because there's anything wrong with it, but because a) I don't know how many Spark users are on Solaris, and b) I am not sure we know it otherwise works, that there aren't other problems like this;;;","17/Oct/16 17:16;erik.oshaughnessy;AFAIK, check-hostname is a Solaris specific script written to provide a fully qualified host name to sendmail.

I am fairly certain that before SPARK-16962 there were no Apache Spark installations on Solaris SPARC platforms external to Oracle. My management assures me that there is significant interest in having Apache Spark work ""out-of-the-box"" on Solaris SPARC, thus my (admittedly recent) participation in Apache Spark.

As far as your second question, whether ""we know it otherwise works"" on Solaris, the only evidence I can offer is the paucity of issues raised as a consequence of Oracle running Apache Spark on Solaris; SPARK-13776 which tickled a Jetty bug and SPARK-16962 which addresses unaligned memory accesses.  The only other major problem we have encountered is with an Apache Spark dependency, snappy-java, that does not support 64-bit Solaris SPARC (yet). We have patches in-house which are under legal review.

I'll be happy to write a {{sbin/spark-funcs.sh}} so we have something concrete to discuss. 

;;;","17/Oct/16 17:46;srowen;To solve this problem -- if it's really the only script-based issue -- just replace these 3 instances with some if-else bash construct to call check-hostname if it exists. I wouldn't introduce a bigger abstraction for now.;;;","19/Oct/16 22:05;apachespark;User 'JnyJny' has created a pull request for this issue:
https://github.com/apache/spark/pull/15557;;;","22/Oct/16 08:38;srowen;Issue resolved by pull request 15557
[https://github.com/apache/spark/pull/15557];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""CodeGenerator - failed to compile: org.codehaus.janino.JaninoRuntimeException: Code of"" method Error",SPARK-17936,13012361,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,jrmiller,jrmiller,14/Oct/16 13:37,15/Oct/16 10:27,14/Jul/23 06:29,15/Oct/16 10:27,2.0.1,,,,,,,,2.1.0,,,,Spark Core,,,,,,,,0,,,,,,"Greetings. I'm currently in the process of migrating a project I'm working on from Spark 1.6.2 to 2.0.1. The project uses Spark Streaming to convert Thrift structs coming from Kafka into Parquet files stored in S3. This conversion process works fine in 1.6.2 but I think there may be a bug in 2.0.1. I'll paste the stack trace below.

org.codehaus.janino.JaninoRuntimeException: Code of method ""(Lorg/apache/spark/sql/catalyst/expressions/GeneratedClass;[Ljava/lang/Object;)V"" of class ""org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection"" grows beyond 64 KB
	at org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:941)
	at org.codehaus.janino.CodeContext.write(CodeContext.java:854)
	at org.codehaus.janino.UnitCompiler.writeShort(UnitCompiler.java:10242)
	at org.codehaus.janino.UnitCompiler.writeLdc(UnitCompiler.java:9058)

Also, later on:
07:35:30.191 ERROR o.a.s.u.SparkUncaughtExceptionHandler - Uncaught exception in thread Thread[Executor task launch worker-6,5,run-main-group-0]
java.lang.OutOfMemoryError: Java heap space

I've seen similar issues posted, but those were always on the query side. I have a hunch that this is happening at write time as the error occurs after batchDuration. Here's the write snippet.

stream.
      flatMap {
        case Success(row) =>
          thriftParseSuccess += 1
          Some(row)
        case Failure(ex) =>
          thriftParseErrors += 1
          logger.error(""Error during deserialization: "", ex)
          None
      }.foreachRDD { rdd =>
        val sqlContext = SQLContext.getOrCreate(rdd.context)
        transformer(sqlContext.createDataFrame(rdd, converter.schema))
          .coalesce(coalesceSize)
          .write
          .mode(Append)
          .partitionBy(partitioning: _*)
          .parquet(parquetPath)
      }

Please let me know if you can be of assistance and if there's anything I can do to help.

Best,
Justin",,jrmiller,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-14887,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 15 10:27:43 UTC 2016,,,,,,,,,,"0|i34wcn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/16 13:41;srowen;Duplicate of several JIRAs -- have a look through first.;;;","14/Oct/16 14:16;jrmiller;I don't believe this is a duplicate. This occurs while trying to write to parquet (with very little data) and happens almost immediately after batchDuration.;;;","14/Oct/16 14:18;jrmiller;I'd also note this wasn't an issue in Spark 1.6.2. The process would run fine for hours and never crashed on this error.;;;","14/Oct/16 14:18;srowen;I doubt it is a different cause given it is the same type of error in the same path. That is, do you expect the resolution differs? It can be reopened if so but otherwise I think this is more likely to fragment discussion about one problem. ;;;","14/Oct/16 14:24;jrmiller;It's just strange that the issue seems to effect previous versions (if they're the same issue) but didn't impact me when I was using 1.6.2 and the 0.8 kafka consumer. Is it possible that Scala 2.10 vs Scala 2.11 makes a difference? There are a lot of variables at play unfortunately.;;;","14/Oct/16 17:31;jrmiller;Hey Sean,

I did a bit more digging this morning looking at SpecificUnsafeProjection and saw this commit: https://github.com/apache/spark/commit/b1b47274bfeba17a9e4e9acebd7385289f31f6c8

I thought I'd try running w/2.1.0-SNAPSHOT and see how things went and it appears to work great now!

[Stage 1:> (0 + 8) / 8]11:28:33.237 INFO  c.p.o.ObservationPersister - (ObservationPersister) - Thrift Parse Success: 0 / Thrift Parse Errors: 0
[Stage 3:> (0 + 8) / 8]11:29:03.236 INFO  c.p.o.ObservationPersister - (ObservationPersister) - Thrift Parse Success: 89 / Thrift Parse Errors: 0
[Stage 5:> (4 + 4) / 8]11:29:33.237 INFO  c.p.o.ObservationPersister - (ObservationPersister) - Thrift Parse Success: 205 / Thrift Parse Errors: 0

Since we're still testing this out that snapshot works great for now. Do you know when 2.1.0 might be available generally?

Best,
Justin                                                    
;;;","15/Oct/16 10:27;srowen;OK, if you're pretty confident that also happened to fix this issue, that's great. It may even resolve the other JIRA I linked.

Evidently resolved by https://github.com/apache/spark/pull/15275;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Failed to run SQL ""show table extended  like table_name""  in Spark2.0.0",SPARK-17932,13012257,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiangxb1987,pin_zhang,pin_zhang,14/Oct/16 05:53,13/Dec/16 03:40,14/Jul/23 06:29,30/Nov/16 11:09,2.0.0,,,,,,,,2.2.0,,,,SQL,,,,,,,,0,,,,,,"SQL ""show table extended  like table_name "" doesn't work in spark 2.0.0
that works in spark1.5.2

Error: org.apache.spark.sql.catalyst.parser.ParseException: 
missing 'FUNCTIONS' at 'extended'(line 1, pos 11)

== SQL ==
show table extended  like test
-----------^^^ (state=,code=0)


",,apachespark,hvanhovell,jiangxb1987,pin_zhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 13 03:40:05 UTC 2016,,,,,,,,,,"0|i34vpj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/16 19:12;hvanhovell;This is currently not implemented in Spark 2.0. Feel free to open up a PR for this.;;;","18/Nov/16 06:58;jiangxb1987;I’m working on this, thanks!;;;","21/Nov/16 07:47;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/15958;;;","13/Dec/16 03:40;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/16262;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock when AM restart and send RemoveExecutor on reset,SPARK-17929,13012224,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,scwf,Sephiroth-Lin,Sephiroth-Lin,14/Oct/16 02:57,21/Oct/16 21:45,14/Jul/23 06:29,21/Oct/16 21:45,2.0.0,,,,,,,,2.0.2,2.1.0,,,Spark Core,,,,,,,,1,,,,,,"We fix SPARK-10582, and add reset in CoarseGrainedSchedulerBackend.scala
{code}
  protected def reset(): Unit = synchronized {
    numPendingExecutors = 0
    executorsPendingToRemove.clear()

    // Remove all the lingering executors that should be removed but not yet. The reason might be
    // because (1) disconnected event is not yet received; (2) executors die silently.
    executorDataMap.toMap.foreach { case (eid, _) =>
      driverEndpoint.askWithRetry[Boolean](
        RemoveExecutor(eid, SlaveLost(""Stale executor after cluster manager re-registered."")))
    }
  }
{code}
but on removeExecutor also need the lock ""CoarseGrainedSchedulerBackend.this.synchronized"", this will cause deadlock, and send RPC will failed, and reset failed
{code}
    private def removeExecutor(executorId: String, reason: ExecutorLossReason): Unit = {
      logDebug(s""Asked to remove executor $executorId with reason $reason"")
      executorDataMap.get(executorId) match {
        case Some(executorInfo) =>
          // This must be synchronized because variables mutated
          // in this block are read when requesting executors
          val killed = CoarseGrainedSchedulerBackend.this.synchronized {
            addressToExecutorId -= executorInfo.executorAddress
            executorDataMap -= executorId
            executorsPendingLossReason -= executorId
            executorsPendingToRemove.remove(executorId).getOrElse(false)
          }
     ...
{code}",,apachespark,Sephiroth-Lin,xuefuz,xwebber,zzr1000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 17 07:04:43 UTC 2016,,,,,,,,,,"0|i34vi7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/16 09:32;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/15481;;;","17/Oct/16 07:04;xwebber;meet the same problem, seems the deadlock is obvious in code. will thy the PR;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No driver.memoryOverhead setting for mesos cluster mode,SPARK-17928,13012223,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,devaraj,drewrobb,drewrobb,14/Oct/16 02:46,15/Jan/19 21:48,14/Jul/23 06:29,15/Jan/19 21:46,2.0.1,,,,,,,,3.0.0,,,,Mesos,,,,,,,,1,,,,,,Mesos cluster mode does not have a configuration setting for the driver's memory overhead. This makes scheduling long running drivers on mesos using dispatcher very unreliable. There is an equivalent setting for yarn-- spark.yarn.driver.memoryOverhead.,,apachespark,devaraj,drewrobb,RickAlm,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 15 21:46:21 UTC 2019,,,,,,,,,,"0|i34vhz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/16 17:49;RickAlm;Also affects 1.6;;;","22/Apr/17 09:00;apachespark;User 'devaraj-kavali' has created a pull request for this issue:
https://github.com/apache/spark/pull/17726;;;","15/Jan/19 21:46;srowen;Issue resolved by pull request 17726
[https://github.com/apache/spark/pull/17726];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HiveWriterContainer passes null configuration to serde.initialize, causing NullPointerException in AvroSerde when using avro.schema.url",SPARK-17920,13012121,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vinodkc,norvellj,norvellj,13/Oct/16 19:43,24/Nov/17 21:20,14/Jul/23 06:29,22/Nov/17 17:22,1.6.2,2.0.0,,,,,,,2.2.1,2.3.0,,,SQL,,,,,,,,2,,,,,,"When HiveWriterContainer intializes a serde it explicitly passes null for the Configuration:

https://github.com/apache/spark/blob/v2.0.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/hiveWriterContainers.scala#L161

When attempting to write to a table stored as Avro with avro.schema.url set, this causes a NullPointerException when it tries to get the FileSystem for the URL:

https://github.com/apache/hive/blob/release-2.1.0-rc3/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerdeUtils.java#L153

Reproduction:
{noformat}
spark-sql> create external table avro_in (a string) stored as avro location '/avro-in/' tblproperties ('avro.schema.url'='/avro-schema/avro.avsc');

spark-sql> create external table avro_out (a string) stored as avro location '/avro-out/' tblproperties ('avro.schema.url'='/avro-schema/avro.avsc');

spark-sql> select * from avro_in;
hello
Time taken: 1.986 seconds, Fetched 1 row(s)

spark-sql> insert overwrite table avro_out select * from avro_in;

16/10/13 19:34:47 WARN AvroSerDe: Encountered exception determining schema. Returning signal schema to indicate problem
java.lang.NullPointerException
	at org.apache.hadoop.fs.FileSystem.getDefaultUri(FileSystem.java:182)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:174)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:359)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFromFS(AvroSerdeUtils.java:131)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:112)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.determineSchemaOrReturnErrorSchema(AvroSerDe.java:167)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:103)
	at org.apache.spark.sql.hive.SparkHiveWriterContainer.newSerializer(hiveWriterContainers.scala:161)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.sideEffectResult$lzycompute(InsertIntoHiveTable.scala:236)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.sideEffectResult(InsertIntoHiveTable.scala:142)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.doExecute(InsertIntoHiveTable.scala:313)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:186)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:167)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:65)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:682)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:62)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:331)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{noformat}

Hive fixed a similar issue in FileSinkOperator in https://issues.apache.org/jira/browse/HIVE-9651","AWS EMR 5.0.0: Spark 2.0.0, Hive 2.1.0",apachespark,hvivani,java8964,kavn,mateo7,mokane,norvellj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19580,SPARK-19878,,,,,,,,,,,,,"13/Oct/16 19:45;norvellj;avro.avsc;https://issues.apache.org/jira/secure/attachment/12833192/avro.avsc","13/Oct/16 19:45;norvellj;avro_data;https://issues.apache.org/jira/secure/attachment/12833193/avro_data",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 24 06:13:04 UTC 2017,,,,,,,,,,"0|i34uvb:",9223372036854775807,,,,,,,,,,,,,2.2.1,2.3.0,,,,,,,,,,"20/Feb/17 16:44;mateo7;[~norvellj]
This is similar to SPARK-19580. I'm also struggling with the same problem in Amazon EMR. Looking for way how to fix it.;;;","27/Feb/17 11:25;kavn;I also get the same problem, i implement a hive serde, and in my implemention i need the parameter Configuration to get some static and dynamic settings, i run it in hive well. but when i use spark's HiveContext to insert data to hive table, in serialize it get the configuration as null, so causing NullPointerException!
I'm using Spark1.5, i found the problem is in class org.apache.spark.sql.hive.execution.InsertIntoHiveTable, the function newSerializer uses serializer.initialize(null, tableDesc.getProperties) to get the serializer.  This also affects Spark1.6 and Spark 2.0;;;","18/Nov/17 09:02;apachespark;User 'vinodkc' has created a pull request for this issue:
https://github.com/apache/spark/pull/19779;;;","22/Nov/17 13:01;apachespark;User 'vinodkc' has created a pull request for this issue:
https://github.com/apache/spark/pull/19795;;;","23/Nov/17 11:43;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19799;;;","24/Nov/17 06:13;apachespark;User 'vinodkc' has created a pull request for this issue:
https://github.com/apache/spark/pull/19809;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CSV data source treats empty string as null no matter what nullValue option is,SPARK-17916,13012096,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,falaki,falaki,13/Oct/16 18:17,12/Dec/22 18:11,14/Jul/23 06:29,14/May/18 02:02,2.0.1,,,,,,,,2.4.0,,,,SQL,,,,,,,,1,,,,,,"When user configures {{nullValue}} in CSV data source, in addition to those values, all empty string values are also converted to null.

{code}
data:
col1,col2
1,""-""
2,""""
{code}

{code}
spark.read.format(""csv"").option(""nullValue"", ""-"")
{code}

We will find a null in both rows.",,apachespark,codingcat,ekhliang,falaki,felixcheung,jzijlstra,koert,maropu,maxgekk,smilegator,tsuresh,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21768,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 11 06:46:49 UTC 2018,,,,,,,,,,"0|i34upr:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,,"14/Oct/16 00:34;gurwls223;Hi [~falaki], this JIRA rings a bell to me. Do you mind if I ask to take a look https://github.com/apache/spark/pull/12904 and SPARK-15125. Could you confirm that it is a duplicate maybe?;;;","14/Oct/16 00:44;falaki;Thanks for linking it. Yes they are very much same issues. However, I slightly disagree with the proposed solution. I will comment on the PR.;;;","20/Oct/16 10:06;gurwls223;[~falaki] I just have thought about this more but I started to feel we might not need this.
I tested {{read.csv()}} in R as well after setting the equalvalent default values in Spark CSV options.

{code}
> d <- ""col1,col2
+ 1,\""-\""
+ 2,\""\""""
> df <- read.csv(text=d, quote=""\"""", na.strings=c(""-""))
> df
  col1 col2
1    1   NA
2    2   NA
{code}

It seems {{read.csv()}} in R seems not having this option[1] and handles equalvalently with the current behaviour of Spark CSV dataource.
Shouldn't we maybe do this as below?
 
{code}
> df <- read.csv(text=d, quote="""", na.strings=c(""\""-\""""))
> df
  col1 col2
1    1 <NA>
2    2   """"
{code}

I am not saying we should exactly match {{read.csv()}} in R to {{read.csv()}} in Spark but maybe I guess it'd be nicer if the behaviour is matched up in general.
I guess we could add this option (as you pointed out) but I am worried if this brings some confusions between {{nullValue}} and {{emptyValue}} as pointed out in the PR.

[1]https://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html;;;","20/Oct/16 10:11;gurwls223;FWIW, the codes I ran in Spark (equalvalent with the last example in R) is as blow.

{code}
spark.read.format(""csv"")
  .option(""nullValue"", ""\""-\"""")
  .option(""quote"", """")
  .option(""header"", ""true"")
  .load(""path"")
  .show()
{code}

{code}
+----+----+
|col1|col2|
+----+----+
|   1|null|
|   2|  """"|
+----+----+
{code};;;","20/Oct/16 21:44;tsuresh;Thank you for trying out the different scenarios. I think output you are getting after setting he quote to empty is not what is expected in the case. You want """" to be recognized as empty string, not actual quotes in the output.

Example (Before my changes on 2.0.1 branch):

input:
col1,col2
1,""-""
2,""""
3,
4,""A,B""

val df = spark.read.format(""csv"").option(""nullValue"", ""\""-\"""").option(""quote"", """").option(""header"", true).load(""/Users/suresht/sparktests/emptystring.csv"")
df: org.apache.spark.sql.DataFrame = [col1: string, col2: string]

scala> df.selectExpr(""length(col2)"").show
+------------+
|length(col2)|
+------------+
|        null|
|           2|
|        null|
|           2|
+------------+


scala> df.show
+----+----+
|col1|col2|
+----+----+
|   1|null|
|   2|  """"|
|   3|null|
|   4|  ""A|
+----+----+



;;;","20/Oct/16 22:49;gurwls223;Oh, yes sure. I just thought the root problem is to differentiate {{""""}}. Once we can distinguish it, we can easily transform it. Also, another point I want to make was.. we already have a great reference in R but it seems not handling this case.
;;;","20/Oct/16 23:03;gurwls223;Could I please ask what you think? cc [~felixcheung];;;","21/Oct/16 04:56;felixcheung;So here's what happen.

First, R read.csv has clearly documented that it treats empty/blank string the same as NA in the following condition: ""Blank fields are also considered to be missing values in logical, integer, numeric and complex fields.""

Second, in this example in R, the 2nd column is turned into ""logical"", instead of ""character"" (ie. string) as expected:
{code}
> d <- ""col1,col2
+ 1,\""-\""
+ 2,\""\""""
> df <- read.csv(text=d, quote=""\"""", na.strings=c(""-""))
> df
  col1 col2
1    1   NA
2    2   NA
> str(df)
'data.frame':	2 obs. of  2 variables:
 $ col1: int  1 2
 $ col2: logi  NA NA
{code}

And that is why the blank string is turned into NA.

Whereas if the data.frame has character/factor column instead, the blank field is retained as blank:
{code}
> d <- ""col1,col2
+ 1,\""###\""
+ 2,\""\""
+ 3,\""this is a string\""""
> df <- read.csv(text=d, quote=""\"""", na.strings=c(""###""))
> df
  col1             col2
1    1             <NA>
2    2
3    3 this is a string
> str(df)
'data.frame':	3 obs. of  2 variables:
 $ col1: int  1 2 3
 $ col2: Factor w/ 2 levels """",""this is a string"": NA 1 2
{code}

IMO this behavior makes sense.;;;","08/Nov/16 22:46;ekhliang;We're hitting this as a regression from 2.0 as well.

Ideally, we don't want the empty string to be treated specially in any scenario. The only logic that converts it to nulls should be due to the nullValue option.;;;","09/Nov/16 15:02;tsuresh;@Eric Liang   If it is possible , can you please share the data, and expected output with the options 

I am trying to fix this issue in PR ; https://github.com/apache/spark/pull/12904

;;;","09/Nov/16 23:49;ekhliang;In our case, a user wants the empty string (whether actually missing, e.g. ,, or quoted ,""""), to resolve as the empty string. It should only turn into null if nullValue is set to """". There doesn't currently appear to be some option combination that allows this.;;;","29/Nov/16 09:03;jzijlstra;I also have the same issue in 2.0.1. This code seems to be the problem:

{code}
private def rowToString(row: InternalRow): Seq[String] = {
    var i = 0
    val values = new Array[String](row.numFields)
    while (i < row.numFields) {
      if (!row.isNullAt(i)) {
        values(i) = valueConverters(i).apply(row, i)
      } else {
        values(i) = params.nullValue
      }
      i += 1
    }
    values
  }

def castTo(
      datum: String,
      castType: DataType,
      nullable: Boolean = true,
      options: CSVOptions = CSVOptions()): Any = {

    if (nullable && datum == options.nullValue) {
      null
    } else {

}{code}


So first the missing value in the data in transformed into the nullValue. Then in the castTo the value is checked against the nullValue, which is always true for a missing value, and casted to null;;;","23/Dec/17 21:36;apachespark;User 'aa8y' has created a pull request for this issue:
https://github.com/apache/spark/pull/20068;;;","08/May/18 21:41;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/21273;;;","14/May/18 02:02;gurwls223;Issue resolved by pull request 21273
[https://github.com/apache/spark/pull/21273];;;","17/Aug/18 19:53;koert;the default behavior in 2.3.x for csv format is that when i write out null value, it comes back in as null. when i write out empty string, it also comes back in as null.

now my nulls are coming back in as empty strings, which would be a very big behavior change. please advice what settings i need to get behavior of 2.3 back, so empty strings read back in as nulls.

to give some background, most csv files have empty values. we have hundreds of spark scripts/programs that read existing csv files and assume empty values are read in as null values, and these programs act/analyze accordingly. i don't think we are alone in this respect. for all these programs this would be a big breaking change, unless i am missing something.;;;","17/Aug/18 20:36;maxgekk;> he default behavior in 2.3.x for csv format is that when i write out null value, it comes back in as null. when i write out empty string, it also comes back in as null.

[~koert] Please, have a look at the added test: [https://github.com/apache/spark/pull/21273/files#diff-219ac8201e443435499123f96e94d29fR1355] . It checks exactly what you described. If you have something different, please, leave the code here.;;;","17/Aug/18 21:01;koert;hi [~maxgekk]
i saw your unit test for the old behavior. let me try to find out what is not working for us.;;;","17/Aug/18 21:02;koert;my first observation is that if i do this:
{code:scala}
val litNull: String = null
val df = Seq(
  (1, ""John Doe""),
  (2, """"),
  (3, ""-""),
  (4, litNull)
).toDF(""id"", ""name"")

df
  .write
  .csv(""/tmp/abc1"")
{code}
and inspect in bash
{code:bash}
cat /tmp/abc1/part-0000*.csv
1,John Doe
2,""""
3,-
4,""""
{code}
notice how for the null value it wrote the empty quoted string. that is emptyValue, not nullValue, which seems incorrect to me.

if i do the same exercise in spark 2.3 i get:
{code:bash}
cat /tmp/abc1/part-0000*.csv
1,John Doe
2,
3,-
4,
{code}

so my actual csv data has changed upon writing. that makes me nervous about compatibility with other systems that read data we produce.
;;;","17/Aug/18 21:06;koert;we also use csv format to write files like for example bsv (bar delimited, interpreted by systems that do not support quoting at all). having an output that used to be:
{code:bash}
1|John Doe
2|
3|-
4|
{code}
become:
{code:bash}
1|John Doe
2|""""
3|-
4|""""
{code}
will no go down well.;;;","17/Aug/18 21:13;koert;now the particular unit test that broke for us, where nulls come back in as quoted strings, is this:
{code:scala}
      val litNull: String = null
      val df = Seq(
        (1, ""John Doe""),
        (2, """"),
        (3, ""-""),
        (4, litNull)
      ).toDF(""id"", ""name"")

      df
        .write
        .format(""csv"")
        .option(""header"", true)
        .option(""delimiter"", ""|"")
        .option(""quote"", ""☃"")
        .save(""/tmp/abc"")

      val df1 = spark
        .read
        .format(""csv"")
        .option(""header"", true)
        .option(""delimiter"", ""|"")
        .option(""quote"", ""☃"")
        .load(""/tmp/abc"")
      df1.show

+---+--------+
| id|    name|
+---+--------+
|  1|John Doe|
|  2|      """"|
|  4|      """"|
|  3|       -|
+---+--------+
{code}

so here we are writing out with bar as delimiter, and quoting is not supported at all (so i set it to a silly character, i cannot think of better option).
i am not sure i understand yet why the nulls come back in as strings, but it has to do something with setting the quote character, because if i dont do that the test passes.;;;","01/Sep/18 19:43;apachespark;User 'koertkuipers' has created a pull request for this issue:
https://github.com/apache/spark/pull/22312;;;","08/Sep/18 21:24;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22367;;;","08/Sep/18 21:25;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22367;;;","11/Sep/18 06:46;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22389;;;",,,,,,,,,,,,,,,,,,
Spark SQL casting to TimestampType with nanosecond results in incorrect timestamp,SPARK-17914,13012079,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aokolnychyi,oromankova@cardlytics.com,oromankova@cardlytics.com,13/Oct/16 17:36,16/Nov/20 10:31,14/Jul/23 06:29,12/Jun/17 20:08,1.6.1,,,,,,,,2.2.0,2.3.0,,,SQL,,,,,,,,0,,,,,,"In some cases when timestamps contain nanoseconds they will be parsed incorrectly. 

Examples: 

""2016-05-14T15:12:14.0034567Z"" -> ""2016-05-14 15:12:14.034567""
""2016-05-14T15:12:14.000345678Z"" -> ""2016-05-14 15:12:14.345678""

The issue seems to be happening in DateTimeUtils.stringToTimestamp(). It assumes that only 6 digit fraction of a second will be passed.

With this being the case I would suggest either discarding nanoseconds automatically, or throw an exception prompting to pre-format timestamps to microsecond precision first before casting to the Timestamp.",,agattiker,apachespark,cchandurkar,jurriaanpruis,oromankova@cardlytics.com,pcocko,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-14428,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 16 10:31:14 UTC 2020,,,,,,,,,,"0|i34ulz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/16 18:09;srowen;I think this is a duplicate of one of a couple possible issues, like https://issues.apache.org/jira/browse/SPARK-14428;;;","13/Oct/16 18:17;oromankova@cardlytics.com;You are correct. It is related to what has been proposed in SPARK-14428. However, current behavior is defective. If SPARK-14428 is not going to be approved to be supported, then at least the defect deserves  consideration to be addressed. ;;;","13/Oct/16 19:09;srowen;Does the ISO8601 format support nanoseconds even? I thought we had a discussion about it but don't recall the conclusion;;;","13/Oct/16 19:41;oromankova@cardlytics.com;Sean, I can't find any evidence of ISO8601 not supporting nanoseconds. All it says that it supports fraction of a second that should be supplied following comma or dot. Different parsing libraries that support ISO8601 have different precision limitations. For instance in Python, datetime.strptime() only supports precision down to microseconds and will throw an exception if nanoseconds were supplied in input string. While it may not be ideal for those who need to be able to retain nanosecond precision after parsing, it is an acceptable behavior. Those who do not need to retain nanosecond precision can catch, or, preemptively, truncate input string. Spark sql DateTimeUtils.stringToTimestamp() doesn't throw, and doesn't truncate properly, which results in incorrect timestamp. In the example above, the acceptable truncation would be:

""2016-05-14T15:12:14.0034567Z"" -> ""2016-05-14 15:12:14.003456""
""2016-05-14T15:12:14.000345678Z"" -> ""2016-05-14 15:12:14.000345""
;;;","09/Jun/17 12:00;apachespark;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/18252;;;","12/Jun/17 20:08;ueshin;Issue resolved by pull request 18252
[https://github.com/apache/spark/pull/18252];;;","23/Jan/19 18:19;cchandurkar;I'm still seeing this issue in Spark 2.4.0 when using from_json() function. In ISO Zulu format datetime, it is not interpreting the timezone accurately after certain number of digits. Every digit added after 3rd digit in the timestamp is adding up more seconds to the parsed datetime.  For example, This datetime: ""2019-01-23T17:50:29.9991Z"" when parsed using spark's build-in from_json() function results in ""2019-01-23T17:50:38.991+0000"" ( Note the number of seconds added )

 

If I'm not wrong from_json() internally uses the Jackson JSON library. I'm not sure if the bug is within that or within spark.

 
{code:java}
// Create Schema to Parse JSON
val sc = StructType(
  StructField(
   ""date"", TimestampType
  ):: Nil
){code}
{code:java}
// Sample JSON Parsing using schema created
Seq( """"""{""date"": ""2019-01-22T18:33:39.134232733Z""}"""""" )
.toDF( ""data"" )
.withColumn( ""parsed"", from_json( $""data"", sc ) )
{code}
This results in date being ""2019-01-24T07:50:51.733+0000"" ( Note the difference of 2 days ) ;;;","26/Mar/19 14:35;jurriaanpruis;I'm also seeing this issue where the millisecond part 'overflows' into the rest of the timestamp in Spark 2.4.0 as described in the comment above. To me it seems like this issue isn't resolved yet. cc [~ueshin];;;","14/Oct/19 11:35;agattiker;As reported by other commenters, the issue is still outstanding with from_json in Spark 2.4.3 (Azure Databricks 5.5 LTS):

{{sc.parallelize(List(""2019-10-14T{color:#00875a}09:39{color}:07.3220000Z"")).toDF}}
{{.select('value.cast(""timestamp""))}}
{{// 2019-10-14T{color:#00875a}09:39{color}:07.322+0000}}
{{// correct time parsing outside of from_json}}

{{val schema = StructType(StructField(""a"", TimestampType, false) :: Nil)}}
{{sc.parallelize(List(""""""{""a"":""2019-10-14T}}{color:#00875a}{{09:39}}{color}{{:07.3220000Z""}"""""")).toDF}}
{{.select(from_json('value, schema))}}
{{// {""a"":""2019-10-14T{color:#de350b}10:32{color}:47.000+0000""}}}
{{// wrong time, corresponds to 09:39+3220 seconds}}

{{val schema = StructType(StructField(""a"", TimestampType, false) :: Nil)}}
{{sc.parallelize(List(""""""{""a"":""2019-10-14T{color:#00875a}09:39{color}:322000Z""}"""""")).toDF}}
{{.select(from_json('value, schema))}}
{{// {""a"":""2019-10-14T{color:#de350b}09:44{color}:29.000+0000""}}}
{{// wrong time, corresponds to 09:39+322 seconds}}

{{val schema = StructType(StructField(""a"", TimestampType, false) :: Nil)}}
{{sc.parallelize(List(""""""{""a"":""2019-10-14T{color:#00875a}09:39{color}:322Z""}"""""")).toDF}}
{{.select(from_json('value, schema))}}
{{// {""a"":""2019-10-14T{color:#00875a}09:39{color}:07.322+0000""}}}
{{// correct time}};;;","16/Nov/20 10:31;pcocko;I think that the solution only apply if there are more than 6 digits as miliseconds.

https://github.com/apache/spark/pull/18252/commits/2f232a7bda28fb42759ee35923044f886a1ff19e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filter/join expressions can return incorrect results when comparing strings to longs,SPARK-17913,13012074,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,mingbeckwith,mingbeckwith,13/Oct/16 17:20,07/Sep/18 03:07,14/Jul/23 06:29,24/Jan/17 18:19,1.6.2,2.0.0,,,,,,,2.2.0,,,,SQL,,,,,,,,0,release_notes,,,,,"Reproducer:

{code}
  case class E(subject: Long, predicate: String, objectNode: String)

  def test(sc: SparkContext) = {
    val sqlContext: SQLContext = new SQLContext(sc)
    import sqlContext.implicits._

    val broken = List(
      (19157170390056969L, ""right"", 19157170390056969L),
      (19157170390056973L, ""wrong"", 19157170390056971L),
      (19157190254313477L, ""wrong"", 19157190254313475L),
      (19157180859056133L, ""wrong"", 19157180859056131L),
      (19157170390056969L, ""number"", 161),
      (19157170390056971L, ""string"", ""a string""),
      (19157190254313475L, ""string"", ""another string""),
      (19157180859056131L, ""number"", 191)
    )

    val brokenDF = sc.parallelize(broken).map(b => E(b._1, b._2, b._3.toString)).toDF()
    val brokenFilter = brokenDF.filter($""subject"" === $""objectNode"")
    val fixed = brokenDF.filter(brokenDF(""subject"").cast(""string"") === brokenDF(""objectNode""))

    println(""***** incorrect filter results *****"")
    println(brokenFilter.show())
    println(""***** correct filter results *****"")
    println(fixed.show())

    println(""***** both sides cast to double *****"")
    println(brokenFilter.explain())
  }

Broken filter returns:

+-----------------+---------+-----------------+
|          subject|predicate|       objectNode|
+-----------------+---------+-----------------+
|19157170390056969|    right|19157170390056969|
|19157170390056973|    wrong|19157170390056971|
|19157190254313477|    wrong|19157190254313475|
|19157180859056133|    wrong|19157180859056131|
+-----------------+---------+-----------------+
{code}

The physical plan shows both sides of the expression are being cast to Double before evaluation. So while comparing numbers to a string number appears to work in many cases, when the numbers are sufficiently large and close together there is enough loss of precision to cause incorrect results. 

{code}
== Physical Plan ==
Filter (cast(subject#0L as double) = cast(objectNode#2 as double))

After casting the left side into strings, the filter returns the expected result:

+-----------------+---------+-----------------+
|          subject|predicate|       objectNode|
+-----------------+---------+-----------------+
|19157170390056969|    right|19157170390056969|
+-----------------+---------+-----------------+
{code}

Expected behavior in this case is probably to choose one side and cast the other (compare string to string or long to long) instead of using a data type with less precision. 
",,apachespark,java8964,maropu,mingbeckwith,rxin,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18489,SPARK-19971,,,,,SPARK-19415,,,,SPARK-21646,,,,SPARK-25039,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 24 18:19:13 UTC 2017,,,,,,,,,,"0|i34ukv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/16 10:56;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/15880;;;","24/Jan/17 18:19;smilegator;Issue resolved by pull request 15880
[https://github.com/apache/spark/pull/15880];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow users to update the comment of a column,SPARK-17910,13012028,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiangxb1987,yhuai,yhuai,13/Oct/16 15:20,12/Oct/19 08:52,14/Jul/23 06:29,15/Dec/16 18:09,,,,,,,,,2.2.0,,,,SQL,,,,,,,,0,,,,,,"Right now, once a user set the comment of a column with create table command, he/she cannot update the comment. It will be useful to provide a public interface (e.g. SQL) to do that. ",,apachespark,jiangxb1987,ouyangxc.zte,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29447,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 06 09:31:34 UTC 2017,,,,,,,,,,"0|i34uan:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"18/Oct/16 10:28;jiangxb1987;Hi, If no one is working on this, I'd like to have a try. Thanks!;;;","21/Oct/16 09:29;jiangxb1987; [~yhuai] Seems we don't support `ALTER TABLE CHANGE COLUMN` statements currently, do we plan to support that? Are there any discussions I can refer to? Thank you! ;;;","21/Oct/16 18:31;yhuai;Maybe we can just enable that but just make it only support setting the comment. ;;;","01/Nov/16 16:09;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/15717;;;","06/Feb/17 09:31;ouyangxc.zte;Hey，I wonder that do we have a plan to support changing column's dataType and name later ? Thanks!  [~yhuai] [~jiangxb];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
collect() ignores stringsAsFactors,SPARK-17902,13011893,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,falaki,falaki,13/Oct/16 05:04,12/Dec/22 18:10,14/Jul/23 06:29,26/Oct/17 11:55,2.0.1,,,,,,,,2.1.3,2.2.1,2.3.0,,SparkR,,,,,,,,0,,,,,,"`collect()` function signature includes an optional flag named `stringsAsFactors`. It seems it is completely ignored.

{code}
str(collect(createDataFrame(iris), stringsAsFactors = TRUE)))
{code}",,apachespark,falaki,felixcheung,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 26 11:55:42 UTC 2017,,,,,,,,,,"0|i34tgn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/16 16:49;shivaram;Good catch - Looks like this was changed in https://github.com/apache/spark/commit/71a138cd0e0a14e8426f97877e3b52a562bbd02c which is a part of 1.6.0. Do you have a small test case that fails ?;;;","13/Oct/16 17:54;falaki;Thanks for the pointer [~shivaram]. I will submit it patch with a regression test. The only obvious side-effect of this bug, is that collected type will be String, while it should have been a Factor. What makes it bad is that it is in our documentation and it used to work, so it is a regression.;;;","18/Oct/17 01:29;gurwls223;Hi [~falaki] and [~shivaram], I was thinking a just simple way such as :

{code}
if (stringsAsFactors) {
  df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)], as.factor)
}
{code}

Would it make sense?;;;","18/Oct/17 19:59;shivaram;I think [~falaki] might have a test case that we could test against ?;;;","18/Oct/17 20:23;falaki;A simple unit test we could add would be:

{code}
> df <- createDataFrame(iris)
> sapply(iris, typeof) == sapply(collect(df, stringsAsFactors = T), typeof)
Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species
        TRUE         TRUE         TRUE         TRUE        FALSE
{code}

As for the solution, I suggest performing the conversion inside [this loop|https://github.com/apache/spark/blob/master/R/pkg/R/DataFrame.R#L1168].;;;","22/Oct/17 10:38;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/19551;;;","26/Oct/17 11:55;gurwls223;Issue resolved by pull request 19551
[https://github.com/apache/spark/pull/19551];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
not isnotnull is converted to the always false condition isnotnull && not isnotnull,SPARK-17897,13011860,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,kuujo,kuujo,13/Oct/16 02:22,17/May/20 17:58,14/Jul/23 06:29,30/Nov/16 11:42,2.0.0,2.0.1,,,,,,,2.0.3,2.1.0,,,Optimizer,SQL,,,,,,,0,correctness,,,,,"When a logical plan is built containing the following somewhat nonsensical filter:
{{Filter (NOT isnotnull($f0#212))}}

During optimization the filter is converted into a condition that will always fail:
{{Filter (isnotnull($f0#212) && NOT isnotnull($f0#212))}}

This appears to be caused by the following check for {{NullIntolerant}}:

https://github.com/apache/spark/commit/df68beb85de59bb6d35b2a8a3b85dbc447798bf5#diff-203ac90583cebe29a92c1d812c07f102R63

Which recurses through the expression and extracts nested {{IsNotNull}} calls, converting them to {{IsNotNull}} calls on the attribute at the root level:

https://github.com/apache/spark/commit/df68beb85de59bb6d35b2a8a3b85dbc447798bf5#diff-203ac90583cebe29a92c1d812c07f102R49

This results in the nonsensical condition above.",,apachespark,cloud_fan,kuujo,rxin,smilegator,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 11 07:54:04 UTC 2017,,,,,,,,,,"0|i34t9b:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"29/Nov/16 06:02;rxin;cc [~cloud_fan], [~smilegator], [~hvanhovell];;;","29/Nov/16 06:11;smilegator;Let me try to reproduce it in master;;;","29/Nov/16 07:20;smilegator;I can reproduce it. Will fix it tomorrow. Thanks for reporting this! ;;;","29/Nov/16 07:42;smilegator;Actually, the fix is super simple. Just one line.;;;","29/Nov/16 08:09;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16055;;;","29/Nov/16 11:37;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16060;;;","29/Nov/16 13:39;cloud_fan;After looking at the code, why we make `Attribute` extends `NullIntolerant`? The definition of `NullIntolerant` is: When an expression inherits this, meaning the expression is null intolerant (i.e. any null input will result in null output). But `Attribute` is input, it doesn't fit the definition of `NullIntolerant`.

Any ideas? cc [~viirya];;;","29/Nov/16 13:46;cloud_fan;oh sorry the web page was not refreshed and I didn't notice that you [~smilegator] already send out the PR. I have the same idea with you, let's see if it breaks any tests.;;;","29/Nov/16 15:35;viirya;I think the original idea should be, in scanNullIntolerantExpr, it scans IsNotNull(_: NullIntolerant) and returns the attributes in the IsNotNull expression.

If Attribute is not NullIntolerant, it won't return the attributes from an expression like IsNotNull('a) && IsNotNull('b).;;;","29/Nov/16 15:37;viirya;And I think Attribute is the input of itself?;;;","29/Nov/16 18:18;smilegator;My first PR does not cover all the cases. Found the root cause. 

The `constraints` of an operator is the expressions that evaluate to `true` for all the rows produced. That means, the expression result should be neither `false` nor `unknown` (NULL). Thus, we can conclude that `IsNotNull` on all the constraints, which are generated by its own predicates or propagated from the children. The constraint can be a complex expression. For better usage of these constraints, we try to push down `IsNotNull` to the lowest-level expressions. `IsNotNull` can be pushed through an expression when it is null intolerant. (When the input is NULL, the null-intolerant expression always evaluates to null.)

Below is the code we have for `IsNotNull` pushdown. 

{noformat}
  private def scanNullIntolerantExpr(expr: Expression): Seq[Attribute] = expr match {
    case a: Attribute => Seq(a)
    case _: NullIntolerant | IsNotNull(_: NullIntolerant) =>
      expr.children.flatMap(scanNullIntolerantExpr)
    case _ => Seq.empty[Attribute]
  }
{noformat}

`IsNotNull` is not null-intolerant. It converts `null` to `false`. If there does not exist any `Not`-like expression, it works; otherwise, it could generate a wrong result. The above function needs to be corrected to 

{noformat}
  private def scanNullIntolerantExpr(expr: Expression): Seq[Attribute] = expr match {
    case a: Attribute => Seq(a)
    case _: NullIntolerant => expr.children.flatMap(scanNullIntolerantExpr)
    case _ => Seq.empty[Attribute]
  }
{noformat}

This fixes the problem, but we need a smarter fix for avoiding regressions. ;;;","29/Nov/16 21:35;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16067;;;","11/Feb/17 07:54;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16894;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure uniqueness of TaskSetManager name,SPARK-17894,13011809,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,erenavsarogullari,erenavsarogullari,erenavsarogullari,12/Oct/16 21:46,17/May/20 17:46,14/Jul/23 06:29,24/Oct/16 22:39,2.1.0,,,,,,,,2.1.0,,,,Scheduler,Spark Core,,,,,,,0,,,,,,"TaskSetManager should have unique name to avoid adding duplicate ones to *Pool* via *SchedulableBuilder*. This problem surfaced with https://issues.apache.org/jira/browse/SPARK-17759 and please find discussion: https://github.com/apache/spark/pull/15326

*Proposal* :
There is 1x1 relationship between Stage Attempt Id and TaskSetManager so taskSet.Id covering both stageId and stageAttemptId looks to be used for TaskSetManager as well. 

*Current TaskSetManager Name* : 
{code:java} var name = ""TaskSet_"" + taskSet.stageId.toString{code}
*Sample*: TaskSet_0

*Proposed TaskSetManager Name* : 
{code:java} var name = ""TaskSet_"" + taskSet.Id (stageId + ""."" + stageAttemptId) {code}
*Sample* : TaskSet_0.0

cc [~kayousterhout] [~markhamstra]",,apachespark,erenavsarogullari,kayousterhout,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17759,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 25 01:41:05 UTC 2016,,,,,,,,,,"0|i34sxz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/16 11:50;apachespark;User 'erenavsarogullari' has created a pull request for this issue:
https://github.com/apache/spark/pull/15463;;;","24/Oct/16 22:39;kayousterhout;Resolved by https://github.com/apache/spark/pull/15463;;;","25/Oct/16 01:41;apachespark;User 'kayousterhout' has created a pull request for this issue:
https://github.com/apache/spark/pull/15617;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Query in CTAS is Optimized Twice (branch-2.0),SPARK-17892,13011765,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,smilegator,yhuai,yhuai,12/Oct/16 19:00,12/Dec/16 17:47,14/Jul/23 06:29,17/Oct/16 07:36,2.0.1,,,,,,,,2.0.2,,,,SQL,,,,,,,,1,,,,,,This tracks the work that fixes the problem shown in  https://issues.apache.org/jira/browse/SPARK-17409 to branch 2.0.,,apachespark,dongjoon,joshrosen,smilegator,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17409,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 12 17:47:11 UTC 2016,,,,,,,,,,"0|i34so7:",9223372036854775807,,,,,,,,,,,,,2.0.2,,,,,,,,,,,"12/Oct/16 19:00;yhuai;cc [~smilegator];;;","12/Oct/16 22:17;smilegator;Will do it! : );;;","15/Oct/16 15:23;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15502;;;","19/Oct/16 03:44;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/15546;;;","19/Oct/16 03:47;dongjoon;Sorry. It's my mistake.;;;","12/Dec/16 17:47;joshrosen;In the future, please re-use the existing JIRA when backporting _OR_ link the JIRA. If you go to SPARK-17409 then it's hard to spot that it's been backported into branch-2.x.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In the cast expression, casting from empty string to interval type throws NullPointerException",SPARK-17884,13011600,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,priyankagargnitk,priyankagargnitk,priyankagargnitk,12/Oct/16 08:36,17/Oct/16 06:36,14/Jul/23 06:29,12/Oct/16 17:15,2.0.1,,,,,,,,1.6.3,2.0.2,2.1.0,,SQL,,,,,,,,0,,,,,,"When the cast expression is applied on empty string """" to cast it to interval type it throws Null pointer exception..

Getting the same exception when I tried reproducing the same through test case
checkEvaluation(Cast(Literal(""""), CalendarIntervalType), null)

Exception i am getting is:
java.lang.NullPointerException was thrown.
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:254)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper$class.checkEvalutionWithUnsafeProjection(ExpressionEvalHelper.scala:181)
	at org.apache.spark.sql.catalyst.expressions.CastSuite.checkEvalutionWithUnsafeProjection(CastSuite.scala:33)
	at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper$class.checkEvaluation(ExpressionEvalHelper.scala:64)
	at org.apache.spark.sql.catalyst.expressions.CastSuite.checkEvaluation(CastSuite.scala:33)
	at org.apache.spark.sql.catalyst.expressions.CastSuite$$anonfun$22.apply$mcV$sp(CastSuite.scala:770)
	at org.apache.spark.sql.catalyst.expressions.CastSuite$$anonfun$22.apply(CastSuite.scala:767)
	at org.apache.spark.sql.catalyst.expressions.CastSuite$$anonfun$22.apply(CastSuite.scala:767)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:57)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:29)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.run(Runner.scala:883)
	at org.scalatest.tools.Runner.run(Runner.scala)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:138)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)

",,apachespark,priyankagargnitk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 17 06:36:46 UTC 2016,,,,,,,,,,"0|i34rnj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/16 08:37;priyankagargnitk;I am working on it.;;;","12/Oct/16 08:55;apachespark;User 'priyankagargnitk' has created a pull request for this issue:
https://github.com/apache/spark/pull/15449;;;","14/Oct/16 05:43;apachespark;User 'priyankagargnitk' has created a pull request for this issue:
https://github.com/apache/spark/pull/15479;;;","17/Oct/16 06:36;priyankagargnitk;Merged into branches 1.6.3, 2.0 and master branches.;;;","17/Oct/16 06:36;priyankagargnitk;Closing this issue... Changes are merged.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RBackendHandler swallowing errors,SPARK-17882,13011564,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jrshust,jrshust,jrshust,12/Oct/16 03:20,13/Oct/16 18:00,14/Jul/23 06:29,13/Oct/16 18:00,2.0.1,,,,,,,,2.0.2,2.1.0,,,SparkR,,,,,,,,0,,,,,,"RBackendHandler is swallowing general exceptions in handleMethodCall which makes it impossible to debug certain issues that happen when doing an invokeJava call.

In my case this was the following error
java.lang.IllegalAccessException: Class org.apache.spark.api.r.RBackendHandler can not access a member of class with modifiers ""public final""

The getCause message that is written back was basically blank.",,apachespark,jrshust,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 13 18:00:23 UTC 2016,,,,,,,,,,"0|i34rfj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/16 03:26;apachespark;User 'jrshust' has created a pull request for this issue:
https://github.com/apache/spark/pull/15446;;;","13/Oct/16 18:00;shivaram;Resolved by https://github.com/apache/spark/pull/15375;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The url linking to `AccumulatorV2` in the document is incorrect.,SPARK-17880,13011534,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,sarutak,sarutak,sarutak,11/Oct/16 23:19,14/Nov/16 14:02,14/Jul/23 06:29,12/Oct/16 05:37,2.0.1,,,,,,,,2.0.2,2.1.0,,,Documentation,,,,,,,,0,,,,,,"In `programming-guide.md`, the url which links to `AccumulatorV2` says `api/scala/index.html#org.apache.spark.AccumulatorV2` but `api/scala/index.html#org.apache.spark.util.AccumulatorV2` is correct.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18435,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 11 23:23:03 UTC 2016,,,,,,,,,,"0|i34r8v:",9223372036854775807,,,,,,,,,,,,,2.0.2,2.1.0,,,,,,,,,,"11/Oct/16 23:23;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/15439;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Write StructuredStreaming WAL to a stream instead of materializing all at once,SPARK-17876,13011455,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,11/Oct/16 18:25,01/Nov/16 22:26,14/Jul/23 06:29,13/Oct/16 04:41,2.0.0,2.0.1,,,,,,,2.0.2,2.1.0,,,Structured Streaming,,,,,,,,0,,,,,,"The CompactibleFileStreamLog materializes the whole metadata log in memory as a String. This can cause issues when there are lots of files that are being committed, especially during a compaction batch. 

You may come across stacktraces that look like:
{code}
java.lang.OutOfMemoryError: Requested array size exceeds VM limit
at java.lang.StringCoding.encode(StringCoding.java:350)
at java.lang.String.getBytes(String.java:941)
at org.apache.spark.sql.execution.streaming.FileStreamSinkLog.serialize(FileStreamSinkLog.scala:127)
at 
{code}

The safer way is to write to an output stream so that we don't have to materialize a huge string.",,apachespark,brkyvz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 11 18:28:05 UTC 2016,,,,,,,,,,"0|i34qrb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/16 18:28;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/15437;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ALTER TABLE ... RENAME TO ... should allow users to specify database in destination table name,SPARK-17873,13011368,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,11/Oct/16 14:33,19/Oct/16 03:24,14/Jul/23 06:29,19/Oct/16 03:24,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 19 03:24:27 UTC 2016,,,,,,,,,,"0|i34q8n:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"11/Oct/16 14:45;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/15434;;;","19/Oct/16 03:24;yhuai;This issue has been resolved by https://github.com/apache/spark/pull/15434. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ML/MLLIB: ChiSquareSelector based on Statistics.chiSqTest(RDD) is wrong ,SPARK-17870,13011295,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,peng.meng@intel.com,peng.meng@intel.com,peng.meng@intel.com,11/Oct/16 08:50,25/Oct/16 19:04,14/Jul/23 06:29,14/Oct/16 11:49,,,,,,,,,2.1.0,,,,ML,MLlib,,,,,,,0,,,,,,"The method to count ChiSqureTestResult in mllib/feature/ChiSqSelector.scala  (line 233) is wrong.

For feature selection method ChiSquareSelector, it is based on the ChiSquareTestResult.statistic (ChiSqure value) to select the features. It select the features with the largest ChiSqure value. But the Degree of Freedom (df) of ChiSqure value is different in Statistics.chiSqTest(RDD), and for different df, you cannot base on ChiSqure value to select features.

Because of the wrong method to count ChiSquare value, the feature selection results are strange.
Take the test suite in ml/feature/ChiSqSelectorSuite.scala as an example:
If use selectKBest to select: the feature 3 will be selected.
If use selectFpr to select: feature 1 and 2 will be selected. 
This is strange. 

I use scikit learn to test the same data with the same parameters. 
When use selectKBest to select: feature 1 will be selected. 
When use selectFpr to select: feature 1 and 2 will be selected. 
This result is make sense. because the df of each feature in scikit learn is the same.

I plan to submit a PR for this problem.
 

 
",,apachespark,avulanov,peng.meng@intel.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17692,,,,,,,,,,,,,,,,SPARK-17017,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 14 11:49:11 UTC 2016,,,,,,,,,,"0|i34psf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/16 09:04;srowen;Oof, I'm pretty certain you're correct. You can rank on the p-value (which is a function of DoF) but not the raw statistic. It's an easy change at least because this is already computed. Can't believe I missed that.;;;","11/Oct/16 09:48;peng.meng@intel.com;hi [~srowen], thanks very much for you quickly reply. 
yes,the p-value is better than raw statistic in this case, because p-value is count  based on DoF and raw statistic.
raw statistic is also popular for feature selection. The SelectKBest and SelectPercentile in scikit learn is based on raw statistic. 
The question here is we should use the same DoF like scikit learn to count ChiSquare value. 
For this JIRA, I propose to change the method to count ChiSquare value like what is done in scikit learn (change Statistics.chiSqTest(RDD)). 

Thanks very much.  ;;;","11/Oct/16 11:13;srowen;I don't think the raw statistic can be directly compared here because the features do not have even nearly the same number of 'buckets', not necessarily. A given test statistic value is ""less remarkable"" when there are more DoF; what's high for a binary-valued feature may not be high at all for one taking on 100 values.

Does scikit really use the statistic? because you're also saying it does something that gives different results from ranking on the statistic.;;;","11/Oct/16 11:44;peng.meng@intel.com;yes, the selectKBest and selectPercentile in scikit learn only use statistic.
Because the method to count ChiSquare value is different, the DoF of all features in scikit learn are the same. so it can do that.

The ChiSquare Value compute process is like this:
 suppose we have data:
X = [ 8 7 0
         0 9 6
         0 9 8
         8 9 5]
y = [0 1 1 2]T, this is the test suite data of ml/feature/ChiSquareSelectorSuite.scala
sci-kit learn to compute chiSquare value is like this:
first:
Y = [1 0 0
        0 1 0
        0  1 0
        0  0 1]
observed = Y'*X=
[8  7    0
 0  18 14
 8   9   5]
expected = 
[4 8.5 4.75
 8 17  9.5
 4  8.5  4.75]
_chisquare(ovserved, expected): to compute all features ChiSquare value, we can see all the DF of each feature is the same.

Bug for spark Statistics.chiSqTest(RDD), is use another method, for each feature, construct a contingency table. So the DF is different for each feature.  

For ""gives different results from ranking on the statistic"", this is because the parameters different.
For previous example, if use SelectKBest(2), the selected feature is the same as SelectFpr(0.2) in scikit learn


         
;;;","11/Oct/16 11:54;srowen;I don't quite understand this example, can you point me to the source? the chi-squared statistic is indeed a function of observed and expected counts, but I'd expect those to be a vector of counts, one for each class. If you're saying that each row contains observed counts for one feature's classes, then yes in this particular construction each of them has the same number of classes (columns). But that isn't generally true; that can't be an assumption scikit makes? I bet I'm missing something.;;;","11/Oct/16 12:02;peng.meng@intel.com;The scikit learn code is here: https://github.com/scikit-learn/scikit-learn/blob/412996f09b6756752dfd3736c306d46fca8f1aa1/sklearn/feature_selection/univariate_selection.py, line 422 for selectKBest, chiSquare compute is also on the same page.

For the last example, each row of X is a sample, it contain three features, totally 4 samples. Y is the label.
Thanks very much.  
;;;","11/Oct/16 12:42;peng.meng@intel.com;https://github.com/apache/spark/pull/1484#issuecomment-51024568
Hi [~mengxr] and [~avulanov] , what do you think about this JIRA. ;;;","11/Oct/16 19:01;srowen;OK I get it, they're doing different things really. The scikit version is computing the statistic for count-valued features vs categorical label, and the Spark version is computing this for categorical features vs categorical labels. Although the number of label classes is constant in both cases, the Spark computation would depend on the number of feature classes too. Yes, it does need to be changed in Spark.;;;","11/Oct/16 20:20;avulanov;[`SelectKBest`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) works with ""a Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) or a single array with scores"". According to what you observe, it uses pvalues for sorting of `chi2` outputs. Indeed, it is the case for all functions that return two arrays: https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/feature_selection/univariate_selection.py#L331. Alternative, one case use raw `chi2` scores for sorting. She need to pass only the first array from `chi2` to `SelectKBest`. As far as I remember, using raw chi2 scores is default in Weka's [ChiSquaredAttributeEval](http://weka.sourceforge.net/doc.stable/weka/attributeSelection/ChiSquaredAttributeEval.html). So, I would not claim that either of approaches is incorrect. According to [Introduction to IR](http://nlp.stanford.edu/IR-book/html/htmledition/assessing-as-a-feature-selection-methodassessing-chi-square-as-a-feature-selection-method-1.html), there might be an issue with computing p-values because then chi2-test is used multiple times. Using plain chi2 values does not involve statistical test, so it might be treated as just some ranking with no statistical implications.;;;","11/Oct/16 20:58;srowen;If the degrees of freedom are the same across the tests, then ranking on p-value or statistic should give the same ranking because the p-value is a monotonically decreasing function of the statistic. That's the case in what the scikit code is effectively doing because there are always (# label classes - 1) degrees of freedom. Really the p-value is the comparable quantity, but there's no point computing it in this case because it's just for ranking.

The Spark code performs a chi-squared test but applies it to answer a different question, where DOF is no longer the same; it's (# label classes - 1) * (# feature classes - 1) in the contingency table here. p-value is no longer always smaller when the statistic is larger. So it's necessary to actually use the p-values for what Spark is doing.;;;","12/Oct/16 01:10;peng.meng@intel.com;hi [~avulanov], the question here is not use raw chi2 scores or pvalues, the question is if use raw chi2 scores, the DoF should be the same.   
""chi2-test is used multiple times"" is another problem.  According to (http://nlp.stanford.edu/IR-book/html/htmledition/assessing-as-a-feature-selection-methodassessing-chi-square-as-a-feature-selection-method-1.html),""whenever a statistical test is used multiple times, then the probability of getting at least one error increases."", this problem is partially solved by Select the p-values corresponding to Family-wise error rate (SelectFwe, SPARK-17645). Thanks very much.

Hi [~srowen], I totally agree with your comments. Based on the DoF is different in Spark ChiSquare value, we can use the p-values for Spark SelectKBest, and SelectPercentile. Thanks very much.

I will submit a pr for this.;;;","12/Oct/16 01:48;apachespark;User 'mpjlu' has created a pull request for this issue:
https://github.com/apache/spark/pull/15444;;;","14/Oct/16 11:49;srowen;Issue resolved by pull request 15444
[https://github.com/apache/spark/pull/15444];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dataset.dropDuplicates (i.e. distinct) should consider the columns with same column name,SPARK-17867,13011257,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,11/Oct/16 05:36,18/May/17 16:20,14/Jul/23 06:29,13/Oct/16 05:29,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"We find and get the first resolved attribute from output with the given column name in Dataset.dropDuplicates. When we have the more than one columns with the same name. Other columns are put into aggregation columns, instead of grouping columns. We should fix this.",,apachespark,cloud_fan,MasterDDT,nitay,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 18 16:20:28 UTC 2017,,,,,,,,,,"0|i34pjz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/16 06:13;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/15427;;;","13/Oct/16 05:29;cloud_fan;Issue resolved by pull request 15427
[https://github.com/apache/spark/pull/15427];;;","18/May/17 13:47;MasterDDT;I'm seeing a regression from this change, the last ""del <> 'hi'"" filter gets pushed down past the dropDuplicates aggregation. cc [~cloud_fan]
 
{code:none}
    val df = Seq((1,2,3,""hi""), (1,2,4,""hi""))
      .toDF(""userid"", ""eventid"", ""vk"", ""del"")
      .filter(""userid is not null and eventid is not null and vk is not null"")
      .repartitionByColumns(Seq(""userid""))
      .sortWithinPartitions(asc(""userid""), asc(""eventid""), desc(""vk""))
      .dropDuplicates(""eventid"")
      .filter(""userid is not null"")
      .repartitionByColumns(Seq(""userid""))
      .sortWithinPartitions(asc(""userid""))
      .filter(""del <> 'hi'"")

    // filter should not be pushed down to the local table scan
    df.queryExecution.sparkPlan.collect {
      case f @ FilterExec(_, t @ LocalTableScanExec(_, _)) =>
        assert(false, s""$f was pushed down to $t"")
{code};;;","18/May/17 14:40;viirya;The above example code can't compile with current codebase. There is no repartitionByColumns but only repartition.

{code}
    val df = Seq((1, 2, 3, ""hi""), (1, 2, 4, ""hi""))
      .toDF(""userid"", ""eventid"", ""vk"", ""del"")
      .filter(""userid is not null and eventid is not null and vk is not null"")
      .repartition($""userid"")
      .sortWithinPartitions(asc(""userid""), asc(""eventid""), desc(""vk""))
      .dropDuplicates(""eventid"")
      .filter(""userid is not null"")
      .repartition($""userid"")
      .sortWithinPartitions(asc(""userid""))
      .filter(""del <> 'hi'"")
{code}

The optimized plan looks like:

{code}
Sort [userid#9 ASC NULLS FIRST], false
+- RepartitionByExpression [userid#9], 5
   +- Filter (isnotnull(del#12) && NOT (del#12 = hi))
      +- Aggregate [eventid#10], [first(userid#9, false) AS userid#9, eventid#10, first(vk#11, false) AS vk#11, first(del#12, false) AS del#12]
         +- Sort [userid#9 ASC NULLS FIRST, eventid#10 ASC NULLS FIRST, vk#11 DESC NULLS LAST], false
            +- RepartitionByExpression [userid#9], 5
               +- LocalRelation [userid#9, eventid#10, vk#11, del#12]
{code}

The spark plan looks like:

{code}
Sort [userid#9 ASC NULLS FIRST], false, 0
+- Exchange hashpartitioning(userid#9, 5)
   +- Filter (isnotnull(del#12) && NOT (del#12 = hi))
      +- SortAggregate(key=[eventid#10], functions=[first(userid#9, false), first(vk#11, false), first(del#12, false)], output=[userid#9, eventid#10, vk#11, del#12])
         +- SortAggregate(key=[eventid#10], functions=[partial_first(userid#9, false), partial_first(vk#11, false), partial_first(del#12, false)], output=[eventid#10, first#35, valueSet#36, first#37, valueSet#38, first#39, valueSet#40])
            +- Sort [userid#9 ASC NULLS FIRST, eventid#10 ASC NULLS FIRST, vk#11 DESC NULLS LAST], false, 0
               +- Exchange hashpartitioning(userid#9, 5)
                  +- LocalTableScan [userid#9, eventid#10, vk#11, del#12]
{code}

Looks like the ""del <> 'hi'"" filter doesn't be pushed down?;;;","18/May/17 16:20;MasterDDT;Ah I see, thanks [~viirya]. The repartitionByColumns is just a short-cut method I created. But I do have some aliasing code changes compared to 2.1, I will try to remove those and see if that is whats breaking it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SELECT distinct does not work if there is a order by clause,SPARK-17863,13011219,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,yhuai,yhuai,11/Oct/16 01:59,14/Oct/16 21:46,14/Jul/23 06:29,14/Oct/16 21:46,,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,correctness,,,,,"{code}
select distinct struct.a, struct.b
from (
  select named_struct('a', 1, 'b', 2, 'c', 3) as struct
  union all
  select named_struct('a', 1, 'b', 2, 'c', 4) as struct) tmp
order by struct.a, struct.b
{code}
This query generates
{code}
+---+---+
|  a|  b|
+---+---+
|  1|  2|
|  1|  2|
+---+---+
{code}
The plan is wrong because the analyze somehow added {{struct#21805}} to the project list, which changes the semantic of the distinct (basically, the query is changed to {{select distinct struct.a, struct.b, struct}} from {{select distinct struct.a, struct.b}}).
{code}
== Parsed Logical Plan ==
'Sort ['struct.a ASC, 'struct.b ASC], true
+- 'Distinct
   +- 'Project ['struct.a, 'struct.b]
      +- 'SubqueryAlias tmp
         +- 'Union
            :- 'Project ['named_struct(a, 1, b, 2, c, 3) AS struct#21805]
            :  +- OneRowRelation$
            +- 'Project ['named_struct(a, 1, b, 2, c, 4) AS struct#21806]
               +- OneRowRelation$

== Analyzed Logical Plan ==
a: int, b: int
Project [a#21819, b#21820]
+- Sort [struct#21805.a ASC, struct#21805.b ASC], true
   +- Distinct
      +- Project [struct#21805.a AS a#21819, struct#21805.b AS b#21820, struct#21805]
         +- SubqueryAlias tmp
            +- Union
               :- Project [named_struct(a, 1, b, 2, c, 3) AS struct#21805]
               :  +- OneRowRelation$
               +- Project [named_struct(a, 1, b, 2, c, 4) AS struct#21806]
                  +- OneRowRelation$

== Optimized Logical Plan ==
Project [a#21819, b#21820]
+- Sort [struct#21805.a ASC, struct#21805.b ASC], true
   +- Aggregate [a#21819, b#21820, struct#21805], [a#21819, b#21820, struct#21805]
      +- Union
         :- Project [1 AS a#21819, 2 AS b#21820, [1,2,3] AS struct#21805]
         :  +- OneRowRelation$
         +- Project [1 AS a#21819, 2 AS b#21820, [1,2,4] AS struct#21806]
            +- OneRowRelation$

== Physical Plan ==
*Project [a#21819, b#21820]
+- *Sort [struct#21805.a ASC, struct#21805.b ASC], true, 0
   +- Exchange rangepartitioning(struct#21805.a ASC, struct#21805.b ASC, 200)
      +- *HashAggregate(keys=[a#21819, b#21820, struct#21805], functions=[], output=[a#21819, b#21820, struct#21805])
         +- Exchange hashpartitioning(a#21819, b#21820, struct#21805, 200)
            +- *HashAggregate(keys=[a#21819, b#21820, struct#21805], functions=[], output=[a#21819, b#21820, struct#21805])
               +- Union
                  :- *Project [1 AS a#21819, 2 AS b#21820, [1,2,3] AS struct#21805]
                  :  +- Scan OneRowRelation[]
                  +- *Project [1 AS a#21819, 2 AS b#21820, [1,2,4] AS struct#21806]
                     +- Scan OneRowRelation[]
{code}

If you use the following query, you will get the correct result
{code}
select distinct struct.a, struct.b
from (
  select named_struct('a', 1, 'b', 2, 'c', 3) as struct
  union all
  select named_struct('a', 1, 'b', 2, 'c', 4) as struct) tmp
order by a, b
{code}",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 14 21:46:01 UTC 2016,,,,,,,,,,"0|i34pbr:",9223372036854775807,,,,,,,,,,,,,2.0.2,2.1.0,,,,,,,,,,"14/Oct/16 18:34;yhuai;Seems it is introduced by https://github.com/apache/spark/pull/11153/files. Let's see if we can actually fix it. Another option is to make it throw an exception and the error message provides the instruction on how to rewrite the query.;;;","14/Oct/16 18:35;yhuai;cc [~davies];;;","14/Oct/16 19:19;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/15489;;;","14/Oct/16 21:46;yhuai;Issue resolved by pull request 15489
[https://github.com/apache/spark/pull/15489];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SHOW COLUMN's database conflict check should respect case sensitivity setting,SPARK-17860,13011200,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dkbiswal,dkbiswal,dkbiswal,10/Oct/16 23:25,20/Oct/16 11:41,14/Jul/23 06:29,20/Oct/16 11:41,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"SHOW COLUMNS command validates the user supplied database 
name with database name from qualified table name name to make
sure both of them are consistent. This comparison should respect
case sensitivity.",,apachespark,cloud_fan,dkbiswal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 20 11:41:08 UTC 2016,,,,,,,,,,"0|i34p7j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/16 23:43;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/15423;;;","20/Oct/16 11:41;cloud_fan;Issue resolved by pull request 15423
[https://github.com/apache/spark/pull/15423];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark worker throw Exception when uber jar's http url contains query string,SPARK-17855,13011023,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,invkrh,invkrh,invkrh,10/Oct/16 13:03,14/Oct/16 11:53,14/Jul/23 06:29,14/Oct/16 11:52,1.6.2,2.0.1,,,,,,,2.1.0,,,,Spark Core,,,,,,,,0,,,,,,"spark-submit support jar url with http protocol

If the url contains any query strings, *worker.DriverRunner.downloadUserJar * method will throw ""Did not see expected jar"" exception. This is because this method checks the existance of a downloaded jar whose name contains query strings.

This is a problem when your jar is located on some web service which requires some additional information to retrieve the file. For example, to download a jar from s3 bucket via http, the url contains signature, datetime, etc as query string.

{code}
https://s3.amazonaws.com/deploy/spark-job.jar
?X-Amz-Algorithm=AWS4-HMAC-SHA256
&X-Amz-Credential=<your-access-key-id>/20130721/us-east-1/s3/aws4_request
&X-Amz-Date=20130721T201207Z
&X-Amz-Expires=86400
&X-Amz-SignedHeaders=host
&X-Amz-Signature=<signature-value>  
{code}

Worker will look for a jar named

""spark-job.jar?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=<your-access-key-id>/20130721/us-east-1/s3/aws4_request&X-Amz-Date=20130721T201207Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=<signature-value>""

instead of

""spark-job.jar""

Hence, all the query string should be removed before checking jar existance.

I created a pr to fix this, if anyone can review it.",,apachespark,gpcuster,invkrh,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 14 11:52:24 UTC 2016,,,,,,,,,,"0|i34o4f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/16 13:11;srowen;Yea, only the URI path should be retained. I'm not sure exactly where the fix should be, but that sounds like about all there is to it.;;;","10/Oct/16 13:45;apachespark;User 'invkrh' has created a pull request for this issue:
https://github.com/apache/spark/pull/15420;;;","14/Oct/16 11:52;srowen;Issue resolved by pull request 15420
[https://github.com/apache/spark/pull/15420];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka OffsetOutOfRangeException on DStreams union from separate Kafka clusters with identical topic names.,SPARK-17853,13011015,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,koeninger,marcin.kuthan,marcin.kuthan,10/Oct/16 12:30,12/Oct/16 07:39,14/Jul/23 06:29,12/Oct/16 07:39,2.0.0,,,,,,,,2.0.2,2.1.0,,,DStreams,,,,,,,,1,,,,,,"During migration from Spark 1.6 to 2.0 I observed OffsetOutOfRangeException  reported by Kafka client. In our scenario we create single DStream as a union of multiple DStreams. One DStream for one Kafka cluster (multi dc solution). Both Kafka clusters have the same topics and number of partitions.

After quick investigation, I found that class DirectKafkaInputDStream keeps offset state for topic and partitions, but it is not aware of different Kafka clusters. 

For every topic, single DStream is created as a union from all configured Kafka clusters.

{code}
class KafkaDStreamSource(configs: Iterable[Map[String, String]]) {
def createSource(ssc: StreamingContext, topic: String): DStream[(String, Array[Byte])] = {
    val streams = configs.map { config =>
      val kafkaParams = config
      val kafkaTopics = Set(topic)

      KafkaUtils.
          createDirectStream[String, Array[Byte]](
        ssc,
        LocationStrategies.PreferConsistent,
        ConsumerStrategies.Subscribe[String, Array[Byte]](kafkaTopics, kafkaParams)
      ).map { record =>
        (record.key, record.value)
      }
    }

    ssc.union(streams.toSeq)
  }
}
{code}

At the end, offsets from one Kafka cluster overwrite offsets from second one. Fortunately OffsetOutOfRangeException was thrown because offsets in both Kafka clusters are significantly different.",,apachespark,koeninger,marcin.kuthan,oleki,pguzik,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 12 01:23:04 UTC 2016,,,,,,,,,,"0|i34o2n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/16 12:22;koeninger;Which version of DStream are you using, 0-10 or 0-8?
Are you using the same group id for both streams?;;;","11/Oct/16 12:37;pguzik;Hi. We are using version 0-10. We are also using the same group id for both streams.;;;","11/Oct/16 13:24;koeninger;Use a different group id.

Let me know if that addresses the issue.


;;;","11/Oct/16 13:47;oleki;Setting different group ids solved the issue.;;;","11/Oct/16 14:23;koeninger;Good, will keep this ticket open at least until documentation is made
clearer.

On Oct 11, 2016 8:48 AM, ""Aleksander Ihnatowicz (JIRA)"" <jira@apache.org>

;;;","12/Oct/16 01:23;apachespark;User 'koeninger' has created a pull request for this issue:
https://github.com/apache/spark/pull/15442;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make sure all test sqls in catalyst pass checkAnalysis,SPARK-17851,13010954,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jiangxb1987,jiangxb1987,jiangxb1987,10/Oct/16 08:56,21/Jun/17 16:41,14/Jul/23 06:29,21/Jun/17 16:41,,,,,,,,,2.3.0,,,,SQL,,,,,,,,0,,,,,,"Currently we have several tens of test sqls in catalyst will fail at `SimpleAnalyzer.checkAnalysis`, we should make sure they are valid.",,apachespark,jiangxb1987,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 10 09:05:05 UTC 2016,,,,,,,,,,"0|i34np3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/16 09:05;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/15417;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HadoopRDD should not swallow EOFException,SPARK-17850,13010917,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,10/Oct/16 07:24,16/Apr/17 08:15,14/Jul/23 06:29,21/Oct/16 21:54,1.4.1,1.5.2,1.6.2,2.0.1,,,,,2.1.0,,,,Spark Core,,,,,,,,0,correctness,,,,,"The code in https://github.com/apache/spark/blob/2bcd5d5ce3eaf0eb1600a12a2b55ddb40927533b/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala#L256 catches EOFException and mark RecordReader finished. However, in some cases, RecordReader will throw EOFException to indicate the stream is corrupted. See the following stack trace as an example:

{code}
Caused by: java.io.EOFException: Unexpected end of input stream
  at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:145)
  at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)
  at java.io.InputStream.read(InputStream.java:101)
  at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211)
  at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
  at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:164)
  at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
  at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:50)
  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:97)
  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:134)
  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:97)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:372)
  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
  at org.apache.spark.scheduler.Task.run(Task.scala:99)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
{code}

Then HadoopRDD doesn't fail the job when files are corrupted (e.g., corrupted gzip files).

Note: NewHadoopRDD doesn't have this issue.

This is reported by Bilal Aslam.",,apachespark,mgrover,tgraves,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18774,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 21 20:55:39 UTC 2016,,,,,,,,,,"0|i34ngv:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"10/Oct/16 07:55;srowen;Yeah, I don't see why it's valid to catch EOFException. It already returns false to signal EOF. An exception would mean it internally didn't expect EOF but read to the end.;;;","10/Oct/16 22:42;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/15422;;;","12/Oct/16 23:05;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/15454;;;","21/Nov/16 20:49;mgrover;Hi [~zsxwing] and [~srowen], the JIRA fix version seems to suggest that it's in both Spark 2.0 branch as well as Spark 2.1 branch, but I don't see it in branch-2.0. Should the fix version be 2.1.0 only?;;;","21/Nov/16 20:55;zsxwing;[~mgrover] you're right. This is only in 2.1. Removed 2.0.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
grouping set throws NPE,SPARK-17849,13010910,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Yang Wang,Yang Wang,Yang Wang,10/Oct/16 06:04,05/Nov/16 13:34,14/Jul/23 06:29,05/Nov/16 13:34,2.0.0,2.0.1,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,,,,,,"spark sql grouping sets throws NullPointerException.
This problem can be recreated using the following lines of code:
case class point(a:String, b:String, c:String)
val data = Seq(
  point(""1"",""2"",""3""),
  point(""4"",""5"",""6""),
  point(""7"",""8"",""9"")
)
sc.parallelize(data).toDF().registerTempTable(""table"")
spark.sql(""select a, b, count(c) from table group by a, b GROUPING SETS (()) "").show()",,apachespark,Yang Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 10 07:33:05 UTC 2016,,,,,,,,,,"0|i34nfb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/16 07:33;apachespark;User 'yangw1234' has created a pull request for this issue:
https://github.com/apache/spark/pull/15416;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka 0.10 commitQueue needs to be drained,SPARK-17841,13010864,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,koeninger,koeninger,koeninger,09/Oct/16 17:51,18/Oct/16 21:01,14/Jul/23 06:29,18/Oct/16 21:01,,,,,,,,,2.0.2,2.1.0,,,,,,,,,,,0,,,,,,"Current implementation is just iterating, not polling and removing.",,apachespark,koeninger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 09 17:58:06 UTC 2016,,,,,,,,,,"0|i34n53:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/16 17:58;apachespark;User 'koeninger' has created a pull request for this issue:
https://github.com/apache/spark/pull/15407;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableIdentifier.quotedString creates un-parseable names when name contains a backtick,SPARK-17832,13010596,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jiangxb1987,hvanhovell,hvanhovell,07/Oct/16 20:45,10/Oct/16 04:54,14/Jul/23 06:29,10/Oct/16 04:54,,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,,,,,,"The {{quotedString}} method in {{TableIdentifier}} and {{FunctionIdentifier}} produce an illegal (un-parseable) name when the name contains a backtick. For example:
{noformat}
import org.apache.spark.sql.catalyst.parser.CatalystSqlParser._
import org.apache.spark.sql.catalyst.TableIdentifier
import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute
val complexName = TableIdentifier(""`weird`table`name"", Some(""`d`b`1""))
parseTableIdentifier(complexName.unquotedString) // Does not work
parseTableIdentifier(complexName.quotedString) // Does not work
UnresolvedAttribute.parseAttributeName(complexName.unquotedString) // Does not work
UnresolvedAttribute.parseAttributeName(complexName.quotedString) // Does not work
{noformat}

The {{quotedString}} method should escape backticks properly.",,apachespark,hvanhovell,ksunitha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 08 11:02:04 UTC 2016,,,,,,,,,,"0|i34lhj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/16 11:02;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/15403;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JVMObjectTracker.objMap may leak JVM objects,SPARK-17822,13010355,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,yhuai,yhuai,07/Oct/16 03:52,23/Feb/17 19:36,14/Jul/23 06:29,09/Dec/16 15:52,,,,,,,,,2.0.3,2.1.0,,,SparkR,,,,,,,,0,,,,,,"JVMObjectTracker.objMap is used to track JVM objects for SparkR. However, we observed that JVM objects that are not used anymore are still trapped in this map, which prevents those object get GCed. 

Seems it makes sense to use weak reference (like persistentRdds in SparkContext). ",,apachespark,felixcheung,josephkb,kiszk,mengxr,rxin,shivaram,techaddict,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17823,,,,,,,,,,,SPARK-17823,,,,,,,,,,,,,,"02/Nov/16 15:28;yhuai;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12836592/screenshot-1.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 09 15:52:17 UTC 2016,,,,,,,,,,"0|i34jzz:",9223372036854775807,,,,,,,,,,,,,2.0.3,2.1.0,,,,,,,,,,"11/Oct/16 13:13;apachespark;User 'techaddict' has created a pull request for this issue:
https://github.com/apache/spark/pull/15433;;;","01/Nov/16 21:10;rxin;cc [~felixcheung], [~shivaram] can one of the R guys take this? It seems like pretty severe.
;;;","02/Nov/16 05:52;felixcheung;I don't have a good handle on what actually is the problem. [~yhuai] could you give us some pointers?
;;;","02/Nov/16 15:32;yhuai;Basically, the problem that I have observed is a long running Spark driver runs out of memory because JVMObjectTracker.objMap prevents objects that are not used anymore from getting GCed. I am attaching a screenshot which shows the objects inside the map.;;;","03/Nov/16 05:10;felixcheung;I see. Is it possible that the R object is alive? Does running gc in R help?
https://stat.ethz.ch/R-manual/R-devel/library/base/html/gc.html

It would be great if there is a way you could share what the R code looks like.

;;;","07/Nov/16 17:58;shivaram;Yeah just to provide some more context, every time an object is added to the JVMObjectTracker we correspondingly make a note of it on the R side (in a map called .validJobjs[1]). 
This map and correspondingly the JVM side objects should get cleared up when the object gets GC'd R[2]. 

I think there are two possibilities here - one is the R side actually is holding on to these references and not clearing them. The other is that the GC messages from R to JVM is somehow not working as expected.

[~yhuai] If we give you some debug scripts can you run them on the R side before the crash ?

[1] https://github.com/apache/spark/blob/daa975f4bfa4f904697bf3365a4be9987032e490/R/pkg/R/jobj.R#L43
[2] https://github.com/apache/spark/blob/daa975f4bfa4f904697bf3365a4be9987032e490/R/pkg/R/jobj.R#L90;;;","02/Dec/16 01:02;josephkb;I've been able to observe something like this bug by creating a DataFrame in SparkR and calling sql queries on it repeatedly.  Java objects from these duplicate queries start to collect in JVMObjectTracker.  But those Java objects do get GCed periodically.  And calling gc() in R completely cleans them up.

The periodic GC I saw only occurred when I ran R commands, so perhaps it is not triggered as frequently as we’d like.  I'm not that familiar with SparkR internals, but is there a good way to make this happen?;;;","02/Dec/16 01:24;josephkb;Since 2.1 is underway and this is not a regression, I'll shift the target.;;;","02/Dec/16 22:13;mengxr;I will take a look.;;;","03/Dec/16 08:39;felixcheung;From what [~josephkb] observed and described, I suspect this is a case of small pointers in R holding larger memory/classes in JVM.

If the memory footprint of the pointer in R is very small, chances are even after thousands of iterations the memory consumption in R is still not high enough to trigger a GC to reclaim. If we have a repro, calling gc() or gcinfo(TRUE) should tell us about memory consumption as it grows.

I'm not sure about the previous attempt to mitigate this with WeakReference though - since we don't know which of the R object is still being referenced, once we remove the JVM object, and the R pointer could become a dangling pointer.

And perhaps then this could be helped by increasing the aggressiveness of R GC:
https://stat.ethz.ch/R-manual/R-devel/library/base/html/Memory.htm
http://adv-r.had.co.nz/memory.html#gc
;;;","05/Dec/16 17:59;mengxr;The issue comes with multiple RBackend connections. It is feasible to create multiple RBackend sessions. But they share the same `JVMObjectTracker`. It cannot tell which JVM object is from which RBackend. If an RBackend died without proper cleaning, we got a memory leak.

I will send a PR to make JVMObjectTracker a member variable of RBackend. There should be more TODOs to allow concurrent RBackend sessions. But this would help solve the most critical issue.;;;","05/Dec/16 20:50;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/16154;;;","09/Dec/16 15:52;mengxr;Issue resolved by pull request 16154
[https://github.com/apache/spark/pull/16154];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Specified database in JDBC URL is ignored when connecting to thriftserver,SPARK-17819,13010338,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,tnemet,tnemet,07/Oct/16 01:22,26/Dec/16 20:31,14/Jul/23 06:29,17/Oct/16 03:16,1.6.2,2.0.0,2.0.1,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,,,,,,"Filing this based on a email thread with Reynold Xin. From the [docs|http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-thrift-jdbcodbc-server], the JDBC connection URL to the thriftserver looks like:

{code}
beeline> !connect jdbc:hive2://<host>:<port>/<database>?hive.server2.transport.mode=http;hive.server2.thrift.http.path=<http_endpoint>
{code}

However, anything specified in <database> results in being put in default schema. I'm running these with -e commands, but the shell shows the same behavior.

In 2.0.1, I created a table foo in schema spark_jira:

{code}
[558|18:01:20] ~/Documents/spark/spark$ bin/beeline -u jdbc:hive2://localhost:10006/spark_jira -n hive -e ""show tables""
Connecting to jdbc:hive2://localhost:10006/spark_jira
16/10/06 18:01:28 INFO jdbc.Utils: Supplied authorities: localhost:10006
16/10/06 18:01:28 INFO jdbc.Utils: Resolved authority: localhost:10006
16/10/06 18:01:28 INFO jdbc.HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://localhost:10006/spark_jira
Connected to: Spark SQL (version 2.0.1)
Driver: Hive JDBC (version 1.2.1.spark2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
+------------+--------------+--+
| tableName  | isTemporary  |
+------------+--------------+--+
+------------+--------------+--+
No rows selected (0.558 seconds)
Beeline version 1.2.1.spark2 by Apache Hive
Closing: 0: jdbc:hive2://localhost:10006/spark_jira
[559|18:01:30] ~/Documents/spark/spark$ bin/beeline -u jdbc:hive2://localhost:10006/spark_jira -n hive -e ""show tables in spark_jira""
Connecting to jdbc:hive2://localhost:10006/spark_jira
16/10/06 18:01:34 INFO jdbc.Utils: Supplied authorities: localhost:10006
16/10/06 18:01:34 INFO jdbc.Utils: Resolved authority: localhost:10006
16/10/06 18:01:34 INFO jdbc.HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://localhost:10006/spark_jira
Connected to: Spark SQL (version 2.0.1)
Driver: Hive JDBC (version 1.2.1.spark2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
+------------+--------------+--+
| tableName  | isTemporary  |
+------------+--------------+--+
| foo        | false        |
+------------+--------------+--+
1 row selected (0.664 seconds)
Beeline version 1.2.1.spark2 by Apache Hive
Closing: 0: jdbc:hive2://localhost:10006/spark_jira
{code}

I also see this in Spark 1.6.2:
{code}
[555|18:13:32] ~/Documents/spark/spark16$ bin/beeline -u jdbc:hive2://localhost:10005/spark_jira -n hive -e ""show tables""
Connecting to jdbc:hive2://localhost:10005/spark_jira
16/10/06 18:13:37 INFO jdbc.Utils: Supplied authorities: localhost:10005
16/10/06 18:13:37 INFO jdbc.Utils: Resolved authority: localhost:10005
16/10/06 18:13:37 INFO jdbc.HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://localhost:10005/spark_jira
Connected to: Spark SQL (version 1.6.2)
Driver: Hive JDBC (version 1.2.1.spark2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
+--------------+--------------+--+
|  tableName   | isTemporary  |
+--------------+--------------+--+
| all_types    | false        |
| order_items  | false        |
| orders       | false        |
| users        | false        |
+--------------+--------------+--+
4 rows selected (0.653 seconds)
Beeline version 1.2.1.spark2 by Apache Hive
Closing: 0: jdbc:hive2://localhost:10005/spark_jira
[556|18:13:39] ~/Documents/spark/spark16$ bin/beeline -u jdbc:hive2://localhost:10005/spark_jira -n hive -e ""show tables in spark_jira""
Connecting to jdbc:hive2://localhost:10005/spark_jira
16/10/06 18:13:45 INFO jdbc.Utils: Supplied authorities: localhost:10005
16/10/06 18:13:45 INFO jdbc.Utils: Resolved authority: localhost:10005
16/10/06 18:13:45 INFO jdbc.HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://localhost:10005/spark_jira
Connected to: Spark SQL (version 1.6.2)
Driver: Hive JDBC (version 1.2.1.spark2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
+------------+--------------+--+
| tableName  | isTemporary  |
+------------+--------------+--+
| foo        | false        |
+------------+--------------+--+
1 row selected (0.633 seconds)
Beeline version 1.2.1.spark2 by Apache Hive
Closing: 0: jdbc:hive2://localhost:10005/spark_jira
{code}

This appeared to work back in Spark 1.4.1:
{code}
[560|18:17:19] ~/Documents/spark/spark14$ bin/beeline -u jdbc:hive2://localhost:11001/imdb -n hive -e ""show tables""
scan complete in 2ms
Connecting to jdbc:hive2://localhost:11001/imdb
Connected to: Spark SQL (version 1.4.1)
Driver: Spark Project Core (version 1.4.1)
Transaction isolation: TRANSACTION_REPEATABLE_READ
+------------------+--------------+
|    tableName     | isTemporary  |
+------------------+--------------+
| aka_name         | false        |
| aka_title        | false        |
| cast_info        | false        |
| char_name        | false        |
| company_name     | false        |
| complete_cast    | false        |
| foo              | false        |
| keyword          | false        |
| movie_companies  | false        |
| movie_info       | false        |
| movie_keyword    | false        |
| movie_link       | false        |
| name             | false        |
| person_info      | false        |
| title            | false        |
+------------------+--------------+
15 rows selected (0.519 seconds)
Beeline version 1.4.1 by Apache Hive
Closing: 0: jdbc:hive2://localhost:11001/imdb
[561|18:17:36] ~/Documents/spark/spark14$ bin/beeline -u jdbc:hive2://localhost:11001/imdb -n hive -e ""show tables in imdb""
scan complete in 1ms
Connecting to jdbc:hive2://localhost:11001/imdb
Connected to: Spark SQL (version 1.4.1)
Driver: Spark Project Core (version 1.4.1)
Transaction isolation: TRANSACTION_REPEATABLE_READ
+------------------+--------------+
|    tableName     | isTemporary  |
+------------------+--------------+
| aka_name         | false        |
| aka_title        | false        |
| cast_info        | false        |
| char_name        | false        |
| company_name     | false        |
| complete_cast    | false        |
| foo              | false        |
| keyword          | false        |
| movie_companies  | false        |
| movie_info       | false        |
| movie_keyword    | false        |
| movie_link       | false        |
| name             | false        |
| person_info      | false        |
| title            | false        |
+------------------+--------------+
15 rows selected (0.509 seconds)
Beeline version 1.4.1 by Apache Hive
Closing: 0: jdbc:hive2://localhost:11001/imdb
[562|18:17:43] ~/Documents/spark/spark14$ bin/beeline -u jdbc:hive2://localhost:11001/imdb -n hive -e ""show tables in default""
scan complete in 2ms
Connecting to jdbc:hive2://localhost:11001/imdb
Connected to: Spark SQL (version 1.4.1)
Driver: Spark Project Core (version 1.4.1)
Transaction isolation: TRANSACTION_REPEATABLE_READ
+------------+--------------+
| tableName  | isTemporary  |
+------------+--------------+
| foo        | false        |
+------------+--------------+
1 row selected (0.511 seconds)
Beeline version 1.4.1 by Apache Hive
Closing: 0: jdbc:hive2://localhost:11001/imdb
{code}
",,apachespark,dongjoon,tnemet,tsuresh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 17 03:59:05 UTC 2016,,,,,,,,,,"0|i34jw7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/16 22:31;dongjoon;Hi, [~tnemet].
I'm working on this and make a PR for this today.;;;","08/Oct/16 01:15;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/15399;;;","17/Oct/16 03:11;dongjoon;Hi, [~rxin].
Could you review the PR? In fact, it was an addition of 3 lines.;;;","17/Oct/16 03:59;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/15507;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark RDD Repartitioning Results in Highly Skewed Partition Sizes,SPARK-17817,13010320,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,dusenberrymw,dusenberrymw,07/Oct/16 00:03,17/Oct/16 07:38,14/Jul/23 06:29,11/Oct/16 18:44,1.6.1,1.6.2,2.0.0,2.0.1,,,,,2.1.0,,,,PySpark,,,,,,,,0,,,,,,"Calling {{repartition}} on a PySpark RDD to increase the number of partitions results in highly skewed partition sizes, with most having 0 rows.  The {{repartition}} method should evenly spread out the rows across the partitions, and this behavior is correctly seen on the Scala side.

Please reference the following code for a reproducible example of this issue:

{code}
# Python
num_partitions = 20000
a = sc.parallelize(range(int(1e6)), 2)  # start with 2 even partitions
l = a.repartition(num_partitions).glom().map(len).collect()  # get length of each partition
min(l), max(l), sum(l)/len(l), len(l)  # skewed!

# Scala
val numPartitions = 20000
val a = sc.parallelize(0 until 1e6.toInt, 2)  # start with 2 even partitions
val l = a.repartition(numPartitions).glom().map(_.length).collect()  # get length of each partition
print(l.min, l.max, l.sum/l.length, l.length)  # even!
{code}

The issue here is that highly skewed partitions can result in severe memory pressure in subsequent steps of a processing pipeline, resulting in OOM errors.",,apachespark,dusenberrymw,jodersky,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 12 03:23:06 UTC 2016,,,,,,,,,,"0|i34js7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/16 05:11;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/15389;;;","12/Oct/16 03:23;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/15445;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Json serialzation of accumulators are failing with ConcurrentModificationException,SPARK-17816,13010302,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eseyfe,eseyfe,eseyfe,06/Oct/16 22:45,02/Nov/16 04:11,14/Jul/23 06:29,11/Oct/16 03:43,2.0.1,2.1.0,,,,,,,2.0.2,2.1.0,,,Spark Core,,,,,,,,0,,,,,,"This is the stack trace: See  {{ConcurrentModificationException}}:

{code}

java.util.ConcurrentModificationException
at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901)
at java.util.ArrayList$Itr.next(ArrayList.java:851)
at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:43)
at scala.collection.Iterator$class.foreach(Iterator.scala:893)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:183)
at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:45)
at scala.collection.TraversableLike$class.to(TraversableLike.scala:590)
at scala.collection.AbstractTraversable.to(Traversable.scala:104)
at scala.collection.TraversableOnce$class.toList(TraversableOnce.scala:294)
at scala.collection.AbstractTraversable.toList(Traversable.scala:104)
at org.apache.spark.util.JsonProtocol$.accumValueToJson(JsonProtocol.scala:314)
at org.apache.spark.util.JsonProtocol$$anonfun$accumulableInfoToJson$5.apply(JsonProtocol.scala:291)
at org.apache.spark.util.JsonProtocol$$anonfun$accumulableInfoToJson$5.apply(JsonProtocol.scala:291)
at scala.Option.map(Option.scala:146)
at org.apache.spark.util.JsonProtocol$.accumulableInfoToJson(JsonProtocol.scala:291)
at org.apache.spark.util.JsonProtocol$$anonfun$taskInfoToJson$12.apply(JsonProtocol.scala:283)
at org.apache.spark.util.JsonProtocol$$anonfun$taskInfoToJson$12.apply(JsonProtocol.scala:283)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.immutable.List.foreach(List.scala:381)
at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)
at scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:45)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
at scala.collection.AbstractTraversable.map(Traversable.scala:104)
at org.apache.spark.util.JsonProtocol$.taskInfoToJson(JsonProtocol.scala:283)
at org.apache.spark.util.JsonProtocol$.taskEndToJson(JsonProtocol.scala:145)
at org.apache.spark.util.JsonProtocol$.sparkEventToJson(JsonProtocol.scala:76)
at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:137)
at org.apache.spark.scheduler.EventLoggingListener.onTaskEnd(EventLoggingListener.scala:157)
at org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent(SparkListenerBus.scala:45)
at org.apache.spark.scheduler.LiveListenerBus.doPostEvent(LiveListenerBus.scala:35)
at org.apache.spark.scheduler.LiveListenerBus.doPostEvent(LiveListenerBus.scala:35)
at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:63)
at org.apache.spark.scheduler.LiveListenerBus.postToAll(LiveListenerBus.scala:35)
at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(LiveListenerBus.scala:81)
at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:66)
at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:66)
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:65)
at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1244)
at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:64)

{code}
",,apachespark,easel,ekeyser,eseyfe,mi2,radostyle@gmail.com,terrasect,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17901,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 02 04:11:56 UTC 2016,,,,,,,,,,"0|i34jo7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/16 23:58;apachespark;User 'seyfe' has created a pull request for this issue:
https://github.com/apache/spark/pull/15371;;;","11/Oct/16 04:19;apachespark;User 'seyfe' has created a pull request for this issue:
https://github.com/apache/spark/pull/15425;;;","02/Nov/16 04:11;radostyle@gmail.com;Can I assume that I can disregard this error for proper operation of my spark job, but I just might lose some logging for the UI?  

The title of this bug says that ""accumulators"" are failing with ConcurrentModificationException.  However, the stacktrace shows the issues arising from the eventlog reporting which I understand to be related to event logging for the UI.  I'm having this error come up during my job and I need accumulators to work correctly for proper operation of my spark job. Can I assume that I can disregard this error for proper operation of my job, but I just might lose some logging for the UI?  Or are the accumulators not working correctly?
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR cannot parallelize data.frame with NA or NULL in Date columns,SPARK-17811,13010268,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,falaki,falaki,falaki,06/Oct/16 20:17,31/Jan/23 05:05,14/Jul/23 06:29,22/Oct/16 05:35,2.0.1,,,,,,,,2.0.2,2.1.0,,,SparkR,,,,,,,,0,,,,,,"To reproduce: 

{code}
df <- data.frame(Date = as.Date(c(rep(""2016-01-10"", 10), ""NA"", ""NA"")), id = 1:12)

dim(createDataFrame(df))
{code}

We don't seem to have this problem with POSIXlt 
{code}
df <- data.frame(Date = as.POSIXlt(as.Date(c(rep(""2016-01-10"", 10), ""NA"", ""NA""))), id = 1:12)
dim(createDataFrame(df))
{code}
",,apachespark,falaki,felixcheung,wm624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-42005,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 11 18:19:41 UTC 2016,,,,,,,,,,"0|i34jgn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/16 05:32;wm624;I am looking at this issue.

Thanks!;;;","07/Oct/16 19:09;wm624;> df <- data.frame(Date = as.POSIXlt(as.Date(c(rep(""2016-01-10"", 10), ""NA"", ""NA""))), id = 1:12)
> re <- createDataFrame(df)
> collect(select(re, ""Date""))
                  Date
1  2016-01-09 16:00:00
2  2016-01-09 16:00:00
3  2016-01-09 16:00:00
4  2016-01-09 16:00:00
5  2016-01-09 16:00:00
6  2016-01-09 16:00:00
7  2016-01-09 16:00:00
8  2016-01-09 16:00:00
9  2016-01-09 16:00:00
10 2016-01-09 16:00:00
11 1969-12-31 16:00:00<====== NA in the data.frame
12 1969-12-31 16:00:00<====== NA in the data.frame

It seems that when creating dataframe, it adds default value for POSIXlt type but not Date type if fields are ""NA"". I trying to find a proper fix.;;;","10/Oct/16 21:00;apachespark;User 'falaki' has created a pull request for this issue:
https://github.com/apache/spark/pull/15421;;;","10/Oct/16 21:07;falaki;Thanks [~wm624] for looking into it. I submitted a small PR to fix the issue.;;;","11/Oct/16 18:19;wm624;:) Just want to submit a PR and found that you have a fix. Good to learn more about R. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default spark.sql.warehouse.dir is relative to local FS but can resolve as HDFS path,SPARK-17810,13010254,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,srowen,srowen,06/Oct/16 19:31,26/Jan/17 13:09,14/Jul/23 06:29,24/Oct/16 10:20,2.0.1,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,,,,,,"Following SPARK-15899 and https://github.com/apache/spark/pull/13868#discussion_r82252372 we have a slightly different problem. 

The change removed the {{file:}} scheme from the default {{spark.sql.warehouse.dir}} as part of its fix, though the path is still clearly intended to be a local FS path and defaults to ""spark-warehouse"" in the user's home dir. However when running on HDFS this path will be resolved as an HDFS path, where it almost surely doesn't exist. 

Although it can be fixed by overriding {{spark.sql.warehouse.dir}} to a path like ""file:/tmp/spark-warehouse"", or any valid HDFS path, this probably won't work on Windows (the original problem) and of course means the default fails to work for most HDFS use cases.

There's a related problem here: the docs say the default should be spark-warehouse relative to the current working dir, not the user home dir. We can adjust that.

PR coming shortly.",,apachespark,mgrover,Purple,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17918,,,,,,,,,,SPARK-19367,,,,SPARK-15899,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 24 10:20:23 UTC 2016,,,,,,,,,,"0|i34jdj:",9223372036854775807,,,,,,,,,,,,,2.0.2,,,,,,,,,,,"06/Oct/16 19:51;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15382;;;","24/Oct/16 10:20;srowen;Issue resolved by pull request 15382
[https://github.com/apache/spark/pull/15382];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scala.MatchError: BooleanType when casting a struct,SPARK-17809,13010220,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,niek,niek,06/Oct/16 17:34,08/Oct/16 09:21,14/Jul/23 06:29,06/Oct/16 19:10,2.0.0,,,,,,,,2.0.1,,,,,,,,,,,,0,,,,,,"I have a Dataframe with a struct and I need to rename some fields to lower case before saving it to cassandra.

It turns out that it's not possible to cast a boolean field of a struct to another boolean field in the renamed struct:

{quote}
case class ClassWithBoolean(flag: Boolean)
case class Parent(cwb: ClassWithBoolean)

val structCwb: DataType = StructType(Seq(
  StructField(""flag"", BooleanType, true)
))

Seq(Parent(ClassWithBoolean(true)))
    .toDF
    .withColumn(""cwb"", $""cwb"".cast(structCwb))
    .collect 

scala.MatchError: BooleanType (of class org.apache.spark.sql.types.BooleanType$)
{quote}

A workaround is to temporarily cast the field to an Integer and back:

{quote}
val structCwbTmp: DataType = StructType(Seq(
  StructField(""flag"", IntegerType, true)
))

Seq(Parent(ClassWithBoolean(true)))
    .toDF
    .withColumn(""cwb"", $""cwb"".cast(structCwbTmp))
    .withColumn(""cwb"", $""cwb"".cast(structCwb))
    .collect 
{quote}

",,joshrosen,niek,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 07 10:23:02 UTC 2016,,,,,,,,,,"0|i34j5z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/16 19:08;joshrosen;For context, here's the stacktrace as of Spark 2.0.0:

{code}
scala.MatchError: BooleanType (of class org.apache.spark.sql.types.BooleanType$)
  at org.apache.spark.sql.catalyst.expressions.Cast.castToBoolean(Cast.scala:148)
  at org.apache.spark.sql.catalyst.expressions.Cast.org$apache$spark$sql$catalyst$expressions$Cast$$cast(Cast.scala:428)
  at org.apache.spark.sql.catalyst.expressions.Cast$$anonfun$1.apply(Cast.scala:405)
  at org.apache.spark.sql.catalyst.expressions.Cast$$anonfun$1.apply(Cast.scala:404)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
  at org.apache.spark.sql.catalyst.expressions.Cast.castStruct(Cast.scala:404)
  at org.apache.spark.sql.catalyst.expressions.Cast.org$apache$spark$sql$catalyst$expressions$Cast$$cast(Cast.scala:437)
  at org.apache.spark.sql.catalyst.expressions.Cast.cast$lzycompute(Cast.scala:445)
  at org.apache.spark.sql.catalyst.expressions.Cast.cast(Cast.scala:445)
  at org.apache.spark.sql.catalyst.expressions.Cast.nullSafeEval(Cast.scala:447)
  at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:324)
  at org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:142)
  at org.apache.spark.sql.catalyst.expressions.InterpretedProjection.apply(Projection.scala:45)
  at org.apache.spark.sql.catalyst.expressions.InterpretedProjection.apply(Projection.scala:29)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.immutable.List.map(List.scala:285)
  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$apply$37.applyOrElse(Optimizer.scala:1484)
  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$apply$37.applyOrElse(Optimizer.scala:1480)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:279)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:279)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:278)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:268)
  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:1480)
  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:1479)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
  at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)
  at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)
  at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:74)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:74)
  at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:78)
  at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:76)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:83)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:83)
  at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2541)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2187)
  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2163)
  ... 50 elided
{code};;;","06/Oct/16 19:10;joshrosen;This appears to be fixed in the Spark 2.0.1 release, so I'm going to mark this as fixed. Please re-open if you encounter this error again after upgrading. Thanks!;;;","07/Oct/16 10:23;niek;Oh didn't notice 2.0.1 is already out (although it doesn't appear yet on maven).

Tested again on 2.0.1 and works correctly now.

Thanks for the help!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinaryType fails in Python 3 due to outdated Pyrolite,SPARK-17808,13010209,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bryanc,pfein,pfein,06/Oct/16 17:03,14/Jul/19 05:22,14/Jul/23 06:29,11/Oct/16 06:30,2.0.1,,,,,,,,2.0.2,2.1.0,,,PySpark,,,,,,,,0,,,,,,"Attempting to create a DataFrame using a BinaryType field fails under Python 3 because the underlying Pyrolite library is out of date. Spark appears to be using Pyrolite 4.9; this issue was fixed in Pyrolite 4.12. See [original bug report|https://github.com/irmen/Pyrolite/issues/36] and [patch|https://github.com/irmen/Pyrolite/commit/eec11786746d933b9d2c3eaeb1e1486319ae436e]

Test case & output attached. I'm just a Python guy, not really sure how to build Spark / do classpath magic to test if this works correctly with updated Pyrolite.",spark-2.0.1-bin-hadoop2.7 with Python 3.4.3 on Ubuntu 14.04.4 LTS,apachespark,bryanc,pfein,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/16 17:04;pfein;demo.py;https://issues.apache.org/jira/secure/attachment/12831990/demo.py","06/Oct/16 17:04;pfein;demo_output.txt;https://issues.apache.org/jira/secure/attachment/12831991/demo_output.txt",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 11 18:02:22 UTC 2016,,,,,,,,,,"0|i34j3j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/16 18:41;bryanc;I was able to reproduce this [~pfein], I can make a PR to upgrade Pyrolite unless you are planning on doing so?;;;","06/Oct/16 19:43;pfein; [~bryanc] go for it - thanks! ;;;","07/Oct/16 00:32;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/15386;;;","11/Oct/16 06:30;srowen;Issue resolved by pull request 15386
[https://github.com/apache/spark/pull/15386];;;","11/Oct/16 13:54;pfein;Any reason this can't be included in the next 2.0.x bug fix release? ;;;","11/Oct/16 18:02;srowen;I think it could be OK. It's a bug fix, and while it is a minor version bump to a dependency in a maintenance release, it looks like a small change.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scalatest listed as compile dependency in spark-tags,SPARK-17807,13010151,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,rdub,lucean,lucean,06/Oct/16 13:27,27/Dec/16 22:46,14/Jul/23 06:29,23/Dec/16 00:09,2.0.0,,,,,,,,2.0.3,2.1.1,2.2.0,,Spark Core,,,,,,,,0,,,,,,"In spark-tags:2.0.0, Scalatest is listed as a compile time dependency - shouldn't this be in test scope?",,apachespark,lucean,rdub,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 16 16:14:06 UTC 2016,,,,,,,,,,"0|i34iqn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/16 13:39;srowen;No, the tags actually depend on scalatests's annotations like TagAnnotation. It wouldn't compile if they were in test scope.

(Might be better to ask questions on dev@ first);;;","06/Oct/16 13:45;lucean;That's great - thanks for clearing that up. I'll make sure to post on the mailing list in future.;;;","16/Dec/16 00:42;vanzin;Reopening since this is a real issue (the dependency leaks when you depend on spark-core in maven and don't have scalatest as an explicit test dependency in your project).;;;","16/Dec/16 01:24;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/16303;;;","16/Dec/16 16:14;apachespark;User 'ryan-williams' has created a pull request for this issue:
https://github.com/apache/spark/pull/16311;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect result when work with data from parquet,SPARK-17806,13010131,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,v-gerasimov,v-gerasimov,06/Oct/16 11:48,27/Oct/16 08:11,14/Jul/23 06:29,07/Oct/16 22:04,2.0.0,2.0.1,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,correctness,,,,,"{code}
  import org.apache.spark.SparkConf
  import org.apache.spark.sql.SparkSession
  import org.apache.spark.sql.types.{StructField, StructType}
  import org.apache.spark.sql.types.DataTypes._

  val sc = SparkSession.builder().config(new SparkConf().setMaster(""local"")).getOrCreate()

  val jsonRDD = sc.sparkContext.parallelize(Seq(
    """"""{""a"":1,""b"":1,""c"":1}"""""",
    """"""{""a"":1,""b"":1,""c"":2}""""""
  ))

  sc.read.schema(StructType(Seq(
    StructField(""a"", IntegerType),
    StructField(""b"", IntegerType),
    StructField(""c"", LongType)
  ))).json(jsonRDD).write.parquet(""/tmp/test"")

  val df = sc.read.load(""/tmp/test"")
  df.join(df, Seq(""a"", ""b"", ""c""), ""left_outer"").show()
{code}

returns:
{code}
+---+---+---+
|  a|  b|  c|
+---+---+---+
|  1|  1|  1|
|  1|  1|  1|
|  1|  1|  2|
|  1|  1|  2|
+---+---+---+
{code}

Expected result:
{code}
+---+---+---+
|  a|  b|  c|
+---+---+---+
|  1|  1|  1|
|  1|  1|  2|
+---+---+---+
{code}

If I use this code without saving to parquet it works fine. If you change type of `c` column to `IntegerType` it also works fine.",,apachespark,davies,joshrosen,rxin,v-gerasimov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17962,SPARK-17891,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 07 22:04:26 UTC 2016,,,,,,,,,,"0|i34im7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/16 18:57;joshrosen;I was able to confirm that this is still a problem as of 2.0.1. To rule out the possibility of this being a self-join bug, I rewrote this to read the parquet file twice, then joined those tables together. Here's the query plan:

{code}
== Parsed Logical Plan ==
GlobalLimit 21
+- LocalLimit 21
   +- Join LeftOuter, (((a#1899 = a#1906) && (b#1900 = b#1907)) && (c#1901L = c#1908L))
      :- Relation[a#1899,b#1900,c#1901L] parquet
      +- Relation[a#1906,b#1907,c#1908L] parquet

== Analyzed Logical Plan ==
a: int, b: int, c: bigint, a: int, b: int, c: bigint
GlobalLimit 21
+- LocalLimit 21
   +- Join LeftOuter, (((a#1899 = a#1906) && (b#1900 = b#1907)) && (c#1901L = c#1908L))
      :- Relation[a#1899,b#1900,c#1901L] parquet
      +- Relation[a#1906,b#1907,c#1908L] parquet

== Optimized Logical Plan ==
GlobalLimit 21
+- LocalLimit 21
   +- Join LeftOuter, (((a#1899 = a#1906) && (b#1900 = b#1907)) && (c#1901L = c#1908L))
      :- LocalLimit 21
      :  +- Relation[a#1899,b#1900,c#1901L] parquet
      +- Relation[a#1906,b#1907,c#1908L] parquet

== Physical Plan ==
CollectLimit 21
+- *BroadcastHashJoin [a#1899, b#1900, c#1901L], [a#1906, b#1907, c#1908L], LeftOuter, BuildRight
   :- *LocalLimit 21
   :  +- *BatchedScan parquet [a#1899,b#1900,c#1901L] Format: ParquetFormat, InputPaths: dbfs:/tmp/test, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:int,b:int,c:bigint>
   +- BroadcastExchange HashedRelationBroadcastMode(List((shiftleft((shiftleft(cast(input[0, int, true] as bigint), 32) | (cast(input[1, int, true] as bigint) & 4294967295)), 64) | (cast(input[2, bigint, true] as bigint) & 0))))
      +- *BatchedScan parquet [a#1906,b#1907,c#1908L] Format: ParquetFormat, InputPaths: dbfs:/tmp/test, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:int,b:int,c:bigint>
{code}
;;;","06/Oct/16 19:01;joshrosen;I think that broadcast join may be the culprit here since I seem to get the correct answer after disabling auto broadcast join ({{set spark.sql.autoBroadcastJoinThreshold=-1}};;;","07/Oct/16 17:28;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/15390;;;","07/Oct/16 22:04;davies;Issue resolved by pull request 15390
[https://github.com/apache/spark/pull/15390];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sqlContext.read.text() does not work with a list of paths,SPARK-17805,13010095,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bryanc,laurent-l,laurent-l,06/Oct/16 08:12,07/Oct/16 07:27,14/Jul/23 06:29,07/Oct/16 07:27,2.0.0,,,,,,,,2.0.2,2.1.0,,,PySpark,,,,,,,,0,,,,,,"When I try to load multiple text files with the sqlContext, I get the
following error:

spark-2.0.0-bin-hadoop2.4/python/lib/pyspark.zip/pyspark/sql/readwriter.py"",
line 282, in text
UnboundLocalError: local variable 'path' referenced before assignment

According to the code
(https://github.com/apache/spark/blob/master/python/pyspark/sql/readwriter.py#L291),
the variable 'path' is not set if the argument is not a string.

Laurent
",,apachespark,laurent-l,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 06 18:39:06 UTC 2016,,,,,,,,,,"0|i34ie7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/16 18:39;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/15379;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove redundant Experimental annotations in sql.streaming package,SPARK-17798,13010066,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,06/Oct/16 04:27,06/Oct/16 17:34,14/Jul/23 06:29,06/Oct/16 17:34,,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,,,,,,,,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 06 04:31:04 UTC 2016,,,,,,,,,,"0|i34i7r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/16 04:31;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/15373;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sorting on stage or job tables doesn’t reload page on that table,SPARK-17795,13010046,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ajbozarth,ajbozarth,ajbozarth,06/Oct/16 00:32,07/Oct/16 10:48,14/Jul/23 06:29,07/Oct/16 10:47,2.1.0,,,,,,,,2.1.0,,,,Web UI,,,,,,,,0,,,,,,"When you sort on a table on the AllJobs, Job, or AllStages pages the page will reload at the top of the page rather than on the table that was just sorted. This can be frustrating if you have large tables to scroll past and want to do quick sorting.",,ajbozarth,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 07 10:47:59 UTC 2016,,,,,,,,,,"0|i34i3b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/16 00:33;ajbozarth;I have a fix for this and will submit a pr soon;;;","06/Oct/16 00:47;apachespark;User 'ajbozarth' has created a pull request for this issue:
https://github.com/apache/spark/pull/15369;;;","07/Oct/16 10:47;srowen;Issue resolved by pull request 15369
[https://github.com/apache/spark/pull/15369];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sorting on the description on the Job or Stage page doesn’t always work,SPARK-17793,13010031,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ajbozarth,ajbozarth,ajbozarth,05/Oct/16 23:26,08/Oct/16 10:24,14/Jul/23 06:29,08/Oct/16 10:24,2.1.0,,,,,,,,2.1.0,,,,Web UI,,,,,,,,0,,,,,,"When you sort on Description on the jobs or stages table when there's no description, just the stage name, it will sort the rows in the same order where it’s supposed to be descending or ascending. This behave oddly when none of the stages have descriptions and the column doesn't sort on what's displayed (the stage names)

Seems to be caused by a doing a getOrElse(“”) on the description, causing them all to be equal. Since description doesn't always exist and the Stage name usually does, the column should secondary sort on stage name for clarity.",,ajbozarth,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 08 10:24:18 UTC 2016,,,,,,,,,,"0|i34hzz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/16 23:27;ajbozarth;I have a fix for this and will be submitting the pr soon;;;","05/Oct/16 23:45;apachespark;User 'ajbozarth' has created a pull request for this issue:
https://github.com/apache/spark/pull/15366;;;","08/Oct/16 10:24;srowen;Issue resolved by pull request 15366
[https://github.com/apache/spark/pull/15366];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
L-BFGS solver for linear regression does not accept general numeric label column types,SPARK-17792,13010028,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sethah,sethah,sethah,05/Oct/16 23:12,07/Oct/16 04:18,14/Jul/23 06:29,07/Oct/16 04:13,,,,,,,,,2.0.2,2.1.0,,,ML,,,,,,,,0,,,,,,"There's a bug in accepting numeric types for linear regression. We cast the label to {{DoubleType}} in one spot where we use normal solver, but not for the l-bfgs solver. The following can reproduce the problem:

{code}
import org.apache.spark.ml.feature.LabeledPoint
import org.apache.spark.ml.linalg.{Vector, DenseVector, Vectors}
import org.apache.spark.ml.regression.LinearRegression
import org.apache.spark.sql.types._

val df = Seq(LabeledPoint(1.0, Vectors.dense(1.0))).toDF().withColumn(""weight"", lit(1.0).cast(LongType))
val lr = new LinearRegression().setSolver(""l-bfgs"").setWeightCol(""weight"")
lr.fit(df)
{code}",,apachespark,sethah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 06 08:55:03 UTC 2016,,,,,,,,,,"0|i34hzb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/16 23:12;sethah;I'll have a PR shortly.;;;","05/Oct/16 23:19;apachespark;User 'sethah' has created a pull request for this issue:
https://github.com/apache/spark/pull/15364;;;","06/Oct/16 08:55;srowen;Related to https://issues.apache.org/jira/browse/SPARK-17747 I guess;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RangePartitioner results in few very large tasks and many small to empty tasks ,SPARK-17788,13009924,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,babak.alipour@gmail.com,babak.alipour@gmail.com,05/Oct/16 17:17,06/Dec/17 14:57,14/Jul/23 06:29,30/Oct/17 16:59,2.0.0,,,,,,,,2.3.0,,,,Spark Core,SQL,,,,,,,0,,,,,,"Greetings everyone,

I was trying to read a single field of a Hive table stored as Parquet in Spark (~140GB for the entire table, this single field is a Double, ~1.4B records) and look at the sorted output using the following:
sql(""SELECT "" + field + "" FROM MY_TABLE ORDER BY "" + field + "" DESC"") 
​But this simple line of code gives:
Caused by: java.lang.IllegalArgumentException: Cannot allocate a page with more than 17179869176 bytes

Same error for:
sql(""SELECT "" + field + "" FROM MY_TABLE).sort(field)
and:
sql(""SELECT "" + field + "" FROM MY_TABLE).orderBy(field)

After doing some searching, the issue seems to lie in the RangePartitioner trying to create equal ranges. [1]

[1] https://spark.apache.org/docs/2.0.0/api/java/org/apache/spark/RangePartitioner.html 

 The Double values I'm trying to sort are mostly in the range [0,1] (~70% of the data which roughly equates 1 billion records), other numbers in the dataset are as high as 2000. With the RangePartitioner trying to create equal ranges, some tasks are becoming almost empty while others are extremely large, due to the heavily skewed distribution. 

This is either a bug in Apache Spark or a major limitation of the framework. I hope one of the devs can help solve this issue.

P.S. Email thread on Spark user mailing list:
http://mail-archives.apache.org/mod_mbox/spark-user/201610.mbox/%3CCA%2B_of14hTVYTUHXC%3DmS9Kqd6qegVvkoF-ry3Yj2%2BRT%2BWSBNzhg%40mail.gmail.com%3E
","Ubuntu 14.04 64bit
Java 1.8.0_101",apachespark,babak.alipour@gmail.com,cloud_fan,dkbiswal,emlyn,gpcuster,holden,hvanhovell,sesshomurai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-9862,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 06 14:57:13 UTC 2017,,,,,,,,,,"0|i34hc7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/16 08:53;cloud_fan;can you provide the full stacktrace? thanks!;;;","14/Nov/16 14:15;babak.alipour@gmail.com;The details were in the email thread.
Here's the full stack trace: 

Caused by: java.lang.IllegalArgumentException: Cannot allocate a page with more than 17179869176 bytes
        at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:241)
        at org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:121)
        at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:374)
        at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:396)
        at org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:94)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
        at org.apache.spark.scheduler.Task.run(Task.scala:85)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
;;;","25/Nov/16 14:14;holden;This is semi-expected behaviour of the range partitioner (and really all Spark partitioners) don't support creating a split on the same key (e.g. 70% of your data has the same key and you are partitioning on that key 70% of that day is going to end up in the same partition).

We could try and fix this in a few ways - either by having Spark SQL do something special in this case or having Spark's sortBy automatically add ""noise"" to the key when the sampling indicates there is too much data for a given key or allowing partitioners to be non-determinstic and updating the general sortBy logic in Spark.

I think this would be something good for us to consider - but it's probably going to take awhile (and certainly not in time for 2.1.0).;;;","25/Nov/16 15:37;hvanhovell;Spark makes a sketch of your data as soon when you want to order the entire dataset. Based on that sketch Spark tries to create equally sized partitions. As [~holdenk] said, your problem is caused by skew (a lot of rows with the same key), and none of the current partitioning schemes can help you with this. On the short run, you could follow her suggestion and add noise to the order (this only works for global ordering and not for joins/aggregation with skewed values). On the long run, there is an ongoing effort to reduce skew for joining, see SPARK-9862 for more information.

I have created the follow little spark program to illustrate how range partitioning works:
{noformat}
import org.apache.spark.sql.Row

// Set the partitions and parallelism to relatively low value so we can read the results.
spark.conf.set(""spark.default.parallelism"", ""20"")
spark.conf.set(""spark.sql.shuffle.partitions"", ""20"")

// Create a skewed data frame.
val df = spark
  .range(10000000)
  .select(
    $""id"",
    (rand(34) * when($""id"" % 10 <= 7, lit(1.0)).otherwise(lit(10.0))).as(""value""))

// Make a summary per partition. The partition intervals should not overlap and the number of
// elements in a partition should roughly be the same for all partitions.
case class PartitionSummary(count: Long, min: Double, max: Double, range: Double)
val res = df.orderBy($""value"").mapPartitions { iterator =>
  val (count, min, max) = iterator.foldLeft((0L, Double.PositiveInfinity, Double.NegativeInfinity)) {
    case ((count, min, max), Row(_, value: Double)) =>
      (count + 1L, Math.min(min, value), Math.max(max, value))
  }
  Iterator.single(PartitionSummary(count, min, max, max - min))
}

// Get results and make them look nice
res.orderBy($""min"")
  .select($""count"", $""min"".cast(""decimal(5,3)""), $""max"".cast(""decimal(5,3)""), $""range"".cast(""decimal(5,3)""))
  .show(30)
{noformat}

This yields the following results (notice how the partition range varies and the row count is relatively similar):
{noformat}
+------+-----+------+-----+                                                     
| count|  min|   max|range|
+------+-----+------+-----+
|484005|0.000| 0.059|0.059|
|426212|0.059| 0.111|0.052|
|381796|0.111| 0.157|0.047|
|519954|0.157| 0.221|0.063|
|496842|0.221| 0.281|0.061|
|539082|0.281| 0.347|0.066|
|516798|0.347| 0.410|0.063|
|558487|0.410| 0.478|0.068|
|419825|0.478| 0.529|0.051|
|402257|0.529| 0.578|0.049|
|557225|0.578| 0.646|0.068|
|518626|0.646| 0.710|0.063|
|611478|0.710| 0.784|0.075|
|544556|0.784| 0.851|0.066|
|454356|0.851| 0.906|0.055|
|450535|0.906| 0.961|0.055|
|575996|0.961| 2.290|1.329|
|525915|2.290| 4.920|2.630|
|518757|4.920| 7.510|2.590|
|497298|7.510|10.000|2.490|
+------+-----+------+-----+
{noformat};;;","25/Nov/16 15:49;hvanhovell;I am closing this one as a duplicate. Feel free to reopen if you disagree.;;;","25/Nov/16 16:05;holden;I don't think this is a duplicate - its related but a join doesn't necessarily use a range partitioner and sortBy is a different operation. I agree the potential solution could share a lot the same underlying implementation.;;;","25/Nov/16 16:05;holden;This is somewhat distinct from the join case, but certainly related.;;;","25/Nov/16 17:09;hvanhovell;That is fair. The solution is not that straightforward TBH:
- Always add some kind of tie breaking value to the range. This could be random, but I'd rather add something like monotonically_increasing_id(). This always incurs some cost.
- Only add a tie-breaker when the you have (suspect) skew. Here we need to add some heavy hitter algorithm, which is potentially much more resource intensive than reservoir sampling. The other thing is that when we suspect skew, we would need to scan the data again (which would make the total of scans 3).

So I would be slightly in favor of option 1 and a flag to disable it.;;;","26/Nov/16 14:44;cloud_fan;Should we investigate this?
{code}
Caused by: java.lang.IllegalArgumentException: Cannot allocate a page with more than 17179869176 bytes
{code}

Although some partitions can be very large, but Spark should be able to process it(slowly), instead of throwing exception.;;;","27/Nov/16 08:10;cloud_fan;After looking at the code, it seems the only way to trigger this exception is setting `spark.buffer.pageSize` to a value larger than `((1L << 31) - 1) * 8L`, [~babak.alipour@gmail.com] did you set this conf?;;;","27/Nov/16 17:14;babak.alipour@gmail.com;No, I didn't change that conf. I did try to change `spark.executor.memory` to various values ranging from 8g to 64g; nothing changes and I get the same exception. ;;;","27/Nov/16 22:48;hvanhovell;[~babak.alipour@gmail.com] A few questions:
- Is it possible to get a reproducible piece of code?
- Could you give us the value of the {{spark.buffer.pageSize}} configuration property? When we allocate the memory for a new record we try to allocate either the page size (which is a Long value) or the size of the record (which is an Int value). The size of the page is larger than the maximum integer value, so this implies the page size is set at a very high value.
- I am also quite surprised why this is not spilling. Could you give us the value of the {{spark.shuffle.spill.numElementsForceSpillThreshold}} configuration property? What is the average row size?;;;","29/Oct/17 04:29;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/18251;;;","30/Oct/17 17:02;cloud_fan;Unfortunately I don't have a reproducible code snippet to prove it has been fixed, but I'm pretty confident my fix should work for it. cc [~babak.alipour@gmail.com] please reopen this ticket if you still hit this issue, thanks!;;;","06/Dec/17 14:57;sesshomurai;I'm also running into this error on spark 2.1.0
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 42 in stage 11.0 failed 4 times, most recent failure: Lost task 42.3 in stage 11.0 (TID 7544,xxx.xxx.xxx.xxx.xx, executor 2): java.lang.IllegalArgumentException: Cannot allocate a page with more than 17179869176 bytes
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hide Credentials in CREATE and DESC FORMATTED/EXTENDED a PERSISTENT/TEMP Table for JDBC,SPARK-17783,13009774,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,smilegator,smilegator,smilegator,05/Oct/16 04:48,07/Aug/19 10:39,14/Jul/23 06:29,28/Nov/16 15:07,2.0.1,2.1.0,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,,,,,,"We should never expose the Credentials in the EXPLAIN and DESC FORMATTED/EXTENDED command. However, below commands exposed the credentials. 

{noformat}
CREATE TABLE tab1 USING org.apache.spark.sql.jdbc
{noformat}

{noformat}
== Physical Plan ==
ExecutedCommand
   +- CreateDataSourceTableCommand CatalogTable(
	Table: `tab1`
	Created: Tue Oct 04 21:39:44 PDT 2016
	Last Access: Wed Dec 31 15:59:59 PST 1969
	Type: MANAGED
	Provider: org.apache.spark.sql.jdbc
	Storage(Properties: [url=jdbc:h2:mem:testdb0;user=testUser;password=testPass, dbtable=TEST.PEOPLE, user=testUser, password=testPass])), false
{noformat}
{noformat}
DESC FORMATTED tab1
{noformat}
{noformat}
...
|# Storage Information       |                                                                  |       |
|Compressed:                 |No                                                                |       |
|Storage Desc Parameters:    |                                                                  |       |
|  path                      |file:/Users/xiaoli/IdeaProjects/sparkDelivery/spark-warehouse/tab1|       |
|  url                       |jdbc:h2:mem:testdb0;user=testUser;password=testPass               |       |
|  dbtable                   |TEST.PEOPLE                                                       |       |
|  user                      |testUser                                                          |       |
|  password                  |testPass                                                          |       |
+----------------------------+------------------------------------------------------------------+-------+
{noformat}


{noformat}
DESC EXTENDED tab1
{noformat}
{noformat}
...
	Storage(Properties: [path=file:/Users/xiaoli/IdeaProjects/sparkDelivery/spark-warehouse/tab1, url=jdbc:h2:mem:testdb0;user=testUser;password=testPass, dbtable=TEST.PEOPLE, user=testUser, password=testPass]))|       |
{noformat}

{noformat}
CREATE TEMP VIEW tab1 USING org.apache.spark.sql.jdbc
{noformat}
{noformat}
== Physical Plan ==
ExecutedCommand
   +- CreateTempViewUsing `tab1`, false, org.apache.spark.sql.jdbc, Map(url -> jdbc:h2:mem:testdb0;user=testUser;password=testPass, dbtable -> TEST.PEOPLE, user -> testUser, password -> testPass)
{noformat}
",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28642,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 28 23:32:05 UTC 2016,,,,,,,,,,"0|i34gev:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/16 04:52;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15358;;;","28/Nov/16 23:32;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16047;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka 010 test is flaky,SPARK-17782,13009758,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,koeninger,hvanhovell,hvanhovell,05/Oct/16 03:10,12/Oct/16 22:23,14/Jul/23 06:29,12/Oct/16 22:23,2.0.0,2.0.1,,,,,,,2.0.2,2.1.0,,,DStreams,,,,,,,,0,,,,,,"The Kafka 010 DirectKafkaStreamSuite {{pattern based subscription}} is flaky. We should disable it, and figure out how we can improve it.",,apachespark,hvanhovell,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 12 22:23:26 UTC 2016,,,,,,,,,,"0|i34gbb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/16 03:15;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/15355;;;","07/Oct/16 01:17;apachespark;User 'koeninger' has created a pull request for this issue:
https://github.com/apache/spark/pull/15387;;;","08/Oct/16 03:02;apachespark;User 'koeninger' has created a pull request for this issue:
https://github.com/apache/spark/pull/15401;;;","12/Oct/16 22:23;zsxwing;Resolved by https://github.com/apache/spark/pull/15401;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveInspector wrapper for JavaVoidObjectInspector is missing ,SPARK-17773,13009389,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,eseyfe,eseyfe,eseyfe,03/Oct/16 23:59,04/Oct/16 17:12,14/Jul/23 06:29,04/Oct/16 06:28,2.0.0,,,,,,,,2.1.0,,,,Input/Output,,,,,,,,0,,,,,,"Executing following query fails.
{noformat}
select SOME_UDAF*(a.arr) 
from (
  select Array(null) as arr from dim_one_row
) a

SOME_UDAF= It's an UDAF which is similar to FIRST(), but skips the null values if it can and adds randomization.

Error message:
scala.MatchError: org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaVoidObjectInspector@39055e0d (of class org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaVoidObjectInspector)
  at org.apache.spark.sql.hive.HiveInspectors$class.wrapperFor(HiveInspectors.scala:257)
  at org.apache.spark.sql.hive.HiveUDAFFunction.wrapperFor(hiveUDFs.scala:269)
  at org.apache.spark.sql.hive.HiveInspectors$class.wrap(HiveInspectors.scala:719)
{noformat}",,apachespark,eseyfe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 04 17:12:05 UTC 2016,,,,,,,,,,"0|i34e1j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/16 00:03;apachespark;User 'seyfe' has created a pull request for this issue:
https://github.com/apache/spark/pull/15337;;;","04/Oct/16 17:12;apachespark;User 'seyfe' has created a pull request for this issue:
https://github.com/apache/spark/pull/15345;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Small {Sum,Count,Mean}Evaluator problems and suboptimalities",SPARK-17768,13009265,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,srowen,srowen,03/Oct/16 15:55,08/Oct/16 10:31,14/Jul/23 06:29,08/Oct/16 10:31,2.0.1,,,,,,,,2.1.0,,,,Spark Core,,,,,,,,0,,,,,,"This tracks a few related issues with org.apache.spark.partial.(Count,Mean,Sum)Evaluator and their ""Grouped"" counterparts:

- GroupedMeanEvaluator and GroupedSumEvaluator are unused, as is the StudentTCacher support class
- CountEvaluator can return a lower bound < 0, when counts can't be negative
- MeanEvaluator will actually fail on exactly 1 datum (yields t-test with 0 DOF)
- CountEvaluator uses a normal distribution, which may be an inappropriate approximation (leading to above)
- Test for SumEvaluator asserts incorrect expected sums -- e.g. after observing 10% of data has sum of 2, expectation should be 20, not 38
- CountEvaluator, MeanEvaluator have no unit tests to catch these
- Duplication of distribution code across CountEvaluator, GroupedCountEvaluator
- The stats in each could use a bit of documentation as I had to guess at them
- (Code could use a few cleanups and optimizations too)

",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 08 10:31:29 UTC 2016,,,,,,,,,,"0|i34d9z:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"04/Oct/16 09:27;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15341;;;","08/Oct/16 10:31;srowen;Issue resolved by pull request 15341
[https://github.com/apache/spark/pull/15341];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.spark.mllib.linalg.VectorUDT cannot be cast to org.apache.spark.sql.types.StructType,SPARK-17765,13009210,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,kxepal,kxepal,03/Oct/16 11:13,12/Dec/22 18:11,14/Jul/23 06:29,21/Nov/16 19:12,1.6.1,2.0.0,,,,,,,2.1.0,,,,MLlib,PySpark,SQL,,,,,,0,,,,,,"The issue in subject happens on attempt to transform DataFrame in Parquet format into ORC while DF contains SparseVector/DenseVector data.

In [sources|https://github.com/apache/spark/blob/v1.6.1/mllib/src/main/scala/org/apache/spark/mllib/linalg/Vectors.scala#L192] it looks like that there shouldn't be any serialization issues, but they happens.

{code}
In[4] pqtdf = hqlctx.read.parquet(pqt_feature)

In[5] pqtdf.take(1)
Out[5]: [Row(foo=u'abc, bar=SparseVector(100, {74: 1.0}))]

In[6]: pqtdf.write.format('orc').save('/tmp/orc')
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-5-57e68fd0c5cb> in <module>()
----> pqtdf.write.format('orc').save('/tmp/orc')

/usr/local/share/spark/python/pyspark/sql/readwriter.pyc in save(self, path, format, mode, partitionBy, **options)
    395             self._jwrite.save()
    396         else:
--> 397             self._jwrite.save(path)
    398 
    399     @since(1.4)

/usr/local/lib/python2.7/site-packages/py4j/java_gateway.pyc in __call__(self, *args)
    811         answer = self.gateway_client.send_command(command)
    812         return_value = get_return_value(
--> 813             answer, self.gateway_client, self.target_id, self.name)
    814 
    815         for temp_arg in temp_args:

/usr/local/share/spark/python/pyspark/sql/utils.pyc in deco(*a, **kw)
     43     def deco(*a, **kw):
     44         try:
---> 45             return f(*a, **kw)
     46         except py4j.protocol.Py4JJavaError as e:
     47             s = e.java_exception.toString()

/usr/local/lib/python2.7/site-packages/py4j/protocol.pyc in get_return_value(answer, gateway_client, target_id, name)
    306                 raise Py4JJavaError(
    307                     ""An error occurred while calling {0}{1}{2}.\n"".
--> 308                     format(target_id, ""."", name), value)
    309             else:
    310                 raise Py4JError(

Py4JJavaError: An error occurred while calling o62.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelation.scala:156)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation.run(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:256)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:148)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:139)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:209)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 185, node123.example.com): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:272)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassCastException: org.apache.spark.mllib.linalg.VectorUDT cannot be cast to org.apache.spark.sql.types.StructType
	at org.apache.spark.sql.hive.HiveInspectors$class.wrap(HiveInspectors.scala:554)
	at org.apache.spark.sql.hive.orc.OrcOutputWriter.wrap(OrcRelation.scala:66)
	at org.apache.spark.sql.hive.orc.OrcOutputWriter.wrapOrcStruct(OrcRelation.scala:128)
	at org.apache.spark.sql.hive.orc.OrcOutputWriter.writeInternal(OrcRelation.scala:139)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:264)
	... 8 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1922)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelation.scala:150)
	... 27 more
Caused by: org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:272)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	... 1 more
Caused by: java.lang.ClassCastException: org.apache.spark.mllib.linalg.VectorUDT cannot be cast to org.apache.spark.sql.types.StructType
	at org.apache.spark.sql.hive.HiveInspectors$class.wrap(HiveInspectors.scala:554)
	at org.apache.spark.sql.hive.orc.OrcOutputWriter.wrap(OrcRelation.scala:66)
	at org.apache.spark.sql.hive.orc.OrcOutputWriter.wrapOrcStruct(OrcRelation.scala:128)
	at org.apache.spark.sql.hive.orc.OrcOutputWriter.writeInternal(OrcRelation.scala:139)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:264)
	... 8 more
{code}",,apachespark,kxepal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22320,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 05 10:45:05 UTC 2016,,,,,,,,,,"0|i34cxr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/16 10:23;gurwls223;It seems this one can be quickly fixed. Let me please submit a PR.;;;","05/Oct/16 10:29;kxepal;[~hyukjin.kwon] Please do! Thank you (:;;;","05/Oct/16 10:45;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/15361;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataFrame's pivot doesn't see column created in groupBy,SPARK-17760,13009129,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,a1ray,Bonsanto,Bonsanto,02/Oct/16 16:35,07/Dec/16 17:31,14/Jul/23 06:29,07/Dec/16 12:47,2.0.0,,,,,,,,2.0.3,2.1.0,,,PySpark,,,,,,,,0,easytest,newbie,,,,"Related to [https://stackoverflow.com/questions/39817993/pivoting-with-missing-values]. I'm not completely sure if this is a bug or expected behavior.

When you `groypBy` by a column generated inside of it, the `pivot` method apparently doesn't find this column during the analysis.

E.g.
{code:none}
df = (sc.parallelize([(1.0, ""2016-03-30 01:00:00""), 
                      (30.2, ""2015-01-02 03:00:02"")])
        .toDF([""amount"", ""Date""])
        .withColumn(""Date"", col(""Date"").cast(""timestamp"")))

(df.withColumn(""hour"",hour(""date""))
   .groupBy(dayofyear(""date"").alias(""date""))
   .pivot(""hour"").sum(""amount"").show()){code}

Shows the following exception.

{quote}
AnalysisException: u'resolved attribute(s) date#140688 missing from dayofyear(date)#140994,hour#140977,sum(`amount`)#140995 in operator !Aggregate \[dayofyear(cast(date#140688 as date))], [dayofyear(cast(date#140688 as date)) AS dayofyear(date)#140994, pivotfirst(hour#140977, sum(`amount`)#140995, 1, 3, 0, 0) AS __pivot_sum(`amount`) AS `sum(``amount``)`#141001\];'
{quote}

To solve it you have to add the column {{date}} before grouping and pivoting.","Databrick's community version, spark 2.0.0, pyspark, python 2.",a1ray,apachespark,Bonsanto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 07 14:50:08 UTC 2016,,,,,,,,,,"0|i34cfr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/16 21:08;apachespark;User 'aray' has created a pull request for this issue:
https://github.com/apache/spark/pull/16177;;;","07/Dec/16 14:50;apachespark;User 'aray' has created a pull request for this issue:
https://github.com/apache/spark/pull/16197;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Aggregate function  LAST returns null on an empty partition ,SPARK-17758,13009098,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,tafranky@gmail.com,tafranky@gmail.com,02/Oct/16 04:16,05/Oct/16 23:06,14/Jul/23 06:29,05/Oct/16 23:06,2.0.0,,,,,,,,2.0.2,2.1.0,,,Input/Output,,,,,,,,0,correctness,,,,,"My Environment 
Spark 2.0.0  

I have included the physical plan of my application below.
Issue description
The result from  a query that uses the LAST function are incorrect. 

The output obtained for the column that corresponds to the last function is null .  

My input data contain 3 rows . 

The application resulted in  2 stages 

The first stage consisted of 3 tasks . 

The first task/partition contains 2 rows
The second task/partition contains 1 row
The last task/partition contain  0 rows

The result from the query executed for the LAST column call is NULL which I believe is due to the  PARTIAL_LAST on the last partition . 

I believe that this behavior is incorrect. The PARTIAL_LAST call on an empty partition should not return null .

{noformat}
== Physical Plan ==
InsertIntoHiveTable MetastoreRelation default, bdm_3449_tgt20, true, false
+- *Project [last(C3_1)#51 AS field#102, cast(round(max(C3_0)#50, 0) as int) AS field1#103, cast(round(max(C3_0)#50, 0) as int) AS field2#104]
   +- SortAggregate(key=[], functions=[max(C3_0#40),last(C3_1#41, false)], output=[max(C3_0)#50,last(C3_1)#51])
      +- SortAggregate(key=[], functions=[partial_max(C3_0#40),partial_last(C3_1#41, false)], output=[max#91,last#92])
         +- *Project [CAST(sum(C1_0) AS DOUBLE)#27 AS C3_0#40, last(C1_1)#28 AS C3_1#41]
            +- SortAggregate(key=[], functions=[sum(cast(C1_0#17 as bigint)),last(C1_1#18, false)], output=[CAST(sum(C1_0) AS DOUBLE)#27,last(C1_1)#28])
               +- Exchange SinglePartition
                  +- SortAggregate(key=[], functions=[partial_sum(cast(C1_0#17 as bigint)),partial_last(C1_1#18, false)], output=[sum#95L,last#96])
                     +- *Project [field1#7 AS C1_0#17, field#6 AS C1_1#18]
                        +- HiveTableScan [field1#7, field#6], MetastoreRelation default, bdm_3449_src, alias
{noformat}",Spark 2.0.0,apachespark,hvanhovell,tafranky@gmail.com,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 05 23:06:05 UTC 2016,,,,,,,,,,"0|i34c8v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/16 18:42;tafranky@gmail.com;I tested the behavior of the min and max function with sort aggregate and an empty partition and the results were correct. 

I would also note that this issue is not reproducible in spark version 1.6;;;","03/Oct/16 19:20;hvanhovell;Why should last return a non null result? It returns the last observation it encounters (which is pretty random in a clustered environment)? Last is only deterministic when you have sorted on the column you are calling last on.;;;","03/Oct/16 21:30;tafranky@gmail.com;[~hvanhovell] It should return a  non null  value in this case because the null  value  is a false representation of my data . 

My initial  input source  had 3 rows . 

a,1
b,2
c,10

From  observations show 

partition 1  contains 2 rows  
a,1
b,2 

partition 2 contains 1 row  
c,10  

partition  3 contains  0 rows 

why should  last return  null in this case  . 

I understand that last would not be deterministic but why should should it return null in this case ?
;;;","03/Oct/16 23:23;hvanhovell;Yeah, you have a point there. It seems to be merging the results the empty partition. We'll need to add a flag to {{Last}} aggregate, and figure out merging. This will be fun to test.;;;","04/Oct/16 00:47;tafranky@gmail.com;is there any workaround that you could think of?  

One  simple solution cold be to call repartition on the data frame  in order to remove the empty partition but performance wise that is  just terrible .  

I am not sure that i comprehend why the concept of an empty partition is even allowed in the spark ecosystem .  Any documentation on why empty partitions are allowed would be greatly appreciated.;;;","04/Oct/16 00:56;hvanhovell;You could write your own UDAF.

Partitions can be come empty for various reasons:
- Data Skew
- A filter applied to the partition can make it empty. Spark pipelines operators, so a filter could effectively prune the partition but we would still aggregate it.;;;","04/Oct/16 01:25;tafranky@gmail.com;Thanks for the pointer 
Is it possible to write a UDAF  which supports  the combiner like  functionality . I am referring to the partial_last , partial_min  .. ;;;","04/Oct/16 01:28;hvanhovell;We do, you have to implement a {{merge}} method which combines partial results. See https://databricks.com/blog/2015/09/16/apache-spark-1-5-dataframe-api-highlights.html for an example.;;;","04/Oct/16 18:22;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/15348;;;","05/Oct/16 01:22;tafranky@gmail.com;My first impression after taking a look at  First.scala leads me to conclude that the issue  depicted here should not happen for the First aggregate function call . 

Is that correct ?;;;","05/Oct/16 01:28;hvanhovell;That is correct. First keeps track of the fact that its value has been set. Last does not.;;;","05/Oct/16 23:06;yhuai;Issue resolved by pull request 15348
[https://github.com/apache/spark/pull/15348];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.ClassCastException when using cartesian with DStream.transform,SPARK-17756,13009073,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,zero323,zero323,01/Oct/16 19:37,12/Dec/22 18:10,14/Jul/23 06:29,08/Jun/18 18:28,2.0.0,2.3.0,,,,,,,2.4.0,,,,DStreams,PySpark,,,,,,,0,,,,,,"Steps to reproduce:

{code}

from pyspark.streaming import StreamingContext

ssc = StreamingContext(spark.sparkContext, 10)
(ssc
    .queueStream([sc.range(10)])
    .transform(lambda rdd: rdd.cartesian(rdd))
    .pprint())

ssc.start()

## 16/10/01 21:34:30 ERROR JobScheduler: Error generating jobs for time 1475350470000 ms
## java.lang.ClassCastException: org.apache.spark.api.java.JavaPairRDD ## cannot be cast to org.apache.spark.api.java.JavaRDD
## 	at com.sun.proxy.$Proxy15.call(Unknown Source)
##    ....
{code}

A dummy fix is to put {{map(lamba x: x)}} which suggests it is a problem similar to https://issues.apache.org/jira/browse/SPARK-16589",,apachespark,rxin,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16589,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,http://stackoverflow.com/q/39804337/1560062,,,,,,,,,,9223372036854775807,,,Fri Jun 08 18:28:20 UTC 2018,,,,,,,,,,"0|i34c3r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/17 17:33;gurwls223;Still happens in the current master. Will set the affected version.;;;","14/Oct/17 14:11;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/19498;;;","08/Jun/18 18:28;gurwls223;Issue resolved by pull request 19498
[https://github.com/apache/spark/pull/19498];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Master may ask a worker to launch an executor before the worker actually got the response of registration,SPARK-17755,13009017,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,yhuai,yhuai,30/Sep/16 23:22,24/Jan/17 18:49,14/Jul/23 06:29,26/Dec/16 07:48,,,,,,,,,2.2.0,,,,Spark Core,,,,,,,,0,,,,,,"I somehow saw a failed test {{org.apache.spark.DistributedSuite.caching in memory, serialized, replicated}}. Its log shows that Spark master asked the worker to launch an executor before the worker actually got the response of registration. So, the master knew that the worker had been registered. But, the worker did not know if it self had been registered. 

{code}
16/09/30 14:53:53.681 dispatcher-event-loop-0 INFO Master: Registering worker localhost:38262 with 1 cores, 1024.0 MB RAM
16/09/30 14:53:53.681 dispatcher-event-loop-0 INFO Master: Launching executor app-20160930145353-0000/1 on worker worker-20160930145353-localhost-38262
16/09/30 14:53:53.682 dispatcher-event-loop-3 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20160930145353-0000/1 on worker-20160930145353-localhost-38262 (localhost:38262) with 1 cores
16/09/30 14:53:53.683 dispatcher-event-loop-3 INFO StandaloneSchedulerBackend: Granted executor ID app-20160930145353-0000/1 on hostPort localhost:38262 with 1 cores, 1024.0 MB RAM
16/09/30 14:53:53.683 dispatcher-event-loop-0 WARN Worker: Invalid Master (spark://localhost:46460) attempted to launch executor.
16/09/30 14:53:53.687 worker-register-master-threadpool-0 INFO Worker: Successfully registered with master spark://localhost:46460
{code}

Then, seems the worker did not launch any executor. ",,apachespark,lins05,yhuai,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-10651,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 20 03:27:01 UTC 2016,,,,,,,,,,"0|i34brj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/16 23:33;yhuai;cc [~joshrosen];;;","21/Nov/16 07:29;zsxwing;It's not easy to fix. The root cause is messages are sent via two different channels and their order is not guaranteed.;;;","19/Dec/16 23:55;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16345;;;","20/Dec/16 03:27;lins05;A (sort-of) similar problem for coarse grained scheduler backends is reported in https://issues.apache.org/jira/browse/SPARK-18820 .;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simple case in spark sql throws ParseException,SPARK-17753,13009008,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,kdhuria,kdhuria,30/Sep/16 22:28,09/Nov/16 10:08,14/Jul/23 06:29,04/Oct/16 02:33,2.0.0,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,1,,,,,,"Simple case in sql throws parser exception in spark 2.0.
The following query as well as similar queries fail in spark 2.0 
{noformat}
scala> spark.sql(""SELECT alias.p_double as a0, alias.p_text as a1, NULL as a2 FROM hadoop_tbl_all alias WHERE  (1 = (CASE ('aaaaabbbbb' = alias.p_text) OR (8 LTE LENGTH(alias.p_text)) WHEN TRUE THEN 1  WHEN FALSE THEN 0 ELSE CAST(NULL AS INT) END))"")
org.apache.spark.sql.catalyst.parser.ParseException:
mismatched input 'FROM' expecting {<EOF>, 'WHERE', 'GROUP', 'ORDER', 'HAVING', 'LIMIT', 'LATERAL', 'WINDOW', 'UNION', 'EXCEPT', 'INTERSECT', 'SORT', 'CLUSTER', 'DISTRIBUTE'}(line 1, pos 60)

== SQL ==
SELECT alias.p_double as a0, alias.p_text as a1, NULL as a2 FROM hadoop_tbl_all alias WHERE  (1 = (CASE ('aaaaabbbbb' = alias.p_text) OR (8 LTE LENGTH(alias.p_text)) WHEN TRUE THEN 1  WHEN FALSE THEN 0 ELSE CAST(NULL AS INT) END))
------------------------------------------------------------^^^

  at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:197)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:99)
  at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:46)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:53)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)
  ... 48 elided
{noformat}",,apachespark,dongjoon,hvanhovell,kdhuria,shubhnd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 09 10:08:49 UTC 2016,,,,,,,,,,"0|i34bpj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Oct/16 18:36;dongjoon;Hi, [~kdhuria].
Right, `CASE WHEN` seems to have a bug on that. I'll investigate this.;;;","01/Oct/16 23:00;dongjoon;Hmm. It's not so each to fix this. Maybe, I'll try later.;;;","01/Oct/16 23:49;hvanhovell;We do not support the GTE/LTE/GT/LT/EQ/NEQ operators. Use the symbolic versions instead.

I will open a PR for using complex expressions in a value based case statement.;;;","01/Oct/16 23:53;kdhuria;Hi Herman,
I have tried with symbolic also, it doesn't work. Even this query fails with same error.
SELECT alias.p_double as a0, alias.p_text as a1, NULL as a2 FROM hadoop_tbl_all alias WHERE  (1 = (CASE ('aaaaabbbbb' = alias.p_text) WHEN TRUE THEN 1  WHEN FALSE THEN 0 ELSE CAST(NULL AS INT) END))

Any boolean condition after case doesn't work.;;;","02/Oct/16 00:01;hvanhovell;Yeah, the current grammar doesn't allow us to use a complex expression as the input for a simple case when statement. I'll create a fix (PR) for this. You can quite easily work around this issue, if it is blocking you.;;;","02/Oct/16 00:07;kdhuria;Yeah, Thanks!;;;","02/Oct/16 01:07;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/15322;;;","09/Nov/16 10:08;shubhnd;how have you solved the following error:-


 at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:197)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:99)
  at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:45)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:53)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)
  at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:682)
  ... 48 elided
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark returns incorrect result when 'collect()'ing a cached Dataset with many columns,SPARK-17752,13009002,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,shivaram,kevinushey,kevinushey,30/Sep/16 22:14,08/Oct/16 09:21,14/Jul/23 06:29,03/Oct/16 19:23,2.0.0,,,,,,,,2.0.1,2.1.0,,,SparkR,,,,,,,,0,,,,,,"Run the following code (modify SPARK_HOME to point to a Spark 2.0.0 installation as necessary):

{code:r}
SPARK_HOME <- path.expand(""~/Library/Caches/spark/spark-2.0.0-bin-hadoop2.7"")
Sys.setenv(SPARK_HOME = SPARK_HOME)

library(SparkR, lib.loc = c(file.path(Sys.getenv(""SPARK_HOME""), ""R"", ""lib"")))
sparkR.session(master = ""local[*]"", sparkConfig = list(spark.driver.memory = ""2g""))

n <- 1E3
df <- as.data.frame(replicate(n, 1L, FALSE))
names(df) <- paste(""X"", 1:n, sep = """")

tbl <- as.DataFrame(df)
cache(tbl) # works fine without this
cl <- collect(tbl)

identical(df, cl) # FALSE
{code}

Although this is reproducible with SparkR, it seems more likely that this is an error in the Java / Scala Spark sources.

For posterity:

> sessionInfo()
R version 3.3.1 Patched (2016-07-30 r71015)
Platform: x86_64-apple-darwin13.4.0 (64-bit)
Running under: macOS Sierra (10.12)
",,kevinushey,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 05 19:49:35 UTC 2016,,,,,,,,,,"0|i34bo7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/16 18:03;shivaram;Thanks for the bug report - I can't seem to reproduce this on a build from master branch or in the 2.0.1 RC4 that just passed the vote, but I am not sure what change actually fixed this. 

It'll be great if you could also verify whether 2.0.1 fixes your problem and if so we can mark this issue as resolved.;;;","03/Oct/16 18:17;kevinushey;I can confirm that everything is okay with Spark 2.0.2-SNAPSHOT (as retrieved from http://people.apache.org/~pwendell/spark-nightly/spark-master-bin/latest/).

I also reproduced this issue with Spark 1.6.2, so it might be worth investigating what fixed this issue, and if that fix should be backported to e.g. Spark 1.6.3. (not sure if long-term support of Spark 1.6 is intended);;;","03/Oct/16 19:22;shivaram;I don't know what the plans are for 1.6.3 -- I'm marking this issue as resolved with fix versions as 2.0.1 and 2.1.0. If we figure out what to backport, we can update the JIRA after merging to branch-1.6 etc.;;;","05/Oct/16 19:49;kevinushey;It looks like this was tracked + fixed in https://issues.apache.org/jira/browse/SPARK-16664, and the fix will be part of Spark 1.6.3. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot create view which includes interval arithmetic,SPARK-17750,13008989,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,andreasdamm,andreasdamm,30/Sep/16 21:26,06/Oct/16 23:11,14/Jul/23 06:29,06/Oct/16 16:44,2.0.0,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,,,,,,"Given table

create table dates (ts timestamp)

the following view creation SQL failes with Failed to analyze the canonicalized SQL. It is possible there is a bug in Spark.

create view test_dates as select ts + interval 1 day from dates
",,andreasdamm,apachespark,dongjoon,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 06 19:53:06 UTC 2016,,,,,,,,,,"0|i34blb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/16 23:20;dongjoon;Hi, [~andreasdamm].
It really does. I'll make a PR for this.;;;","01/Oct/16 03:20;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/15318;;;","01/Oct/16 17:45;dongjoon;Hi, [~andreasdamm].
You can see your cases in the test case of first commit . 

https://github.com/apache/spark/pull/15318/commits/a99a247b1efd433738eeb57195ea94fe8ec6250e;;;","06/Oct/16 16:44;smilegator;Issue resolved by pull request 15318
[https://github.com/apache/spark/pull/15318];;;","06/Oct/16 19:53;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/15383;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Launcher does not get failed state in Listener ,SPARK-17742,13008817,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,anshbansal,anshbansal,30/Sep/16 07:25,30/Aug/18 07:45,14/Jul/23 06:29,25/Aug/17 17:04,2.0.0,,,,,,,,2.3.0,,,,Spark Submit,,,,,,,,3,,,,,,"I tried to launch an application using the below code. This is dummy code to reproduce the problem. I tried exiting spark with status -1, throwing an exception etc. but in no case did the listener give me failed status. But if a spark job returns -1 or throws an exception from the main method it should be considered as a failure. 

{code}
package com.example;

import org.apache.spark.launcher.SparkAppHandle;
import org.apache.spark.launcher.SparkLauncher;

import java.io.IOException;

public class Main2 {

    public static void main(String[] args) throws IOException, InterruptedException {
        SparkLauncher launcher = new SparkLauncher()
                .setSparkHome(""/opt/spark2"")
                .setAppResource(""/home/aseem/projects/testsparkjob/build/libs/testsparkjob-1.0-SNAPSHOT.jar"")
                .setMainClass(""com.example.Main"")
                .setMaster(""local[2]"");

        launcher.startApplication(new MyListener());

        Thread.sleep(1000 * 60);
    }

}

class MyListener implements SparkAppHandle.Listener {

    @Override
    public void stateChanged(SparkAppHandle handle) {

        System.out.println(""state changed "" + handle.getState());
    }

    @Override
    public void infoChanged(SparkAppHandle handle) {
        System.out.println(""info changed "" + handle.getState());
    }
}
{code}

The spark job is 
{code}
package com.example;

import org.apache.spark.sql.SparkSession;
import java.io.IOException;

public class Main {

    public static void main(String[] args) throws IOException {
        SparkSession sparkSession = SparkSession
                .builder()
                .appName("""" + System.currentTimeMillis())
                .getOrCreate();


        try {
            for (int i = 0; i < 15; i++) {
                Thread.sleep(1000);
                System.out.println(""sleeping 1"");
            }
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
//        sparkSession.stop();

        System.exit(-1);
    }

}
{code}",,anshbansal,apachespark,daanvdn,devaraj,roman.kovalik,sleefd,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18241,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 30 07:45:05 UTC 2018,,,,,,,,,,"0|i34aj3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/16 07:35;anshbansal;I dug into the launcher code to see if I can figure out how it is working and see if I could find the bug. But when I reached LauncherServer's ServerConnection's handle method and found that this is socket programming I found it harder to find where the messages are coming from. Still trying to figure out but maybe someone who knows spark code better will find it easier to find the bug.;;;","03/Oct/16 18:19;vanzin;This code in ""LocalSchedulerBackend"" causes the issue you're seeing:

{code}
  override def stop() {
    stop(SparkAppHandle.State.FINISHED)
  }
{code}

That code is run both when you explicitly stop a SparkContext, or when the VM shuts down, via a shutdown hook. You could remove that line (and similar lines in other backends), and change so that if the child process exist with a 0 exit code, then the app is ""successful"". But need to make sure it still works with YARN (both client and cluster mode), since the logic to report app state is different there.;;;","04/Apr/17 07:24;daanvdn;Hi, is there already a fix for this issue? Because it effectively renders the SparkLauncher unusable.. Or does anybody know at least a workaround for it? Any thoughts [~anshbansal], [~vanzin] ? Thanks!;;;","04/Apr/17 07:32;anshbansal;[~daanvdn] We ended up using kafka messages to communicate to the web app that was using the launcher to launch the job  whether the job was complete or failed. Dumped Launcher's states as they are broken.;;;","04/Apr/17 09:29;daanvdn;Thanks for the advice [~anshbansal], I'll give that a try. 
A question to the spark developers: is a fix for this bug already on the roadmap for a next release of spark?

;;;","04/Apr/17 09:31;srowen;[~daanvdn] did you implement Marcelo's suggestion? this is pretty WYSIWYG -- if you want to push a change, investigate it and test and then open a PR. If you don't see anything here, nobody's working on it.;;;","04/Apr/17 10:57;daanvdn;Hi [~srowen], unfortunately I'm a java dev, so changing stuff in a scala code base is quite a bit out of my comfort zone. In case that changes, I will definitely look into it :-), 

Cheers;;;","10/Jul/17 07:10;sleefd;Is anybody working on this? If no, I can help.;;;","15/Aug/17 18:26;vanzin;https://github.com/apache/spark/pull/18877;;;","21/Aug/17 21:22;vanzin;Reopening temporarily for a follow up fix:
https://github.com/apache/spark/pull/19012;;;","01/Sep/17 18:02;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/19012;;;","30/Aug/18 07:45;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/18877;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Grammar to parse top level and nested data fields separately,SPARK-17741,13008796,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,jiangxb1987,tejasp,tejasp,30/Sep/16 03:09,10/Oct/16 05:03,14/Jul/23 06:29,10/Oct/16 05:03,2.0.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,1,,,,,,"Based on discussion over the dev list:

{noformat}
Is there any reason why Spark SQL supports ""<column name>"" "":"" ""<data type>"" while specifying columns ?
eg. sql(""CREATE TABLE t1 (column1:INT)"") works fine. 
Here is relevant snippet in the grammar [0]:

```
colType
    : identifier ':'? dataType (COMMENT STRING)?
    ;
```

I do not see MySQL[1], Hive[2], Presto[3] and PostgreSQL [4] supporting "":"" while specifying columns.
They all use space as a delimiter.

[0] : https://github.com/apache/spark/blob/master/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4#L596
[1] : http://dev.mysql.com/doc/refman/5.7/en/create-table.html
[2] : https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-CreateTable
[3] : https://prestodb.io/docs/current/sql/create-table.html
[4] : https://www.postgresql.org/docs/9.1/static/sql-createtable.html
{noformat}

Herman's response:

{noformat}
This is because we use the same rule to parse top level and nested data fields. For example:

create table tbl_x(
  id bigint,
  nested struct<col1:string,col2:string>
)

Shows both syntaxes. We should split this rule in a top-level and nested rule.
{noformat}",,apachespark,dongjoon,jiangxb1987,tejasp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 04 17:22:05 UTC 2016,,,,,,,,,,"0|i34aef:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/16 04:19;jiangxb1987;I'm working on this - Perhaps we can also add more testcases in `SparkSqlParserSuite`.;;;","04/Oct/16 17:22;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/15346;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InferFiltersFromConstraints rule never terminates for query,SPARK-17733,13008709,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,jiangxb1987,joshrosen,joshrosen,29/Sep/16 20:20,10/May/17 21:31,14/Jul/23 06:29,26/Oct/16 15:11,2.0.0,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,,,,,,"The following (complicated) example becomes stuck in the {{InferFiltersFromConstraints}} rule and never runs. However, it doesn't fail with a stack overflow and doesn't hit the limit on optimization passes, so I think there's some sort of non-obvious infinite loop within the rule itself.


{code:title=Table Creation|borderStyle=solid}
 -- Query #0

CREATE TEMPORARY VIEW table_4(float_col_1, boolean_col_2, decimal2610_col_3, boolean_col_4, timestamp_col_5, boolean_col_6, bigint_col_7, timestamp_col_8) AS VALUES
  (CAST(21.920416 AS FLOAT), false, -182.0700000000BD, true, TIMESTAMP('1996-10-24 00:00:00.0'), true, CAST(-993 AS BIGINT), TIMESTAMP('2007-01-13 00:00:00.0')),
  (CAST(722.4906 AS FLOAT), true, 497.5400000000BD, true, TIMESTAMP('2015-12-14 00:00:00.0'), false, CAST(268 AS BIGINT), TIMESTAMP('2021-04-19 00:00:00.0')),
  (CAST(534.9996 AS FLOAT), true, -470.8300000000BD, true, TIMESTAMP('1996-01-31 00:00:00.0'), false, CAST(-910 AS BIGINT), TIMESTAMP('2019-10-16 00:00:00.0')),
  (CAST(-289.6454 AS FLOAT), false, 892.2500000000BD, false, TIMESTAMP('2014-03-14 00:00:00.0'), false, CAST(-462 AS BIGINT), CAST(NULL AS TIMESTAMP)),
  (CAST(46.395535 AS FLOAT), true, -662.8900000000BD, true, TIMESTAMP('2000-10-16 00:00:00.0'), false, CAST(-656 AS BIGINT), TIMESTAMP('2024-09-01 00:00:00.0')),
  (CAST(-555.36285 AS FLOAT), true, -938.9300000000BD, true, TIMESTAMP('2007-04-10 00:00:00.0'), true, CAST(252 AS BIGINT), TIMESTAMP('2028-12-03 00:00:00.0')),
  (CAST(826.29004 AS FLOAT), true, 53.1800000000BD, false, TIMESTAMP('2004-06-11 00:00:00.0'), false, CAST(437 AS BIGINT), TIMESTAMP('1994-04-04 00:00:00.0')),
  (CAST(-15.276999 AS FLOAT), CAST(NULL AS BOOLEAN), -889.3100000000BD, true, TIMESTAMP('1991-05-23 00:00:00.0'), true, CAST(226 AS BIGINT), TIMESTAMP('2023-07-08 00:00:00.0')),
  (CAST(385.27386 AS FLOAT), CAST(NULL AS BOOLEAN), -9.9500000000BD, false, TIMESTAMP('2022-10-22 00:00:00.0'), true, CAST(430 AS BIGINT), TIMESTAMP('2013-09-29 00:00:00.0')),
  (CAST(988.7868 AS FLOAT), CAST(NULL AS BOOLEAN), 715.1700000000BD, false, TIMESTAMP('2026-10-03 00:00:00.0'), true, CAST(-696 AS BIGINT), TIMESTAMP('1990-08-10 00:00:00.0'))
     ;


 -- Query #1

CREATE TEMPORARY VIEW table_1(double_col_1, boolean_col_2, timestamp_col_3, smallint_col_4, boolean_col_5, int_col_6, timestamp_col_7, varchar0008_col_8, int_col_9, string_col_10) AS VALUES
  (CAST(-147.818640624 AS DOUBLE), CAST(NULL AS BOOLEAN), TIMESTAMP('2012-10-19 00:00:00.0'), CAST(9 AS SMALLINT), false, 77, TIMESTAMP('2014-07-01 00:00:00.0'), '-945', -646, '722'),
  (CAST(594.195125271 AS DOUBLE), false, TIMESTAMP('2016-12-04 00:00:00.0'), CAST(NULL AS SMALLINT), CAST(NULL AS BOOLEAN), CAST(NULL AS INT), TIMESTAMP('1999-12-26 00:00:00.0'), '250', -861, '55'),
  (CAST(-454.171126363 AS DOUBLE), false, TIMESTAMP('2008-12-13 00:00:00.0'), CAST(NULL AS SMALLINT), false, -783, TIMESTAMP('2010-05-28 00:00:00.0'), '211', -959, CAST(NULL AS STRING)),
  (CAST(437.670945524 AS DOUBLE), true, TIMESTAMP('2011-10-16 00:00:00.0'), CAST(952 AS SMALLINT), true, 297, TIMESTAMP('2013-01-13 00:00:00.0'), '262', CAST(NULL AS INT), '936'),
  (CAST(-387.226759334 AS DOUBLE), false, TIMESTAMP('2019-10-03 00:00:00.0'), CAST(-496 AS SMALLINT), CAST(NULL AS BOOLEAN), -925, TIMESTAMP('2028-06-27 00:00:00.0'), '-657', 948, '18'),
  (CAST(-306.138230875 AS DOUBLE), true, TIMESTAMP('1997-10-07 00:00:00.0'), CAST(332 AS SMALLINT), false, 744, TIMESTAMP('1990-09-22 00:00:00.0'), '-345', 566, '-574'),
  (CAST(675.402140308 AS DOUBLE), false, TIMESTAMP('2017-06-26 00:00:00.0'), CAST(972 AS SMALLINT), true, CAST(NULL AS INT), TIMESTAMP('2026-06-10 00:00:00.0'), '518', 683, '-320'),
  (CAST(734.839647174 AS DOUBLE), true, TIMESTAMP('1995-06-01 00:00:00.0'), CAST(-792 AS SMALLINT), CAST(NULL AS BOOLEAN), CAST(NULL AS INT), TIMESTAMP('2021-07-11 00:00:00.0'), '-318', 564, '142'),
  (CAST(-836.513475295 AS DOUBLE), true, TIMESTAMP('2027-01-02 00:00:00.0'), CAST(-446 AS SMALLINT), true, CAST(NULL AS INT), TIMESTAMP('1993-09-01 00:00:00.0'), '771', CAST(NULL AS INT), '977'),
  (CAST(-768.883638815 AS DOUBLE), false, TIMESTAMP('1994-02-11 00:00:00.0'), CAST(-244 AS SMALLINT), true, -493, TIMESTAMP('1994-01-02 00:00:00.0'), '-921', CAST(NULL AS INT), '-409')
     ;


 -- Query #2

CREATE TEMPORARY VIEW table_5(float_col_1, varchar0138_col_2, string_col_3, decimal2211_col_4, float_col_5, string_col_6, timestamp_col_7, varchar0207_col_8) AS VALUES
  (CAST(-885.7606 AS FLOAT), '-740', '680', -929.06000000000BD, CAST(NULL AS FLOAT), '-915', TIMESTAMP('1994-09-12 00:00:00.0'), CAST(NULL AS STRING)),
  (CAST(NULL AS FLOAT), '489', '692', -220.60000000000BD, CAST(939.18964 AS FLOAT), '-514', CAST(NULL AS TIMESTAMP), '181'),
  (CAST(210.7055 AS FLOAT), '44', CAST(NULL AS STRING), -174.70000000000BD, CAST(760.21045 AS FLOAT), '325', TIMESTAMP('2019-09-25 00:00:00.0'), '505'),
  (CAST(952.8074 AS FLOAT), '838', '705', CAST(NULL AS DECIMAL(22,11)), CAST(NULL AS FLOAT), '-62', TIMESTAMP('2029-05-22 00:00:00.0'), CAST(NULL AS STRING)),
  (CAST(-113.300446 AS FLOAT), '-210', '765', CAST(NULL AS DECIMAL(22,11)), CAST(-819.2468 AS FLOAT), '-829', CAST(NULL AS TIMESTAMP), '465'),
  (CAST(-739.9902 AS FLOAT), '614', '-393', -509.22000000000BD, CAST(-339.78568 AS FLOAT), '568', TIMESTAMP('2013-05-14 00:00:00.0'), '305'),
  (CAST(976.0611 AS FLOAT), '670', '71', 663.23000000000BD, CAST(-685.9362 AS FLOAT), '42', CAST(NULL AS TIMESTAMP), '150'),
  (CAST(NULL AS FLOAT), '302', '-404', -349.42000000000BD, CAST(2.113715 AS FLOAT), '-703', TIMESTAMP('2003-01-09 00:00:00.0'), '-863'),
  (CAST(-40.604317 AS FLOAT), '856', '632', 844.57000000000BD, CAST(-730.8376 AS FLOAT), '151', TIMESTAMP('2021-05-11 00:00:00.0'), '494'),
  (CAST(884.62714 AS FLOAT), '-195', '960', -664.40000000000BD, CAST(374.4844 AS FLOAT), '814', TIMESTAMP('2006-06-12 00:00:00.0'), '-900')
     ;


 -- Query #3

CREATE TEMPORARY VIEW table_2(bigint_col_1, boolean_col_2, double_col_3, double_col_4, double_col_5, varchar0164_col_6) AS VALUES
  (CAST(-374 AS BIGINT), CAST(NULL AS BOOLEAN), CAST(939.626553676 AS DOUBLE), CAST(-777.275379746 AS DOUBLE), CAST(235.613760023 AS DOUBLE), '86'),
  (CAST(324 AS BIGINT), true, CAST(-507.23760783 AS DOUBLE), CAST(NULL AS DOUBLE), CAST(966.753434439 AS DOUBLE), '304'),
  (CAST(882 AS BIGINT), false, CAST(-366.529706229 AS DOUBLE), CAST(787.000491043 AS DOUBLE), CAST(-331.333188698 AS DOUBLE), '158'),
  (CAST(-510 AS BIGINT), CAST(NULL AS BOOLEAN), CAST(-855.344932257 AS DOUBLE), CAST(-858.167264921 AS DOUBLE), CAST(NULL AS DOUBLE), '-419'),
  (CAST(-13 AS BIGINT), false, CAST(589.966987492 AS DOUBLE), CAST(NULL AS DOUBLE), CAST(-653.515783257 AS DOUBLE), '970'),
  (CAST(-361 AS BIGINT), true, CAST(-413.021011259 AS DOUBLE), CAST(-716.638705947 AS DOUBLE), CAST(-936.480108205 AS DOUBLE), '807'),
  (CAST(815 AS BIGINT), true, CAST(-643.690268711 AS DOUBLE), CAST(-684.206112496 AS DOUBLE), CAST(335.557479371 AS DOUBLE), '-872'),
  (CAST(617 AS BIGINT), true, CAST(-93.3806447556 AS DOUBLE), CAST(-322.66171021 AS DOUBLE), CAST(-951.18299435 AS DOUBLE), '-167'),
  (CAST(-876 AS BIGINT), false, CAST(-481.774062168 AS DOUBLE), CAST(-204.40537387 AS DOUBLE), CAST(224.889845986 AS DOUBLE), '-986'),
  (CAST(2 AS BIGINT), false, CAST(462.843898322 AS DOUBLE), CAST(-9.85549856798 AS DOUBLE), CAST(-549.875829922 AS DOUBLE), '121')
     ;


 -- Query #4

CREATE TEMPORARY VIEW table_3(string_col_1, float_col_2, timestamp_col_3, boolean_col_4, timestamp_col_5, decimal3317_col_6) AS VALUES
  ('-450', CAST(-903.6053 AS FLOAT), CAST(NULL AS TIMESTAMP), true, TIMESTAMP('2020-08-22 00:00:00.0'), -376.39000000000000000BD),
  ('698', CAST(402.56534 AS FLOAT), TIMESTAMP('2013-10-13 00:00:00.0'), true, TIMESTAMP('2012-11-06 00:00:00.0'), -498.81000000000000000BD),
  ('139', CAST(-895.7336 AS FLOAT), TIMESTAMP('2018-09-08 00:00:00.0'), true, TIMESTAMP('2019-03-13 00:00:00.0'), CAST(NULL AS DECIMAL(33,17))),
  ('616', CAST(-464.9475 AS FLOAT), TIMESTAMP('2028-05-18 00:00:00.0'), true, TIMESTAMP('2016-05-22 00:00:00.0'), -109.88000000000000000BD),
  ('943', CAST(605.42303 AS FLOAT), TIMESTAMP('1996-08-04 00:00:00.0'), false, TIMESTAMP('2028-05-18 00:00:00.0'), 201.36000000000000000BD),
  ('-764', CAST(-503.56726 AS FLOAT), TIMESTAMP('1990-02-28 00:00:00.0'), false, CAST(NULL AS TIMESTAMP), 211.25000000000000000BD),
  ('-587', CAST(84.67886 AS FLOAT), TIMESTAMP('2013-06-06 00:00:00.0'), true, TIMESTAMP('2022-05-07 00:00:00.0'), 90.75000000000000000BD),
  ('712', CAST(141.08926 AS FLOAT), TIMESTAMP('2001-05-12 00:00:00.0'), true, TIMESTAMP('2019-11-22 00:00:00.0'), 929.89000000000000000BD),
  ('948', CAST(0.74294764 AS FLOAT), TIMESTAMP('2002-06-14 00:00:00.0'), false, TIMESTAMP('1990-01-13 00:00:00.0'), -100.90000000000000000BD),
  ('-201', CAST(366.82578 AS FLOAT), TIMESTAMP('2015-11-28 00:00:00.0'), false, CAST(NULL AS TIMESTAMP), 196.33000000000000000BD)
     ;
{code}

{code:title=Query|borderStyle=solid}
SELECT
t1.int_col_2,
(t1.bigint_col_7) / (t2.double_col_3) AS float_col,
TRIM(t2.varchar0164_col_6) AS char_col
FROM (
SELECT
COALESCE(t1.bigint_col_7, t2.bigint_col_7, t2.bigint_col_7) AS int_col,
t1.bigint_col_7,
t2.bigint_col_7 AS int_col_1,
t1.bigint_col_7 AS int_col_2,
COALESCE(t2.bigint_col_7, (COALESCE(t1.bigint_col_7, t2.bigint_col_7, t2.bigint_col_7)) - (t1.bigint_col_7), MIN(t2.bigint_col_7)) AS int_col_3
FROM table_4 t1
INNER JOIN table_4 t2 ON ((t2.timestamp_col_5) = (t1.timestamp_col_8)) AND ((t2.decimal2610_col_3) = (t1.decimal2610_col_3))
WHERE
(t1.bigint_col_7) IN (t2.bigint_col_7, t2.bigint_col_7)
GROUP BY
COALESCE(t1.bigint_col_7, t2.bigint_col_7, t2.bigint_col_7),
t1.bigint_col_7,
t2.bigint_col_7,
t1.bigint_col_7
HAVING
(MIN(COALESCE(t1.bigint_col_7, t2.bigint_col_7, t2.bigint_col_7))) NOT IN (423.13, t2.bigint_col_7)
UNION
SELECT
MIN((436) * (927)) OVER (ORDER BY (t2.int_col_6) * (NULL) DESC, (t1.smallint_col_4) - (t2.int_col_6) DESC ROWS BETWEEN 96 PRECEDING AND 16 PRECEDING) AS int_col,
(t2.int_col_6) * (NULL) AS decimal_col,
(t1.smallint_col_4) - (t2.int_col_6) AS int_col_1,
LAG((449) * (-157.519107824), 46) OVER (ORDER BY (t2.int_col_6) * (NULL) DESC, (t1.smallint_col_4) - (t2.int_col_6) DESC) AS float_col,
AVG((669) - (-773)) OVER (ORDER BY (t2.int_col_6) * (NULL) ASC, (t1.smallint_col_4) - (t2.int_col_6) ASC ROWS BETWEEN CURRENT ROW AND 62 FOLLOWING) AS float_col_1
FROM table_1 t1
INNER JOIN table_1 t2 ON ((t2.smallint_col_4) = (t1.int_col_9)) AND ((t2.smallint_col_4) = (t1.int_col_6))
) t1
INNER JOIN table_2 t2 ON (((t2.bigint_col_1) = (t1.bigint_col_7)) AND ((t2.bigint_col_1) = (t1.int_col))) AND ((t2.bigint_col_1) = (t1.int_col_1))
GROUP BY
t1.int_col_2,
(t1.bigint_col_7) / (t2.double_col_3),
TRIM(t2.varchar0164_col_6)
{code}

I attached YourKit to my Spark process and recorded some stack traces. See the attached screenshots showing the distribution of time for the hung query.

",,apachespark,jiangxb1987,joshrosen,maropu,rxin,sameerag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20700,,,,,,,,,,,,,,"29/Sep/16 20:21;joshrosen;SparkSubmit-2016-09-29-1_snapshot___Users_joshrosen_Snapshots__-_YourKit_Java_Profiler_2013_build_13088_-_64-bit.png;https://issues.apache.org/jira/secure/attachment/12830985/SparkSubmit-2016-09-29-1_snapshot___Users_joshrosen_Snapshots__-_YourKit_Java_Profiler_2013_build_13088_-_64-bit.png","29/Sep/16 20:22;joshrosen;constraints.png;https://issues.apache.org/jira/secure/attachment/12830986/constraints.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 01 05:04:05 UTC 2016,,,,,,,,,,"0|i349v3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/16 20:32;joshrosen;I think the problem is that {{InferFiltersFromConstraints}} appears to be constructing bigger-and-bigger constraints, causing it to become slower and slower. If I let this run long enough I imagine that it would OOM.

Here's trace logging showing the plans after the first two applications of InferFiltersFromConstraints

{code}
16/09/29 13:26:17 TRACE SparkOptimizer:
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints ===
 Aggregate [count(1) AS count#179L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Aggregate [count(1) AS count#179L]
 +- Project                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            +- Project
    +- Aggregate [int_col_2#165, (cast(bigint_col_7#14L as double) / cast(double_col_3#63 as double)), trim(varchar0164_col_6#66)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        +- Aggregate [int_col_2#165, (cast(bigint_col_7#14L as double) / cast(double_col_3#63 as double)), trim(varchar0164_col_6#66)]
       +- Project [bigint_col_7#14L, int_col_2#165, double_col_3#63, varchar0164_col_6#66]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   +- Project [bigint_col_7#14L, int_col_2#165, double_col_3#63, varchar0164_col_6#66]
!         +- Join Inner, (((bigint_col_1#61L = bigint_col_7#14L) && (bigint_col_1#61L = int_col#81L)) && (bigint_col_1#61L = int_col_1#82L))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    +- Join Inner, (((((((int_col#81L = int_col_1#82L) && (bigint_col_7#14L = int_col#81L)) && (bigint_col_7#14L = int_col_1#82L)) && isnotnull(bigint_col_7#14L)) && isnotnull(int_col#81L)) && isnotnull(int_col_1#82L)) && (((bigint_col_1#61L = bigint_col_7#14L) && (bigint_col_1#61L = int_col#81L)) && (bigint_col_1#61L = int_col_1#82L)))
             :- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              :- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165]
             :  +- Aggregate [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165, int_col_3#166], [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           :  +- Aggregate [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165, int_col_3#166], [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165]
             :     +- Union                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        :     +- Union
             :        :- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, cast(int_col_2#83L as decimal(29,9)) AS int_col_2#165, cast(int_col_3#84L as double) AS int_col_3#166]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             :        :- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, cast(int_col_2#83L as decimal(29,9)) AS int_col_2#165, cast(int_col_3#84L as double) AS int_col_3#166]
             :        :  +- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#83L, int_col_3#84L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   :        :  +- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#83L, int_col_3#84L]
             :        :     +- Filter NOT cast(min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L as decimal(22,2)) IN (cast(423.13 as decimal(22,2)),cast(bigint_col_7#140L as decimal(22,2)))                                                                                                                                                                                                                                                                                                                                                                                                                                                            :        :     +- Filter NOT cast(min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L as decimal(22,2)) IN (cast(423.13 as decimal(22,2)),cast(bigint_col_7#140L as decimal(22,2)))
             :        :        +- Aggregate [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L), bigint_col_7#14L, bigint_col_7#140L], [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) AS int_col#81L, bigint_col_7#14L, bigint_col_7#140L AS int_col_1#82L, bigint_col_7#14L AS int_col_2#83L, coalesce(bigint_col_7#140L, (coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) - bigint_col_7#14L), min(bigint_col_7#140L)) AS int_col_3#84L, min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L)) AS min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L, bigint_col_7#140L]               :        :        +- Aggregate [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L), bigint_col_7#14L, bigint_col_7#140L], [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) AS int_col#81L, bigint_col_7#14L, bigint_col_7#140L AS int_col_1#82L, bigint_col_7#14L AS int_col_2#83L, coalesce(bigint_col_7#140L, (coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) - bigint_col_7#14L), min(bigint_col_7#140L)) AS int_col_3#84L, min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L)) AS min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L, bigint_col_7#140L]
             :        :           +- Project [bigint_col_7#14L, bigint_col_7#140L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 :        :           +- Project [bigint_col_7#14L, bigint_col_7#140L]
!            :        :              +- Join Inner, (bigint_col_7#14L IN (bigint_col_7#140L,bigint_col_7#140L) && ((timestamp_col_5#138 = timestamp_col_8#15) && (decimal2610_col_3#136 = decimal2610_col_3#10)))                                                                                                                                                                                                                                                                                                                                                                                                                                                                  :        :              +- Join Inner, ((isnotnull(timestamp_col_8#15) && isnotnull(timestamp_col_5#138)) && (bigint_col_7#14L IN (bigint_col_7#140L,bigint_col_7#140L) && ((timestamp_col_5#138 = timestamp_col_8#15) && (decimal2610_col_3#136 = decimal2610_col_3#10))))
             :        :                 :- Project [decimal2610_col_3#10, bigint_col_7#14L, timestamp_col_8#15]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    :        :                 :- Project [decimal2610_col_3#10, bigint_col_7#14L, timestamp_col_8#15]
             :        :                 :  +- Project [col3#2 AS decimal2610_col_3#10, col7#6L AS bigint_col_7#14L, col8#7 AS timestamp_col_8#15]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  :        :                 :  +- Project [col3#2 AS decimal2610_col_3#10, col7#6L AS bigint_col_7#14L, col8#7 AS timestamp_col_8#15]
             :        :                 :     +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   :        :                 :     +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]
             :        :                 +- Project [decimal2610_col_3#136, timestamp_col_5#138, bigint_col_7#140L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 :        :                 +- Project [decimal2610_col_3#136, timestamp_col_5#138, bigint_col_7#140L]
             :        :                    +- Project [col3#2 AS decimal2610_col_3#136, col5#4 AS timestamp_col_5#138, col7#6L AS bigint_col_7#140L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               :        :                    +- Project [col3#2 AS decimal2610_col_3#136, col5#4 AS timestamp_col_5#138, col7#6L AS bigint_col_7#140L]
             :        :                       +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   :        :                       +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]
             :        +- Project [cast(int_col#85 as bigint) AS int_col#167L, cast(decimal_col#86 as bigint) AS decimal_col#168L, cast(int_col_1#87 as bigint) AS int_col_1#169L, cast(float_col#88 as decimal(29,9)) AS float_col#170, float_col_1#89]                                                                                                                                                                                                                                                                                                                                                                                                                            :        +- Project [cast(int_col#85 as bigint) AS int_col#167L, cast(decimal_col#86 as bigint) AS decimal_col#168L, cast(int_col_1#87 as bigint) AS int_col_1#169L, cast(float_col#88 as decimal(29,9)) AS float_col#170, float_col_1#89]
             :           +- Project [int_col#85, decimal_col#86, int_col_1#87, float_col#88, float_col_1#89]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       :           +- Project [int_col#85, decimal_col#86, int_col_1#87, float_col#88, float_col_1#89]
             :              +- Project [decimal_col#86, int_col_1#87, float_col_1#89, int_col#85, float_col#88, int_col#85, float_col#88, float_col_1#89]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          :              +- Project [decimal_col#86, int_col_1#87, float_col_1#89, int_col#85, float_col#88, int_col#85, float_col#88, float_col_1#89]
             :                 +- Project [decimal_col#86, int_col_1#87, _w0#158, _w1#159, _w2#160, _w3#161, _w4#162, _w5#163, float_col_1#89, int_col#85, float_col#88, int_col#85, float_col#88, float_col_1#89]                                                                                                                                                                                                                                                                                                                                                                                                                                                                 :                 +- Project [decimal_col#86, int_col_1#87, _w0#158, _w1#159, _w2#160, _w3#161, _w4#162, _w5#163, float_col_1#89, int_col#85, float_col#88, int_col#85, float_col#88, float_col_1#89]
             :                    +- Window [lag(CheckOverflow((promote_precision(cast(cast(449 as decimal(10,0)) as decimal(19,9))) * promote_precision(cast(-157.519107824 as decimal(19,9)))), DecimalType(23,9)), 46, null) windowspecdefinition(_w2#160 DESC, _w3#161 DESC, ROWS BETWEEN 46 PRECEDING AND 46 PRECEDING) AS float_col#88], [_w2#160 DESC, _w3#161 DESC]                                                                                                                                                                                                                                                                                                        :                    +- Window [lag(CheckOverflow((promote_precision(cast(cast(449 as decimal(10,0)) as decimal(19,9))) * promote_precision(cast(-157.519107824 as decimal(19,9)))), DecimalType(23,9)), 46, null) windowspecdefinition(_w2#160 DESC, _w3#161 DESC, ROWS BETWEEN 46 PRECEDING AND 46 PRECEDING) AS float_col#88], [_w2#160 DESC, _w3#161 DESC]
             :                       +- Window [min((436 * 927)) windowspecdefinition(_w0#158 DESC, _w1#159 DESC, ROWS BETWEEN 96 PRECEDING AND 16 PRECEDING) AS int_col#85], [_w0#158 DESC, _w1#159 DESC]                                                                                                                                                                                                                                                                                                                                                                                                                                                                         :                       +- Window [min((436 * 927)) windowspecdefinition(_w0#158 DESC, _w1#159 DESC, ROWS BETWEEN 96 PRECEDING AND 16 PRECEDING) AS int_col#85], [_w0#158 DESC, _w1#159 DESC]
             :                          +- Window [avg(cast((669 - -773) as bigint)) windowspecdefinition(_w4#162 ASC, _w5#163 ASC, ROWS BETWEEN CURRENT ROW AND 62 FOLLOWING) AS float_col_1#89], [_w4#162 ASC, _w5#163 ASC]                                                                                                                                                                                                                                                                                                                                                                                                                                                      :                          +- Window [avg(cast((669 - -773) as bigint)) windowspecdefinition(_w4#162 ASC, _w5#163 ASC, ROWS BETWEEN CURRENT ROW AND 62 FOLLOWING) AS float_col_1#89], [_w4#162 ASC, _w5#163 ASC]
             :                             +- Project [(int_col_6#147 * cast(null as int)) AS decimal_col#86, (cast(smallint_col_4#30 as int) - int_col_6#147) AS int_col_1#87, (int_col_6#147 * cast(null as int)) AS _w0#158, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w1#159, (int_col_6#147 * cast(null as int)) AS _w2#160, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w3#161, (int_col_6#147 * cast(null as int)) AS _w4#162, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w5#163]                                                                                                                                                             :                             +- Project [(int_col_6#147 * cast(null as int)) AS decimal_col#86, (cast(smallint_col_4#30 as int) - int_col_6#147) AS int_col_1#87, (int_col_6#147 * cast(null as int)) AS _w0#158, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w1#159, (int_col_6#147 * cast(null as int)) AS _w2#160, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w3#161, (int_col_6#147 * cast(null as int)) AS _w4#162, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w5#163]
!            :                                +- Join Inner, ((cast(smallint_col_4#145 as int) = int_col_9#35) && (cast(smallint_col_4#145 as int) = int_col_6#32))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                :                                +- Join Inner, (((isnotnull(smallint_col_4#145) && isnotnull(int_col_6#32)) && isnotnull(int_col_9#35)) && ((cast(smallint_col_4#145 as int) = int_col_9#35) && (cast(smallint_col_4#145 as int) = int_col_6#32)))
             :                                   :- Project [smallint_col_4#30, int_col_6#32, int_col_9#35]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        :                                   :- Project [smallint_col_4#30, int_col_6#32, int_col_9#35]
             :                                   :  +- Project [col4#20 AS smallint_col_4#30, col6#22 AS int_col_6#32, col9#25 AS int_col_9#35]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    :                                   :  +- Project [col4#20 AS smallint_col_4#30, col6#22 AS int_col_6#32, col9#25 AS int_col_9#35]
             :                                   :     +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                :                                   :     +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]
             :                                   +- Project [smallint_col_4#145, int_col_6#147]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    :                                   +- Project [smallint_col_4#145, int_col_6#147]
             :                                      +- Project [col4#20 AS smallint_col_4#145, col6#22 AS int_col_6#147]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           :                                      +- Project [col4#20 AS smallint_col_4#145, col6#22 AS int_col_6#147]
             :                                         +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                :                                         +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]
             +- Project [bigint_col_1#61L, double_col_3#63, varchar0164_col_6#66]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  +- Project [bigint_col_1#61L, double_col_3#63, varchar0164_col_6#66]
                +- Project [col1#55L AS bigint_col_1#61L, col3#57 AS double_col_3#63, col6#60 AS varchar0164_col_6#66]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                +- Project [col1#55L AS bigint_col_1#61L, col3#57 AS double_col_3#63, col6#60 AS varchar0164_col_6#66]
                   +- LocalRelation [col1#55L, col2#56, col3#57, col4#58, col5#59, col6#60]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              +- LocalRelation [col1#55L, col2#56, col3#57, col4#58, col5#59, col6#60]

16/09/29 13:26:17 TRACE SparkOptimizer:
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.CollapseProject ===
 Aggregate [count(1) AS count#179L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Aggregate [count(1) AS count#179L]
!+- Project                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            +- Aggregate [int_col_2#165, (cast(bigint_col_7#14L as double) / cast(double_col_3#63 as double)), trim(varchar0164_col_6#66)]
!   +- Aggregate [int_col_2#165, (cast(bigint_col_7#14L as double) / cast(double_col_3#63 as double)), trim(varchar0164_col_6#66)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        +- Project [bigint_col_7#14L, int_col_2#165, double_col_3#63, varchar0164_col_6#66]
!      +- Project [bigint_col_7#14L, int_col_2#165, double_col_3#63, varchar0164_col_6#66]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   +- Join Inner, (((((((int_col#81L = int_col_1#82L) && (bigint_col_7#14L = int_col#81L)) && (bigint_col_7#14L = int_col_1#82L)) && isnotnull(bigint_col_7#14L)) && isnotnull(int_col#81L)) && isnotnull(int_col_1#82L)) && (((bigint_col_1#61L = bigint_col_7#14L) && (bigint_col_1#61L = int_col#81L)) && (bigint_col_1#61L = int_col_1#82L)))
!         +- Join Inner, (((((((int_col#81L = int_col_1#82L) && (bigint_col_7#14L = int_col#81L)) && (bigint_col_7#14L = int_col_1#82L)) && isnotnull(bigint_col_7#14L)) && isnotnull(int_col#81L)) && isnotnull(int_col_1#82L)) && (((bigint_col_1#61L = bigint_col_7#14L) && (bigint_col_1#61L = int_col#81L)) && (bigint_col_1#61L = int_col_1#82L)))                                                                                                                                                                                                                                                                                                                        :- Aggregate [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165, int_col_3#166], [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165]
!            :- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           :  +- Union
!            :  +- Aggregate [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165, int_col_3#166], [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        :     :- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, cast(int_col_2#83L as decimal(29,9)) AS int_col_2#165, cast(int_col_3#84L as double) AS int_col_3#166]
!            :     +- Union                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     :     :  +- Filter NOT cast(min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L as decimal(22,2)) IN (cast(423.13 as decimal(22,2)),cast(bigint_col_7#140L as decimal(22,2)))
!            :        :- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, cast(int_col_2#83L as decimal(29,9)) AS int_col_2#165, cast(int_col_3#84L as double) AS int_col_3#166]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          :     :     +- Aggregate [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L), bigint_col_7#14L, bigint_col_7#140L], [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) AS int_col#81L, bigint_col_7#14L, bigint_col_7#140L AS int_col_1#82L, bigint_col_7#14L AS int_col_2#83L, coalesce(bigint_col_7#140L, (coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) - bigint_col_7#14L), min(bigint_col_7#140L)) AS int_col_3#84L, min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L)) AS min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L, bigint_col_7#140L]
!            :        :  +- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#83L, int_col_3#84L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                :     :        +- Project [bigint_col_7#14L, bigint_col_7#140L]
!            :        :     +- Filter NOT cast(min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L as decimal(22,2)) IN (cast(423.13 as decimal(22,2)),cast(bigint_col_7#140L as decimal(22,2)))                                                                                                                                                                                                                                                                                                                                                                                                                                                         :     :           +- Join Inner, ((isnotnull(timestamp_col_8#15) && isnotnull(timestamp_col_5#138)) && (bigint_col_7#14L IN (bigint_col_7#140L,bigint_col_7#140L) && ((timestamp_col_5#138 = timestamp_col_8#15) && (decimal2610_col_3#136 = decimal2610_col_3#10))))
!            :        :        +- Aggregate [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L), bigint_col_7#14L, bigint_col_7#140L], [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) AS int_col#81L, bigint_col_7#14L, bigint_col_7#140L AS int_col_1#82L, bigint_col_7#14L AS int_col_2#83L, coalesce(bigint_col_7#140L, (coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) - bigint_col_7#14L), min(bigint_col_7#140L)) AS int_col_3#84L, min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L)) AS min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L, bigint_col_7#140L]            :     :              :- Project [col3#2 AS decimal2610_col_3#10, col7#6L AS bigint_col_7#14L, col8#7 AS timestamp_col_8#15]
!            :        :           +- Project [bigint_col_7#14L, bigint_col_7#140L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              :     :              :  +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]
!            :        :              +- Join Inner, ((isnotnull(timestamp_col_8#15) && isnotnull(timestamp_col_5#138)) && (bigint_col_7#14L IN (bigint_col_7#140L,bigint_col_7#140L) && ((timestamp_col_5#138 = timestamp_col_8#15) && (decimal2610_col_3#136 = decimal2610_col_3#10))))                                                                                                                                                                                                                                                                                                                                                                                        :     :              +- Project [col3#2 AS decimal2610_col_3#136, col5#4 AS timestamp_col_5#138, col7#6L AS bigint_col_7#140L]
!            :        :                 :- Project [decimal2610_col_3#10, bigint_col_7#14L, timestamp_col_8#15]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 :     :                 +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]
!            :        :                 :  +- Project [col3#2 AS decimal2610_col_3#10, col7#6L AS bigint_col_7#14L, col8#7 AS timestamp_col_8#15]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               :     +- Project [cast(int_col#85 as bigint) AS int_col#167L, cast(decimal_col#86 as bigint) AS decimal_col#168L, cast(int_col_1#87 as bigint) AS int_col_1#169L, cast(float_col#88 as decimal(29,9)) AS float_col#170, float_col_1#89]
!            :        :                 :     +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                :        +- Window [lag(CheckOverflow((promote_precision(cast(cast(449 as decimal(10,0)) as decimal(19,9))) * promote_precision(cast(-157.519107824 as decimal(19,9)))), DecimalType(23,9)), 46, null) windowspecdefinition(_w2#160 DESC, _w3#161 DESC, ROWS BETWEEN 46 PRECEDING AND 46 PRECEDING) AS float_col#88], [_w2#160 DESC, _w3#161 DESC]
!            :        :                 +- Project [decimal2610_col_3#136, timestamp_col_5#138, bigint_col_7#140L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              :           +- Window [min((436 * 927)) windowspecdefinition(_w0#158 DESC, _w1#159 DESC, ROWS BETWEEN 96 PRECEDING AND 16 PRECEDING) AS int_col#85], [_w0#158 DESC, _w1#159 DESC]
!            :        :                    +- Project [col3#2 AS decimal2610_col_3#136, col5#4 AS timestamp_col_5#138, col7#6L AS bigint_col_7#140L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            :              +- Window [avg(cast((669 - -773) as bigint)) windowspecdefinition(_w4#162 ASC, _w5#163 ASC, ROWS BETWEEN CURRENT ROW AND 62 FOLLOWING) AS float_col_1#89], [_w4#162 ASC, _w5#163 ASC]
!            :        :                       +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                :                 +- Project [(int_col_6#147 * cast(null as int)) AS decimal_col#86, (cast(smallint_col_4#30 as int) - int_col_6#147) AS int_col_1#87, (int_col_6#147 * cast(null as int)) AS _w0#158, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w1#159, (int_col_6#147 * cast(null as int)) AS _w2#160, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w3#161, (int_col_6#147 * cast(null as int)) AS _w4#162, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w5#163]
!            :        +- Project [cast(int_col#85 as bigint) AS int_col#167L, cast(decimal_col#86 as bigint) AS decimal_col#168L, cast(int_col_1#87 as bigint) AS int_col_1#169L, cast(float_col#88 as decimal(29,9)) AS float_col#170, float_col_1#89]                                                                                                                                                                                                                                                                                                                                                                                                                         :                    +- Join Inner, (((isnotnull(smallint_col_4#145) && isnotnull(int_col_6#32)) && isnotnull(int_col_9#35)) && ((cast(smallint_col_4#145 as int) = int_col_9#35) && (cast(smallint_col_4#145 as int) = int_col_6#32)))
!            :           +- Project [int_col#85, decimal_col#86, int_col_1#87, float_col#88, float_col_1#89]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    :                       :- Project [col4#20 AS smallint_col_4#30, col6#22 AS int_col_6#32, col9#25 AS int_col_9#35]
!            :              +- Project [decimal_col#86, int_col_1#87, float_col_1#89, int_col#85, float_col#88, int_col#85, float_col#88, float_col_1#89]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       :                       :  +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]
!            :                 +- Project [decimal_col#86, int_col_1#87, _w0#158, _w1#159, _w2#160, _w3#161, _w4#162, _w5#163, float_col_1#89, int_col#85, float_col#88, int_col#85, float_col#88, float_col_1#89]                                                                                                                                                                                                                                                                                                                                                                                                                                                              :                       +- Project [col4#20 AS smallint_col_4#145, col6#22 AS int_col_6#147]
!            :                    +- Window [lag(CheckOverflow((promote_precision(cast(cast(449 as decimal(10,0)) as decimal(19,9))) * promote_precision(cast(-157.519107824 as decimal(19,9)))), DecimalType(23,9)), 46, null) windowspecdefinition(_w2#160 DESC, _w3#161 DESC, ROWS BETWEEN 46 PRECEDING AND 46 PRECEDING) AS float_col#88], [_w2#160 DESC, _w3#161 DESC]                                                                                                                                                                                                                                                                                                     :                          +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]
!            :                       +- Window [min((436 * 927)) windowspecdefinition(_w0#158 DESC, _w1#159 DESC, ROWS BETWEEN 96 PRECEDING AND 16 PRECEDING) AS int_col#85], [_w0#158 DESC, _w1#159 DESC]                                                                                                                                                                                                                                                                                                                                                                                                                                                                      +- Project [col1#55L AS bigint_col_1#61L, col3#57 AS double_col_3#63, col6#60 AS varchar0164_col_6#66]
!            :                          +- Window [avg(cast((669 - -773) as bigint)) windowspecdefinition(_w4#162 ASC, _w5#163 ASC, ROWS BETWEEN CURRENT ROW AND 62 FOLLOWING) AS float_col_1#89], [_w4#162 ASC, _w5#163 ASC]                                                                                                                                                                                                                                                                                                                                                                                                                                                      +- LocalRelation [col1#55L, col2#56, col3#57, col4#58, col5#59, col6#60]
!            :                             +- Project [(int_col_6#147 * cast(null as int)) AS decimal_col#86, (cast(smallint_col_4#30 as int) - int_col_6#147) AS int_col_1#87, (int_col_6#147 * cast(null as int)) AS _w0#158, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w1#159, (int_col_6#147 * cast(null as int)) AS _w2#160, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w3#161, (int_col_6#147 * cast(null as int)) AS _w4#162, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w5#163]
!            :                                +- Join Inner, (((isnotnull(smallint_col_4#145) && isnotnull(int_col_6#32)) && isnotnull(int_col_9#35)) && ((cast(smallint_col_4#145 as int) = int_col_9#35) && (cast(smallint_col_4#145 as int) = int_col_6#32)))
!            :                                   :- Project [smallint_col_4#30, int_col_6#32, int_col_9#35]
!            :                                   :  +- Project [col4#20 AS smallint_col_4#30, col6#22 AS int_col_6#32, col9#25 AS int_col_9#35]
!            :                                   :     +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]
!            :                                   +- Project [smallint_col_4#145, int_col_6#147]
!            :                                      +- Project [col4#20 AS smallint_col_4#145, col6#22 AS int_col_6#147]
!            :                                         +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]
!            +- Project [bigint_col_1#61L, double_col_3#63, varchar0164_col_6#66]
!               +- Project [col1#55L AS bigint_col_1#61L, col3#57 AS double_col_3#63, col6#60 AS varchar0164_col_6#66]
!                  +- LocalRelation [col1#55L, col2#56, col3#57, col4#58, col5#59, col6#60]

16/09/29 13:26:17 TRACE SparkOptimizer:
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.ConstantFolding ===
 Aggregate [count(1) AS count#179L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Aggregate [count(1) AS count#179L]
 +- Aggregate [int_col_2#165, (cast(bigint_col_7#14L as double) / cast(double_col_3#63 as double)), trim(varchar0164_col_6#66)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               +- Aggregate [int_col_2#165, (cast(bigint_col_7#14L as double) / cast(double_col_3#63 as double)), trim(varchar0164_col_6#66)]
    +- Project [bigint_col_7#14L, int_col_2#165, double_col_3#63, varchar0164_col_6#66]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          +- Project [bigint_col_7#14L, int_col_2#165, double_col_3#63, varchar0164_col_6#66]
       +- Join Inner, (((((((int_col#81L = int_col_1#82L) && (bigint_col_7#14L = int_col#81L)) && (bigint_col_7#14L = int_col_1#82L)) && isnotnull(bigint_col_7#14L)) && isnotnull(int_col#81L)) && isnotnull(int_col_1#82L)) && (((bigint_col_1#61L = bigint_col_7#14L) && (bigint_col_1#61L = int_col#81L)) && (bigint_col_1#61L = int_col_1#82L)))                                                                                                                                                                                                                                                                                                               +- Join Inner, (((((((int_col#81L = int_col_1#82L) && (bigint_col_7#14L = int_col#81L)) && (bigint_col_7#14L = int_col_1#82L)) && isnotnull(bigint_col_7#14L)) && isnotnull(int_col#81L)) && isnotnull(int_col_1#82L)) && (((bigint_col_1#61L = bigint_col_7#14L) && (bigint_col_1#61L = int_col#81L)) && (bigint_col_1#61L = int_col_1#82L)))
          :- Aggregate [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165, int_col_3#166], [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     :- Aggregate [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165, int_col_3#166], [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165]
          :  +- Union                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  :  +- Union
          :     :- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, cast(int_col_2#83L as decimal(29,9)) AS int_col_2#165, cast(int_col_3#84L as double) AS int_col_3#166]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       :     :- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, cast(int_col_2#83L as decimal(29,9)) AS int_col_2#165, cast(int_col_3#84L as double) AS int_col_3#166]
!         :     :  +- Filter NOT cast(min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L as decimal(22,2)) IN (cast(423.13 as decimal(22,2)),cast(bigint_col_7#140L as decimal(22,2)))                                                                                                                                                                                                                                                                                                                                                                                                                                                         :     :  +- Filter NOT cast(min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L as decimal(22,2)) IN (423.13,cast(bigint_col_7#140L as decimal(22,2)))
          :     :     +- Aggregate [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L), bigint_col_7#14L, bigint_col_7#140L], [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) AS int_col#81L, bigint_col_7#14L, bigint_col_7#140L AS int_col_1#82L, bigint_col_7#14L AS int_col_2#83L, coalesce(bigint_col_7#140L, (coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) - bigint_col_7#14L), min(bigint_col_7#140L)) AS int_col_3#84L, min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L)) AS min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L, bigint_col_7#140L]            :     :     +- Aggregate [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L), bigint_col_7#14L, bigint_col_7#140L], [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) AS int_col#81L, bigint_col_7#14L, bigint_col_7#140L AS int_col_1#82L, bigint_col_7#14L AS int_col_2#83L, coalesce(bigint_col_7#140L, (coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) - bigint_col_7#14L), min(bigint_col_7#140L)) AS int_col_3#84L, min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L)) AS min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L, bigint_col_7#140L]
          :     :        +- Project [bigint_col_7#14L, bigint_col_7#140L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              :     :        +- Project [bigint_col_7#14L, bigint_col_7#140L]
          :     :           +- Join Inner, ((isnotnull(timestamp_col_8#15) && isnotnull(timestamp_col_5#138)) && (bigint_col_7#14L IN (bigint_col_7#140L,bigint_col_7#140L) && ((timestamp_col_5#138 = timestamp_col_8#15) && (decimal2610_col_3#136 = decimal2610_col_3#10))))                                                                                                                                                                                                                                                                                                                                                                                        :     :           +- Join Inner, ((isnotnull(timestamp_col_8#15) && isnotnull(timestamp_col_5#138)) && (bigint_col_7#14L IN (bigint_col_7#140L,bigint_col_7#140L) && ((timestamp_col_5#138 = timestamp_col_8#15) && (decimal2610_col_3#136 = decimal2610_col_3#10))))
          :     :              :- Project [col3#2 AS decimal2610_col_3#10, col7#6L AS bigint_col_7#14L, col8#7 AS timestamp_col_8#15]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  :     :              :- Project [col3#2 AS decimal2610_col_3#10, col7#6L AS bigint_col_7#14L, col8#7 AS timestamp_col_8#15]
          :     :              :  +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   :     :              :  +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]
          :     :              +- Project [col3#2 AS decimal2610_col_3#136, col5#4 AS timestamp_col_5#138, col7#6L AS bigint_col_7#140L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               :     :              +- Project [col3#2 AS decimal2610_col_3#136, col5#4 AS timestamp_col_5#138, col7#6L AS bigint_col_7#140L]
          :     :                 +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   :     :                 +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]
          :     +- Project [cast(int_col#85 as bigint) AS int_col#167L, cast(decimal_col#86 as bigint) AS decimal_col#168L, cast(int_col_1#87 as bigint) AS int_col_1#169L, cast(float_col#88 as decimal(29,9)) AS float_col#170, float_col_1#89]                                                                                                                                                                                                                                                                                                                                                                                                                      :     +- Project [cast(int_col#85 as bigint) AS int_col#167L, cast(decimal_col#86 as bigint) AS decimal_col#168L, cast(int_col_1#87 as bigint) AS int_col_1#169L, cast(float_col#88 as decimal(29,9)) AS float_col#170, float_col_1#89]
!         :        +- Window [lag(CheckOverflow((promote_precision(cast(cast(449 as decimal(10,0)) as decimal(19,9))) * promote_precision(cast(-157.519107824 as decimal(19,9)))), DecimalType(23,9)), 46, null) windowspecdefinition(_w2#160 DESC, _w3#161 DESC, ROWS BETWEEN 46 PRECEDING AND 46 PRECEDING) AS float_col#88], [_w2#160 DESC, _w3#161 DESC]                                                                                                                                                                                                                                                                                                           :        +- Window [lag(-70726.079412976, 46, null) windowspecdefinition(_w2#160 DESC, _w3#161 DESC, ROWS BETWEEN 46 PRECEDING AND 46 PRECEDING) AS float_col#88], [_w2#160 DESC, _w3#161 DESC]
!         :           +- Window [min((436 * 927)) windowspecdefinition(_w0#158 DESC, _w1#159 DESC, ROWS BETWEEN 96 PRECEDING AND 16 PRECEDING) AS int_col#85], [_w0#158 DESC, _w1#159 DESC]                                                                                                                                                                                                                                                                                                                                                                                                                                                                            :           +- Window [min(404172) windowspecdefinition(_w0#158 DESC, _w1#159 DESC, ROWS BETWEEN 96 PRECEDING AND 16 PRECEDING) AS int_col#85], [_w0#158 DESC, _w1#159 DESC]
!         :              +- Window [avg(cast((669 - -773) as bigint)) windowspecdefinition(_w4#162 ASC, _w5#163 ASC, ROWS BETWEEN CURRENT ROW AND 62 FOLLOWING) AS float_col_1#89], [_w4#162 ASC, _w5#163 ASC]                                                                                                                                                                                                                                                                                                                                                                                                                                                         :              +- Window [avg(1442) windowspecdefinition(_w4#162 ASC, _w5#163 ASC, ROWS BETWEEN CURRENT ROW AND 62 FOLLOWING) AS float_col_1#89], [_w4#162 ASC, _w5#163 ASC]
!         :                 +- Project [(int_col_6#147 * cast(null as int)) AS decimal_col#86, (cast(smallint_col_4#30 as int) - int_col_6#147) AS int_col_1#87, (int_col_6#147 * cast(null as int)) AS _w0#158, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w1#159, (int_col_6#147 * cast(null as int)) AS _w2#160, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w3#161, (int_col_6#147 * cast(null as int)) AS _w4#162, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w5#163]                                                                                                                                                                :                 +- Project [(int_col_6#147 * null) AS decimal_col#86, (cast(smallint_col_4#30 as int) - int_col_6#147) AS int_col_1#87, (int_col_6#147 * null) AS _w0#158, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w1#159, (int_col_6#147 * null) AS _w2#160, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w3#161, (int_col_6#147 * null) AS _w4#162, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w5#163]
          :                    +- Join Inner, (((isnotnull(smallint_col_4#145) && isnotnull(int_col_6#32)) && isnotnull(int_col_9#35)) && ((cast(smallint_col_4#145 as int) = int_col_9#35) && (cast(smallint_col_4#145 as int) = int_col_6#32)))                                                                                                                                                                                                                                                                                                                                                                                                                      :                    +- Join Inner, (((isnotnull(smallint_col_4#145) && isnotnull(int_col_6#32)) && isnotnull(int_col_9#35)) && ((cast(smallint_col_4#145 as int) = int_col_9#35) && (cast(smallint_col_4#145 as int) = int_col_6#32)))
          :                       :- Project [col4#20 AS smallint_col_4#30, col6#22 AS int_col_6#32, col9#25 AS int_col_9#35]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          :                       :- Project [col4#20 AS smallint_col_4#30, col6#22 AS int_col_6#32, col9#25 AS int_col_9#35]
          :                       :  +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      :                       :  +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]
          :                       +- Project [col4#20 AS smallint_col_4#145, col6#22 AS int_col_6#147]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 :                       +- Project [col4#20 AS smallint_col_4#145, col6#22 AS int_col_6#147]
          :                          +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      :                          +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]
          +- Project [col1#55L AS bigint_col_1#61L, col3#57 AS double_col_3#63, col6#60 AS varchar0164_col_6#66]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       +- Project [col1#55L AS bigint_col_1#61L, col3#57 AS double_col_3#63, col6#60 AS varchar0164_col_6#66]
             +- LocalRelation [col1#55L, col2#56, col3#57, col4#58, col5#59, col6#60]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     +- LocalRelation [col1#55L, col2#56, col3#57, col4#58, col5#59, col6#60]

16/09/29 13:26:17 TRACE SparkOptimizer:
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.SimplifyCasts ===
 Aggregate [count(1) AS count#179L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Aggregate [count(1) AS count#179L]
!+- Aggregate [int_col_2#165, (cast(bigint_col_7#14L as double) / cast(double_col_3#63 as double)), trim(varchar0164_col_6#66)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               +- Aggregate [int_col_2#165, (cast(bigint_col_7#14L as double) / double_col_3#63), trim(varchar0164_col_6#66)]
    +- Project [bigint_col_7#14L, int_col_2#165, double_col_3#63, varchar0164_col_6#66]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          +- Project [bigint_col_7#14L, int_col_2#165, double_col_3#63, varchar0164_col_6#66]
       +- Join Inner, (((((((int_col#81L = int_col_1#82L) && (bigint_col_7#14L = int_col#81L)) && (bigint_col_7#14L = int_col_1#82L)) && isnotnull(bigint_col_7#14L)) && isnotnull(int_col#81L)) && isnotnull(int_col_1#82L)) && (((bigint_col_1#61L = bigint_col_7#14L) && (bigint_col_1#61L = int_col#81L)) && (bigint_col_1#61L = int_col_1#82L)))                                                                                                                                                                                                                                                                                                               +- Join Inner, (((((((int_col#81L = int_col_1#82L) && (bigint_col_7#14L = int_col#81L)) && (bigint_col_7#14L = int_col_1#82L)) && isnotnull(bigint_col_7#14L)) && isnotnull(int_col#81L)) && isnotnull(int_col_1#82L)) && (((bigint_col_1#61L = bigint_col_7#14L) && (bigint_col_1#61L = int_col#81L)) && (bigint_col_1#61L = int_col_1#82L)))
          :- Aggregate [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165, int_col_3#166], [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     :- Aggregate [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165, int_col_3#166], [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165]
          :  +- Union                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  :  +- Union
          :     :- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, cast(int_col_2#83L as decimal(29,9)) AS int_col_2#165, cast(int_col_3#84L as double) AS int_col_3#166]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       :     :- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, cast(int_col_2#83L as decimal(29,9)) AS int_col_2#165, cast(int_col_3#84L as double) AS int_col_3#166]
          :     :  +- Filter NOT cast(min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L as decimal(22,2)) IN (423.13,cast(bigint_col_7#140L as decimal(22,2)))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                :     :  +- Filter NOT cast(min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L as decimal(22,2)) IN (423.13,cast(bigint_col_7#140L as decimal(22,2)))
          :     :     +- Aggregate [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L), bigint_col_7#14L, bigint_col_7#140L], [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) AS int_col#81L, bigint_col_7#14L, bigint_col_7#140L AS int_col_1#82L, bigint_col_7#14L AS int_col_2#83L, coalesce(bigint_col_7#140L, (coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) - bigint_col_7#14L), min(bigint_col_7#140L)) AS int_col_3#84L, min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L)) AS min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L, bigint_col_7#140L]            :     :     +- Aggregate [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L), bigint_col_7#14L, bigint_col_7#140L], [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) AS int_col#81L, bigint_col_7#14L, bigint_col_7#140L AS int_col_1#82L, bigint_col_7#14L AS int_col_2#83L, coalesce(bigint_col_7#140L, (coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) - bigint_col_7#14L), min(bigint_col_7#140L)) AS int_col_3#84L, min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L)) AS min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L, bigint_col_7#140L]
          :     :        +- Project [bigint_col_7#14L, bigint_col_7#140L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              :     :        +- Project [bigint_col_7#14L, bigint_col_7#140L]
          :     :           +- Join Inner, ((isnotnull(timestamp_col_8#15) && isnotnull(timestamp_col_5#138)) && (bigint_col_7#14L IN (bigint_col_7#140L,bigint_col_7#140L) && ((timestamp_col_5#138 = timestamp_col_8#15) && (decimal2610_col_3#136 = decimal2610_col_3#10))))                                                                                                                                                                                                                                                                                                                                                                                        :     :           +- Join Inner, ((isnotnull(timestamp_col_8#15) && isnotnull(timestamp_col_5#138)) && (bigint_col_7#14L IN (bigint_col_7#140L,bigint_col_7#140L) && ((timestamp_col_5#138 = timestamp_col_8#15) && (decimal2610_col_3#136 = decimal2610_col_3#10))))
          :     :              :- Project [col3#2 AS decimal2610_col_3#10, col7#6L AS bigint_col_7#14L, col8#7 AS timestamp_col_8#15]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  :     :              :- Project [col3#2 AS decimal2610_col_3#10, col7#6L AS bigint_col_7#14L, col8#7 AS timestamp_col_8#15]
          :     :              :  +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   :     :              :  +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]
          :     :              +- Project [col3#2 AS decimal2610_col_3#136, col5#4 AS timestamp_col_5#138, col7#6L AS bigint_col_7#140L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               :     :              +- Project [col3#2 AS decimal2610_col_3#136, col5#4 AS timestamp_col_5#138, col7#6L AS bigint_col_7#140L]
          :     :                 +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   :     :                 +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]
          :     +- Project [cast(int_col#85 as bigint) AS int_col#167L, cast(decimal_col#86 as bigint) AS decimal_col#168L, cast(int_col_1#87 as bigint) AS int_col_1#169L, cast(float_col#88 as decimal(29,9)) AS float_col#170, float_col_1#89]                                                                                                                                                                                                                                                                                                                                                                                                                      :     +- Project [cast(int_col#85 as bigint) AS int_col#167L, cast(decimal_col#86 as bigint) AS decimal_col#168L, cast(int_col_1#87 as bigint) AS int_col_1#169L, cast(float_col#88 as decimal(29,9)) AS float_col#170, float_col_1#89]
          :        +- Window [lag(-70726.079412976, 46, null) windowspecdefinition(_w2#160 DESC, _w3#161 DESC, ROWS BETWEEN 46 PRECEDING AND 46 PRECEDING) AS float_col#88], [_w2#160 DESC, _w3#161 DESC]                                                                                                                                                                                                                                                                                                                                                                                                                                                              :        +- Window [lag(-70726.079412976, 46, null) windowspecdefinition(_w2#160 DESC, _w3#161 DESC, ROWS BETWEEN 46 PRECEDING AND 46 PRECEDING) AS float_col#88], [_w2#160 DESC, _w3#161 DESC]
          :           +- Window [min(404172) windowspecdefinition(_w0#158 DESC, _w1#159 DESC, ROWS BETWEEN 96 PRECEDING AND 16 PRECEDING) AS int_col#85], [_w0#158 DESC, _w1#159 DESC]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 :           +- Window [min(404172) windowspecdefinition(_w0#158 DESC, _w1#159 DESC, ROWS BETWEEN 96 PRECEDING AND 16 PRECEDING) AS int_col#85], [_w0#158 DESC, _w1#159 DESC]
          :              +- Window [avg(1442) windowspecdefinition(_w4#162 ASC, _w5#163 ASC, ROWS BETWEEN CURRENT ROW AND 62 FOLLOWING) AS float_col_1#89], [_w4#162 ASC, _w5#163 ASC]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 :              +- Window [avg(1442) windowspecdefinition(_w4#162 ASC, _w5#163 ASC, ROWS BETWEEN CURRENT ROW AND 62 FOLLOWING) AS float_col_1#89], [_w4#162 ASC, _w5#163 ASC]
          :                 +- Project [(int_col_6#147 * null) AS decimal_col#86, (cast(smallint_col_4#30 as int) - int_col_6#147) AS int_col_1#87, (int_col_6#147 * null) AS _w0#158, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w1#159, (int_col_6#147 * null) AS _w2#160, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w3#161, (int_col_6#147 * null) AS _w4#162, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w5#163]                                                                                                                                                                                                                    :                 +- Project [(int_col_6#147 * null) AS decimal_col#86, (cast(smallint_col_4#30 as int) - int_col_6#147) AS int_col_1#87, (int_col_6#147 * null) AS _w0#158, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w1#159, (int_col_6#147 * null) AS _w2#160, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w3#161, (int_col_6#147 * null) AS _w4#162, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w5#163]
          :                    +- Join Inner, (((isnotnull(smallint_col_4#145) && isnotnull(int_col_6#32)) && isnotnull(int_col_9#35)) && ((cast(smallint_col_4#145 as int) = int_col_9#35) && (cast(smallint_col_4#145 as int) = int_col_6#32)))                                                                                                                                                                                                                                                                                                                                                                                                                      :                    +- Join Inner, (((isnotnull(smallint_col_4#145) && isnotnull(int_col_6#32)) && isnotnull(int_col_9#35)) && ((cast(smallint_col_4#145 as int) = int_col_9#35) && (cast(smallint_col_4#145 as int) = int_col_6#32)))
          :                       :- Project [col4#20 AS smallint_col_4#30, col6#22 AS int_col_6#32, col9#25 AS int_col_9#35]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          :                       :- Project [col4#20 AS smallint_col_4#30, col6#22 AS int_col_6#32, col9#25 AS int_col_9#35]
          :                       :  +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      :                       :  +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]
          :                       +- Project [col4#20 AS smallint_col_4#145, col6#22 AS int_col_6#147]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 :                       +- Project [col4#20 AS smallint_col_4#145, col6#22 AS int_col_6#147]
          :                          +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      :                          +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]
          +- Project [col1#55L AS bigint_col_1#61L, col3#57 AS double_col_3#63, col6#60 AS varchar0164_col_6#66]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       +- Project [col1#55L AS bigint_col_1#61L, col3#57 AS double_col_3#63, col6#60 AS varchar0164_col_6#66]
             +- LocalRelation [col1#55L, col2#56, col3#57, col4#58, col5#59, col6#60]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     +- LocalRelation [col1#55L, col2#56, col3#57, col4#58, col5#59, col6#60]

16/09/29 13:26:17 TRACE SparkOptimizer:
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.PushPredicateThroughJoin ===
 Aggregate [count(1) AS count#179L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Aggregate [count(1) AS count#179L]
 +- Aggregate [int_col_2#165, (cast(bigint_col_7#14L as double) / double_col_3#63), trim(varchar0164_col_6#66)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               +- Aggregate [int_col_2#165, (cast(bigint_col_7#14L as double) / double_col_3#63), trim(varchar0164_col_6#66)]
    +- Project [bigint_col_7#14L, int_col_2#165, double_col_3#63, varchar0164_col_6#66]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          +- Project [bigint_col_7#14L, int_col_2#165, double_col_3#63, varchar0164_col_6#66]
!      +- Join Inner, (((((((int_col#81L = int_col_1#82L) && (bigint_col_7#14L = int_col#81L)) && (bigint_col_7#14L = int_col_1#82L)) && isnotnull(bigint_col_7#14L)) && isnotnull(int_col#81L)) && isnotnull(int_col_1#82L)) && (((bigint_col_1#61L = bigint_col_7#14L) && (bigint_col_1#61L = int_col#81L)) && (bigint_col_1#61L = int_col_1#82L)))                                                                                                                                                                                                                                                                                                               +- Join Inner, (((bigint_col_1#61L = bigint_col_7#14L) && (bigint_col_1#61L = int_col#81L)) && (bigint_col_1#61L = int_col_1#82L))
!         :- Aggregate [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165, int_col_3#166], [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     :- Filter ((((((int_col#81L = int_col_1#82L) && (bigint_col_7#14L = int_col#81L)) && (bigint_col_7#14L = int_col_1#82L)) && isnotnull(bigint_col_7#14L)) && isnotnull(int_col#81L)) && isnotnull(int_col_1#82L))
!         :  +- Union                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  :  +- Aggregate [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165, int_col_3#166], [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165]
!         :     :- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, cast(int_col_2#83L as decimal(29,9)) AS int_col_2#165, cast(int_col_3#84L as double) AS int_col_3#166]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       :     +- Union
!         :     :  +- Filter NOT cast(min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L as decimal(22,2)) IN (423.13,cast(bigint_col_7#140L as decimal(22,2)))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                :        :- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, cast(int_col_2#83L as decimal(29,9)) AS int_col_2#165, cast(int_col_3#84L as double) AS int_col_3#166]
!         :     :     +- Aggregate [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L), bigint_col_7#14L, bigint_col_7#140L], [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) AS int_col#81L, bigint_col_7#14L, bigint_col_7#140L AS int_col_1#82L, bigint_col_7#14L AS int_col_2#83L, coalesce(bigint_col_7#140L, (coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) - bigint_col_7#14L), min(bigint_col_7#140L)) AS int_col_3#84L, min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L)) AS min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L, bigint_col_7#140L]            :        :  +- Filter NOT cast(min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L as decimal(22,2)) IN (423.13,cast(bigint_col_7#140L as decimal(22,2)))
!         :     :        +- Project [bigint_col_7#14L, bigint_col_7#140L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              :        :     +- Aggregate [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L), bigint_col_7#14L, bigint_col_7#140L], [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) AS int_col#81L, bigint_col_7#14L, bigint_col_7#140L AS int_col_1#82L, bigint_col_7#14L AS int_col_2#83L, coalesce(bigint_col_7#140L, (coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) - bigint_col_7#14L), min(bigint_col_7#140L)) AS int_col_3#84L, min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L)) AS min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L, bigint_col_7#140L]
!         :     :           +- Join Inner, ((isnotnull(timestamp_col_8#15) && isnotnull(timestamp_col_5#138)) && (bigint_col_7#14L IN (bigint_col_7#140L,bigint_col_7#140L) && ((timestamp_col_5#138 = timestamp_col_8#15) && (decimal2610_col_3#136 = decimal2610_col_3#10))))                                                                                                                                                                                                                                                                                                                                                                                        :        :        +- Project [bigint_col_7#14L, bigint_col_7#140L]
!         :     :              :- Project [col3#2 AS decimal2610_col_3#10, col7#6L AS bigint_col_7#14L, col8#7 AS timestamp_col_8#15]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  :        :           +- Join Inner, ((bigint_col_7#14L IN (bigint_col_7#140L,bigint_col_7#140L) && (timestamp_col_5#138 = timestamp_col_8#15)) && (decimal2610_col_3#136 = decimal2610_col_3#10))
!         :     :              :  +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   :        :              :- Filter isnotnull(timestamp_col_8#15)
!         :     :              +- Project [col3#2 AS decimal2610_col_3#136, col5#4 AS timestamp_col_5#138, col7#6L AS bigint_col_7#140L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               :        :              :  +- Project [col3#2 AS decimal2610_col_3#10, col7#6L AS bigint_col_7#14L, col8#7 AS timestamp_col_8#15]
!         :     :                 +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   :        :              :     +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]
!         :     +- Project [cast(int_col#85 as bigint) AS int_col#167L, cast(decimal_col#86 as bigint) AS decimal_col#168L, cast(int_col_1#87 as bigint) AS int_col_1#169L, cast(float_col#88 as decimal(29,9)) AS float_col#170, float_col_1#89]                                                                                                                                                                                                                                                                                                                                                                                                                      :        :              +- Filter isnotnull(timestamp_col_5#138)
!         :        +- Window [lag(-70726.079412976, 46, null) windowspecdefinition(_w2#160 DESC, _w3#161 DESC, ROWS BETWEEN 46 PRECEDING AND 46 PRECEDING) AS float_col#88], [_w2#160 DESC, _w3#161 DESC]                                                                                                                                                                                                                                                                                                                                                                                                                                                              :        :                 +- Project [col3#2 AS decimal2610_col_3#136, col5#4 AS timestamp_col_5#138, col7#6L AS bigint_col_7#140L]
!         :           +- Window [min(404172) windowspecdefinition(_w0#158 DESC, _w1#159 DESC, ROWS BETWEEN 96 PRECEDING AND 16 PRECEDING) AS int_col#85], [_w0#158 DESC, _w1#159 DESC]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 :        :                    +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]
!         :              +- Window [avg(1442) windowspecdefinition(_w4#162 ASC, _w5#163 ASC, ROWS BETWEEN CURRENT ROW AND 62 FOLLOWING) AS float_col_1#89], [_w4#162 ASC, _w5#163 ASC]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 :        +- Project [cast(int_col#85 as bigint) AS int_col#167L, cast(decimal_col#86 as bigint) AS decimal_col#168L, cast(int_col_1#87 as bigint) AS int_col_1#169L, cast(float_col#88 as decimal(29,9)) AS float_col#170, float_col_1#89]
!         :                 +- Project [(int_col_6#147 * null) AS decimal_col#86, (cast(smallint_col_4#30 as int) - int_col_6#147) AS int_col_1#87, (int_col_6#147 * null) AS _w0#158, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w1#159, (int_col_6#147 * null) AS _w2#160, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w3#161, (int_col_6#147 * null) AS _w4#162, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w5#163]                                                                                                                                                                                                                    :           +- Window [lag(-70726.079412976, 46, null) windowspecdefinition(_w2#160 DESC, _w3#161 DESC, ROWS BETWEEN 46 PRECEDING AND 46 PRECEDING) AS float_col#88], [_w2#160 DESC, _w3#161 DESC]
!         :                    +- Join Inner, (((isnotnull(smallint_col_4#145) && isnotnull(int_col_6#32)) && isnotnull(int_col_9#35)) && ((cast(smallint_col_4#145 as int) = int_col_9#35) && (cast(smallint_col_4#145 as int) = int_col_6#32)))                                                                                                                                                                                                                                                                                                                                                                                                                      :              +- Window [min(404172) windowspecdefinition(_w0#158 DESC, _w1#159 DESC, ROWS BETWEEN 96 PRECEDING AND 16 PRECEDING) AS int_col#85], [_w0#158 DESC, _w1#159 DESC]
!         :                       :- Project [col4#20 AS smallint_col_4#30, col6#22 AS int_col_6#32, col9#25 AS int_col_9#35]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          :                 +- Window [avg(1442) windowspecdefinition(_w4#162 ASC, _w5#163 ASC, ROWS BETWEEN CURRENT ROW AND 62 FOLLOWING) AS float_col_1#89], [_w4#162 ASC, _w5#163 ASC]
!         :                       :  +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      :                    +- Project [(int_col_6#147 * null) AS decimal_col#86, (cast(smallint_col_4#30 as int) - int_col_6#147) AS int_col_1#87, (int_col_6#147 * null) AS _w0#158, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w1#159, (int_col_6#147 * null) AS _w2#160, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w3#161, (int_col_6#147 * null) AS _w4#162, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w5#163]
!         :                       +- Project [col4#20 AS smallint_col_4#145, col6#22 AS int_col_6#147]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 :                       +- Join Inner, ((cast(smallint_col_4#145 as int) = int_col_9#35) && (cast(smallint_col_4#145 as int) = int_col_6#32))
!         :                          +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      :                          :- Filter (isnotnull(int_col_6#32) && isnotnull(int_col_9#35))
!         +- Project [col1#55L AS bigint_col_1#61L, col3#57 AS double_col_3#63, col6#60 AS varchar0164_col_6#66]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       :                          :  +- Project [col4#20 AS smallint_col_4#30, col6#22 AS int_col_6#32, col9#25 AS int_col_9#35]
!            +- LocalRelation [col1#55L, col2#56, col3#57, col4#58, col5#59, col6#60]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  :                          :     +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]
!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      :                          +- Filter isnotnull(smallint_col_4#145)
!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      :                             +- Project [col4#20 AS smallint_col_4#145, col6#22 AS int_col_6#147]
!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      :                                +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]
!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      +- Project [col1#55L AS bigint_col_1#61L, col3#57 AS double_col_3#63, col6#60 AS varchar0164_col_6#66]
!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         +- LocalRelation [col1#55L, col2#56, col3#57, col4#58, col5#59, col6#60]

16/09/29 13:26:17 TRACE SparkOptimizer:
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.PushDownPredicate ===
 Aggregate [count(1) AS count#179L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Aggregate [count(1) AS count#179L]
 +- Aggregate [int_col_2#165, (cast(bigint_col_7#14L as double) / double_col_3#63), trim(varchar0164_col_6#66)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  +- Aggregate [int_col_2#165, (cast(bigint_col_7#14L as double) / double_col_3#63), trim(varchar0164_col_6#66)]
    +- Project [bigint_col_7#14L, int_col_2#165, double_col_3#63, varchar0164_col_6#66]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             +- Project [bigint_col_7#14L, int_col_2#165, double_col_3#63, varchar0164_col_6#66]
       +- Join Inner, (((bigint_col_1#61L = bigint_col_7#14L) && (bigint_col_1#61L = int_col#81L)) && (bigint_col_1#61L = int_col_1#82L))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              +- Join Inner, (((bigint_col_1#61L = bigint_col_7#14L) && (bigint_col_1#61L = int_col#81L)) && (bigint_col_1#61L = int_col_1#82L))
!         :- Filter ((((((int_col#81L = int_col_1#82L) && (bigint_col_7#14L = int_col#81L)) && (bigint_col_7#14L = int_col_1#82L)) && isnotnull(bigint_col_7#14L)) && isnotnull(int_col#81L)) && isnotnull(int_col_1#82L))                                                                                                                                                                                                                                                                                                                                                                                                                                                :- Aggregate [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165, int_col_3#166], [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165]
!         :  +- Aggregate [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165, int_col_3#166], [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     :  +- Union
!         :     +- Union                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  :     :- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, cast(int_col_2#83L as decimal(29,9)) AS int_col_2#165, cast(int_col_3#84L as double) AS int_col_3#166]
!         :        :- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, cast(int_col_2#83L as decimal(29,9)) AS int_col_2#165, cast(int_col_3#84L as double) AS int_col_3#166]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       :     :  +- Filter ((((((int_col#81L = int_col_1#82L) && (bigint_col_7#14L = int_col#81L)) && (bigint_col_7#14L = int_col_1#82L)) && isnotnull(bigint_col_7#14L)) && isnotnull(int_col#81L)) && isnotnull(int_col_1#82L))
!         :        :  +- Filter NOT cast(min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L as decimal(22,2)) IN (423.13,cast(bigint_col_7#140L as decimal(22,2)))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                :     :     +- Filter NOT cast(min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L as decimal(22,2)) IN (423.13,cast(bigint_col_7#140L as decimal(22,2)))
!         :        :     +- Aggregate [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L), bigint_col_7#14L, bigint_col_7#140L], [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) AS int_col#81L, bigint_col_7#14L, bigint_col_7#140L AS int_col_1#82L, bigint_col_7#14L AS int_col_2#83L, coalesce(bigint_col_7#140L, (coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) - bigint_col_7#14L), min(bigint_col_7#140L)) AS int_col_3#84L, min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L)) AS min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L, bigint_col_7#140L]            :     :        +- Aggregate [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L), bigint_col_7#14L, bigint_col_7#140L], [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) AS int_col#81L, bigint_col_7#14L, bigint_col_7#140L AS int_col_1#82L, bigint_col_7#14L AS int_col_2#83L, coalesce(bigint_col_7#140L, (coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) - bigint_col_7#14L), min(bigint_col_7#140L)) AS int_col_3#84L, min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L)) AS min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L, bigint_col_7#140L]
!         :        :        +- Project [bigint_col_7#14L, bigint_col_7#140L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              :     :           +- Project [bigint_col_7#14L, bigint_col_7#140L]
!         :        :           +- Join Inner, ((bigint_col_7#14L IN (bigint_col_7#140L,bigint_col_7#140L) && (timestamp_col_5#138 = timestamp_col_8#15)) && (decimal2610_col_3#136 = decimal2610_col_3#10))                                                                                                                                                                                                                                                                                                                                                                                                                                                               :     :              +- Join Inner, ((bigint_col_7#14L IN (bigint_col_7#140L,bigint_col_7#140L) && (timestamp_col_5#138 = timestamp_col_8#15)) && (decimal2610_col_3#136 = decimal2610_col_3#10))
!         :        :              :- Filter isnotnull(timestamp_col_8#15)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 :     :                 :- Project [col3#2 AS decimal2610_col_3#10, col7#6L AS bigint_col_7#14L, col8#7 AS timestamp_col_8#15]
!         :        :              :  +- Project [col3#2 AS decimal2610_col_3#10, col7#6L AS bigint_col_7#14L, col8#7 AS timestamp_col_8#15]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               :     :                 :  +- Filter isnotnull(col8#7)
!         :        :              :     +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                :     :                 :     +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]
!         :        :              +- Filter isnotnull(timestamp_col_5#138)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                :     :                 +- Project [col3#2 AS decimal2610_col_3#136, col5#4 AS timestamp_col_5#138, col7#6L AS bigint_col_7#140L]
!         :        :                 +- Project [col3#2 AS decimal2610_col_3#136, col5#4 AS timestamp_col_5#138, col7#6L AS bigint_col_7#140L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            :     :                    +- Filter isnotnull(col5#4)
!         :        :                    +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                :     :                       +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]
!         :        +- Project [cast(int_col#85 as bigint) AS int_col#167L, cast(decimal_col#86 as bigint) AS decimal_col#168L, cast(int_col_1#87 as bigint) AS int_col_1#169L, cast(float_col#88 as decimal(29,9)) AS float_col#170, float_col_1#89]                                                                                                                                                                                                                                                                                                                                                                                                                      :     +- Project [cast(int_col#85 as bigint) AS int_col#167L, cast(decimal_col#86 as bigint) AS decimal_col#168L, cast(int_col_1#87 as bigint) AS int_col_1#169L, cast(float_col#88 as decimal(29,9)) AS float_col#170, float_col_1#89]
!         :           +- Window [lag(-70726.079412976, 46, null) windowspecdefinition(_w2#160 DESC, _w3#161 DESC, ROWS BETWEEN 46 PRECEDING AND 46 PRECEDING) AS float_col#88], [_w2#160 DESC, _w3#161 DESC]                                                                                                                                                                                                                                                                                                                                                                                                                                                              :        +- Filter ((((((cast(int_col#85 as bigint) = cast(int_col_1#87 as bigint)) && (cast(decimal_col#86 as bigint) = cast(int_col#85 as bigint))) && (cast(decimal_col#86 as bigint) = cast(int_col_1#87 as bigint))) && isnotnull(cast(decimal_col#86 as bigint))) && isnotnull(cast(int_col#85 as bigint))) && isnotnull(cast(int_col_1#87 as bigint)))
!         :              +- Window [min(404172) windowspecdefinition(_w0#158 DESC, _w1#159 DESC, ROWS BETWEEN 96 PRECEDING AND 16 PRECEDING) AS int_col#85], [_w0#158 DESC, _w1#159 DESC]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 :           +- Window [lag(-70726.079412976, 46, null) windowspecdefinition(_w2#160 DESC, _w3#161 DESC, ROWS BETWEEN 46 PRECEDING AND 46 PRECEDING) AS float_col#88], [_w2#160 DESC, _w3#161 DESC]
!         :                 +- Window [avg(1442) windowspecdefinition(_w4#162 ASC, _w5#163 ASC, ROWS BETWEEN CURRENT ROW AND 62 FOLLOWING) AS float_col_1#89], [_w4#162 ASC, _w5#163 ASC]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 :              +- Window [min(404172) windowspecdefinition(_w0#158 DESC, _w1#159 DESC, ROWS BETWEEN 96 PRECEDING AND 16 PRECEDING) AS int_col#85], [_w0#158 DESC, _w1#159 DESC]
!         :                    +- Project [(int_col_6#147 * null) AS decimal_col#86, (cast(smallint_col_4#30 as int) - int_col_6#147) AS int_col_1#87, (int_col_6#147 * null) AS _w0#158, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w1#159, (int_col_6#147 * null) AS _w2#160, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w3#161, (int_col_6#147 * null) AS _w4#162, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w5#163]                                                                                                                                                                                                                    :                 +- Window [avg(1442) windowspecdefinition(_w4#162 ASC, _w5#163 ASC, ROWS BETWEEN CURRENT ROW AND 62 FOLLOWING) AS float_col_1#89], [_w4#162 ASC, _w5#163 ASC]
!         :                       +- Join Inner, ((cast(smallint_col_4#145 as int) = int_col_9#35) && (cast(smallint_col_4#145 as int) = int_col_6#32))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   :                    +- Project [(int_col_6#147 * null) AS decimal_col#86, (cast(smallint_col_4#30 as int) - int_col_6#147) AS int_col_1#87, (int_col_6#147 * null) AS _w0#158, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w1#159, (int_col_6#147 * null) AS _w2#160, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w3#161, (int_col_6#147 * null) AS _w4#162, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w5#163]
!         :                          :- Filter (isnotnull(int_col_6#32) && isnotnull(int_col_9#35))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       :                       +- Join Inner, ((cast(smallint_col_4#145 as int) = int_col_9#35) && (cast(smallint_col_4#145 as int) = int_col_6#32))
!         :                          :  +- Project [col4#20 AS smallint_col_4#30, col6#22 AS int_col_6#32, col9#25 AS int_col_9#35]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       :                          :- Project [col4#20 AS smallint_col_4#30, col6#22 AS int_col_6#32, col9#25 AS int_col_9#35]
!         :                          :     +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   :                          :  +- Filter (isnotnull(col6#22) && isnotnull(col9#25))
!         :                          +- Filter isnotnull(smallint_col_4#145)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              :                          :     +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]
!         :                             +- Project [col4#20 AS smallint_col_4#145, col6#22 AS int_col_6#147]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              :                          +- Project [col4#20 AS smallint_col_4#145, col6#22 AS int_col_6#147]
!         :                                +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   :                             +- Filter isnotnull(col4#20)
!         +- Project [col1#55L AS bigint_col_1#61L, col3#57 AS double_col_3#63, col6#60 AS varchar0164_col_6#66]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          :                                +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]
!            +- LocalRelation [col1#55L, col2#56, col3#57, col4#58, col5#59, col6#60]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     +- Project [col1#55L AS bigint_col_1#61L, col3#57 AS double_col_3#63, col6#60 AS varchar0164_col_6#66]
!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            +- LocalRelation [col1#55L, col2#56, col3#57, col4#58, col5#59, col6#60]

16/09/29 13:26:17 TRACE SparkOptimizer:
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.ColumnPruning ===
 Aggregate [count(1) AS count#179L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Aggregate [count(1) AS count#179L]
 +- Aggregate [int_col_2#165, (cast(bigint_col_7#14L as double) / double_col_3#63), trim(varchar0164_col_6#66)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  +- Aggregate [int_col_2#165, (cast(bigint_col_7#14L as double) / double_col_3#63), trim(varchar0164_col_6#66)]
    +- Project [bigint_col_7#14L, int_col_2#165, double_col_3#63, varchar0164_col_6#66]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             +- Project [bigint_col_7#14L, int_col_2#165, double_col_3#63, varchar0164_col_6#66]
       +- Join Inner, (((bigint_col_1#61L = bigint_col_7#14L) && (bigint_col_1#61L = int_col#81L)) && (bigint_col_1#61L = int_col_1#82L))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              +- Join Inner, (((bigint_col_1#61L = bigint_col_7#14L) && (bigint_col_1#61L = int_col#81L)) && (bigint_col_1#61L = int_col_1#82L))
          :- Aggregate [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165, int_col_3#166], [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        :- Aggregate [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165, int_col_3#166], [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165]
          :  +- Union                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     :  +- Union
          :     :- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, cast(int_col_2#83L as decimal(29,9)) AS int_col_2#165, cast(int_col_3#84L as double) AS int_col_3#166]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          :     :- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, cast(int_col_2#83L as decimal(29,9)) AS int_col_2#165, cast(int_col_3#84L as double) AS int_col_3#166]
          :     :  +- Filter ((((((int_col#81L = int_col_1#82L) && (bigint_col_7#14L = int_col#81L)) && (bigint_col_7#14L = int_col_1#82L)) && isnotnull(bigint_col_7#14L)) && isnotnull(int_col#81L)) && isnotnull(int_col_1#82L))                                                                                                                                                                                                                                                                                                                                                                                                                                       :     :  +- Filter ((((((int_col#81L = int_col_1#82L) && (bigint_col_7#14L = int_col#81L)) && (bigint_col_7#14L = int_col_1#82L)) && isnotnull(bigint_col_7#14L)) && isnotnull(int_col#81L)) && isnotnull(int_col_1#82L))
          :     :     +- Filter NOT cast(min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L as decimal(22,2)) IN (423.13,cast(bigint_col_7#140L as decimal(22,2)))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                :     :     +- Filter NOT cast(min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L as decimal(22,2)) IN (423.13,cast(bigint_col_7#140L as decimal(22,2)))
          :     :        +- Aggregate [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L), bigint_col_7#14L, bigint_col_7#140L], [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) AS int_col#81L, bigint_col_7#14L, bigint_col_7#140L AS int_col_1#82L, bigint_col_7#14L AS int_col_2#83L, coalesce(bigint_col_7#140L, (coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) - bigint_col_7#14L), min(bigint_col_7#140L)) AS int_col_3#84L, min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L)) AS min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L, bigint_col_7#140L]            :     :        +- Aggregate [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L), bigint_col_7#14L, bigint_col_7#140L], [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) AS int_col#81L, bigint_col_7#14L, bigint_col_7#140L AS int_col_1#82L, bigint_col_7#14L AS int_col_2#83L, coalesce(bigint_col_7#140L, (coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) - bigint_col_7#14L), min(bigint_col_7#140L)) AS int_col_3#84L, min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L)) AS min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L, bigint_col_7#140L]
          :     :           +- Project [bigint_col_7#14L, bigint_col_7#140L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              :     :           +- Project [bigint_col_7#14L, bigint_col_7#140L]
          :     :              +- Join Inner, ((bigint_col_7#14L IN (bigint_col_7#140L,bigint_col_7#140L) && (timestamp_col_5#138 = timestamp_col_8#15)) && (decimal2610_col_3#136 = decimal2610_col_3#10))                                                                                                                                                                                                                                                                                                                                                                                                                                                               :     :              +- Join Inner, ((bigint_col_7#14L IN (bigint_col_7#140L,bigint_col_7#140L) && (timestamp_col_5#138 = timestamp_col_8#15)) && (decimal2610_col_3#136 = decimal2610_col_3#10))
          :     :                 :- Project [col3#2 AS decimal2610_col_3#10, col7#6L AS bigint_col_7#14L, col8#7 AS timestamp_col_8#15]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  :     :                 :- Project [col3#2 AS decimal2610_col_3#10, col7#6L AS bigint_col_7#14L, col8#7 AS timestamp_col_8#15]
          :     :                 :  +- Filter isnotnull(col8#7)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          :     :                 :  +- Filter isnotnull(col8#7)
          :     :                 :     +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                :     :                 :     +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]
          :     :                 +- Project [col3#2 AS decimal2610_col_3#136, col5#4 AS timestamp_col_5#138, col7#6L AS bigint_col_7#140L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               :     :                 +- Project [col3#2 AS decimal2610_col_3#136, col5#4 AS timestamp_col_5#138, col7#6L AS bigint_col_7#140L]
          :     :                    +- Filter isnotnull(col5#4)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          :     :                    +- Filter isnotnull(col5#4)
          :     :                       +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                :     :                       +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]
          :     +- Project [cast(int_col#85 as bigint) AS int_col#167L, cast(decimal_col#86 as bigint) AS decimal_col#168L, cast(int_col_1#87 as bigint) AS int_col_1#169L, cast(float_col#88 as decimal(29,9)) AS float_col#170, float_col_1#89]                                                                                                                                                                                                                                                                                                                                                                                                                         :     +- Project [cast(int_col#85 as bigint) AS int_col#167L, cast(decimal_col#86 as bigint) AS decimal_col#168L, cast(int_col_1#87 as bigint) AS int_col_1#169L, cast(float_col#88 as decimal(29,9)) AS float_col#170, float_col_1#89]
          :        +- Filter ((((((cast(int_col#85 as bigint) = cast(int_col_1#87 as bigint)) && (cast(decimal_col#86 as bigint) = cast(int_col#85 as bigint))) && (cast(decimal_col#86 as bigint) = cast(int_col_1#87 as bigint))) && isnotnull(cast(decimal_col#86 as bigint))) && isnotnull(cast(int_col#85 as bigint))) && isnotnull(cast(int_col_1#87 as bigint)))                                                                                                                                                                                                                                                                                                   :        +- Filter ((((((cast(int_col#85 as bigint) = cast(int_col_1#87 as bigint)) && (cast(decimal_col#86 as bigint) = cast(int_col#85 as bigint))) && (cast(decimal_col#86 as bigint) = cast(int_col_1#87 as bigint))) && isnotnull(cast(decimal_col#86 as bigint))) && isnotnull(cast(int_col#85 as bigint))) && isnotnull(cast(int_col_1#87 as bigint)))
          :           +- Window [lag(-70726.079412976, 46, null) windowspecdefinition(_w2#160 DESC, _w3#161 DESC, ROWS BETWEEN 46 PRECEDING AND 46 PRECEDING) AS float_col#88], [_w2#160 DESC, _w3#161 DESC]                                                                                                                                                                                                                                                                                                                                                                                                                                                              :           +- Window [lag(-70726.079412976, 46, null) windowspecdefinition(_w2#160 DESC, _w3#161 DESC, ROWS BETWEEN 46 PRECEDING AND 46 PRECEDING) AS float_col#88], [_w2#160 DESC, _w3#161 DESC]
!         :              +- Window [min(404172) windowspecdefinition(_w0#158 DESC, _w1#159 DESC, ROWS BETWEEN 96 PRECEDING AND 16 PRECEDING) AS int_col#85], [_w0#158 DESC, _w1#159 DESC]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 :              +- Project [decimal_col#86, int_col_1#87, _w2#160, _w3#161, float_col_1#89, int_col#85]
!         :                 +- Window [avg(1442) windowspecdefinition(_w4#162 ASC, _w5#163 ASC, ROWS BETWEEN CURRENT ROW AND 62 FOLLOWING) AS float_col_1#89], [_w4#162 ASC, _w5#163 ASC]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 :                 +- Window [min(404172) windowspecdefinition(_w0#158 DESC, _w1#159 DESC, ROWS BETWEEN 96 PRECEDING AND 16 PRECEDING) AS int_col#85], [_w0#158 DESC, _w1#159 DESC]
!         :                    +- Project [(int_col_6#147 * null) AS decimal_col#86, (cast(smallint_col_4#30 as int) - int_col_6#147) AS int_col_1#87, (int_col_6#147 * null) AS _w0#158, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w1#159, (int_col_6#147 * null) AS _w2#160, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w3#161, (int_col_6#147 * null) AS _w4#162, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w5#163]                                                                                                                                                                                                                    :                    +- Project [decimal_col#86, int_col_1#87, _w0#158, _w1#159, _w2#160, _w3#161, float_col_1#89]
!         :                       +- Join Inner, ((cast(smallint_col_4#145 as int) = int_col_9#35) && (cast(smallint_col_4#145 as int) = int_col_6#32))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   :                       +- Window [avg(1442) windowspecdefinition(_w4#162 ASC, _w5#163 ASC, ROWS BETWEEN CURRENT ROW AND 62 FOLLOWING) AS float_col_1#89], [_w4#162 ASC, _w5#163 ASC]
!         :                          :- Project [col4#20 AS smallint_col_4#30, col6#22 AS int_col_6#32, col9#25 AS int_col_9#35]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          :                          +- Project [(int_col_6#147 * null) AS decimal_col#86, (cast(smallint_col_4#30 as int) - int_col_6#147) AS int_col_1#87, (int_col_6#147 * null) AS _w0#158, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w1#159, (int_col_6#147 * null) AS _w2#160, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w3#161, (int_col_6#147 * null) AS _w4#162, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w5#163]
!         :                          :  +- Filter (isnotnull(col6#22) && isnotnull(col9#25))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              :                             +- Join Inner, ((cast(smallint_col_4#145 as int) = int_col_9#35) && (cast(smallint_col_4#145 as int) = int_col_6#32))
!         :                          :     +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   :                                :- Project [col4#20 AS smallint_col_4#30, col6#22 AS int_col_6#32, col9#25 AS int_col_9#35]
!         :                          +- Project [col4#20 AS smallint_col_4#145, col6#22 AS int_col_6#147]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 :                                :  +- Filter (isnotnull(col6#22) && isnotnull(col9#25))
!         :                             +- Filter isnotnull(col4#20)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      :                                :     +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]
!         :                                +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   :                                +- Project [col4#20 AS smallint_col_4#145, col6#22 AS int_col_6#147]
!         +- Project [col1#55L AS bigint_col_1#61L, col3#57 AS double_col_3#63, col6#60 AS varchar0164_col_6#66]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          :                                   +- Filter isnotnull(col4#20)
!            +- LocalRelation [col1#55L, col2#56, col3#57, col4#58, col5#59, col6#60]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     :                                      +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]
!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         +- Project [col1#55L AS bigint_col_1#61L, col3#57 AS double_col_3#63, col6#60 AS varchar0164_col_6#66]
!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            +- LocalRelation [col1#55L, col2#56, col3#57, col4#58, col5#59, col6#60]

16/09/29 13:26:17 TRACE SparkOptimizer:
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints ===
 Aggregate [count(1) AS count#179L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Aggregate [count(1) AS count#179L]
 +- Aggregate [int_col_2#165, (cast(bigint_col_7#14L as double) / double_col_3#63), trim(varchar0164_col_6#66)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  +- Aggregate [int_col_2#165, (cast(bigint_col_7#14L as double) / double_col_3#63), trim(varchar0164_col_6#66)]
    +- Project [bigint_col_7#14L, int_col_2#165, double_col_3#63, varchar0164_col_6#66]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             +- Project [bigint_col_7#14L, int_col_2#165, double_col_3#63, varchar0164_col_6#66]
!      +- Join Inner, (((bigint_col_1#61L = bigint_col_7#14L) && (bigint_col_1#61L = int_col#81L)) && (bigint_col_1#61L = int_col_1#82L))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              +- Join Inner, ((((int_col#81L = int_col_1#82L) && (bigint_col_7#14L = int_col#81L)) && (bigint_col_7#14L = int_col_1#82L)) && (((bigint_col_1#61L = bigint_col_7#14L) && (bigint_col_1#61L = int_col#81L)) && (bigint_col_1#61L = int_col_1#82L)))
          :- Aggregate [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165, int_col_3#166], [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        :- Aggregate [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165, int_col_3#166], [int_col#81L, bigint_col_7#14L, int_col_1#82L, int_col_2#165]
          :  +- Union                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     :  +- Union
          :     :- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, cast(int_col_2#83L as decimal(29,9)) AS int_col_2#165, cast(int_col_3#84L as double) AS int_col_3#166]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          :     :- Project [int_col#81L, bigint_col_7#14L, int_col_1#82L, cast(int_col_2#83L as decimal(29,9)) AS int_col_2#165, cast(int_col_3#84L as double) AS int_col_3#166]
!         :     :  +- Filter ((((((int_col#81L = int_col_1#82L) && (bigint_col_7#14L = int_col#81L)) && (bigint_col_7#14L = int_col_1#82L)) && isnotnull(bigint_col_7#14L)) && isnotnull(int_col#81L)) && isnotnull(int_col_1#82L))                                                                                                                                                                                                                                                                                                                                                                                                                                       :     :  +- Filter (((((((((((((((coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) <=> int_col_1#82L) && (int_col#81L <=> int_col_2#83L)) && (int_col_1#82L <=> int_col_2#83L)) && (bigint_col_7#140L <=> int_col#81L)) && int_col#81L IN (int_col_1#82L,int_col_1#82L)) && (bigint_col_7#140L <=> bigint_col_7#14L)) && (coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) <=> bigint_col_7#14L)) && (coalesce(int_col_1#82L, bigint_col_7#140L, bigint_col_7#140L) <=> int_col#81L)) && bigint_col_7#14L IN (bigint_col_7#14L,bigint_col_7#14L)) && int_col_1#82L IN (int_col_1#82L,int_col_1#82L)) && int_col#81L IN (bigint_col_7#140L,bigint_col_7#140L)) && int_col_1#82L IN (bigint_col_7#140L,bigint_col_7#140L)) && (coalesce(int_col#81L, bigint_col_7#140L, bigint_col_7#140L) <=> int_col#81L)) && bigint_col_7#14L IN (int_col#81L,int_col#81L)) && ((((((int_col#81L = int_col_1#82L) && (bigint_col_7#14L = int_col#81L)) && (bigint_col_7#14L = int_col_1#82L)) && isnotnull(bigint_col_7#14L)) && isnotnull(int_col#81L)) && isnotnull(int_col_1#82L)))
          :     :     +- Filter NOT cast(min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L as decimal(22,2)) IN (423.13,cast(bigint_col_7#140L as decimal(22,2)))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                :     :     +- Filter NOT cast(min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L as decimal(22,2)) IN (423.13,cast(bigint_col_7#140L as decimal(22,2)))
          :     :        +- Aggregate [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L), bigint_col_7#14L, bigint_col_7#140L], [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) AS int_col#81L, bigint_col_7#14L, bigint_col_7#140L AS int_col_1#82L, bigint_col_7#14L AS int_col_2#83L, coalesce(bigint_col_7#140L, (coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) - bigint_col_7#14L), min(bigint_col_7#140L)) AS int_col_3#84L, min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L)) AS min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L, bigint_col_7#140L]            :     :        +- Aggregate [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L), bigint_col_7#14L, bigint_col_7#140L], [coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) AS int_col#81L, bigint_col_7#14L, bigint_col_7#140L AS int_col_1#82L, bigint_col_7#14L AS int_col_2#83L, coalesce(bigint_col_7#140L, (coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L) - bigint_col_7#14L), min(bigint_col_7#140L)) AS int_col_3#84L, min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L)) AS min(coalesce(bigint_col_7#14L, bigint_col_7#140L, bigint_col_7#140L))#157L, bigint_col_7#140L]
          :     :           +- Project [bigint_col_7#14L, bigint_col_7#140L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              :     :           +- Project [bigint_col_7#14L, bigint_col_7#140L]
          :     :              +- Join Inner, ((bigint_col_7#14L IN (bigint_col_7#140L,bigint_col_7#140L) && (timestamp_col_5#138 = timestamp_col_8#15)) && (decimal2610_col_3#136 = decimal2610_col_3#10))                                                                                                                                                                                                                                                                                                                                                                                                                                                               :     :              +- Join Inner, ((bigint_col_7#14L IN (bigint_col_7#140L,bigint_col_7#140L) && (timestamp_col_5#138 = timestamp_col_8#15)) && (decimal2610_col_3#136 = decimal2610_col_3#10))
          :     :                 :- Project [col3#2 AS decimal2610_col_3#10, col7#6L AS bigint_col_7#14L, col8#7 AS timestamp_col_8#15]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  :     :                 :- Project [col3#2 AS decimal2610_col_3#10, col7#6L AS bigint_col_7#14L, col8#7 AS timestamp_col_8#15]
          :     :                 :  +- Filter isnotnull(col8#7)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          :     :                 :  +- Filter isnotnull(col8#7)
          :     :                 :     +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                :     :                 :     +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]
          :     :                 +- Project [col3#2 AS decimal2610_col_3#136, col5#4 AS timestamp_col_5#138, col7#6L AS bigint_col_7#140L]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               :     :                 +- Project [col3#2 AS decimal2610_col_3#136, col5#4 AS timestamp_col_5#138, col7#6L AS bigint_col_7#140L]
          :     :                    +- Filter isnotnull(col5#4)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          :     :                    +- Filter isnotnull(col5#4)
          :     :                       +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                :     :                       +- LocalRelation [col1#0, col2#1, col3#2, col4#3, col5#4, col6#5, col7#6L, col8#7]
          :     +- Project [cast(int_col#85 as bigint) AS int_col#167L, cast(decimal_col#86 as bigint) AS decimal_col#168L, cast(int_col_1#87 as bigint) AS int_col_1#169L, cast(float_col#88 as decimal(29,9)) AS float_col#170, float_col_1#89]                                                                                                                                                                                                                                                                                                                                                                                                                         :     +- Project [cast(int_col#85 as bigint) AS int_col#167L, cast(decimal_col#86 as bigint) AS decimal_col#168L, cast(int_col_1#87 as bigint) AS int_col_1#169L, cast(float_col#88 as decimal(29,9)) AS float_col#170, float_col_1#89]
!         :        +- Filter ((((((cast(int_col#85 as bigint) = cast(int_col_1#87 as bigint)) && (cast(decimal_col#86 as bigint) = cast(int_col#85 as bigint))) && (cast(decimal_col#86 as bigint) = cast(int_col_1#87 as bigint))) && isnotnull(cast(decimal_col#86 as bigint))) && isnotnull(cast(int_col#85 as bigint))) && isnotnull(cast(int_col_1#87 as bigint)))                                                                                                                                                                                                                                                                                                   :        +- Filter (((isnotnull(int_col_1#87) && isnotnull(int_col#85)) && isnotnull(decimal_col#86)) && ((((((cast(int_col#85 as bigint) = cast(int_col_1#87 as bigint)) && (cast(decimal_col#86 as bigint) = cast(int_col#85 as bigint))) && (cast(decimal_col#86 as bigint) = cast(int_col_1#87 as bigint))) && isnotnull(cast(decimal_col#86 as bigint))) && isnotnull(cast(int_col#85 as bigint))) && isnotnull(cast(int_col_1#87 as bigint))))
          :           +- Window [lag(-70726.079412976, 46, null) windowspecdefinition(_w2#160 DESC, _w3#161 DESC, ROWS BETWEEN 46 PRECEDING AND 46 PRECEDING) AS float_col#88], [_w2#160 DESC, _w3#161 DESC]                                                                                                                                                                                                                                                                                                                                                                                                                                                              :           +- Window [lag(-70726.079412976, 46, null) windowspecdefinition(_w2#160 DESC, _w3#161 DESC, ROWS BETWEEN 46 PRECEDING AND 46 PRECEDING) AS float_col#88], [_w2#160 DESC, _w3#161 DESC]
          :              +- Project [decimal_col#86, int_col_1#87, _w2#160, _w3#161, float_col_1#89, int_col#85]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          :              +- Project [decimal_col#86, int_col_1#87, _w2#160, _w3#161, float_col_1#89, int_col#85]
          :                 +- Window [min(404172) windowspecdefinition(_w0#158 DESC, _w1#159 DESC, ROWS BETWEEN 96 PRECEDING AND 16 PRECEDING) AS int_col#85], [_w0#158 DESC, _w1#159 DESC]                                                                                                                                                                                                                                                                                                                                                                                                                                                                              :                 +- Window [min(404172) windowspecdefinition(_w0#158 DESC, _w1#159 DESC, ROWS BETWEEN 96 PRECEDING AND 16 PRECEDING) AS int_col#85], [_w0#158 DESC, _w1#159 DESC]
          :                    +- Project [decimal_col#86, int_col_1#87, _w0#158, _w1#159, _w2#160, _w3#161, float_col_1#89]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              :                    +- Project [decimal_col#86, int_col_1#87, _w0#158, _w1#159, _w2#160, _w3#161, float_col_1#89]
          :                       +- Window [avg(1442) windowspecdefinition(_w4#162 ASC, _w5#163 ASC, ROWS BETWEEN CURRENT ROW AND 62 FOLLOWING) AS float_col_1#89], [_w4#162 ASC, _w5#163 ASC]                                                                                                                                                                                                                                                                                                                                                                                                                                                                           :                       +- Window [avg(1442) windowspecdefinition(_w4#162 ASC, _w5#163 ASC, ROWS BETWEEN CURRENT ROW AND 62 FOLLOWING) AS float_col_1#89], [_w4#162 ASC, _w5#163 ASC]
          :                          +- Project [(int_col_6#147 * null) AS decimal_col#86, (cast(smallint_col_4#30 as int) - int_col_6#147) AS int_col_1#87, (int_col_6#147 * null) AS _w0#158, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w1#159, (int_col_6#147 * null) AS _w2#160, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w3#161, (int_col_6#147 * null) AS _w4#162, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w5#163]                                                                                                                                                                                                              :                          +- Project [(int_col_6#147 * null) AS decimal_col#86, (cast(smallint_col_4#30 as int) - int_col_6#147) AS int_col_1#87, (int_col_6#147 * null) AS _w0#158, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w1#159, (int_col_6#147 * null) AS _w2#160, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w3#161, (int_col_6#147 * null) AS _w4#162, (cast(smallint_col_4#30 as int) - int_col_6#147) AS _w5#163]
          :                             +- Join Inner, ((cast(smallint_col_4#145 as int) = int_col_9#35) && (cast(smallint_col_4#145 as int) = int_col_6#32))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             :                             +- Join Inner, ((cast(smallint_col_4#145 as int) = int_col_9#35) && (cast(smallint_col_4#145 as int) = int_col_6#32))
          :                                :- Project [col4#20 AS smallint_col_4#30, col6#22 AS int_col_6#32, col9#25 AS int_col_9#35]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    :                                :- Project [col4#20 AS smallint_col_4#30, col6#22 AS int_col_6#32, col9#25 AS int_col_9#35]
          :                                :  +- Filter (isnotnull(col6#22) && isnotnull(col9#25))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        :                                :  +- Filter (isnotnull(col6#22) && isnotnull(col9#25))
          :                                :     +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             :                                :     +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]
          :                                +- Project [col4#20 AS smallint_col_4#145, col6#22 AS int_col_6#147]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           :                                +- Project [col4#20 AS smallint_col_4#145, col6#22 AS int_col_6#147]
          :                                   +- Filter isnotnull(col4#20)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                :                                   +- Filter isnotnull(col4#20)
          :                                      +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             :                                      +- LocalRelation [col1#17, col2#18, col3#19, col4#20, col5#21, col6#22, col7#23, col8#24, col9#25, col10#26]
          +- Project [col1#55L AS bigint_col_1#61L, col3#57 AS double_col_3#63, col6#60 AS varchar0164_col_6#66]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          +- Project [col1#55L AS bigint_col_1#61L, col3#57 AS double_col_3#63, col6#60 AS varchar0164_col_6#66]
             +- LocalRelation [col1#55L, col2#56, col3#57, col4#58, col5#59, col6#60]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        +- LocalRelation [col1#55L, col2#56, col3#57, col4#58, col5#59, col6#60]
{code};;;","29/Sep/16 20:43;joshrosen;Actually, the above log segment wasn't super useful, so let me post some of the inferred filters instead, since that might make the pattern easier to spot:

{code}
Filter ((isnotnull(col8#7) && (((((col7#6L IN (coalesce(col7#6L, col7#6L, col7#6L),coalesce(col7#6L, col7#6L, col7#6L)) && (coalesce(coalesce(col7#6L, col7#6L, col7#6L), col7#6L, col7#6L) <=> coalesce(col7#6L, col7#6L, col7#6L))) && (col7#6L = coalesce(col7#6L, col7#6L, col7#6L))) && (col7#6L <=> coalesce(col7#6L, col7#6L, col7#6L))) && coalesce(col7#6L, col7#6L, col7#6L) IN (col7#6L,col7#6L)) && col7#6L IN (col7#6L,col7#6L))) && ((((((((((((((((col7#6L = coalesce(col7#6L, coalesce(col7#6L, col7#6L, col7#6L), coalesce(col7#6L, col7#6L, col7#6L))) && (coalesce(coalesce(col7#6L, col7#6L, col7#6L), col7#6L, col7#6L) = col7#6L)) && (coalesce(coalesce(col7#6L, col7#6L, col7#6L), col7#6L, col7#6L) <=> col7#6L)) && (coalesce(col7#6L, coalesce(col7#6L, col7#6L, col7#6L), coalesce(col7#6L, col7#6L, col7#6L)) <=> col7#6L)) && (coalesce(coalesce(coalesce(col7#6L, col7#6L, col7#6L), col7#6L, col7#6L), col7#6L, col7#6L) <=> coalesce(coalesce(col7#6L, col7#6L, col7#6L), col7#6L, col7#6L))) && (coalesce(col7#6L, col7#6L, col7#6L) = coalesce(coalesce(col7#6L, col7#6L, col7#6L), col7#6L, col7#6L))) && (coalesce(coalesce(col7#6L, col7#6L, col7#6L), coalesce(col7#6L, col7#6L, col7#6L), coalesce(col7#6L, col7#6L, col7#6L)) <=> col7#6L)) && (coalesce(coalesce(col7#6L, col7#6L, col7#6L), coalesce(col7#6L, col7#6L, col7#6L), coalesce(col7#6L, col7#6L, col7#6L)) <=> coalesce(col7#6L, coalesce(col7#6L, col7#6L, col7#6L), coalesce(col7#6L, col7#6L, col7#6L)))) && coalesce(coalesce(col7#6L, col7#6L, col7#6L), col7#6L, col7#6L) IN (col7#6L,col7#6L)) && col7#6L IN (coalesce(col7#6L, coalesce(col7#6L, col7#6L, col7#6L), coalesce(col7#6L, col7#6L, col7#6L)),coalesce(col7#6L, coalesce(col7#6L, col7#6L, col7#6L), coalesce(col7#6L, col7#6L, col7#6L)))) && coalesce(col7#6L, coalesce(col7#6L, col7#6L, col7#6L), coalesce(col7#6L, col7#6L, col7#6L)) IN (coalesce(col7#6L, col7#6L, col7#6L),coalesce(col7#6L, col7#6L, col7#6L))) && (coalesce(coalesce(col7#6L, coalesce(col7#6L, col7#6L, col7#6L), coalesce(col7#6L, col7#6L, col7#6L)), coalesce(col7#6L, col7#6L, col7#6L), coalesce(col7#6L, col7#6L, col7#6L)) <=> coalesce(col7#6L, coalesce(col7#6L, col7#6L, col7#6L), coalesce(col7#6L, col7#6L, col7#6L)))) && (coalesce(col7#6L, col7#6L, col7#6L) <=> coalesce(col7#6L, coalesce(col7#6L, col7#6L, col7#6L), coalesce(col7#6L, col7#6L, col7#6L)))) && coalesce(col7#6L, col7#6L, col7#6L) IN (coalesce(coalesce(col7#6L, col7#6L, col7#6L), col7#6L, col7#6L),coalesce(coalesce(col7#6L, col7#6L, col7#6L), col7#6L, col7#6L))) && (coalesce(col7#6L, coalesce(col7#6L, col7#6L, col7#6L), coalesce(col7#6L, col7#6L, col7#6L)) = coalesce(col7#6L, col7#6L, col7#6L))) && true))
{code}

and 

{code}
 Filter ((((isnotnull(int_col_1#87) && isnotnull(int_col#85)) && (((cast(int_col#85 as bigint) = cast(int_col_1#87 as bigint)) && null) && null)) && null) && ((((((((((((((((cast(int_col_1#87 as bigint) IN (cast(int_col_1#87 as bigint),cast(int_col_1#87 as bigint)) && (coalesce(coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)) <=> coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)))) && (cast(int_col_1#87 as bigint) <=> coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)))) && (cast(int_col_1#87 as bigint) = coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)))) && cast(int_col_1#87 as bigint) IN (coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)),coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)))) && coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)) IN (cast(int_col_1#87 as bigint),cast(int_col_1#87 as bigint))) || null) && ((((((null && isnull(null)) && isnull(null)) && null) && null) && null) || null)) && null) && ((((((cast(int_col#85 as bigint) IN (coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint)),coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint))) && (coalesce(coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint)), cast(int_col#85 as bigint), cast(int_col#85 as bigint)) <=> coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint)))) && (cast(int_col#85 as bigint) <=> coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint)))) && (cast(int_col#85 as bigint) = coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint)))) && coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint)) IN (cast(int_col#85 as bigint),cast(int_col#85 as bigint))) && cast(int_col#85 as bigint) IN (cast(int_col#85 as bigint),cast(int_col#85 as bigint))) || isnull(cast(int_col#85 as bigint)))) && null) && ((((((cast(int_col_1#87 as bigint) IN (coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)),coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint))) && (coalesce(coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)) <=> coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)))) && (cast(int_col_1#87 as bigint) <=> coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)))) && (cast(int_col_1#87 as bigint) = coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)))) && coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)) IN (cast(int_col_1#87 as bigint),cast(int_col_1#87 as bigint))) && cast(int_col_1#87 as bigint) IN (cast(int_col_1#87 as bigint),cast(int_col_1#87 as bigint))) || isnull(cast(int_col_1#87 as bigint)))) && ((((((null && isnull(null)) && null) && isnull(null)) && null) && null) || null)) && ((((((null && isnull(null)) && isnull(null)) && null) && null) && null) || isnull(null))) && ((((((coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)) IN (cast(int_col_1#87 as bigint),cast(int_col_1#87 as bigint)) && (coalesce(coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)) <=> coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)))) && cast(int_col_1#87 as bigint) IN (cast(int_col_1#87 as bigint),cast(int_col_1#87 as bigint))) && (cast(int_col_1#87 as bigint) <=> coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)))) && (cast(int_col_1#87 as bigint) = coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)))) && cast(int_col_1#87 as bigint) IN (coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)),coalesce(cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint), cast(int_col_1#87 as bigint)))) || null)) && ((((((cast(int_col#85 as bigint) IN (cast(int_col#85 as bigint),cast(int_col#85 as bigint)) && (coalesce(coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint)), cast(int_col#85 as bigint), cast(int_col#85 as bigint)) <=> coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint)))) && (cast(int_col#85 as bigint) <=> coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint)))) && (cast(int_col#85 as bigint) = coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint)))) && cast(int_col#85 as bigint) IN (coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint)),coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint)))) && coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint)) IN (cast(int_col#85 as bigint),cast(int_col#85 as bigint))) || null)) && ((((((coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint)) IN (cast(int_col#85 as bigint),cast(int_col#85 as bigint)) && (coalesce(coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint)), cast(int_col#85 as bigint), cast(int_col#85 as bigint)) <=> coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint)))) && cast(int_col#85 as bigint) IN (cast(int_col#85 as bigint),cast(int_col#85 as bigint))) && (cast(int_col#85 as bigint) <=> coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint)))) && (cast(int_col#85 as bigint) = coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint)))) && cast(int_col#85 as bigint) IN (coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint)),coalesce(cast(int_col#85 as bigint), cast(int_col#85 as bigint), cast(int_col#85 as bigint)))) || null)))
{code};;;","29/Sep/16 21:18;joshrosen;I managed to shrink to a smaller case which freezes {{explain}}:

{code}
sql(""""""
    CREATE TEMPORARY VIEW table_4(float_col_1, boolean_col_2, decimal2610_col_3, boolean_col_4, timestamp_col_5, boolean_col_6, bigint_col_7, timestamp_col_8) AS VALUES
  (CAST(21.920416 AS FLOAT), false, -182.0700000000BD, true, TIMESTAMP('1996-10-24 00:00:00.0'), true, CAST(-993 AS BIGINT), TIMESTAMP('2007-01-13 00:00:00.0'))
"""""")

sql(""""""
    CREATE TEMPORARY VIEW table_2(bigint_col_1, boolean_col_2, double_col_3, double_col_4, double_col_5, varchar0164_col_6) AS VALUES
  (CAST(-374 AS BIGINT), CAST(NULL AS BOOLEAN), CAST(939.626553676 AS DOUBLE), CAST(-777.275379746 AS DOUBLE), CAST(235.613760023 AS DOUBLE), '86')
  """""")

sql(""""""
    SELECT
    *
    FROM (
        SELECT
        COALESCE(t1.bigint_col_7, t2.bigint_col_7) AS int_col,
        t1.bigint_col_7,
        t2.bigint_col_7 AS int_col_1
        FROM table_4 t1
        CROSS JOIN table_4 t2
    ) t1
        INNER JOIN table_2 t2 ON (((t2.bigint_col_1) = (t1.bigint_col_7)) AND ((t2.bigint_col_1) = (t1.int_col))) AND ((t2.bigint_col_1) = (t1.int_col_1))
   """""")
{code};;;","29/Sep/16 21:37;joshrosen;Here's an even simpler test case:

{code}
sql(""""""CREATE TEMPORARY VIEW foo(a) AS VALUES (CAST(-993 AS BIGINT))"""""")

sql(""""""
    SELECT
    *
    FROM (
        SELECT
        COALESCE(t1.a, t2.a) AS int_col,
        t1.a,
        t2.a AS b
        FROM foo t1
        CROSS JOIN foo t2
    ) t1
        INNER JOIN foo t2 ON (((t2.a) = (t1.a)) AND ((t2.a) = (t1.int_col))) AND ((t2.a) = (t1.b))
   """""")
{code};;;","30/Sep/16 11:19;jiangxb1987;The problem lies in function `QueryPlan.inferAdditionalConstraints`, this functions infers an set of additional constraints from given equality constraints, and it's possible to cause expressions to propagate, For instance:
{code:sql}
Set(a = b, a = c) => b = c
(a、b、c are all `Attribute`s.)
{code}
When c is created from an `Alias`, like `Alias(Coalesce(a, b))`, then we deduct {code}b = Alias(Coalesce(a, b)){code}, after the `PushPredicateThroughJoin` push this predicate through `Join` operator, it will appear in constraints. Such process repeated, the constraints grows larger and larger.

It will be complecated to adapt the `PushPredicateThroughJoin` rule to this case, so maybe we should remove the wrongly propagated constraints in rule `InferFiltersFromConstraints`. I'll submit a PR to resolve this problem soon. Thank you!;;;","30/Sep/16 18:22;sameerag;Thanks [~jiangxb], we came to the same conclusion yesterday. More generally, the issue here is that the `QueryPlan.inferAdditionalConstraints` and `UnaryNode.getAliasedConstraints` can produce a non-converging set of constraints for recursive functions. So if we have 2 constraints of the form (where a is an alias):

{code}
a = b, a = f(b, c)
{code}

Applying both these rules in the next iteration would infer:

{code}
b = f(f(b, c), c)
{code}

and next would infer: 

{code}
b = f(f(f(b, c), c), c)
{code}

and so on...

These rules aren't incorrect per se, but are obviously useless and causes problems of this kind. I think the right fix here would be to modify `UnaryNode.getAliasedConstraints` to not produce these recursive constraints. Would you like to submit a patch?;;;","01/Oct/16 03:54;jiangxb1987;[~sameer]Thank you for your help! Since `UnaryNode.getAliasedConstraints` don't generate recursive constraints, I think we'd better modify `QueryPlan.inferAdditionalConstraints` to avoid problems of this kind, I've finished a naive version of this fix and will soon submit a patch. Thank you!;;;","01/Oct/16 05:04;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/15319;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Erroneous computation in multiplication of transposed SparseMatrix with SparseVector,SPARK-17721,13008479,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,bfruergaard,bfruergaard,bfruergaard,29/Sep/16 09:22,02/Oct/16 02:30,14/Jul/23 06:29,02/Oct/16 02:29,1.4.1,1.5.2,1.6.2,2.0.0,,,,,1.5.3,1.6.3,2.0.2,2.1.0,ML,MLlib,,,,,,,0,correctness,,,,,"There is a bug in how a transposed SparseMatrix (isTransposed=true) does multiplication with a SparseVector. The bug is present (for v. > 2.0.0) in both org.apache.spark.mllib.linalg.BLAS (mllib) and org.apache.spark.ml.linalg.BLAS (mllib-local) in the private gemv method with signature:
bq. gemv(alpha: Double, A: SparseMatrix, x: SparseVector, beta: Double, y: DenseVector).

This bug can be verified by running the following snippet in a Spark shell (here using v1.6.1):
{code:java}
import com.holdenkarau.spark.testing.SharedSparkContext
import org.apache.spark.mllib.linalg._

val A = Matrices.dense(3, 2, Array[Double](0, 2, 1, 1, 2, 0)).asInstanceOf[DenseMatrix].toSparse.transpose
val b = Vectors.sparse(3, Seq[(Int, Double)]((1, 2), (2, 1))).asInstanceOf[SparseVector]

A.multiply(b)
A.multiply(b.toDense)
{code}
The first multiply with the SparseMatrix returns the incorrect result:
{code:java}
org.apache.spark.mllib.linalg.DenseVector = [5.0,0.0]
{code}
whereas the correct result is returned by the second multiply:
{code:java}
org.apache.spark.mllib.linalg.DenseVector = [5.0,4.0]
{code}",Verified on OS X with Spark 1.6.1 and on Databricks running Spark 1.6.1,apachespark,bfruergaard,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 02 02:29:43 UTC 2016,,,,,,,,,,"0|i348fz:",9223372036854775807,,,,,,,,,,,,,1.5.3,1.6.3,2.0.2,2.1.0,,,,,,,,"29/Sep/16 09:34;bfruergaard;I already have a suggested fix in the pipeline; expect a PR soon;;;","29/Sep/16 10:26;srowen;Am I missing something or should this result in an error? A is 3x2 and b is 3x1 and so A x b is invalid. ;;;","29/Sep/16 10:59;bfruergaard;The 3x2 matrix gets transposed, so it becomes a 2x3 matrix which is multiplied with the 3x1, so it is valid. Also note, that if I construct an equivalent SparseMatrix that is 2x3 directly (i.e., not the transpose of the 3x2 matrix), then it works. There only exists an error when the matrix I'm calling multiply on is transposed.;;;","29/Sep/16 11:27;apachespark;User 'bwahlgreen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15296;;;","29/Sep/16 11:36;srowen;Oh right I somehow didn't read the .transpose.;;;","29/Sep/16 21:02;josephkb;Noting here: We should audit MLlib for uses of this multiply to see what algorithms it might have affected.  I'm hoping the effects will have been minimal.;;;","29/Sep/16 23:13;josephkb;OK I did an audit, and this will not have affected any algorithms in 2.0 or before.  But it will affect sparse logistic regression in 2.1!  Thanks for finding this bug.

If users have called Matrix.multiply directly, then they could be affected.;;;","30/Sep/16 09:26;apachespark;User 'bwahlgreen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15311;;;","02/Oct/16 02:29;josephkb;Issue resolved by pull request 15311
[https://github.com/apache/spark/pull/15311];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassCircularityError is thrown when using org.apache.spark.util.Utils.classForName ,SPARK-17714,13008388,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,WeiqingYang,WeiqingYang,28/Sep/16 23:26,13/Feb/17 20:04,14/Jul/23 06:29,13/Feb/17 20:04,,,,,,,,,2.1.1,2.2.0,,,Spark Core,,,,,,,,0,,,,,,"This jira is a follow up to [SPARK-15857| https://issues.apache.org/jira/browse/SPARK-15857] .

Task invokes CallerContext. SetCurrentContext() to set its callerContext to HDFS. In SetCurrentContext(), it tries looking for class {{org.apache.hadoop.ipc.CallerContext}} by using {{org.apache.spark.util.Utils.classForName}}. This causes ClassCircularityError to be thrown when running ReplSuite in master Maven builds (The same tests pass in the SBT build). A hotfix [SPARK-17710|https://issues.apache.org/jira/browse/SPARK-17710] has been made by using Class.forName instead, but it needs further investigation.

Error:
https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.3/2000/testReport/junit/org.apache.spark.repl/ReplSuite/simple_foreach_with_accumulator/
{code}
scala> accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: None, value: 0)
scala> org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ClassCircularityError: io/netty/util/internal/_matchers_/org/apache/spark/network/protocol/MessageMatcher
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:348)
at io.netty.util.internal.JavassistTypeParameterMatcherGenerator.generate(JavassistTypeParameterMatcherGenerator.java:62)
at io.netty.util.internal.JavassistTypeParameterMatcherGenerator.generate(JavassistTypeParameterMatcherGenerator.java:54)
at io.netty.util.internal.TypeParameterMatcher.get(TypeParameterMatcher.java:42)
at io.netty.util.internal.TypeParameterMatcher.find(TypeParameterMatcher.java:78)
at io.netty.handler.codec.MessageToMessageEncoder.<init>(MessageToMessageEncoder.java:59)
at org.apache.spark.network.protocol.MessageEncoder.<init>(MessageEncoder.java:34)
at org.apache.spark.network.TransportContext.<init>(TransportContext.java:78)
at org.apache.spark.rpc.netty.NettyRpcEnv.downloadClient(NettyRpcEnv.scala:354)
at org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:324)
at org.apache.spark.repl.ExecutorClassLoader.org$apache$spark$repl$ExecutorClassLoader$$getClassFileInputStreamFromSparkRPC(ExecutorClassLoader.scala:90)
at org.apache.spark.repl.ExecutorClassLoader$$anonfun$1.apply(ExecutorClassLoader.scala:57)
at org.apache.spark.repl.ExecutorClassLoader$$anonfun$1.apply(ExecutorClassLoader.scala:57)
at org.apache.spark.repl.ExecutorClassLoader.findClassLocally(ExecutorClassLoader.scala:162)
at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:80)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:348)
at io.netty.util.internal.JavassistTypeParameterMatcherGenerator.generate(JavassistTypeParameterMatcherGenerator.java:62)
at io.netty.util.internal.JavassistTypeParameterMatcherGenerator.generate(JavassistTypeParameterMatcherGenerator.java:54)
at io.netty.util.internal.TypeParameterMatcher.get(TypeParameterMatcher.java:42)
at io.netty.util.internal.TypeParameterMatcher.find(TypeParameterMatcher.java:78)
at io.netty.handler.codec.MessageToMessageEncoder.<init>(MessageToMessageEncoder.java:59)
at org.apache.spark.network.protocol.MessageEncoder.<init>(MessageEncoder.java:34)
at org.apache.spark.network.TransportContext.<init>(TransportContext.java:78)
at org.apache.spark.rpc.netty.NettyRpcEnv.downloadClient(NettyRpcEnv.scala:354)
at org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:324)
at org.apache.spark.repl.ExecutorClassLoader.org$apache$spark$repl$ExecutorClassLoader$$getClassFileInputStreamFromSparkRPC(ExecutorClassLoader.scala:90)
at org.apache.spark.repl.ExecutorClassLoader$$anonfun$1.apply(ExecutorClassLoader.scala:57)
at org.apache.spark.repl.ExecutorClassLoader$$anonfun$1.apply(ExecutorClassLoader.scala:57)
at org.apache.spark.repl.ExecutorClassLoader.findClassLocally(ExecutorClassLoader.scala:162)
at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:80)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:348)
at org.apache.spark.util.Utils$.classForName(Utils.scala:225)
at org.apache.spark.util.CallerContext.setCurrentContext(Utils.scala:2492)
at org.apache.spark.scheduler.Task.run(Task.scala:96)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Driver stacktrace:
at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1433)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1421)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1420)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1420)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
at scala.Option.foreach(Option.scala:257)
at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1648)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1603)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1592)
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1901)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1914)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1927)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1941)
at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:894)
at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:892)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
at org.apache.spark.rdd.RDD.foreach(RDD.scala:892)
... 94 elided
Caused by: java.lang.ClassCircularityError: io/netty/util/internal/_matchers_/org/apache/spark/network/protocol/MessageMatcher
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:348)
at io.netty.util.internal.JavassistTypeParameterMatcherGenerator.generate(JavassistTypeParameterMatcherGenerator.java:62)
at io.netty.util.internal.JavassistTypeParameterMatcherGenerator.generate(JavassistTypeParameterMatcherGenerator.java:54)
at io.netty.util.internal.TypeParameterMatcher.get(TypeParameterMatcher.java:42)
at io.netty.util.internal.TypeParameterMatcher.find(TypeParameterMatcher.java:78)
at io.netty.handler.codec.MessageToMessageEncoder.<init>(MessageToMessageEncoder.java:59)
at org.apache.spark.network.protocol.MessageEncoder.<init>(MessageEncoder.java:34)
at org.apache.spark.network.TransportContext.<init>(TransportContext.java:78)
at org.apache.spark.rpc.netty.NettyRpcEnv.downloadClient(NettyRpcEnv.scala:354)
at org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:324)
at org.apache.spark.repl.ExecutorClassLoader.org$apache$spark$repl$ExecutorClassLoader$$getClassFileInputStreamFromSparkRPC(ExecutorClassLoader.scala:90)
at org.apache.spark.repl.ExecutorClassLoader$$anonfun$1.apply(ExecutorClassLoader.scala:57)
at org.apache.spark.repl.ExecutorClassLoader$$anonfun$1.apply(ExecutorClassLoader.scala:57)
at org.apache.spark.repl.ExecutorClassLoader.findClassLocally(ExecutorClassLoader.scala:162)
at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:80)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:348)
at io.netty.util.internal.JavassistTypeParameterMatcherGenerator.generate(JavassistTypeParameterMatcherGenerator.java:62)
at io.netty.util.internal.JavassistTypeParameterMatcherGenerator.generate(JavassistTypeParameterMatcherGenerator.java:54)
at io.netty.util.internal.TypeParameterMatcher.get(TypeParameterMatcher.java:42)
at io.netty.util.internal.TypeParameterMatcher.find(TypeParameterMatcher.java:78)
at io.netty.handler.codec.MessageToMessageEncoder.<init>(MessageToMessageEncoder.java:59)
at org.apache.spark.network.protocol.MessageEncoder.<init>(MessageEncoder.java:34)
at org.apache.spark.network.TransportContext.<init>(TransportContext.java:78)
at org.apache.spark.rpc.netty.NettyRpcEnv.downloadClient(NettyRpcEnv.scala:354)
at org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:324)
at org.apache.spark.repl.ExecutorClassLoader.org$apache$spark$repl$ExecutorClassLoader$$getClassFileInputStreamFromSparkRPC(ExecutorClassLoader.scala:90)
at org.apache.spark.repl.ExecutorClassLoader$$anonfun$1.apply(ExecutorClassLoader.scala:57)
at org.apache.spark.repl.ExecutorClassLoader$$anonfun$1.apply(ExecutorClassLoader.scala:57)
at org.apache.spark.repl.ExecutorClassLoader.findClassLocally(ExecutorClassLoader.scala:162)
at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:80)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:348)
at org.apache.spark.util.Utils$.classForName(Utils.scala:225)
at org.apache.spark.util.CallerContext.setCurrentContext(Utils.scala:2492)
at org.apache.spark.scheduler.Task.run(Task.scala:96)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
scala> res1: Long = 0
{code}
",,apachespark,lian cheng,robmoore,WeiqingYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-15857,SPARK-17710,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 08 20:13:04 UTC 2017,,,,,,,,,,"0|i347vr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/16 16:34;srowen;This is resolved now, right?;;;","13/Oct/16 17:17;WeiqingYang;Not yet, need to investigate more. Could we pull in people more familiar with the Repl classloader stuff? Thanks.;;;","07/Feb/17 19:38;lian cheng;Although I've no idea why this error occurs, it seems that setting system property {{io.netty.noJavassist}} to {{true}} can workaround this issue by disabling Javassist usage in Netty.;;;","08/Feb/17 20:13;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16859;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect result due to invalid pushdown of data-independent filter beneath aggregate,SPARK-17712,13008373,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joshrosen,joshrosen,joshrosen,28/Sep/16 22:16,29/Sep/16 19:12,14/Jul/23 06:29,29/Sep/16 02:05,1.6.2,2.0.0,2.0.2,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,correctness,,,,,"Let {{diamonds}} be a non-empty table. The following two queries should both return no rows, but the first returns a single row:

{code}
SELECT
1
FROM (
    SELECT
    count(*)
    FROM diamonds
) t1
WHERE
false
{code}

{code}
SELECT
1
FROM (
    SELECT
    *
    FROM diamonds
) t1
WHERE
false
{code}",,apachespark,hvanhovell,joshrosen,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 29 02:05:14 UTC 2016,,,,,,,,,,"0|i347sf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/16 22:22;joshrosen;This appears to be an optimizer bug:

{code}
16/09/28 15:18:57 TRACE SparkOptimizer:
=== Applying Rule org.apache.spark.sql.catalyst.analysis.EliminateSubqueryAliases ===
 Project [1 AS 1#10]                                    Project [1 AS 1#10]
 +- Filter false                                        +- Filter false
!   +- SubqueryAlias t1                                    +- Aggregate [count(1) AS count(1)#9L]
!      +- Aggregate [count(1) AS count(1)#9L]                 +- Range (1, 10, step=1, splits=Some(8))
!         +- SubqueryAlias diamonds
!            +- Range (1, 10, step=1, splits=Some(8))

16/09/28 15:18:57 DEBUG SparkOptimizer:
=== Result of Batch Finish Analysis ===
 Project [1 AS 1#10]                                    Project [1 AS 1#10]
 +- Filter false                                        +- Filter false
!   +- SubqueryAlias t1                                    +- Aggregate [count(1) AS count(1)#9L]
!      +- Aggregate [count(1) AS count(1)#9L]                 +- Range (1, 10, step=1, splits=Some(8))
!         +- SubqueryAlias diamonds
!            +- Range (1, 10, step=1, splits=Some(8))

16/09/28 15:18:57 TRACE SparkOptimizer: Fixed point reached for batch Union after 1 iterations.
16/09/28 15:18:57 TRACE SparkOptimizer: Batch Union has no effect.
16/09/28 15:18:57 TRACE SparkOptimizer: Fixed point reached for batch Subquery after 1 iterations.
16/09/28 15:18:57 TRACE SparkOptimizer: Batch Subquery has no effect.
16/09/28 15:18:57 TRACE SparkOptimizer: Fixed point reached for batch Replace Operators after 1 iterations.
16/09/28 15:18:57 TRACE SparkOptimizer: Batch Replace Operators has no effect.
16/09/28 15:18:57 TRACE SparkOptimizer: Fixed point reached for batch Aggregate after 1 iterations.
16/09/28 15:18:57 TRACE SparkOptimizer: Batch Aggregate has no effect.
16/09/28 15:18:57 TRACE SparkOptimizer:
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.PushDownPredicate ===
 Project [1 AS 1#10]                              Project [1 AS 1#10]
!+- Filter false                                  +- Aggregate [count(1) AS count(1)#9L]
!   +- Aggregate [count(1) AS count(1)#9L]           +- Filter false
       +- Range (1, 10, step=1, splits=Some(8))         +- Range (1, 10, step=1, splits=Some(8))

16/09/28 15:18:57 TRACE SparkOptimizer:
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.ColumnPruning ===
 Project [1 AS 1#10]                              Project [1 AS 1#10]
!+- Aggregate [count(1) AS count(1)#9L]           +- Aggregate
!   +- Filter false                                  +- Project
!      +- Range (1, 10, step=1, splits=Some(8))         +- Filter false
!                                                          +- Range (1, 10, step=1, splits=Some(8))

16/09/28 15:18:57 TRACE SparkOptimizer:
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.CollapseProject ===
!Project [1 AS 1#10]                                 Aggregate [1 AS 1#10]
!+- Aggregate                                        +- Project
!   +- Project                                          +- Filter false
!      +- Filter false                                     +- Range (1, 10, step=1, splits=Some(8))
!         +- Range (1, 10, step=1, splits=Some(8))

16/09/28 15:18:57 TRACE SparkOptimizer:
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.PruneFilters ===
 Aggregate [1 AS 1#10]                            Aggregate [1 AS 1#10]
 +- Project                                       +- Project
!   +- Filter false                                  +- LocalRelation <empty>, [id#0L]
!      +- Range (1, 10, step=1, splits=Some(8))

16/09/28 15:18:57 TRACE SparkOptimizer: Fixed point reached for batch Operator Optimizations after 2 iterations.
{code}

It looks like the {{PushDownPredicate}} rule is pushing the filter beneath an aggregate, which is unsound.;;;","28/Sep/16 22:44;joshrosen;Intuitively, the only case where you can push a filter beneath an aggregate is when that filter is defined over the grouping columns / expressions, since in that case the filter is acting to exclude entire groups from the query (like a HAVING clause).

However, our implementation of this logic is wrong because it checks whether a filter condition's references are a subset of the grouping columns without handling the case where an expression references no columns / attributes (as in my {{false}} case (or any expression that the optimizer folds to false)).;;;","28/Sep/16 23:39;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15289;;;","29/Sep/16 02:05;hvanhovell;Resolved per Josh's PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReplSuite fails with ClassCircularityError in master Maven builds,SPARK-17710,13008360,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,WeiqingYang,joshrosen,joshrosen,28/Sep/16 21:11,07/Feb/17 19:50,14/Jul/23 06:29,29/Sep/16 01:21,2.1.0,,,,,,,,2.1.0,,,,Tests,,,,,,,,0,,,,,,"The master Maven build is currently broken because ReplSuite consistently fails with ClassCircularityErrors. See https://spark-tests.appspot.com/jobs/spark-master-test-maven-hadoop-2.3 for a timeline of the failure.

Here's the first build where this failed: https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-2.3/2000/

This appears to correspond to https://github.com/apache/spark/commit/6a68c5d7b4eb07e4ed6b702dd1536cd08d9bba7d

The same tests pass in the SBT build.",,apachespark,joshrosen,WeiqingYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17714,,,,,,SPARK-15857,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 04 23:59:07 UTC 2016,,,,,,,,,,"0|i347pj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/16 22:26;WeiqingYang;A PR https://github.com/apache/spark/pull/15286 has been created to resolve this. 
;;;","28/Sep/16 22:33;apachespark;User 'Sherry302' has created a pull request for this issue:
https://github.com/apache/spark/pull/15286;;;","04/Nov/16 23:59;apachespark;User 'weiqingy' has created a pull request for this issue:
https://github.com/apache/spark/pull/15776;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Web UI prevents spark-submit application to be finished,SPARK-17707,13008266,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,nickorka,nickorka,28/Sep/16 15:47,10/Nov/16 10:28,14/Jul/23 06:29,07/Oct/16 17:39,2.0.0,2.0.1,,,,,,,2.0.2,2.1.0,,,,,,,,,,,0,,,,,,"Here are re-production steps:
1. create any scala spark application which will work long enough to open the application details in Web UI
2. run spark-submit command for standalone cluster, like: --master spark:\\localhost:7077
3. open running application details in Web UI, like: localhost:4040
4. spark-submit will never finish, you will have to kill the process

Cause: The application creates a thread with infinite loop for web UI communication  and never stops it. The application is waiting for the thread to be finished instead, even if you close the web page.",,abridgett,apachespark,bryanc,ekeyser,maver1ck,nickorka,tgraves,umesh9794@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18343,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 07 09:33:44 UTC 2016,,,,,,,,,,"0|i3474n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/16 11:44;srowen;I have never observed this. It would mean that anyone running an app that ever looks at the web UI sees it doesn't finish. Are you sure it's finished? ;;;","01/Oct/16 00:15;nickorka;I noticed this a couple of days ago. We've just switched to Spark 2.0. I'm on Mac OS usitng start-all to spin up standalone spark. While I was developing just spark part on scala, I was using Scala Console for testing and debugging and it looks fine there because it holds all the variables during one session. 

We are using Luigi workflow management for our data pipeline. This is where I've noticed that the same Spark task has unstable behavior. Sometime it passes through sometime it stucks. Luigi opens separate thread to run spark-submit as a shell command. I intercepted the exact command line and started it just in shell. And I've noticed that if you open the running application details in Web UI the application opens socket for piping the details to web listener as a separate thread. I can see the very last statement execution of my app (println(""I'm done"")) but the shell is still waiting. Ctrl-C is the only way to finish the process. If I don't open app details in Web UI, it prints ""I'm done"" closes all accumulators' and shuffles' processes and returns to shell without any problem.;;;","04/Oct/16 09:51;srowen;Are you sure it's not Luigi? is it not consuming output streams / input streams?;;;","04/Oct/16 11:41;nickorka;100% sure that this is not Luigi. If you read my previous comment, I've said, that I noticed that with luigi, but re-produced the same behavior in shell with spark-submit.
Yesterday I re-produced the same issue with --master local as well.;;;","04/Oct/16 12:22;srowen;Yeah I see that, but, the thing is, if this were true then spark-submit would just about never finish, and it does in all the tests harnesses. I just tried it and can't reproduce with SparkPi locally or on YARN, on 1.6 or 2.0. It's got to be something in your setup, like a non-daemon thread started by a dependent library? use kill -QUIT on the driver to dump its threads and see what's still running.;;;","05/Oct/16 11:28;maver1ck;I have similar problem with Spark Submit.
I'm not sure if it's connected with Web UI.

As a workaround I'm trying to use sc.stop() in application.

My problem is also connected with PySpark.
When I'm running python app - at the end python process exits but spark-submit process stucks.;;;","05/Oct/16 13:57;maver1ck;I did some tests.
{code}
mbrynski@jupyter:~$ cat submit.py
from pyspark.sql import SparkSession
spark = SparkSession.builder.enableHiveSupport().getOrCreate()

c = spark.table(""some_table"").count()

print(c)
{code}

1) Running without Spark Submit working with or without using WebUI works fine.
{code}
$ echo $PYTHONPATH
/opt/spark/python:/opt/spark/python/lib/py4j-0.10.3-src.zip:
$ python3 submit.py
10000000

$ ps xf
27756 ?        S      0:00 sshd: mbrynski@pts/20
27757 pts/20   Ss     0:00  \_ -bash
12521 pts/20   Sl+    0:00      \_ python3 submit.py
12524 pts/20   Sl+    0:42          \_ /usr/lib/jvm/java-8-oracle//bin/java -cp /opt/spark/lib/mysql-connector-java-5.1.36-SNAPSHOT-bin.jar:/opt/spark/lib/dwh-hbase-connector.jar:/opt/spark/lib/hive-hbase-handler-1.2.1.spark.jar:/opt/spa
{code}

2) Running with Spark Submit and without Web UI works fine.
{code}
$ /opt/spark/bin/spark-submit submit.py
10000000

$ ps xf
27756 ?        S      0:00 sshd: mbrynski@pts/20
27757 pts/20   Ss     0:00  \_ -bash
14137 pts/20   Sl+    0:16      \_ /usr/lib/jvm/java-8-oracle//bin/java -cp /opt/spark/lib/mysql-connector-java-5.1.36-SNAPSHOT-bin.jar:/opt/spark/lib/dwh-hbase-connector.jar:/opt/spark/lib/hive-hbase-handler-1.2.1.spark.jar:/opt/spark/l
14257 pts/20   S+     0:00          \_ /usr/bin/python3 /home/mbrynski/submit.py
{code}

3) Running with Spark Submit and Web UI. Spark-submit stuck after job finished.
Processes after job's end. (compare to 2)
{code}
$ ps xf
27756 ?        S      0:00 sshd: mbrynski@pts/20
27757 pts/20   Ss     0:00  \_ -bash
16077 pts/20   Sl+    0:43      \_ /usr/lib/jvm/java-8-oracle//bin/java -cp /opt/spark/lib/mysql-connector-java-5.1.36-SNAPSHOT-bin.jar:/opt/spark/lib/dwh-hbase-connector.jar:/opt/spark/lib/hive-hbase-handler-1.2.1.spark.jar:/opt/spark/l
{code}

4) Same like 3. I added spark.stop() to submit.py. Everything works just fine.
{code}
mbrynski@jupyter:~$ cat submit.py
from pyspark.sql import SparkSession
spark = SparkSession.builder.enableHiveSupport().getOrCreate()

c = spark.table(""some_table"").count()

print(c)

spark.stop()
{code};;;","05/Oct/16 14:06;maver1ck;Ad 3) Output from kill -QUIT after job finished.

{code}
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.66-b17 mixed mode):

""DestroyJavaVM"" #189 prio=5 os_prio=0 tid=0x00007f6e50c96000 nid=0x6017 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""shuffle-server-3"" #139 daemon prio=5 os_prio=0 tid=0x00007f6c18004800 nid=0x6246 runnable [0x00007f6dbcdf9000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x000000008087bb78> (a io.netty.channel.nio.SelectedSelectionKeySet)
        - locked <0x000000008087dbf8> (a java.util.Collections$UnmodifiableSet)
        - locked <0x000000008087bae0> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)

""shuffle-server-2"" #138 daemon prio=5 os_prio=0 tid=0x00007f6c18003000 nid=0x6244 runnable [0x00007f6dbcefa000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x000000008087f3f8> (a io.netty.channel.nio.SelectedSelectionKeySet)
        - locked <0x0000000080881478> (a java.util.Collections$UnmodifiableSet)
        - locked <0x000000008087f360> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)

""shuffle-server-7"" #86 daemon prio=5 os_prio=0 tid=0x00007f6c98018000 nid=0x6234 runnable [0x00007f6dbcffb000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x00000000800d93a8> (a io.netty.channel.nio.SelectedSelectionKeySet)
        - locked <0x00000000800db428> (a java.util.Collections$UnmodifiableSet)
        - locked <0x00000000800d9300> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)

""shuffle-server-6"" #85 daemon prio=5 os_prio=0 tid=0x00007f6c98016000 nid=0x6233 runnable [0x00007f6dc41e1000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x00000000800dcbc0> (a io.netty.channel.nio.SelectedSelectionKeySet)
        - locked <0x00000000800dec40> (a java.util.Collections$UnmodifiableSet)
        - locked <0x00000000800dcb28> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)

""shuffle-server-5"" #84 daemon prio=5 os_prio=0 tid=0x00007f6c98014800 nid=0x6232 runnable [0x00007f6dc42e2000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x00000000800ef900> (a io.netty.channel.nio.SelectedSelectionKeySet)
        - locked <0x00000000800f1980> (a java.util.Collections$UnmodifiableSet)
        - locked <0x00000000800ef868> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-188"" #188 daemon prio=5 os_prio=0 tid=0x00007f6bd4001000 nid=0x6214 waiting on condition [0x00007f6dc43e3000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008049f730> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.spark_project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:531)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:47)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:590)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-187"" #187 daemon prio=5 os_prio=0 tid=0x00007f6bd0001000 nid=0x6213 waiting on condition [0x00007f6dc44e4000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008049f730> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.spark_project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:531)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:47)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:590)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-186"" #186 daemon prio=5 os_prio=0 tid=0x00007f6bdc001000 nid=0x6212 waiting on condition [0x00007f6dc45e5000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008049f730> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.spark_project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:531)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:47)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:590)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-185"" #185 daemon prio=5 os_prio=0 tid=0x00007f6bd8001000 nid=0x6211 waiting on condition [0x00007f6dc46e6000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008049f730> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.spark_project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:531)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:47)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:590)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-184"" #184 daemon prio=5 os_prio=0 tid=0x00007f6c54087800 nid=0x6210 waiting on condition [0x00007f6dc47e7000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008049f730> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.spark_project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:531)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:47)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:590)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-183"" #183 daemon prio=5 os_prio=0 tid=0x00007f6c54001000 nid=0x620d waiting on condition [0x00007f6dc48e8000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008049f730> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.spark_project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:531)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:47)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:590)
        at java.lang.Thread.run(Thread.java:745)

""Scheduler-23010977"" #182 prio=5 os_prio=0 tid=0x00007f6c70001800 nid=0x620c waiting on condition [0x00007f6dc4be9000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000808001a0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""task-result-getter-3"" #181 daemon prio=5 os_prio=0 tid=0x00007f6cc0016800 nid=0x61e9 waiting on condition [0x00007f6dc4cea000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008062dcb0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""task-result-getter-2"" #180 daemon prio=5 os_prio=0 tid=0x00007f6cc000d000 nid=0x61e8 waiting on condition [0x00007f6dc4deb000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008062dcb0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""task-result-getter-1"" #179 daemon prio=5 os_prio=0 tid=0x00007f6cc000c000 nid=0x61e7 waiting on condition [0x00007f6dc4eec000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008062dcb0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""task-result-getter-0"" #178 daemon prio=5 os_prio=0 tid=0x00007f6cc000a000 nid=0x61e6 waiting on condition [0x00007f6dc4fed000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008062dcb0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""shuffle-server-1"" #137 daemon prio=5 os_prio=0 tid=0x00007f6c18001000 nid=0x61ca runnable [0x00007f6dc50ee000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x0000000080882c78> (a io.netty.channel.nio.SelectedSelectionKeySet)
        - locked <0x0000000080884cf8> (a java.util.Collections$UnmodifiableSet)
        - locked <0x0000000080882be0> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)

""shuffle-server-4"" #83 daemon prio=5 os_prio=0 tid=0x00007f6c98013000 nid=0x61c6 runnable [0x00007f6dc61f0000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x00000000800f3118> (a io.netty.channel.nio.SelectedSelectionKeySet)
        - locked <0x00000000800f5198> (a java.util.Collections$UnmodifiableSet)
        - locked <0x00000000800f3080> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)

""shuffle-server-3"" #82 daemon prio=5 os_prio=0 tid=0x00007f6c98012000 nid=0x61c5 runnable [0x00007f6dcd1f6000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x0000000080106378> (a io.netty.channel.nio.SelectedSelectionKeySet)
        - locked <0x00000000801083f8> (a java.util.Collections$UnmodifiableSet)
        - locked <0x00000000801062e0> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)

""ResponseProcessor for block BP-1099807597-10.32.32.3-1425981656094:blk_1136380417_62643328"" #156 daemon prio=5 os_prio=0 tid=0x00007f6c1c001000 nid=0x610a runnable [0x00007f6dcd8f7000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x000000008085e378> (a sun.nio.ch.Util$2)
        - locked <0x000000008085e368> (a java.util.Collections$UnmodifiableSet)
        - locked <0x000000008085def0> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:335)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)
        at java.io.FilterInputStream.read(FilterInputStream.java:83)
        at java.io.FilterInputStream.read(FilterInputStream.java:83)
        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2201)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:176)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:867)

""SparkListenerBus"" #28 daemon prio=5 os_prio=0 tid=0x00007f6d40f9a000 nid=0x6107 waiting on condition [0x00007f6dcd9f8000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080346448> (a java.util.concurrent.Semaphore$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at java.util.concurrent.Semaphore.acquire(Semaphore.java:312)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(LiveListenerBus.scala:80)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)
        at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:78)
        at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1249)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:77)

""context-cleaner-periodic-gc"" #155 daemon prio=5 os_prio=0 tid=0x00007f6d40f98000 nid=0x6106 waiting on condition [0x00007f6dcdaf9000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008086b780> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""Spark Context Cleaner"" #154 daemon prio=5 os_prio=0 tid=0x00007f6d40f93000 nid=0x6103 in Object.wait() [0x00007f6dcdbfa000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
        - locked <0x000000008086be98> (a java.lang.ref.ReferenceQueue$Lock)
        at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:175)
        at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1249)
        at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:172)
        at org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:67)

""spark-dynamic-executor-allocation"" #153 daemon prio=5 os_prio=0 tid=0x00007f6d40f8e800 nid=0x6101 waiting on condition [0x00007f6dcdefb000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008086cc20> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""DataStreamer for file /apps/spark/application_1475584896243_0248.inprogress block BP-1099807597-10.32.32.3-1425981656094:blk_1136380417_62643328"" #152 daemon prio=5 os_prio=0 tid=0x00007f6d40a5c000 nid=0x60ff in Object.wait() [0x00007f6dcdffc000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:564)
        - locked <0x000000008086b3b0> (a java.util.LinkedList)

""shuffle-server-0"" #136 daemon prio=5 os_prio=0 tid=0x00007f6d40aed000 nid=0x60fe runnable [0x00007f6dd43df000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x0000000080886268> (a io.netty.channel.nio.SelectedSelectionKeySet)
        - locked <0x000000008089cac0> (a java.util.Collections$UnmodifiableSet)
        - locked <0x00000000808861c0> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)

""Yarn application state monitor"" #127 daemon prio=5 os_prio=0 tid=0x00007f6d4095b000 nid=0x60fd waiting on condition [0x00007f6dd44e0000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
        at java.lang.Thread.sleep(Native Method)
        at org.apache.spark.deploy.yarn.Client.monitorApplication(Client.scala:1024)
        at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:105)

""shuffle-server-2"" #81 daemon prio=5 os_prio=0 tid=0x00007f6c98010800 nid=0x60fa runnable [0x00007f6dd45e1000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x0000000080109b78> (a io.netty.channel.nio.SelectedSelectionKeySet)
        - locked <0x000000008010bbf8> (a java.util.Collections$UnmodifiableSet)
        - locked <0x0000000080109ae0> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)

""threadDeathWatcher-2-1"" #126 daemon prio=1 os_prio=0 tid=0x00007f6c28010000 nid=0x60f9 waiting on condition [0x00007f6dd46e2000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.ThreadDeathWatcher$Watcher.run(ThreadDeathWatcher.java:137)
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
        at java.lang.Thread.run(Thread.java:745)

""shuffle-server-1"" #80 daemon prio=5 os_prio=0 tid=0x00007f6c98010000 nid=0x60f8 runnable [0x00007f6dd49e3000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x0000000080111370> (a io.netty.channel.nio.SelectedSelectionKeySet)
        - locked <0x00000000801133f0> (a java.util.Collections$UnmodifiableSet)
        - locked <0x00000000801112d8> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)

""driver-revive-thread"" #125 daemon prio=5 os_prio=0 tid=0x00007f6d1c003000 nid=0x60ca waiting on condition [0x00007f6dd4be5000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008197f348> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""LeaseRenewer:mbrynski@hdfs1"" #117 daemon prio=5 os_prio=0 tid=0x00007f6d40f7a000 nid=0x60c2 waiting on condition [0x00007f6dd4ae4000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:438)
        at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
        at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
        at java.lang.Thread.run(Thread.java:745)

""IPC Client (15321284) connection to dwh-hm3.adpilot.co/10.32.32.3:8020 from mbrynski"" #115 daemon prio=5 os_prio=0 tid=0x00007f6d40eea000 nid=0x60b3 in Object.wait() [0x00007f6dd4ee6000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:920)
        - locked <0x0000000081b29240> (a org.apache.hadoop.ipc.Client$Connection)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:965)

""IPC Parameter Sending Thread #0"" #114 daemon prio=5 os_prio=0 tid=0x00007f6d40ec7800 nid=0x60b2 waiting on condition [0x00007f6dd51e7000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080610e50> (a java.util.concurrent.SynchronousQueue$TransferStack)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""IPC Client (15321284) connection to dwh-hm3.adpilot.co/10.32.32.3:8032 from mbrynski"" #113 daemon prio=5 os_prio=0 tid=0x00007f6d40ec5800 nid=0x60b1 in Object.wait() [0x00007f6dd52e8000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:920)
        - locked <0x0000000081b9c3e0> (a org.apache.hadoop.ipc.Client$Connection)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:965)

""dag-scheduler-event-loop"" #112 daemon prio=5 os_prio=0 tid=0x00007f6d407b5000 nid=0x60ab waiting on condition [0x00007f6dd64d8000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080437938> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:46)

""Timer-0"" #111 daemon prio=5 os_prio=0 tid=0x00007f6d40786800 nid=0x60aa in Object.wait() [0x00007f6dd65d9000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:502)
        at java.util.TimerThread.mainLoop(Timer.java:526)
        - locked <0x0000000080420e80> (a java.util.TaskQueue)
        at java.util.TimerThread.run(Timer.java:505)

""netty-rpc-env-timeout"" #110 daemon prio=5 os_prio=0 tid=0x00007f6c5000c800 nid=0x60a9 waiting on condition [0x00007f6dd66da000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080121ae0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""heartbeat-receiver-event-loop-thread"" #109 daemon prio=5 os_prio=0 tid=0x00007f6d2c001800 nid=0x60a8 waiting on condition [0x00007f6dd68dc000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008041ac50> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-108"" #108 daemon prio=5 os_prio=0 tid=0x00007f6c50001000 nid=0x60a7 waiting on condition [0x00007f6dd67db000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008049f730> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.spark_project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:531)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:47)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:590)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-106-acceptor-3@111d97fd-ServerConnector@62c9a4d8{HTTP/1.1}{10.32.32.227:48397}"" #106 daemon prio=5 os_prio=0 tid=0x00007f6d40738800 nid=0x60a5 waiting for monitor entry [0x00007f6dd69dd000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:234)
        - waiting to lock <0x0000000080455b88> (a java.lang.Object)
        at org.spark_project.jetty.server.ServerConnector.accept(ServerConnector.java:377)
        at org.spark_project.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:500)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-105-acceptor-2@28d6022e-ServerConnector@62c9a4d8{HTTP/1.1}{10.32.32.227:48397}"" #105 daemon prio=5 os_prio=0 tid=0x00007f6d40736800 nid=0x60a4 waiting for monitor entry [0x00007f6dd6ade000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:234)
        - waiting to lock <0x0000000080455b88> (a java.lang.Object)
        at org.spark_project.jetty.server.ServerConnector.accept(ServerConnector.java:377)
        at org.spark_project.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:500)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-104-acceptor-1@517a00f1-ServerConnector@62c9a4d8{HTTP/1.1}{10.32.32.227:48397}"" #104 daemon prio=5 os_prio=0 tid=0x00007f6d40735000 nid=0x60a3 runnable [0x00007f6dd6bdf000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:422)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:250)
        - locked <0x0000000080455b88> (a java.lang.Object)
        at org.spark_project.jetty.server.ServerConnector.accept(ServerConnector.java:377)
        at org.spark_project.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:500)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-103-acceptor-0@6c0e4729-ServerConnector@62c9a4d8{HTTP/1.1}{10.32.32.227:48397}"" #103 daemon prio=5 os_prio=0 tid=0x00007f6d40733000 nid=0x60a2 waiting for monitor entry [0x00007f6dd6ce0000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:234)
        - waiting to lock <0x0000000080455b88> (a java.lang.Object)
        at org.spark_project.jetty.server.ServerConnector.accept(ServerConnector.java:377)
        at org.spark_project.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:500)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-102-selector-ServerConnectorManager@246f2547/3"" #102 daemon prio=5 os_prio=0 tid=0x00007f6d40731800 nid=0x60a1 runnable [0x00007f6dd6de1000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x000000008049e348> (a sun.nio.ch.Util$2)
        - locked <0x000000008049e338> (a java.util.Collections$UnmodifiableSet)
        - locked <0x000000008049e220> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.spark_project.jetty.io.SelectorManager$ManagedSelector.select(SelectorManager.java:601)
        at org.spark_project.jetty.io.SelectorManager$ManagedSelector.run(SelectorManager.java:550)
        at org.spark_project.jetty.util.thread.NonBlockingThread.run(NonBlockingThread.java:52)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-101-selector-ServerConnectorManager@246f2547/2"" #101 daemon prio=5 os_prio=0 tid=0x00007f6d4072f800 nid=0x60a0 runnable [0x00007f6dd6ee2000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x000000008049b408> (a sun.nio.ch.Util$2)
        - locked <0x000000008049b3f8> (a java.util.Collections$UnmodifiableSet)
        - locked <0x000000008049b2e0> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.spark_project.jetty.io.SelectorManager$ManagedSelector.select(SelectorManager.java:601)
        at org.spark_project.jetty.io.SelectorManager$ManagedSelector.run(SelectorManager.java:550)
        at org.spark_project.jetty.util.thread.NonBlockingThread.run(NonBlockingThread.java:52)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-100-selector-ServerConnectorManager@246f2547/1"" #100 daemon prio=5 os_prio=0 tid=0x00007f6d4072e800 nid=0x609f runnable [0x00007f6dd6fe3000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x000000008049cd00> (a sun.nio.ch.Util$2)
        - locked <0x000000008049ccf0> (a java.util.Collections$UnmodifiableSet)
        - locked <0x000000008049cbd8> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.spark_project.jetty.io.SelectorManager$ManagedSelector.select(SelectorManager.java:601)
        at org.spark_project.jetty.io.SelectorManager$ManagedSelector.run(SelectorManager.java:550)
        at org.spark_project.jetty.util.thread.NonBlockingThread.run(NonBlockingThread.java:52)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-99-selector-ServerConnectorManager@246f2547/0"" #99 daemon prio=5 os_prio=0 tid=0x00007f6d40728000 nid=0x609e runnable [0x00007f6dd72e4000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x00000000804bdf30> (a sun.nio.ch.Util$2)
        - locked <0x00000000804bdf40> (a java.util.Collections$UnmodifiableSet)
        - locked <0x00000000804bdee8> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.spark_project.jetty.io.SelectorManager$ManagedSelector.select(SelectorManager.java:601)
        at org.spark_project.jetty.io.SelectorManager$ManagedSelector.run(SelectorManager.java:550)
        at org.spark_project.jetty.util.thread.NonBlockingThread.run(NonBlockingThread.java:52)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
        at java.lang.Thread.run(Thread.java:745)

""refresh progress"" #97 daemon prio=5 os_prio=0 tid=0x00007f6d4066c000 nid=0x609c in Object.wait() [0x00007f6dd73e5000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        - locked <0x00000000804683a0> (a java.util.TaskQueue)
        at java.util.TimerThread.run(Timer.java:505)

""map-output-dispatcher-7"" #95 daemon prio=5 os_prio=0 tid=0x00007f6d405e8800 nid=0x609b waiting on condition [0x00007f6dd76e6000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080021208> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:338)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""map-output-dispatcher-6"" #94 daemon prio=5 os_prio=0 tid=0x00007f6d405e7000 nid=0x609a waiting on condition [0x00007f6dd77e7000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080021208> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:338)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""map-output-dispatcher-5"" #93 daemon prio=5 os_prio=0 tid=0x00007f6d405e5000 nid=0x6099 waiting on condition [0x00007f6dd78e8000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080021208> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:338)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""map-output-dispatcher-4"" #92 daemon prio=5 os_prio=0 tid=0x00007f6d405e3800 nid=0x6098 waiting on condition [0x00007f6dd79e9000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080021208> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:338)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""map-output-dispatcher-3"" #91 daemon prio=5 os_prio=0 tid=0x00007f6d405e2000 nid=0x6097 waiting on condition [0x00007f6dd7aea000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080021208> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:338)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""map-output-dispatcher-2"" #90 daemon prio=5 os_prio=0 tid=0x00007f6d405e0000 nid=0x6096 waiting on condition [0x00007f6dd7beb000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080021208> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:338)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""map-output-dispatcher-1"" #89 daemon prio=5 os_prio=0 tid=0x00007f6d405de800 nid=0x6095 waiting on condition [0x00007f6dd7cec000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080021208> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:338)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""map-output-dispatcher-0"" #88 daemon prio=5 os_prio=0 tid=0x00007f6d405d9800 nid=0x6091 waiting on condition [0x00007f6dd7ded000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080021208> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:338)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""shuffle-server-0"" #79 daemon prio=5 os_prio=0 tid=0x00007f6d405c1800 nid=0x6090 runnable [0x00007f6ddc19d000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x0000000080021aa8> (a io.netty.channel.nio.SelectedSelectionKeySet)
        - locked <0x0000000080021ac8> (a java.util.Collections$UnmodifiableSet)
        - locked <0x0000000080021a60> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-39"" #70 daemon prio=5 os_prio=0 tid=0x00007f6d40428000 nid=0x608e waiting on condition [0x00007f6ddc4b0000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-38"" #69 daemon prio=5 os_prio=0 tid=0x00007f6d40426800 nid=0x608d waiting on condition [0x00007f6ddc5b1000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-37"" #68 daemon prio=5 os_prio=0 tid=0x00007f6d40425000 nid=0x608c waiting on condition [0x00007f6ddc6b2000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-36"" #67 daemon prio=5 os_prio=0 tid=0x00007f6d40423000 nid=0x608a waiting on condition [0x00007f6ddc7b3000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-35"" #66 daemon prio=5 os_prio=0 tid=0x00007f6d40421800 nid=0x6089 waiting on condition [0x00007f6ddc8b4000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-34"" #65 daemon prio=5 os_prio=0 tid=0x00007f6d4041f800 nid=0x6088 waiting on condition [0x00007f6ddc9b5000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-33"" #64 daemon prio=5 os_prio=0 tid=0x00007f6d4041e000 nid=0x6087 waiting on condition [0x00007f6ddcab6000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-32"" #63 daemon prio=5 os_prio=0 tid=0x00007f6d4041c000 nid=0x6086 waiting on condition [0x00007f6ddcbb7000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-31"" #62 daemon prio=5 os_prio=0 tid=0x00007f6d4041a800 nid=0x6085 waiting on condition [0x00007f6ddccb8000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-30"" #61 daemon prio=5 os_prio=0 tid=0x00007f6d40418800 nid=0x6084 waiting on condition [0x00007f6ddcdb9000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-29"" #60 daemon prio=5 os_prio=0 tid=0x00007f6d40417000 nid=0x6083 waiting on condition [0x00007f6ddceba000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-28"" #59 daemon prio=5 os_prio=0 tid=0x00007f6d40415800 nid=0x6082 waiting on condition [0x00007f6ddcfbb000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-27"" #58 daemon prio=5 os_prio=0 tid=0x00007f6d40413800 nid=0x6081 waiting on condition [0x00007f6ddd0bc000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-26"" #57 daemon prio=5 os_prio=0 tid=0x00007f6d40412000 nid=0x6080 waiting on condition [0x00007f6ddd1bd000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-25"" #56 daemon prio=5 os_prio=0 tid=0x00007f6d40410000 nid=0x607f waiting on condition [0x00007f6ddd2be000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-24"" #55 daemon prio=5 os_prio=0 tid=0x00007f6d4040e800 nid=0x607e waiting on condition [0x00007f6ddd3bf000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-23"" #54 daemon prio=5 os_prio=0 tid=0x00007f6d4040c800 nid=0x607d waiting on condition [0x00007f6ddd4c0000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-22"" #53 daemon prio=5 os_prio=0 tid=0x00007f6d4040b000 nid=0x607c waiting on condition [0x00007f6ddd5c1000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-21"" #52 daemon prio=5 os_prio=0 tid=0x00007f6d40409000 nid=0x607b waiting on condition [0x00007f6ddd6c2000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-20"" #51 daemon prio=5 os_prio=0 tid=0x00007f6d40407800 nid=0x607a waiting on condition [0x00007f6ddd7c3000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-19"" #50 daemon prio=5 os_prio=0 tid=0x00007f6d40406000 nid=0x6079 waiting on condition [0x00007f6ddd8c4000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-18"" #49 daemon prio=5 os_prio=0 tid=0x00007f6d40404000 nid=0x6078 waiting on condition [0x00007f6ddd9c5000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-17"" #48 daemon prio=5 os_prio=0 tid=0x00007f6d40402800 nid=0x6077 waiting on condition [0x00007f6dddac6000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-16"" #47 daemon prio=5 os_prio=0 tid=0x00007f6d40400800 nid=0x6076 waiting on condition [0x00007f6dddbc7000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-15"" #46 daemon prio=5 os_prio=0 tid=0x00007f6d403ff000 nid=0x6075 waiting on condition [0x00007f6dddcc8000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-14"" #45 daemon prio=5 os_prio=0 tid=0x00007f6d403fd000 nid=0x6074 waiting on condition [0x00007f6ddddc9000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-13"" #44 daemon prio=5 os_prio=0 tid=0x00007f6d403fb800 nid=0x6073 waiting on condition [0x00007f6dddeca000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-12"" #43 daemon prio=5 os_prio=0 tid=0x00007f6d403fa000 nid=0x6072 waiting on condition [0x00007f6dddfcb000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-11"" #42 daemon prio=5 os_prio=0 tid=0x00007f6d403f8000 nid=0x6071 waiting on condition [0x00007f6dde0cc000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-10"" #41 daemon prio=5 os_prio=0 tid=0x00007f6d403f6800 nid=0x6070 waiting on condition [0x00007f6dde1cd000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-9"" #40 daemon prio=5 os_prio=0 tid=0x00007f6d403f4800 nid=0x606f waiting on condition [0x00007f6dde2ce000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-8"" #39 daemon prio=5 os_prio=0 tid=0x00007f6d403f3000 nid=0x606e waiting on condition [0x00007f6dde3cf000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-7"" #38 daemon prio=5 os_prio=0 tid=0x00007f6d403f1000 nid=0x606d waiting on condition [0x00007f6dde4d0000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-6"" #37 daemon prio=5 os_prio=0 tid=0x00007f6d403ef800 nid=0x606c waiting on condition [0x00007f6dde5d1000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-5"" #36 daemon prio=5 os_prio=0 tid=0x00007f6d403ed800 nid=0x606b waiting on condition [0x00007f6dde6d2000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-4"" #35 daemon prio=5 os_prio=0 tid=0x00007f6d403ec000 nid=0x606a waiting on condition [0x00007f6dde7d3000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-3"" #34 daemon prio=5 os_prio=0 tid=0x00007f6d403ea800 nid=0x6069 waiting on condition [0x00007f6dde8d4000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-2"" #33 daemon prio=5 os_prio=0 tid=0x00007f6d403e1800 nid=0x6068 waiting on condition [0x00007f6dde9d5000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-1"" #32 daemon prio=5 os_prio=0 tid=0x00007f6d403e0000 nid=0x6067 waiting on condition [0x00007f6ddead6000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-0"" #31 daemon prio=5 os_prio=0 tid=0x00007f6d403df000 nid=0x6066 waiting on condition [0x00007f6ddefdb000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800159d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""process reaper"" #29 daemon prio=10 os_prio=0 tid=0x00007f6d40359000 nid=0x6058 waiting on condition [0x00007f6ddf014000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008003b828> (a java.util.concurrent.SynchronousQueue$TransferStack)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""process reaper"" #25 daemon prio=10 os_prio=0 tid=0x00007f6e50c94000 nid=0x6052 waiting on condition [0x00007f6de40a9000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008003b828> (a java.util.concurrent.SynchronousQueue$TransferStack)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""Service Thread"" #20 daemon prio=9 os_prio=0 tid=0x00007f6e5010d800 nid=0x6047 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C1 CompilerThread14"" #19 daemon prio=9 os_prio=0 tid=0x00007f6e50108800 nid=0x6046 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C1 CompilerThread13"" #18 daemon prio=9 os_prio=0 tid=0x00007f6e50107000 nid=0x6045 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C1 CompilerThread12"" #17 daemon prio=9 os_prio=0 tid=0x00007f6e50104800 nid=0x6044 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C1 CompilerThread11"" #16 daemon prio=9 os_prio=0 tid=0x00007f6e50103000 nid=0x6043 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C1 CompilerThread10"" #15 daemon prio=9 os_prio=0 tid=0x00007f6e50100800 nid=0x6042 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread9"" #14 daemon prio=9 os_prio=0 tid=0x00007f6e500fe800 nid=0x6041 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread8"" #13 daemon prio=9 os_prio=0 tid=0x00007f6e500fc800 nid=0x6040 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread7"" #12 daemon prio=9 os_prio=0 tid=0x00007f6e500fa000 nid=0x603f waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread6"" #11 daemon prio=9 os_prio=0 tid=0x00007f6e500f8800 nid=0x603e waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread5"" #10 daemon prio=9 os_prio=0 tid=0x00007f6e500f6000 nid=0x603d waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread4"" #9 daemon prio=9 os_prio=0 tid=0x00007f6e500f4000 nid=0x603c waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread3"" #8 daemon prio=9 os_prio=0 tid=0x00007f6e500f2000 nid=0x603b waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread2"" #7 daemon prio=9 os_prio=0 tid=0x00007f6e500f0000 nid=0x603a waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread1"" #6 daemon prio=9 os_prio=0 tid=0x00007f6e500ee000 nid=0x6039 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread0"" #5 daemon prio=9 os_prio=0 tid=0x00007f6e500eb000 nid=0x6038 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Signal Dispatcher"" #4 daemon prio=9 os_prio=0 tid=0x00007f6e500e9800 nid=0x6037 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007f6e500b2000 nid=0x6036 in Object.wait() [0x00007f6de61e0000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
        - locked <0x0000000080020960> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)
        at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)

""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007f6e500b0000 nid=0x6035 in Object.wait() [0x00007f6de62e1000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:502)
        at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:157)
        - locked <0x0000000080033b28> (a java.lang.ref.Reference$Lock)

""VM Thread"" os_prio=0 tid=0x00007f6e500aa800 nid=0x6034 runnable

""GC task thread#0 (ParallelGC)"" os_prio=0 tid=0x00007f6e50021800 nid=0x6018 runnable

""GC task thread#1 (ParallelGC)"" os_prio=0 tid=0x00007f6e50023000 nid=0x6019 runnable

""GC task thread#2 (ParallelGC)"" os_prio=0 tid=0x00007f6e50025000 nid=0x601a runnable

""GC task thread#3 (ParallelGC)"" os_prio=0 tid=0x00007f6e50026800 nid=0x601b runnable

""GC task thread#4 (ParallelGC)"" os_prio=0 tid=0x00007f6e50028800 nid=0x601c runnable

""GC task thread#5 (ParallelGC)"" os_prio=0 tid=0x00007f6e5002a000 nid=0x601d runnable

""GC task thread#6 (ParallelGC)"" os_prio=0 tid=0x00007f6e5002c000 nid=0x601e runnable

""GC task thread#7 (ParallelGC)"" os_prio=0 tid=0x00007f6e5002d800 nid=0x601f runnable

""GC task thread#8 (ParallelGC)"" os_prio=0 tid=0x00007f6e5002f800 nid=0x6020 runnable

""GC task thread#9 (ParallelGC)"" os_prio=0 tid=0x00007f6e50031000 nid=0x6021 runnable

""GC task thread#10 (ParallelGC)"" os_prio=0 tid=0x00007f6e50033000 nid=0x6022 runnable

""GC task thread#11 (ParallelGC)"" os_prio=0 tid=0x00007f6e50034800 nid=0x6023 runnable

""GC task thread#12 (ParallelGC)"" os_prio=0 tid=0x00007f6e50036800 nid=0x6024 runnable

""GC task thread#13 (ParallelGC)"" os_prio=0 tid=0x00007f6e50038000 nid=0x6025 runnable

""GC task thread#14 (ParallelGC)"" os_prio=0 tid=0x00007f6e5003a000 nid=0x6026 runnable

""GC task thread#15 (ParallelGC)"" os_prio=0 tid=0x00007f6e5003b800 nid=0x6027 runnable

""GC task thread#16 (ParallelGC)"" os_prio=0 tid=0x00007f6e5003d800 nid=0x6028 runnable

""GC task thread#17 (ParallelGC)"" os_prio=0 tid=0x00007f6e5003f000 nid=0x6029 runnable

""GC task thread#18 (ParallelGC)"" os_prio=0 tid=0x00007f6e50041000 nid=0x602a runnable

""GC task thread#19 (ParallelGC)"" os_prio=0 tid=0x00007f6e50042800 nid=0x602b runnable

""GC task thread#20 (ParallelGC)"" os_prio=0 tid=0x00007f6e50044800 nid=0x602c runnable

""GC task thread#21 (ParallelGC)"" os_prio=0 tid=0x00007f6e50046000 nid=0x602d runnable

""GC task thread#22 (ParallelGC)"" os_prio=0 tid=0x00007f6e50048000 nid=0x602e runnable

""GC task thread#23 (ParallelGC)"" os_prio=0 tid=0x00007f6e50049800 nid=0x602f runnable

""GC task thread#24 (ParallelGC)"" os_prio=0 tid=0x00007f6e5004b800 nid=0x6030 runnable

""GC task thread#25 (ParallelGC)"" os_prio=0 tid=0x00007f6e5004d000 nid=0x6031 runnable

""GC task thread#26 (ParallelGC)"" os_prio=0 tid=0x00007f6e5004f000 nid=0x6032 runnable

""GC task thread#27 (ParallelGC)"" os_prio=0 tid=0x00007f6e50050800 nid=0x6033 runnable

""VM Periodic Task Thread"" os_prio=0 tid=0x00007f6e50110800 nid=0x6048 waiting on condition

JNI global references: 291

Heap
 PSYoungGen      total 570368K, used 374904K [0x00000000d5580000, 0x0000000100000000, 0x0000000100000000)
  eden space 530432K, 63% used [0x00000000d5580000,0x00000000e9ca4c98,0x00000000f5b80000)
  from space 39936K, 99% used [0x00000000f5b80000,0x00000000f8279380,0x00000000f8280000)
  to   space 94208K, 0% used [0x00000000fa400000,0x00000000fa400000,0x0000000100000000)
 ParOldGen       total 1398272K, used 135592K [0x0000000080000000, 0x00000000d5580000, 0x00000000d5580000)
  object space 1398272K, 9% used [0x0000000080000000,0x000000008846a328,0x00000000d5580000)
 Metaspace       used 94146K, capacity 94946K, committed 95104K, reserved 1130496K
  class space    used 12861K, capacity 13064K, committed 13184K, reserved 1048576K
{code};;;","05/Oct/16 14:37;srowen;Ah, yes you do need to call stop() at the end of your app. That could be why I'm not seeing this. 

There is one non-daemon thread:

{code}
""Scheduler-23010977"" #182 prio=5 os_prio=0 tid=0x00007f6c70001800 nid=0x620c waiting on condition [0x00007f6dc4be9000]
{code}

I admit, I am not sure if this is a Spark thread, though I assume it is; I can't find something that makes threads with this name. If we can track that down maybe it could be made a daemon. In any event that's the cause, although, calling stop() in a finally block at the top level of the program may be the right thing to do in any event.;;;","05/Oct/16 14:45;maver1ck;I understand.
But Spark 1.6 works without stopping SparkContext.

Threads without Web UI
{code}
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.66-b17 mixed mode):

""task-result-getter-3"" #181 daemon prio=5 os_prio=0 tid=0x00007f5700006800 nid=0x30e5 waiting on condition [0x00007f582cdf9000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008067e7a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""task-result-getter-2"" #180 daemon prio=5 os_prio=0 tid=0x00007f5700005000 nid=0x30e4 waiting on condition [0x00007f582cefa000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008067e7a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""task-result-getter-1"" #179 daemon prio=5 os_prio=0 tid=0x00007f5700003800 nid=0x30e3 waiting on condition [0x00007f582cffb000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008067e7a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""task-result-getter-0"" #178 daemon prio=5 os_prio=0 tid=0x00007f5700002000 nid=0x30e2 waiting on condition [0x00007f58341e8000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008067e7a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""shuffle-server-1"" #137 daemon prio=5 os_prio=0 tid=0x00007f563c001000 nid=0x30da runnable [0x00007f58342e9000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x0000000081bc96e8> (a io.netty.channel.nio.SelectedSelectionKeySet)
        - locked <0x0000000081bd8f28> (a java.util.Collections$UnmodifiableSet)
        - locked <0x0000000081bc9650> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)

""shuffle-server-4"" #83 daemon prio=5 os_prio=0 tid=0x00007f56bc013000 nid=0x30d9 runnable [0x00007f58343ea000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x00000000801124f0> (a io.netty.channel.nio.SelectedSelectionKeySet)
        - locked <0x00000000801393a0> (a java.util.Collections$UnmodifiableSet)
        - locked <0x0000000080112458> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)

""shuffle-server-3"" #82 daemon prio=5 os_prio=0 tid=0x00007f56bc012000 nid=0x30d8 runnable [0x00007f58362f8000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x000000008010ecd8> (a io.netty.channel.nio.SelectedSelectionKeySet)
        - locked <0x0000000080110d58> (a java.util.Collections$UnmodifiableSet)
        - locked <0x000000008010ec40> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)

""ResponseProcessor for block BP-1099807597-10.32.32.3-1425981656094:blk_1136381829_62644740"" #156 daemon prio=5 os_prio=0 tid=0x00007f5640001000 nid=0x303d runnable [0x00007f58369f9000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x0000000081b70230> (a sun.nio.ch.Util$2)
        - locked <0x0000000081b70220> (a java.util.Collections$UnmodifiableSet)
        - locked <0x0000000081b6fda8> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:335)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)
        at java.io.FilterInputStream.read(FilterInputStream.java:83)
        at java.io.FilterInputStream.read(FilterInputStream.java:83)
        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2201)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:176)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:867)

""SparkListenerBus"" #28 daemon prio=5 os_prio=0 tid=0x00007f5765411000 nid=0x3037 waiting on condition [0x00007f5836afa000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080061df8> (a java.util.concurrent.Semaphore$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at java.util.concurrent.Semaphore.acquire(Semaphore.java:312)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(LiveListenerBus.scala:80)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)
        at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:78)
        at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1249)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:77)

""context-cleaner-periodic-gc"" #155 daemon prio=5 os_prio=0 tid=0x00007f576540f800 nid=0x3036 waiting on condition [0x00007f5836bfb000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000081b75588> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""Spark Context Cleaner"" #154 daemon prio=5 os_prio=0 tid=0x00007f576540c800 nid=0x3035 in Object.wait() [0x00007f5836cfc000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
        - locked <0x0000000081b75ca0> (a java.lang.ref.ReferenceQueue$Lock)
        at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:175)
        at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1249)
        at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:172)
        at org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:67)

""spark-dynamic-executor-allocation"" #153 daemon prio=5 os_prio=0 tid=0x00007f5765408000 nid=0x3034 waiting on condition [0x00007f5836ffd000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000081b76a28> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""DataStreamer for file /apps/spark/application_1475584896243_0256.inprogress block BP-1099807597-10.32.32.3-1425981656094:blk_1136381829_62644740"" #152 daemon prio=5 os_prio=0 tid=0x00007f57652be000 nid=0x3033 in Object.wait() [0x00007f585418a000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:564)
        - locked <0x0000000081b751b8> (a java.util.LinkedList)

""shuffle-server-0"" #136 daemon prio=5 os_prio=0 tid=0x00007f5764dee800 nid=0x3032 runnable [0x00007f585448b000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x0000000081bda498> (a io.netty.channel.nio.SelectedSelectionKeySet)
        - locked <0x0000000081c30de8> (a java.util.Collections$UnmodifiableSet)
        - locked <0x0000000081bda3f0> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)

""Yarn application state monitor"" #127 daemon prio=5 os_prio=0 tid=0x00007f57652a4000 nid=0x3031 waiting on condition [0x00007f585458c000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
        at java.lang.Thread.sleep(Native Method)
        at org.apache.spark.deploy.yarn.Client.monitorApplication(Client.scala:1024)
        at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:105)

""shuffle-server-2"" #81 daemon prio=5 os_prio=0 tid=0x00007f56bc010000 nid=0x3030 runnable [0x00007f585468d000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x00000000800d88c8> (a io.netty.channel.nio.SelectedSelectionKeySet)
        - locked <0x000000008012b0f0> (a java.util.Collections$UnmodifiableSet)
        - locked <0x00000000800d8830> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)

""threadDeathWatcher-2-1"" #126 daemon prio=1 os_prio=0 tid=0x00007f564c010000 nid=0x302f waiting on condition [0x00007f585578f000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.ThreadDeathWatcher$Watcher.run(ThreadDeathWatcher.java:137)
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
        at java.lang.Thread.run(Thread.java:745)

""shuffle-server-1"" #80 daemon prio=5 os_prio=0 tid=0x00007f56bc00f800 nid=0x302e runnable [0x00007f5855a90000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x000000008007a498> (a io.netty.channel.nio.SelectedSelectionKeySet)
        - locked <0x000000008007c518> (a java.util.Collections$UnmodifiableSet)
        - locked <0x000000008007a400> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)

""driver-revive-thread"" #125 daemon prio=5 os_prio=0 tid=0x00007f5740003000 nid=0x2fef waiting on condition [0x00007f5855c92000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000808e05a0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""LeaseRenewer:mbrynski@hdfs1"" #117 daemon prio=5 os_prio=0 tid=0x00007f5765289000 nid=0x2fd4 waiting on condition [0x00007f5855b91000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:438)
        at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
        at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
        at java.lang.Thread.run(Thread.java:745)

""IPC Client (550242927) connection to dwh-hm3.adpilot.co/10.32.32.3:8020 from mbrynski"" #115 daemon prio=5 os_prio=0 tid=0x00007f57651f9000 nid=0x2fc2 in Object.wait() [0x00007f5855f93000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:920)
        - locked <0x0000000081a61428> (a org.apache.hadoop.ipc.Client$Connection)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:965)

""IPC Parameter Sending Thread #0"" #114 daemon prio=5 os_prio=0 tid=0x00007f57651d6000 nid=0x2fc1 waiting on condition [0x00007f5856294000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000804e1d70> (a java.util.concurrent.SynchronousQueue$TransferStack)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""IPC Client (550242927) connection to dwh-hm3.adpilot.co/10.32.32.3:8032 from mbrynski"" #113 daemon prio=5 os_prio=0 tid=0x00007f57651d5000 nid=0x2fc0 in Object.wait() [0x00007f5856395000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:920)
        - locked <0x0000000081a66db0> (a org.apache.hadoop.ipc.Client$Connection)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:965)

""dag-scheduler-event-loop"" #112 daemon prio=5 os_prio=0 tid=0x00007f57648d2000 nid=0x2fb1 waiting on condition [0x00007f58574f3000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008046ada0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:46)

""Timer-0"" #111 daemon prio=5 os_prio=0 tid=0x00007f5764764800 nid=0x2fb0 in Object.wait() [0x00007f58575f4000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        - locked <0x0000000080492ca8> (a java.util.TaskQueue)
        at java.util.TimerThread.run(Timer.java:505)

""netty-rpc-env-timeout"" #110 daemon prio=5 os_prio=0 tid=0x00007f567400e800 nid=0x2faf waiting on condition [0x00007f58576f5000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000800d5fc8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""heartbeat-receiver-event-loop-thread"" #109 daemon prio=5 os_prio=0 tid=0x00007f575000b800 nid=0x2fae waiting on condition [0x00007f58578f7000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080465e80> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-108"" #108 daemon prio=5 os_prio=0 tid=0x00007f5674001000 nid=0x2fad waiting on condition [0x00007f58577f6000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080449238> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.spark_project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:531)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:47)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:590)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-106-acceptor-3@5f5f8f8a-ServerConnector@61a163b6{HTTP/1.1}{10.32.32.227:55062}"" #106 daemon prio=5 os_prio=0 tid=0x00007f5764723000 nid=0x2fab waiting for monitor entry [0x00007f58579f8000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:234)
        - waiting to lock <0x000000008048cf00> (a java.lang.Object)
        at org.spark_project.jetty.server.ServerConnector.accept(ServerConnector.java:377)
        at org.spark_project.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:500)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-105-acceptor-2@5afa7c48-ServerConnector@61a163b6{HTTP/1.1}{10.32.32.227:55062}"" #105 daemon prio=5 os_prio=0 tid=0x00007f5764721000 nid=0x2faa waiting for monitor entry [0x00007f5857af9000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:234)
        - waiting to lock <0x000000008048cf00> (a java.lang.Object)
        at org.spark_project.jetty.server.ServerConnector.accept(ServerConnector.java:377)
        at org.spark_project.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:500)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-104-acceptor-1@24a06cf5-ServerConnector@61a163b6{HTTP/1.1}{10.32.32.227:55062}"" #104 daemon prio=5 os_prio=0 tid=0x00007f576471f800 nid=0x2fa9 waiting for monitor entry [0x00007f5857bfa000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:234)
        - waiting to lock <0x000000008048cf00> (a java.lang.Object)
        at org.spark_project.jetty.server.ServerConnector.accept(ServerConnector.java:377)
        at org.spark_project.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:500)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-103-acceptor-0@6f66802a-ServerConnector@61a163b6{HTTP/1.1}{10.32.32.227:55062}"" #103 daemon prio=5 os_prio=0 tid=0x00007f576471e000 nid=0x2fa8 runnable [0x00007f5857cfb000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:422)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:250)
        - locked <0x000000008048cf00> (a java.lang.Object)
        at org.spark_project.jetty.server.ServerConnector.accept(ServerConnector.java:377)
        at org.spark_project.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:500)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-102-selector-ServerConnectorManager@273066d9/3"" #102 daemon prio=5 os_prio=0 tid=0x00007f576471c000 nid=0x2fa7 runnable [0x00007f5857dfc000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x0000000080447e08> (a sun.nio.ch.Util$2)
        - locked <0x0000000080447df8> (a java.util.Collections$UnmodifiableSet)
        - locked <0x0000000080447ce0> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.spark_project.jetty.io.SelectorManager$ManagedSelector.select(SelectorManager.java:601)
        at org.spark_project.jetty.io.SelectorManager$ManagedSelector.run(SelectorManager.java:550)
        at org.spark_project.jetty.util.thread.NonBlockingThread.run(NonBlockingThread.java:52)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-101-selector-ServerConnectorManager@273066d9/2"" #101 daemon prio=5 os_prio=0 tid=0x00007f576471a800 nid=0x2fa6 runnable [0x00007f5857efd000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x000000008048d470> (a sun.nio.ch.Util$2)
        - locked <0x000000008048d460> (a java.util.Collections$UnmodifiableSet)
        - locked <0x000000008048d348> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.spark_project.jetty.io.SelectorManager$ManagedSelector.select(SelectorManager.java:601)
        at org.spark_project.jetty.io.SelectorManager$ManagedSelector.run(SelectorManager.java:550)
        at org.spark_project.jetty.util.thread.NonBlockingThread.run(NonBlockingThread.java:52)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-100-selector-ServerConnectorManager@273066d9/1"" #100 daemon prio=5 os_prio=0 tid=0x00007f5764719000 nid=0x2fa5 runnable [0x00007f5857ffe000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x00000000804496e0> (a sun.nio.ch.Util$2)
        - locked <0x00000000804496d0> (a java.util.Collections$UnmodifiableSet)
        - locked <0x00000000804495b8> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.spark_project.jetty.io.SelectorManager$ManagedSelector.select(SelectorManager.java:601)
        at org.spark_project.jetty.io.SelectorManager$ManagedSelector.run(SelectorManager.java:550)
        at org.spark_project.jetty.util.thread.NonBlockingThread.run(NonBlockingThread.java:52)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
        at java.lang.Thread.run(Thread.java:745)

""SparkUI-99-selector-ServerConnectorManager@273066d9/0"" #99 daemon prio=5 os_prio=0 tid=0x00007f5764712800 nid=0x2fa4 runnable [0x00007f585c3ba000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x000000008048b6c8> (a sun.nio.ch.Util$2)
        - locked <0x000000008048b6b8> (a java.util.Collections$UnmodifiableSet)
        - locked <0x000000008048b5a0> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.spark_project.jetty.io.SelectorManager$ManagedSelector.select(SelectorManager.java:601)
        at org.spark_project.jetty.io.SelectorManager$ManagedSelector.run(SelectorManager.java:550)
        at org.spark_project.jetty.util.thread.NonBlockingThread.run(NonBlockingThread.java:52)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
        at java.lang.Thread.run(Thread.java:745)

""refresh progress"" #97 daemon prio=5 os_prio=0 tid=0x00007f576465d000 nid=0x2fa0 in Object.wait() [0x00007f585c4bb000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        - locked <0x000000008048edd8> (a java.util.TaskQueue)
        at java.util.TimerThread.run(Timer.java:505)

""map-output-dispatcher-7"" #95 daemon prio=5 os_prio=0 tid=0x00007f57645f1000 nid=0x2f97 waiting on condition [0x00007f585c7bc000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080015ad8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:338)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""map-output-dispatcher-6"" #94 daemon prio=5 os_prio=0 tid=0x00007f57645ef000 nid=0x2f96 waiting on condition [0x00007f585c8bd000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080015ad8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:338)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""map-output-dispatcher-5"" #93 daemon prio=5 os_prio=0 tid=0x00007f57645ed800 nid=0x2f95 waiting on condition [0x00007f585c9be000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080015ad8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:338)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""map-output-dispatcher-4"" #92 daemon prio=5 os_prio=0 tid=0x00007f57645eb800 nid=0x2f94 waiting on condition [0x00007f585cabf000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080015ad8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:338)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""map-output-dispatcher-3"" #91 daemon prio=5 os_prio=0 tid=0x00007f57645ea000 nid=0x2f93 waiting on condition [0x00007f585cbc0000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080015ad8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:338)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""map-output-dispatcher-2"" #90 daemon prio=5 os_prio=0 tid=0x00007f57645e8800 nid=0x2f92 waiting on condition [0x00007f585ccc1000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080015ad8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:338)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""map-output-dispatcher-1"" #89 daemon prio=5 os_prio=0 tid=0x00007f57645e7000 nid=0x2f91 waiting on condition [0x00007f585cdc2000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080015ad8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:338)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""map-output-dispatcher-0"" #88 daemon prio=5 os_prio=0 tid=0x00007f57645e2000 nid=0x2f90 waiting on condition [0x00007f585cec3000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080015ad8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:338)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""shuffle-server-0"" #79 daemon prio=5 os_prio=0 tid=0x00007f57645ca000 nid=0x2f8f runnable [0x00007f585cfc4000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x0000000080027ae0> (a io.netty.channel.nio.SelectedSelectionKeySet)
        - locked <0x0000000080027b00> (a java.util.Collections$UnmodifiableSet)
        - locked <0x0000000080027a98> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-39"" #70 daemon prio=5 os_prio=0 tid=0x00007f5764430800 nid=0x2f82 waiting on condition [0x00007f585d4d6000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-38"" #69 daemon prio=5 os_prio=0 tid=0x00007f576442e800 nid=0x2f81 waiting on condition [0x00007f585d5d7000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-37"" #68 daemon prio=5 os_prio=0 tid=0x00007f576442d000 nid=0x2f80 waiting on condition [0x00007f585d6d8000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-36"" #67 daemon prio=5 os_prio=0 tid=0x00007f576442b800 nid=0x2f7f waiting on condition [0x00007f585d7d9000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-35"" #66 daemon prio=5 os_prio=0 tid=0x00007f5764429800 nid=0x2f7e waiting on condition [0x00007f585d8da000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-34"" #65 daemon prio=5 os_prio=0 tid=0x00007f5764428000 nid=0x2f7d waiting on condition [0x00007f585d9db000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-33"" #64 daemon prio=5 os_prio=0 tid=0x00007f5764426000 nid=0x2f7c waiting on condition [0x00007f585dadc000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-32"" #63 daemon prio=5 os_prio=0 tid=0x00007f5764424800 nid=0x2f7b waiting on condition [0x00007f585dbdd000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-31"" #62 daemon prio=5 os_prio=0 tid=0x00007f5764422800 nid=0x2f7a waiting on condition [0x00007f585dcde000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-30"" #61 daemon prio=5 os_prio=0 tid=0x00007f5764421000 nid=0x2f79 waiting on condition [0x00007f585dddf000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-29"" #60 daemon prio=5 os_prio=0 tid=0x00007f576441f000 nid=0x2f77 waiting on condition [0x00007f585dee0000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-28"" #59 daemon prio=5 os_prio=0 tid=0x00007f576441d800 nid=0x2f76 waiting on condition [0x00007f585dfe1000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-27"" #58 daemon prio=5 os_prio=0 tid=0x00007f576441c000 nid=0x2f75 waiting on condition [0x00007f585e0e2000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-26"" #57 daemon prio=5 os_prio=0 tid=0x00007f576441a000 nid=0x2f74 waiting on condition [0x00007f585e1e3000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-25"" #56 daemon prio=5 os_prio=0 tid=0x00007f5764418800 nid=0x2f73 waiting on condition [0x00007f585e2e4000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-24"" #55 daemon prio=5 os_prio=0 tid=0x00007f5764416800 nid=0x2f72 waiting on condition [0x00007f585e3e5000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-23"" #54 daemon prio=5 os_prio=0 tid=0x00007f5764415000 nid=0x2f71 waiting on condition [0x00007f585e4e6000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-22"" #53 daemon prio=5 os_prio=0 tid=0x00007f5764413000 nid=0x2f70 waiting on condition [0x00007f585e5e7000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-21"" #52 daemon prio=5 os_prio=0 tid=0x00007f5764411800 nid=0x2f6f waiting on condition [0x00007f585e6e8000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-20"" #51 daemon prio=5 os_prio=0 tid=0x00007f576440f800 nid=0x2f6e waiting on condition [0x00007f585e7e9000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-19"" #50 daemon prio=5 os_prio=0 tid=0x00007f576440e000 nid=0x2f6d waiting on condition [0x00007f585e8ea000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-18"" #49 daemon prio=5 os_prio=0 tid=0x00007f576440c800 nid=0x2f6c waiting on condition [0x00007f585e9eb000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-17"" #48 daemon prio=5 os_prio=0 tid=0x00007f576440a800 nid=0x2f6b waiting on condition [0x00007f585eaec000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-16"" #47 daemon prio=5 os_prio=0 tid=0x00007f5764409000 nid=0x2f6a waiting on condition [0x00007f585ebed000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-15"" #46 daemon prio=5 os_prio=0 tid=0x00007f5764407000 nid=0x2f69 waiting on condition [0x00007f585ecee000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-14"" #45 daemon prio=5 os_prio=0 tid=0x00007f5764405800 nid=0x2f68 waiting on condition [0x00007f585edef000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-13"" #44 daemon prio=5 os_prio=0 tid=0x00007f5764403800 nid=0x2f67 waiting on condition [0x00007f585eef0000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-12"" #43 daemon prio=5 os_prio=0 tid=0x00007f5764402000 nid=0x2f66 waiting on condition [0x00007f585eff1000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-11"" #42 daemon prio=5 os_prio=0 tid=0x00007f5764400800 nid=0x2f65 waiting on condition [0x00007f585f0f2000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-10"" #41 daemon prio=5 os_prio=0 tid=0x00007f57643fe800 nid=0x2f64 waiting on condition [0x00007f585f1f3000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-9"" #40 daemon prio=5 os_prio=0 tid=0x00007f57643fd000 nid=0x2f63 waiting on condition [0x00007f585f2f4000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-8"" #39 daemon prio=5 os_prio=0 tid=0x00007f57643fb000 nid=0x2f62 waiting on condition [0x00007f585f3f5000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-7"" #38 daemon prio=5 os_prio=0 tid=0x00007f57643f9800 nid=0x2f61 waiting on condition [0x00007f585f4f6000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-6"" #37 daemon prio=5 os_prio=0 tid=0x00007f57643f7800 nid=0x2f60 waiting on condition [0x00007f585f5f7000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-5"" #36 daemon prio=5 os_prio=0 tid=0x00007f57643f6000 nid=0x2f5f waiting on condition [0x00007f585f6f8000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-4"" #35 daemon prio=5 os_prio=0 tid=0x00007f57643f4000 nid=0x2f5d waiting on condition [0x00007f585f7f9000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-3"" #34 daemon prio=5 os_prio=0 tid=0x00007f57643f2800 nid=0x2f5c waiting on condition [0x00007f585f8fa000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-2"" #33 daemon prio=5 os_prio=0 tid=0x00007f57643e9800 nid=0x2f5b waiting on condition [0x00007f585f9fb000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-1"" #32 daemon prio=5 os_prio=0 tid=0x00007f57643e8000 nid=0x2f5a waiting on condition [0x00007f585fafc000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""dispatcher-event-loop-0"" #31 daemon prio=5 os_prio=0 tid=0x00007f57643e7000 nid=0x2f59 waiting on condition [0x00007f585fffe000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000008002fa38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:207)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""process reaper"" #29 daemon prio=10 os_prio=0 tid=0x00007f5764362800 nid=0x2f57 waiting on condition [0x00007f5864090000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080049a38> (a java.util.concurrent.SynchronousQueue$TransferStack)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""Thread-3"" #27 daemon prio=5 os_prio=0 tid=0x00007f576c00b000 nid=0x2f53 waiting on condition [0x00007f58648fa000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000f843d438> (a scala.concurrent.impl.Promise$CompletionLatch)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:202)
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:153)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:623)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1916)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1930)
        at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
        at org.apache.spark.rdd.RDD.collect(RDD.scala:911)
        at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:290)
        at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2193)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
        at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2546)
        at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2192)
        at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2199)
        at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2227)
        at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2226)
        at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2559)
        at org.apache.spark.sql.Dataset.count(Dataset.scala:2226)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:280)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:214)
        at java.lang.Thread.run(Thread.java:745)

""redirect output"" #26 daemon prio=5 os_prio=0 tid=0x00007f5894c86800 nid=0x2f52 runnable [0x00007f5864d37000]
   java.lang.Thread.State: RUNNABLE
        at java.io.FileInputStream.readBytes(Native Method)
        at java.io.FileInputStream.read(FileInputStream.java:255)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
        - locked <0x0000000080076b00> (a java.lang.UNIXProcess$ProcessPipeInputStream)
        at java.io.FilterInputStream.read(FilterInputStream.java:107)
        at org.apache.spark.util.RedirectThread$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2455)
        at org.apache.spark.util.RedirectThread$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2453)
        at org.apache.spark.util.RedirectThread$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2453)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1307)
        at org.apache.spark.util.RedirectThread$$anonfun$run$1.apply$mcV$sp(Utils.scala:2461)
        at org.apache.spark.util.RedirectThread$$anonfun$run$1.apply(Utils.scala:2461)
        at org.apache.spark.util.RedirectThread$$anonfun$run$1.apply(Utils.scala:2461)
        at scala.util.control.Exception$Catch.apply(Exception.scala:103)
        at org.apache.spark.util.RedirectThread.run(Utils.scala:2451)

""process reaper"" #25 daemon prio=10 os_prio=0 tid=0x00007f5894c84800 nid=0x2f51 runnable [0x00007f5864b35000]
   java.lang.Thread.State: RUNNABLE
        at java.lang.UNIXProcess.waitForProcessExit(Native Method)
        at java.lang.UNIXProcess.lambda$initStreams$275(UNIXProcess.java:290)
        at java.lang.UNIXProcess$$Lambda$7/1014166943.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""Thread-2"" #24 daemon prio=5 os_prio=0 tid=0x00007f5778004800 nid=0x2f4f runnable [0x00007f5864c36000]
   java.lang.Thread.State: RUNNABLE
        at java.net.PlainSocketImpl.socketAccept(Native Method)
        at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)
        at java.net.ServerSocket.implAccept(ServerSocket.java:545)
        at java.net.ServerSocket.accept(ServerSocket.java:513)
        at py4j.GatewayServer.run(GatewayServer.java:634)
        at java.lang.Thread.run(Thread.java:745)

""pool-1-thread-1"" #22 prio=5 os_prio=0 tid=0x00007f5894c44800 nid=0x2f4d waiting on condition [0x00007f5864e38000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000080018120> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

""Service Thread"" #20 daemon prio=9 os_prio=0 tid=0x00007f589410c800 nid=0x2f4a runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C1 CompilerThread14"" #19 daemon prio=9 os_prio=0 tid=0x00007f5894107800 nid=0x2f49 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C1 CompilerThread13"" #18 daemon prio=9 os_prio=0 tid=0x00007f5894105800 nid=0x2f48 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C1 CompilerThread12"" #17 daemon prio=9 os_prio=0 tid=0x00007f5894103000 nid=0x2f47 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C1 CompilerThread11"" #16 daemon prio=9 os_prio=0 tid=0x00007f5894101800 nid=0x2f44 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C1 CompilerThread10"" #15 daemon prio=9 os_prio=0 tid=0x00007f58940ff000 nid=0x2f40 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread9"" #14 daemon prio=9 os_prio=0 tid=0x00007f58940fd000 nid=0x2f3e runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread8"" #13 daemon prio=9 os_prio=0 tid=0x00007f58940fb800 nid=0x2f3d waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread7"" #12 daemon prio=9 os_prio=0 tid=0x00007f58940f9800 nid=0x2f3c waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread6"" #11 daemon prio=9 os_prio=0 tid=0x00007f58940f7800 nid=0x2f3b waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread5"" #10 daemon prio=9 os_prio=0 tid=0x00007f58940f5800 nid=0x2f3a waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread4"" #9 daemon prio=9 os_prio=0 tid=0x00007f58940f3800 nid=0x2f39 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread3"" #8 daemon prio=9 os_prio=0 tid=0x00007f58940f2000 nid=0x2f38 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread2"" #7 daemon prio=9 os_prio=0 tid=0x00007f58940f0000 nid=0x2f37 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread1"" #6 daemon prio=9 os_prio=0 tid=0x00007f58940ee000 nid=0x2f36 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread0"" #5 daemon prio=9 os_prio=0 tid=0x00007f58940eb000 nid=0x2f35 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Signal Dispatcher"" #4 daemon prio=9 os_prio=0 tid=0x00007f58940e9800 nid=0x2f34 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007f58940b2000 nid=0x2f33 in Object.wait() [0x00007f58671f0000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
        - locked <0x0000000080047a78> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)
        at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)

""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007f58940b0000 nid=0x2f32 in Object.wait() [0x00007f58672f1000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:502)
        at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:157)
        - locked <0x000000008007d850> (a java.lang.ref.Reference$Lock)

""main"" #1 prio=5 os_prio=0 tid=0x00007f589400c000 nid=0x2f14 in Object.wait() [0x00007f589dc1f000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x0000000080017b68> (a java.lang.UNIXProcess)
        at java.lang.Object.wait(Object.java:502)
        at java.lang.UNIXProcess.waitFor(UNIXProcess.java:396)
        - locked <0x0000000080017b68> (a java.lang.UNIXProcess)
        at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:86)
        at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

""VM Thread"" os_prio=0 tid=0x00007f58940aa800 nid=0x2f31 runnable

""GC task thread#0 (ParallelGC)"" os_prio=0 tid=0x00007f5894021800 nid=0x2f15 runnable

""GC task thread#1 (ParallelGC)"" os_prio=0 tid=0x00007f5894023000 nid=0x2f16 runnable

""GC task thread#2 (ParallelGC)"" os_prio=0 tid=0x00007f5894025000 nid=0x2f17 runnable

""GC task thread#3 (ParallelGC)"" os_prio=0 tid=0x00007f5894026800 nid=0x2f18 runnable

""GC task thread#4 (ParallelGC)"" os_prio=0 tid=0x00007f5894028800 nid=0x2f19 runnable

""GC task thread#5 (ParallelGC)"" os_prio=0 tid=0x00007f589402a000 nid=0x2f1a runnable

""GC task thread#6 (ParallelGC)"" os_prio=0 tid=0x00007f589402c000 nid=0x2f1b runnable

""GC task thread#7 (ParallelGC)"" os_prio=0 tid=0x00007f589402d800 nid=0x2f1c runnable

""GC task thread#8 (ParallelGC)"" os_prio=0 tid=0x00007f589402f800 nid=0x2f1d runnable

""GC task thread#9 (ParallelGC)"" os_prio=0 tid=0x00007f5894031000 nid=0x2f1e runnable

""GC task thread#10 (ParallelGC)"" os_prio=0 tid=0x00007f5894033000 nid=0x2f1f runnable

""GC task thread#11 (ParallelGC)"" os_prio=0 tid=0x00007f5894034800 nid=0x2f20 runnable

""GC task thread#12 (ParallelGC)"" os_prio=0 tid=0x00007f5894036800 nid=0x2f21 runnable

""GC task thread#13 (ParallelGC)"" os_prio=0 tid=0x00007f5894038000 nid=0x2f22 runnable

""GC task thread#14 (ParallelGC)"" os_prio=0 tid=0x00007f589403a000 nid=0x2f23 runnable

""GC task thread#15 (ParallelGC)"" os_prio=0 tid=0x00007f589403b800 nid=0x2f24 runnable

""GC task thread#16 (ParallelGC)"" os_prio=0 tid=0x00007f589403d800 nid=0x2f25 runnable

""GC task thread#17 (ParallelGC)"" os_prio=0 tid=0x00007f589403f000 nid=0x2f26 runnable

""GC task thread#18 (ParallelGC)"" os_prio=0 tid=0x00007f5894041000 nid=0x2f27 runnable

""GC task thread#19 (ParallelGC)"" os_prio=0 tid=0x00007f5894042800 nid=0x2f28 runnable

""GC task thread#20 (ParallelGC)"" os_prio=0 tid=0x00007f5894044800 nid=0x2f29 runnable

""GC task thread#21 (ParallelGC)"" os_prio=0 tid=0x00007f5894046000 nid=0x2f2a runnable

""GC task thread#22 (ParallelGC)"" os_prio=0 tid=0x00007f5894048000 nid=0x2f2b runnable

""GC task thread#23 (ParallelGC)"" os_prio=0 tid=0x00007f5894049800 nid=0x2f2c runnable

""GC task thread#24 (ParallelGC)"" os_prio=0 tid=0x00007f589404b800 nid=0x2f2d runnable

""GC task thread#25 (ParallelGC)"" os_prio=0 tid=0x00007f589404d000 nid=0x2f2e runnable

""GC task thread#26 (ParallelGC)"" os_prio=0 tid=0x00007f589404f000 nid=0x2f2f runnable

""GC task thread#27 (ParallelGC)"" os_prio=0 tid=0x00007f5894050800 nid=0x2f30 runnable

""VM Periodic Task Thread"" os_prio=0 tid=0x00007f589410f800 nid=0x2f4b waiting on condition

JNI global references: 296

Heap
 PSYoungGen      total 572416K, used 351384K [0x00000000d5580000, 0x0000000100000000, 0x0000000100000000)
  eden space 530432K, 61% used [0x00000000d5580000,0x00000000e93b6600,0x00000000f5b80000)
  from space 41984K, 60% used [0x00000000f8280000,0x00000000f9b6fbd0,0x00000000fab80000)
  to   space 39936K, 0% used [0x00000000f5b80000,0x00000000f5b80000,0x00000000f8280000)
 ParOldGen       total 708608K, used 41445K [0x0000000080000000, 0x00000000ab400000, 0x00000000d5580000)
  object space 708608K, 5% used [0x0000000080000000,0x00000000828796c0,0x00000000ab400000)
 Metaspace       used 91447K, capacity 92274K, committed 92416K, reserved 1128448K
  class space    used 12530K, capacity 12735K, committed 12800K, reserved 1048576K
{code};;;","05/Oct/16 14:50;srowen;OK so it's pretty clear the ""Scheduler-x"" thread is somehow started only when the UI is touched. maybe it's from inside Jetty.
I don't know why it might have happened to work before, but in general I'd expect that an app that has long-running threads has to shut itself down explicitly.
If there's an easy way to make this a daemon thread, OK, seems worth changing that, but stop() is still the intended way to close the app correctly.;;;","05/Oct/16 15:06;maver1ck;Maybe It's connected with this patch
https://github.com/apache/spark/pull/12916/files

Or we just doesn't stop Jetty ?;;;","05/Oct/16 15:11;srowen;Could be, though the two pools that touches appear to (still) make daemon threads.
Yeah, that's the point I guess -- how would you know when to stop Jetty except when sc.stop() is called?;;;","05/Oct/16 15:59;maver1ck;At the end of submitted python script ?;;;","05/Oct/16 16:12;srowen;That's a non-event from Spark's perspective, like main() ending in Java. How does Jetty know that's happened?;;;","05/Oct/16 16:20;maver1ck;Maybe this could help.
http://download.eclipse.org/jetty/9.3.11.v20160721/apidocs/org/eclipse/jetty/server/Server.html#setStopAtShutdown-boolean-
I'll try to test this tomorrow.;;;","05/Oct/16 16:26;srowen;Maybe so; it looks like it registers a shutdown hook. I'm not sure we reach shutdown in this case because a thread is still running. CC [~zsxwing] who may know more.;;;","05/Oct/16 16:28;maver1ck;I think so.
There is DestroyJavaVM thread.;;;","05/Oct/16 16:54;maver1ck;Tested.
setStopAtShutdown(true) doesn't help.;;;","06/Oct/16 07:33;maver1ck;I reverted this PR.
https://github.com/apache/spark/pull/12916/files

That solved this problem. 
Is it possible to go back to Jetty 8 ?

cc: [~bomeng];;;","06/Oct/16 07:53;srowen;No, that's certainly not a good solution. At the least, this is something that only comes up when you don't properly close the context, which should be done by all apps. ;;;","06/Oct/16 08:02;maver1ck;And wasn't needed before Spark 2.0.
I think there will be many submitted Jira like this.

Even samples from official documentation don't have sc.stop()
http://spark.apache.org/docs/latest/quick-start.html#self-contained-applications
;;;","06/Oct/16 08:09;maver1ck;For python code I have workaround.
We can use atexit package to register hook when creating SparkContext or SparkSession.;;;","06/Oct/16 08:11;srowen;... why not just call stop()? 
stop() was always what you're supposed to do. Look at the examples in examples/. That example you cite though does need a fix.;;;","06/Oct/16 11:30;nickorka;Calling stop() would be a good idea if there were not getOrCreate(). Spark context is reusable. If you start two simultaneous threads when are you supposed to stop() the context explicitly? 
Is it better to fix the issue for backward compatibility or make context non-reusable?;;;","06/Oct/16 12:59;srowen;I don't think that changes things. You still have one context per JVM, regardless of how you get or create it, and it should be stopped when the job is done.;;;","06/Oct/16 18:43;srowen;[~maver1ck] I looked into the Jetty code and found several call sites that would make a non-daemon thread called ""Scheduler-X"" using its ScheduledExecutorScheduler class. I am reasonably sure it comes from using ServerConnector. We can manage to construct this with a different impl that will make daemon threads.

I can make a pull request to try that out, though I suppose none of our existing tests catch this case. Let me see if I can make up a test or at least check it locally, though it may be of interest to you to try out too.;;;","06/Oct/16 19:03;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15381;;;","07/Oct/16 09:33;maver1ck;[~srowen]
I tested your branch and it solves problem.
;;;",,,,,,,,,,,,,,
Code generation including too many mutable states exceeds JVM size limit.,SPARK-17702,13008126,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,28/Sep/16 04:18,04/Oct/16 04:48,14/Jul/23 06:29,04/Oct/16 04:48,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Code generation including too many mutable states exceeds JVM size limit to extract values from {{references}} into fields in the constructor.
We should split the generated extractions in the constructor into smaller functions.",,apachespark,kiszk,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 28 04:31:04 UTC 2016,,,,,,,,,,"0|i3469j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/16 04:31;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/15275;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Join predicates should not contain filter clauses,SPARK-17698,13008104,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tejasp,tejasp,tejasp,28/Sep/16 01:22,22/Oct/16 23:32,14/Jul/23 06:29,20/Oct/16 16:51,2.0.0,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,,,,,,"`ExtractEquiJoinKeys` is incorrectly using filter predicates as the join condition for joins. While this does not lead to incorrect results but in case of bucketed + sorted tables, we might miss out on avoiding un-necessary shuffle + sort. eg.

{code}
val df = (1 until 10).toDF(""id"").coalesce(1)
hc.sql(""DROP TABLE IF EXISTS table1"").collect
df.write.bucketBy(8, ""id"").sortBy(""id"").saveAsTable(""table1"")
hc.sql(""DROP TABLE IF EXISTS table2"").collect
df.write.bucketBy(8, ""id"").sortBy(""id"").saveAsTable(""table2"")

sqlContext.sql(""""""
  SELECT a.id, b.id
  FROM table1 a
  FULL OUTER JOIN table2 b
  ON a.id = b.id AND a.id='1' AND b.id='1'
"""""").explain(true)
{code}

This is doing shuffle + sort over table scan outputs which is not needed as both tables are bucketed and sorted on the same columns and have same number of buckets. This should be a single stage job.

{code}
SortMergeJoin [id#38, cast(id#38 as double), 1.0], [id#39, 1.0, cast(id#39 as double)], FullOuter
:- *Sort [id#38 ASC NULLS FIRST, cast(id#38 as double) ASC NULLS FIRST, 1.0 ASC NULLS FIRST], false, 0
:  +- Exchange hashpartitioning(id#38, cast(id#38 as double), 1.0, 200)
:     +- *FileScan parquet default.table1[id#38] Batched: true, Format: ParquetFormat, InputPaths: file:spark-warehouse/table1, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int>
+- *Sort [id#39 ASC NULLS FIRST, 1.0 ASC NULLS FIRST, cast(id#39 as double) ASC NULLS FIRST], false, 0
   +- Exchange hashpartitioning(id#39, 1.0, cast(id#39 as double), 200)
      +- *FileScan parquet default.table2[id#39] Batched: true, Format: ParquetFormat, InputPaths: file:spark-warehouse/table2, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int>
{code}",,apachespark,h_o,nsyca,rajeshhadoop,tejasp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 22 20:30:05 UTC 2016,,,,,,,,,,"0|i3464n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/16 01:39;apachespark;User 'tejasapatil' has created a pull request for this issue:
https://github.com/apache/spark/pull/15272;;;","22/Oct/16 20:30;apachespark;User 'tejasapatil' has created a pull request for this issue:
https://github.com/apache/spark/pull/15600;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"BinaryLogisticRegressionSummary, GLM Summary should handle non-Double numeric types",SPARK-17697,13008099,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bryanc,josephkb,josephkb,28/Sep/16 01:02,01/Oct/16 18:02,14/Jul/23 06:29,01/Oct/16 18:02,2.0.1,2.1.0,,,,,,,2.0.2,2.1.0,,,ML,,,,,,,,0,,,,,,"Say you have a DataFrame with a label column of Integer type.  You can fit a LogisticRegresionModel since LR handles casting to DoubleType internally.

However, if you call evaluate() on it, then this line does not handle casting properly, so you get a runtime error (MatchError) for an invalid schema: [https://github.com/apache/spark/blob/2cd327ef5e4c3f6b8468ebb2352479a1686b7888/mllib/src/main/scala/org/apache/spark/ml/classification/LogisticRegression.scala#L863]

We should handle casting.  And test evaluate() with other numeric types.

**ALSO** We should check elsewhere in logreg and other algorithms to see if we can catch the same issue elsewhere.",,apachespark,bryanc,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 28 23:07:04 UTC 2016,,,,,,,,,,"0|i3463j:",9223372036854775807,,,,,josephkb,,,,,,,,2.0.2,2.1.0,,,,,,,,,,"28/Sep/16 18:26;bryanc;I can work on this, thanks!;;;","28/Sep/16 23:07;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/15288;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race in CoarseGrainedExecutorBackend shutdown can lead to wrong exit status,SPARK-17696,13008076,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,drcrallen,vanzin,vanzin,27/Sep/16 22:15,28/Sep/16 21:50,14/Jul/23 06:29,28/Sep/16 21:40,1.6.0,,,,,,,,1.6.3,,,,Spark Core,YARN,,,,,,,0,,,,,,"There's a race in the shutdown path of CoarseGrainedExecutorBackend that may lead to the process exiting with the wrong status. When the race triggers, you can see things like this in the driver logs in yarn-cluster mode:

{noformat}
14:38:20,114 [Driver] INFO  org.apache.spark.SparkContext - Successfully stopped SparkContext
{noformat}

And later:

{noformat}
14:38:22,455 [Reporter] WARN  org.apache.spark.deploy.yarn.YarnAllocator - Container marked as failed: container_1470951093505_0001_01_000002 on host: xxx.com. Exit status: 1. Diagnostics: Exception from container-launch.
Container id: container_1470951093505_0001_01_000002
Exit code: 1
{noformat}

This happens because the user class is still running after the SparkContext is shut down, so the YarnAllocator instance is alive for long enough to fetch the exit status of the container. If the race is triggered, the container exits with the wrong status. In this case, enough containers hit the race that the application ended up failing due to too many container failures, even though the app would probably succeed otherwise.

The race is as follows:

- CoarseGrainedExecutorBackend receives a StopExecutor
- Before it can enqueue a ""Shutdown"" message, the socket is disconnected and NettyRpcEnv enqueues a ""RemoteProcessDisconnected"" message
- ""RemoteProcessDisconnected"" is processed first, and calls ""System.exit"" with wrong exit code for this case.

You can see that in the executor logs: both messages are being processed.

{noformat}
14:38:20,093 [dispatcher-event-loop-9] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend - Driver commanded a shutdown
14:38:20,286 [dispatcher-event-loop-9] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend - Driver xxx:40988 disassociated! Shutting down.
{noformat}

The code needs to avoid this situation by ignoring the disconnect event if it's already shutting down.",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 27 22:58:05 UTC 2016,,,,,,,,,,"0|i345yf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/16 22:28;vanzin;This is actually fixed in 2.0 by SPARK-12330 (I was looking at a failed 1.6 app). The fix in CoarseGrainedExecutorBackend is simple enough that it would be a good addition for 1.6.;;;","27/Sep/16 22:58;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/15270;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixed Insert Failure To Data Source Tables when the Schema has the Comment Field,SPARK-17693,13008020,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,27/Sep/16 18:32,05/Nov/16 10:46,14/Jul/23 06:29,26/Oct/16 07:39,2.0.0,2.1.0,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,1,,,,,,"{noformat}
CREATE TABLE tab1(col1 int COMMENT 'a', col2 int) USING parquet
INSERT INTO TABLE tab1 SELECT 1, 2
{noformat}
The insert attempt will fail if the target table has a column with comments. The error is strange to the external users:
{noformat}
assertion failed: No plan for InsertIntoTable Relation[col1#15,col2#16] parquet, false, false
+- Project [1 AS col1#19, 2 AS col2#20]
   +- OneRowRelation$
{noformat}",,Antonbl,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 05 06:34:05 UTC 2016,,,,,,,,,,"0|i345lz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/16 18:35;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15266;;;","24/Oct/16 20:17;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15615;;;","26/Oct/16 07:39;smilegator;Issue resolved by pull request 15615
[https://github.com/apache/spark/pull/15615];;;","05/Nov/16 06:34;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15782;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WholeStageCodegenExec throws IndexOutOfBoundsException,SPARK-17685,13007849,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yumwang,yumwang,yumwang,27/Sep/16 08:46,11/May/17 19:59,14/Jul/23 06:29,10/May/17 02:46,2.0.0,,,,,,,,2.1.2,2.2.0,,,SQL,,,,,,,,1,,,,,,"The following SQL query reproduces this issue:

{code:sql}
CREATE TABLE tab1(int int, int2 int, str string);
CREATE TABLE tab2(int int, int2 int, str string);
INSERT INTO tab1 values(1,1,'str');
INSERT INTO tab1 values(2,2,'str');
INSERT INTO tab2 values(1,1,'str');
INSERT INTO tab2 values(2,3,'str');

SELECT
  count(*)
FROM
  (
    SELECT t1.int, t2.int2 
    FROM (SELECT * FROM tab1 LIMIT 1310721) t1
    INNER JOIN (SELECT * FROM tab2 LIMIT 1310721) t2 
    ON (t1.int = t2.int AND t1.int2 = t2.int2)
  ) t;
{code}

Exception thrown:

{noformat}
java.lang.IndexOutOfBoundsException: 1
	at scala.collection.LinearSeqOptimized$class.apply(LinearSeqOptimized.scala:65)
	at scala.collection.immutable.List.apply(List.scala:84)
	at org.apache.spark.sql.catalyst.expressions.BoundReference.doGenCode(BoundAttribute.scala:64)
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:104)
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:101)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:101)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$createJoinKey$1.apply(SortMergeJoinExec.scala:334)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$createJoinKey$1.apply(SortMergeJoinExec.scala:334)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.createJoinKey(SortMergeJoinExec.scala:334)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.genScanner(SortMergeJoinExec.scala:369)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.doProduce(SortMergeJoinExec.scala:512)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.produce(SortMergeJoinExec.scala:35)
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:40)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:30)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithoutKeys(HashAggregateExec.scala:215)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:143)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:37)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithoutKeys(HashAggregateExec.scala:215)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:143)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:37)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:309)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:347)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:240)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:287)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:310)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$hiveResultString$3.apply(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$hiveResultString$3.apply(QueryExecution.scala:128)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.hiveResultString(QueryExecution.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:331)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
java.lang.IndexOutOfBoundsException: 1
	at scala.collection.LinearSeqOptimized$class.apply(LinearSeqOptimized.scala:65)
	at scala.collection.immutable.List.apply(List.scala:84)
	at org.apache.spark.sql.catalyst.expressions.BoundReference.doGenCode(BoundAttribute.scala:64)
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:104)
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:101)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:101)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$createJoinKey$1.apply(SortMergeJoinExec.scala:334)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$createJoinKey$1.apply(SortMergeJoinExec.scala:334)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.createJoinKey(SortMergeJoinExec.scala:334)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.genScanner(SortMergeJoinExec.scala:369)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.doProduce(SortMergeJoinExec.scala:512)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.produce(SortMergeJoinExec.scala:35)
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:40)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:30)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithoutKeys(HashAggregateExec.scala:215)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:143)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:37)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithoutKeys(HashAggregateExec.scala:215)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:143)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:37)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:309)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:347)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:240)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:287)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:310)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$hiveResultString$3.apply(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$hiveResultString$3.apply(QueryExecution.scala:128)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.hiveResultString(QueryExecution.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:331)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

{noformat}

This bug is a regression. Spark 1.6 doesn't have this issue.",,apachespark,dmcwhorter,kiszk,yumwang,zhaiyuyong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 09 08:58:03 UTC 2017,,,,,,,,,,"0|i344jz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/16 11:08;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/15259;;;","09/May/17 08:58;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/17920;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unnecessary Py4J ListConverter patch,SPARK-17679,13007805,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jason.white,jason.white,jason.white,27/Sep/16 04:36,08/Oct/16 09:20,14/Jul/23 06:29,03/Oct/16 21:10,2.0.0,,,,,,,,2.1.0,,,,PySpark,,,,,,,,0,,,,,,"In SPARK-6949 davies documented a couple of bugs with Py4J that prevented Spark from registering a converter for date and datetime objects. Patched in https://github.com/apache/spark/pull/5570.

Specifically https://github.com/bartdag/py4j/issues/160 dealt with ListConverter automatically converting bytearrays into ArrayList instead of leaving it alone.

Py4J #160 has since been fixed in Py4J, since the 0.9 release a couple of months after Spark #5570. According to spark-core's pom.xml, we're using 0.10.3.

We should remove this patch on ListConverter since the upstream package no longer has this issue.",,apachespark,davies,jason.white,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 03 21:10:59 UTC 2016,,,,,,,,,,"0|i344a7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/16 04:45;apachespark;User 'JasonMWhite' has created a pull request for this issue:
https://github.com/apache/spark/pull/15254;;;","03/Oct/16 21:10;davies;Issue resolved by pull request 15254
[https://github.com/apache/spark/pull/15254];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spark 1.6 Scala-2.11 repl doesn't honor ""spark.replClassServer.port""",SPARK-17678,13007799,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,jerryshao,jerryshao,27/Sep/16 03:43,13/Oct/16 23:50,14/Jul/23 06:29,13/Oct/16 23:49,1.6.2,,,,,,,,1.6.3,,,,Spark Shell,,,,,,,,0,,,,,,"Spark 1.6 Scala-2.11 repl doesn't honor ""spark.replClassServer.port"" configuration, so user cannot set a fixed port number through ""spark.replClassServer.port"".

There's no issue in Spark2.0+, since this class is removed.",,apachespark,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 27 03:56:07 UTC 2016,,,,,,,,,,"0|i3448v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/16 03:56;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/15253;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FsHistoryProvider should ignore hidden files,SPARK-17676,13007793,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,irashid,irashid,irashid,27/Sep/16 02:53,29/Sep/16 22:41,14/Jul/23 06:29,29/Sep/16 22:41,,,,,,,,,2.1.0,,,,Spark Core,,,,,,,,0,,,,,,"FsHistoryProvider currently reads hidden files (beginning with ""."") from the log dir.  However, it is writing a hidden file *itself* to that dir, which cannot be parsed, as part of a trick to find the scan time according to the file system:

{code}
    val fileName = ""."" + UUID.randomUUID().toString
    val path = new Path(logDir, fileName)
    val fos = fs.create(path)
{code}

It does delete the tmp file immediately, but we've seen cases where that race ends badly, and there is a logged error.  The error is harmless (the log file is ignored and spark moves on to the other log files), but the logged error is very confusing for users, so we should avoid it.

{noformat}
2016-09-26 09:10:03,016 ERROR org.apache.spark.deploy.history.FsHistoryProvider: Exception encountered when attempting to load application log hdfs://XXX/user/spark/applicationHistory/.3a5e987c-ace5-4568-9ccd-6285010e399a 
java.lang.IllegalArgumentException: Codec [3a5e987c-ace5-4568-9ccd-6285010e399a] is not available. Consider setting spark.io.compression.codec=lzf 
at org.apache.spark.io.CompressionCodec$$anonfun$createCodec$1.apply(CompressionCodec.scala:72) 
at org.apache.spark.io.CompressionCodec$$anonfun$createCodec$1.apply(CompressionCodec.scala:72) 
at scala.Option.getOrElse(Option.scala:120) 
at org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:72) 
at org.apache.spark.scheduler.EventLoggingListener$$anonfun$8$$anonfun$apply$1.apply(EventLoggingListener.scala:309) 
at org.apache.spark.scheduler.EventLoggingListener$$anonfun$8$$anonfun$apply$1.apply(EventLoggingListener.scala:309) 
at scala.collection.mutable.MapLike$class.getOrElseUpdate(MapLike.scala:189) 
at scala.collection.mutable.AbstractMap.getOrElseUpdate(Map.scala:91) 
at org.apache.spark.scheduler.EventLoggingListener$$anonfun$8.apply(EventLoggingListener.scala:309) 
at org.apache.spark.scheduler.EventLoggingListener$$anonfun$8.apply(EventLoggingListener.scala:308) 
at scala.Option.map(Option.scala:145) 
at org.apache.spark.scheduler.EventLoggingListener$.openEventLog(EventLoggingListener.scala:308) 
at org.apache.spark.deploy.history.FsHistoryProvider.org$apache$spark$deploy$history$FsHistoryProvider$$replay(FsHistoryProvider.scala:518) 
at org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$10.apply(FsHistoryProvider.scala:359) 
at org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$10.apply(FsHistoryProvider.scala:356) 
at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251) 
at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251) 
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) 
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) 
at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251) 
at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105) 
at org.apache.spark.deploy.history.FsHistoryProvider.org$apache$spark$deploy$history$FsHistoryProvider$$mergeApplicationListing(FsHistoryProvider.scala:356)
at org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$checkForLogs$1$$anon$4.run(FsHistoryProvider.scala:277) 
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) 
at java.util.concurrent.FutureTask.run(FutureTask.java:262) 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) 
at java.lang.Thread.run(Thread.java:745)
{noformat}",,apachespark,irashid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 27 03:10:06 UTC 2016,,,,,,,,,,"0|i3447j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/16 03:10;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/15250;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reused Exchange Aggregations Produce Incorrect Results,SPARK-17673,13007762,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ekhliang,rspitzer,rspitzer,26/Sep/16 23:42,28/Sep/16 23:44,14/Jul/23 06:29,28/Sep/16 20:23,2.0.0,2.0.1,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,correctness,,,,,"https://datastax-oss.atlassian.net/browse/SPARKC-429

Was brought to my attention where the following code produces incorrect results

{code}
 val data = List(TestData(""A"", 1, 7))
    val frame = session.sqlContext.createDataFrame(session.sparkContext.parallelize(data))

    frame.createCassandraTable(
      keySpaceName,
      table,
      partitionKeyColumns = Some(Seq(""id"")))

    frame
      .write
      .format(""org.apache.spark.sql.cassandra"")
      .mode(SaveMode.Append)
      .options(Map(""table"" -> table, ""keyspace"" -> keySpaceName))
      .save()

val loaded = sparkSession.sqlContext
  .read
  .format(""org.apache.spark.sql.cassandra"")
  .options(Map(""table"" -> table, ""keyspace"" -> ks))
  .load()
  .select(""id"", ""col1"", ""col2"")
val min1 = loaded.groupBy(""id"").agg(min(""col1"").as(""min""))
val min2 = loaded.groupBy(""id"").agg(min(""col2"").as(""min""))
 min1.union(min2).show()
    /* prints:
      +---+---+
      | id|min|
      +---+---+
      |  A|  1|
      |  A|  1|
      +---+---+
     Should be 
      | A| 1|
      | A| 7|
     */
{code}

I looked into the explain pattern and saw 
{code}
Union
:- *HashAggregate(keys=[id#93], functions=[min(col1#94)])
:  +- Exchange hashpartitioning(id#93, 200)
:     +- *HashAggregate(keys=[id#93], functions=[partial_min(col1#94)])
:        +- *Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@7ec20844 [id#93,col1#94]
+- *HashAggregate(keys=[id#93], functions=[min(col2#95)])
   +- ReusedExchange [id#93, min#153], Exchange hashpartitioning(id#93, 200)
{code}

Which was different than using a parallelized collection as the DF backing. So I tested the same code with a Parquet backed DF and saw the same results.

{code}
    frame.write.parquet(""garbagetest"")
    val parquet = sparkSession.read.parquet(""garbagetest"").select(""id"", ""col1"", ""col2"")
    println(""PDF"")
    parquetmin1.union(parquetmin2).explain()
    parquetmin1.union(parquetmin2).show()
    /* prints:
      +---+---+
      | id|min|
      +---+---+
      |  A|  1|
      |  A|  1|
      +---+---+
*/
{code}

Which leads me to believe there is something wrong with the reused exchange. ",,apachespark,bcantoni,BToldbod,ekhliang,emlyn,hvanhovell,joshrosen,mhornbech,rspitzer,rxin,tgraves,wkinney,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 28 23:44:50 UTC 2016,,,,,,,,,,"0|i3440n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/16 00:15;hvanhovell;[~rspitzer] Are you using Spark 2.0 or the latest master?;;;","27/Sep/16 00:42;rspitzer;I only ran this on 2.0.0 and 2.0.1;;;","27/Sep/16 01:12;rxin;Can you help create a repro (without the need to connect Cassandra)?;;;","27/Sep/16 01:30;rspitzer;Ugh I made a typo in my Parquet Example I don't see it repoing there now. Let me run investigate a little more as to why this would affect the C* Source...;;;","27/Sep/16 01:32;rxin;It's possible if hashCode and equals are not defined properly in the Cassandra data source.
;;;","27/Sep/16 01:37;rspitzer;Well in this case they are equal correct? We are using the same Dataframe with two different aggregation steps.

The Parquet example doesn't end up using the reusedExchange. It does the same plan as the parallelized one

{code}
PDF
== Physical Plan ==
Union
:- *HashAggregate(keys=[id#112], functions=[min(col1#113)])
:  +- Exchange hashpartitioning(id#112, 200)
:     +- *HashAggregate(keys=[id#112], functions=[partial_min(col1#113)])
:        +- *BatchedScan parquet [id#112,col1#113] Format: ParquetFormat, InputPaths: file:/Users/russellspitzer/repos/spark-cassandra-connector/ꟾ뫳㼎麡䰨틖㇗ཨᎪ贬, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,col1:int>
+- *HashAggregate(keys=[id#112], functions=[min(col2#114)])
   +- Exchange hashpartitioning(id#112, 200)
      +- *HashAggregate(keys=[id#112], functions=[partial_min(col2#114)])
         +- *BatchedScan parquet [id#112,col2#114] Format: ParquetFormat, InputPaths: file:/Users/russellspitzer/repos/spark-cassandra-connector/ꟾ뫳㼎麡䰨틖㇗ཨᎪ贬, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,col2:int>
{code};;;","27/Sep/16 02:43;rspitzer;I couldn't get this to happen without C*, hopefully tomorrow I can get some guidance tomorrow :/ It could be a hashcode / equals thing but we don't override those in the base class. Also i'm a little confused because this should be the same ""grouping"" operation on the RDD just with a different aggregate. I don't know enough about the ReusedExchange to know when it's applied and why.;;;","27/Sep/16 04:06;hvanhovell;[~rspitzer] we only reuse an exchange when they produce the same result. You can find the implementation for row datasource scans (what you are probably using) here https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala#L124-L128

I have taken a look at the cassandra datasource code, but that does not seem to implement equals at all and should not cause a problem. Do you cache instances of CassandraRelation at some point?;;;","27/Sep/16 04:08;rspitzer;We shouldn't be ... The only thing we cache are underlying database connections and queries which shouldn't factor into this I would think :/;;;","27/Sep/16 04:09;rxin;RowDataSourceScanExec.sameResult is probably the problem:

{code}
  // Ignore rdd when checking results
  override def sameResult(plan: SparkPlan): Boolean = plan match {
    case other: RowDataSourceScanExec => relation == other.relation && metadata == other.metadata
    case _ => false
  }
{code}

First glance -- it looks like it ignores predicate pushdown or column pruning entirely.
;;;","27/Sep/16 04:12;rspitzer;Looking at this plan 
{code}```
Union
:- *HashAggregate(keys=[id#93], functions=[min(col1#94)])
:  +- Exchange hashpartitioning(id#93, 200)
:     +- *HashAggregate(keys=[id#93], functions=[partial_min(col1#94)])
:        +- *Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@7ec20844 [id#93,col1#94]
+- *HashAggregate(keys=[id#93], functions=[min(col2#95)])
   +- ReusedExchange [id#93, min#153], Exchange hashpartitioning(id#93, 200)```
{code}
I see it reuses the hash aggregate from the partial min on col1 even though the hash aggregate that runs it does min col2. Am I reading that right? The column names don't even match so I'm confused how that gets through?;;;","27/Sep/16 04:14;rxin;The only thing differentiating the two sides of the plan is column pruning right? It is possible the issue I mentioned earlier is the culprit.
;;;","27/Sep/16 04:31;hvanhovell;Could you also share the optimized plan {{df.explain(true)}}? I am wondering if the attributes are the same.;;;","27/Sep/16 05:12;rspitzer;Ah yeah there would definitely be different pruning in both ""source""s  Getting the optimized plan now;;;","27/Sep/16 05:16;rspitzer;{code}== Parsed Logical Plan ==
Union
:- Aggregate [id#101], [id#101, min(col1#102) AS min#128]
:  +- Project [id#101, col1#102, col2#103]
:     +- Relation[id#101,col1#102,col2#103] org.apache.spark.sql.cassandra.CassandraSourceRelation@486a9118
+- Aggregate [id#101], [id#101, min(col2#103) AS min#137]
   +- Project [id#101, col1#102, col2#103]
      +- Relation[id#101,col1#102,col2#103] org.apache.spark.sql.cassandra.CassandraSourceRelation@486a9118

== Analyzed Logical Plan ==
id: string, min: int
Union
:- Aggregate [id#101], [id#101, min(col1#102) AS min#128]
:  +- Project [id#101, col1#102, col2#103]
:     +- Relation[id#101,col1#102,col2#103] org.apache.spark.sql.cassandra.CassandraSourceRelation@486a9118
+- Aggregate [id#101], [id#101, min(col2#103) AS min#137]
   +- Project [id#101, col1#102, col2#103]
      +- Relation[id#101,col1#102,col2#103] org.apache.spark.sql.cassandra.CassandraSourceRelation@486a9118

== Optimized Logical Plan ==
Union
:- Aggregate [id#101], [id#101, min(col1#102) AS min#128]
:  +- Project [id#101, col1#102]
:     +- Relation[id#101,col1#102,col2#103] org.apache.spark.sql.cassandra.CassandraSourceRelation@486a9118
+- Aggregate [id#101], [id#101, min(col2#103) AS min#137]
   +- Project [id#101, col2#103]
      +- Relation[id#101,col1#102,col2#103] org.apache.spark.sql.cassandra.CassandraSourceRelation@486a9118

== Physical Plan ==
Union
:- *HashAggregate(keys=[id#101], functions=[min(col1#102)], output=[id#101, min#128])
:  +- Exchange hashpartitioning(id#101, 200)
:     +- *HashAggregate(keys=[id#101], functions=[partial_min(col1#102)], output=[id#101, min#182])
:        +- *Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@486a9118 [id#101,col1#102]
+- *HashAggregate(keys=[id#101], functions=[min(col2#103)], output=[id#101, min#137])
   +- ReusedExchange [id#101, min#190], Exchange hashpartitioning(id#101, 200)
{code}

So the relations do look exactly the same (even though they are not) in the optimized plan;;;","27/Sep/16 20:31;rxin;I'm upgrading this to a blocker level issue.
;;;","27/Sep/16 20:34;ekhliang;I'm looking at this now.;;;","28/Sep/16 02:23;apachespark;User 'ericl' has created a pull request for this issue:
https://github.com/apache/spark/pull/15273;;;","28/Sep/16 02:24;ekhliang;Russell, could you try applying this patch (wip) to see if it resolves the issue? https://github.com/apache/spark/pull/15273/files

It fixes equality comparison for row datasource scans to take into account the output schema of the scan.;;;","28/Sep/16 20:24;rxin;I merged this in master. There was a conflict with branch-2.0.

[~ekhliang] can you submit a pr for 2.0?
;;;","28/Sep/16 20:35;apachespark;User 'ericl' has created a pull request for this issue:
https://github.com/apache/spark/pull/15282;;;","28/Sep/16 23:44;rspitzer;Looks good on my end

{code}
scala>     min2.union(min1).show()
+---+---+
| id|min|
+---+---+
|  A|  7|
|  A|  1|
+---+---+


scala>     min1.union(min2).show()
+---+---+
| id|min|
+---+---+
|  A|  1|
|  A|  7|
+---+---+
{code};;;",,,,,,,,,,,,,,,,,,,,,
Spark 2.0 history server web Ui takes too long for a single application,SPARK-17672,13007748,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wgtmac,wgtmac,wgtmac,26/Sep/16 22:53,29/Sep/16 19:49,14/Jul/23 06:29,29/Sep/16 19:48,2.0.0,,,,,,,,2.0.1,,,,Web UI,,,,,,,,0,,,,,,"When there are 10K application history in the history server back end, it can take a very long time to even get a single application history page. After some investigation, I found the root cause was the following piece of code: 

{code:title=OneApplicationResource.scala|borderStyle=solid}
@Produces(Array(MediaType.APPLICATION_JSON))
private[v1] class OneApplicationResource(uiRoot: UIRoot) {

  @GET
  def getApp(@PathParam(""appId"") appId: String): ApplicationInfo = {
    val apps = uiRoot.getApplicationInfoList.find { _.id == appId }
    apps.getOrElse(throw new NotFoundException(""unknown app: "" + appId))
  }

}
{code}

Although all application history infos are stored in a LinkedHashMap, here to code transforms the map to an iterator and then uses the find() api which is O( n) instead of O(1) from a map.get() operation.",,apachespark,rxin,wgtmac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 27 05:43:37 UTC 2016,,,,,,,,,,"0|i343xj:",9223372036854775807,,,,,,,,,,,,,2.0.1,,,,,,,,,,,"26/Sep/16 22:54;wgtmac;I'm working on a fix and will send a PR soon.;;;","26/Sep/16 23:45;apachespark;User 'wgtmac' has created a pull request for this issue:
https://github.com/apache/spark/pull/15247;;;","27/Sep/16 00:19;wgtmac;Hi [~ajbozarth], can you take a look at the PR? Thanks!;;;","27/Sep/16 05:39;srowen;I don't see how this is separate from SPARK-17671?;;;","27/Sep/16 05:43;wgtmac;They are similar but different.
This JIRA deals with the approach to get one specific appId from the whole list (returned from the map). SPARK-17671 deals with the number of app infos to get from the map.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
take() or isEmpty() on dataset leaks s3a connections,SPARK-17666,13007595,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,igor.berman,igor.berman,26/Sep/16 14:44,30/Sep/16 02:07,14/Jul/23 06:29,28/Sep/16 01:14,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"I'm experiensing problems with s3a and working with parquet with dataset api
the symptom of problem - tasks failing with 
{code}
Caused by: org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool
	at org.apache.http.impl.conn.PoolingClientConnectionManager.leaseConnection(PoolingClientConnectionManager.java:232)
	at org.apache.http.impl.conn.PoolingClientConnectionManager$1.getConnection(PoolingClientConnectionManager.java:199)
	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
{code}

Checking CoarseGrainedExecutorBackend with lsof gave me many sockets in CLOSE_WAIT state

reproduction of problem:
{code}
package com.test;

import java.text.ParseException;

import org.apache.spark.SparkConf;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class ConnectionLeakTest {
	public static void main(String[] args) throws ParseException {
		SparkConf sparkConf = new SparkConf();

		sparkConf.setMaster(""local[*]"");
		sparkConf.setAppName(""Test"");
		sparkConf.set(""spark.local.dir"", ""/tmp/spark"");
		sparkConf.set(""spark.sql.shuffle.partitions"", ""2"");

		SparkSession session = SparkSession.builder().config(sparkConf).getOrCreate();
//set your credentials to your bucket

		for (int i = 0; i < 100; i++) {
			Dataset<Row> df = session
					.sqlContext()
					.read()
					.parquet(""s3a://test/*"");//contains multiple snappy compressed parquet files
			if (df.rdd().isEmpty()) {//same problem with takeAsList().isEmpty()
				System.out.println(""Yes"");
			} else {
				System.out.println(""No"");
			}
		}
		System.out.println(""Done"");
	}
}
{code}
so when program runs, you can jps for pid and do lsof -p <pid> | grep https
and you'll see constant grow of CLOSE_WAITs

Our way to bypass problem is to use count() == 0
In addition we've seen that df.dropDuplicates(""a"",""b"").rdd().isEmpty() doesn't produce problem too



","ubuntu/centos, java 7, java 8, spark 2.0, java api",apachespark,emlyn,igor.berman,joshrosen,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17740,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 28 01:33:03 UTC 2016,,,,,,,,,,"0|i342zj:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"26/Sep/16 14:48;srowen;Where are you saying the leak is, from take()?;;;","26/Sep/16 14:51;igor.berman;[~sowen], yes, I believe so
rdd.isEmpty() uses take inside as far as I understand
;;;","26/Sep/16 14:52;srowen;No, I mean what about take() do you believe leaks a connection? is it a Spark problem or S3 problem?;;;","26/Sep/16 14:56;igor.berman;sorry, still no idea. What's the difference between take and count ? Why count is not producing such problem? So for me as ""end-user"" spark has a problem, something like ""not returning connection to s3a connection pool"", but I might be wrong. I'll try to investigate it further.;;;","26/Sep/16 17:53;joshrosen;My hunch is that there's cleanup which is performed in a {{CompletionIterator}} once that iterator is completely consumed, but that there is not final ""safety net"" cleanup logic to ensure cleanup if the iterator is _not_ fully-consumed (which happens in take).;;;","26/Sep/16 18:17;srowen;Hm, I wonder if a couple problems of this form could be solved by handling the case of a CompletionIterator in ResultTask.runTask, which is the thing that would eventually get a CompletionIterator from something and process it, if anything, in this case. It could call completion() when it knows it's done. I might give that a shot to see if it works at all; not sure that's the right fix.;;;","26/Sep/16 19:13;joshrosen;I think that one problem with that approach is that any transformation of a CompletionIterator will yield a new iterator which doesn't forward the close() call (i.e. if you {{map}} over the iterator then it will break your proposed check in {{Task}}).;;;","26/Sep/16 19:33;srowen;Agree, it might happen to help some cases or even this one but can't help all cases. Does a finalize() help at all? only if somehow the iterator were regularly GCed well before the underlying stream or something, but I don't see how.;;;","26/Sep/16 20:39;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15245;;;","28/Sep/16 01:14;rxin;This still needs to be backported to branch-2.0.
;;;","28/Sep/16 01:33;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15271;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SchedulableBuilder should handle invalid data access via scheduler.allocation.file,SPARK-17663,13007465,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,erenavsarogullari,erenavsarogullari,erenavsarogullari,25/Sep/16 21:30,17/May/20 17:47,14/Jul/23 06:29,06/Feb/17 14:25,2.1.0,,,,,,,,2.2.0,,,,Scheduler,Spark Core,,,,,,,0,,,,,,"If spark.scheduler.allocation.file has invalid minShare or/and weight values, these cause :

- NumberFormatException due to toInt function
- SparkContext can not be initialized.
- It does not show meaningful error message to user.

In a nutshell, this functionality can be more robust by selecting one of the following flows :

*1-* Currently, if schedulingMode has an invalid value, a warning message is logged and default value is set as FIFO. Same pattern can be used for minShare(default: 0) and weight(default: 1) as well
*2-* Meaningful error message can be shown to the user for all invalid cases.

*Code to Reproduce* :
{code}
val conf = new SparkConf().setAppName(""spark-fairscheduler"").setMaster(""local"")
conf.set(""spark.scheduler.mode"", ""FAIR"")
conf.set(""spark.scheduler.allocation.file"", ""src/main/resources/fairscheduler-invalid-data.xml"")
val sc = new SparkContext(conf)
{code}

*fairscheduler-invalid-data.xml* :
{code}
<allocations>
    <pool name=""production"">
        <schedulingMode>FIFO</schedulingMode>
        <weight>invalid_weight</weight>
        <minShare>2</minShare>
    </pool>
</allocations>
{code}

*Stacktrace* :
{code}
Exception in thread ""main"" java.lang.NumberFormatException: For input string: ""invalid_weight""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at org.apache.spark.scheduler.FairSchedulableBuilder$$anonfun$org$apache$spark$scheduler$FairSchedulableBuilder$$buildFairSchedulerPool$1.apply(SchedulableBuilder.scala:127)
	at org.apache.spark.scheduler.FairSchedulableBuilder$$anonfun$org$apache$spark$scheduler$FairSchedulableBuilder$$buildFairSchedulerPool$1.apply(SchedulableBuilder.scala:102)
{code}
",,apachespark,erenavsarogullari,irashid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 06 14:25:54 UTC 2017,,,,,,,,,,"0|i3426n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/16 22:22;apachespark;User 'erenavsarogullari' has created a pull request for this issue:
https://github.com/apache/spark/pull/15237;;;","06/Feb/17 14:25;irashid;Issue resolved by pull request 15237
[https://github.com/apache/spark/pull/15237];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DESC FORMATTED for VIEW Lacks View Definition,SPARK-17660,13007411,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,25/Sep/16 04:51,27/Sep/16 17:53,14/Jul/23 06:29,27/Sep/16 17:53,2.0.0,2.1.0,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Currently, DESC FORMATTED does not have a section for the view definition. We should add it for permanent views, like what Hive does. Below is an example with the desired view definition.

{noformat}
+----------------------------+-------------------------------------------------------------------------------------------------------------------------------------+-------+
|col_name                    |data_type                                                                                                                            |comment|
+----------------------------+-------------------------------------------------------------------------------------------------------------------------------------+-------+
|a                           |int                                                                                                                                  |null   |
|                            |                                                                                                                                     |       |
|# Detailed Table Information|                                                                                                                                     |       |
|Database:                   |default                                                                                                                              |       |
|Owner:                      |xiaoli                                                                                                                               |       |
|Create Time:                |Sat Sep 24 21:46:19 PDT 2016                                                                                                         |       |
|Last Access Time:           |Wed Dec 31 16:00:00 PST 1969                                                                                                         |       |
|Location:                   |                                                                                                                                     |       |
|Table Type:                 |VIEW                                                                                                                                 |       |
|Table Parameters:           |                                                                                                                                     |       |
|  transient_lastDdlTime     |1474778779                                                                                                                           |       |
|                            |                                                                                                                                     |       |
|# Storage Information       |                                                                                                                                     |       |
|SerDe Library:              |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                                                                                   |       |
|InputFormat:                |org.apache.hadoop.mapred.SequenceFileInputFormat                                                                                     |       |
|OutputFormat:               |org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat                                                                            |       |
|Compressed:                 |No                                                                                                                                   |       |
|Storage Desc Parameters:    |                                                                                                                                     |       |
|  serialization.format      |1                                                                                                                                    |       |
|                            |                                                                                                                                     |       |
|# View Information          |                                                                                                                                     |       |
|View Original Text:         |SELECT * FROM tbl                                                                                                                    |       |
|View Expanded Text:         |SELECT `gen_attr_0` AS `a` FROM (SELECT `gen_attr_0` FROM (SELECT `a` AS `gen_attr_0` FROM `default`.`tbl`) AS gen_subquery_0) AS tbl|       |
+----------------------------+-------------------------------------------------------------------------------------------------------------------------------------+-------+
{noformat}",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 25 04:52:04 UTC 2016,,,,,,,,,,"0|i341un:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/16 04:52;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15234;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partitioned View is Not Supported In SHOW CREATE TABLE,SPARK-17659,13007407,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,25/Sep/16 03:33,09/Nov/16 08:13,14/Jul/23 06:29,09/Nov/16 08:13,2.0.0,2.1.0,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"`Partitioned View` is not supported by SPARK SQL. For Hive partitioned view, SHOW CREATE TABLE is unable to generate the right DDL. Thus, SHOW CREATE TABLE should not support it like the other Hive-only features.
",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 25 03:36:04 UTC 2016,,,,,,,,,,"0|i341tr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/16 03:36;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15233;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
write.df API requires path which is not actually always nessasary in SparkR,SPARK-17658,13007359,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,gurwls223,,24/Sep/16 10:20,12/Dec/22 17:50,14/Jul/23 06:29,05/Oct/16 06:03,2.0.0,,,,,,,,2.1.0,,,,SparkR,,,,,,,,0,,,,,,"It seems {{write.df}} in SparkR always requires taking {{path}}. This is actually not always nessasary.

For example, if we have a datasource extending {{CreatableRelationProvider}}, it might not request {{path}}. 

FWIW, Python/Scala do not require this in the API already.",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 24 10:47:04 UTC 2016,,,,,,,,,,"0|i341j3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/16 10:47;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/15231;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disallow Users to Change Table Type ,SPARK-17657,13007352,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,24/Sep/16 06:50,13/Oct/16 13:38,14/Jul/23 06:29,13/Oct/16 13:38,2.0.0,2.1.0,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Hive allows users to change the table type from `Managed` to `External` or from `External` to `Managed` by altering table's property `EXTERNAL`. See the JIRA: https://issues.apache.org/jira/browse/HIVE-1329

So far, Spark SQL does not correctly support it, although users can do it. Many assumptions are broken in the implementation. Thus, this PR is to disallow users to do it. 
",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 13 13:38:49 UTC 2016,,,,,,,,,,"0|i341hj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/16 06:51;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15230;;;","13/Oct/16 13:38;cloud_fan;Issue resolved by pull request 15230
[https://github.com/apache/spark/pull/15230];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix confusing exception message while reserving capacity,SPARK-17652,13007281,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sameerag,sameerag,sameerag,23/Sep/16 22:15,28/Sep/16 23:27,14/Jul/23 06:29,26/Sep/16 20:21,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,,,apachespark,sameerag,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 26 20:21:43 UTC 2016,,,,,,,,,,"0|i3411j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/16 22:16;apachespark;User 'sameeragarwal' has created a pull request for this issue:
https://github.com/apache/spark/pull/15225;;;","26/Sep/16 20:21;yhuai;Issue resolved by pull request 15225
[https://github.com/apache/spark/pull/15225];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding a malformed URL to sc.addJar and/or sc.addFile bricks Executors,SPARK-17650,13007230,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,23/Sep/16 19:00,28/Sep/16 23:27,14/Jul/23 06:29,26/Sep/16 05:58,,,,,,,,,2.0.1,2.1.0,,,Spark Core,,,,,,,,0,,,,,,"Using a malformed URL in sc.addJar or sc.addFile bricks the executors forever. The executors try to update their dependencies, but because the URL is malformed, they always throw a malformedURL exception. Then your cluster is unusable until you restart it.",,apachespark,brkyvz,joshrosen,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 23 21:14:05 UTC 2016,,,,,,,,,,"0|i340q7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/16 21:14;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/15224;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL LIKE does not handle backslashes correctly,SPARK-17647,13007185,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,23/Sep/16 15:55,13/Jan/21 22:02,14/Jul/23 06:29,17/Apr/17 18:20,,,,,,,,,2.1.1,2.2.0,,,SQL,,,,,,,,0,correctness,,,,,"Try the following in SQL shell:

{code}
select '\\\\' like '%\\%';
{code}

It returned false, which is wrong.


cc: [~yhuai] [~joshrosen]


A false-negative considered previously:


{code}
select '\\\\' rlike '.*\\\\\\\\.*';
{code}

It returned true, which is correct if we assume that the pattern is treated as a Java string but not raw string.",,apachespark,cloud_fan,djiangxu,jodersky,joshrosen,maropu,mengxr,noahkawasakigoogle,robert3005,smilegator,swiegleb,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19555,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 13 22:02:52 UTC 2021,,,,,,,,,,"0|i340g7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/16 21:12;joshrosen;I think that the first case is clearly a bug (and have a fix) but I'm not so sure about the second case. Consider:

{code}
scala> "".*\\\\\\\\.*"".r.findFirstIn(""\\\\"")
res8: Option[String] = Some(\\)
{code}

In a regular expression, two backslashes denote an escaped backslash. Setting Java strings aside for a moment, consider using pencil/paper to writing a regex which matches a single backslash character: in the context of a regex a backslash character acts as an escape character, so you need two consecutive backslashes. When we take our handwritten regex with two backslashes and encode this into a Java string we need to add an additional layer of backslash escaping to work around the character escaping for Java strings, yielding four consecutive backslashes.

One illustration of this is the fact that the Java string literal {code}""\\""{code} is not considered a valid regex:

{code}
scala> ""\\"".r
java.util.regex.PatternSyntaxException: Unexpected internal error near index 1
\
 ^
  at java.util.regex.Pattern.error(Pattern.java:1955)
  at java.util.regex.Pattern.compile(Pattern.java:1702)
  at java.util.regex.Pattern.<init>(Pattern.java:1351)
  at java.util.regex.Pattern.compile(Pattern.java:1028)
  at scala.util.matching.Regex.<init>(Regex.scala:191)
  at scala.collection.immutable.StringLike$class.r(StringLike.scala:284)
  at scala.collection.immutable.StringOps.r(StringOps.scala:29)
  at scala.collection.immutable.StringLike$class.r(StringLike.scala:273)
  at scala.collection.immutable.StringOps.r(StringOps.scala:29)
  ... 28 elided
{code}

The second example returns {{true}} on MySQL.

On MySQL, running {code}select '\\' rlike '\\'{code} will fail with a syntax error because this will be interpreted as a trailing escape character rather than as a backslash literal, while {code}select '\\' rlike '\\\\'{code} will return true.;;;","23/Sep/16 21:33;joshrosen;On the other hand, running 

{code}
select '\\\\' like '%\\%'
{code}

seems to return false in MySQL (because I think this is being interpreted as ""any characters followed by a literal percent""), but it returns true in Postgres. I'm running postgres via {{psql}} and MySQL via MySQLWorkbench.;;;","23/Sep/16 21:43;joshrosen;Another piece of evidence to help untangle this:

In MySQL,

{code}
select '\\' like '\\', '\\' rlike '\\\\';
{code}

returns {{true}} for both columns, illustrating that {{like}} and {{rlike}} seem to have different escaping rules.

However,

{code}
select 
  '\\' like '\\\\',
  '\\' like '\\'
{code}

returns {{true}} for both columns in MySQL and {{true, false}} in Postgres.;;;","26/Sep/16 17:05;mengxr;Thanks [~joshrosen]! I updated the JIRA description. The LIKE escaping behaviors in MySQL/PostgreSQL are documented here:

* MySQL: http://dev.mysql.com/doc/refman/5.7/en/string-comparison-functions.html#operator_like
* PostgreSQL: https://www.postgresql.org/docs/8.3/static/functions-matching.html

In particular, MySQL:

{noformat}
Exception: At the end of the pattern string, backslash can be specified as “\\”.
At the end of the string, backslash stands for itself because there is nothing following to escape.
{noformat}

That explains why MySQL returns true for both

{code}
'\\' like '\\\\'
'\\' like '\\'
{code};;;","06/Oct/16 22:08;yhuai;[~smilegator] Anyone from your side has time to take a look at it?;;;","06/Oct/16 22:34;smilegator;Sure, let me check it with my teammates. Thanks!;;;","06/Oct/16 23:15;jodersky;Xiao pointed me to this issue, I can take a look at it;;;","08/Oct/16 00:35;apachespark;User 'jodersky' has created a pull request for this issue:
https://github.com/apache/spark/pull/15398;;;","09/Dec/16 08:19;mengxr;[~rxin@databricks.com] [~yhuai] I think this is a critical correctness bug, which should be fixed in 2.1. Thoughts?;;;","09/Dec/16 19:37;jodersky;I rebased the PR and resolved the conflict. However, there is still the incompatibility issue with the sql ANTLR parser. I talk about it in my last [two comments | https://github.com/apache/spark/pull/15398#issuecomment-255917940 ] and propose a few solutions. Any feedback is welcome!;;;","17/Apr/17 21:39;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/17663;;;","18/Dec/17 17:03;djiangxu;Are we sure this issue is resolved, I tested the following on spark-shell 2.2.0
{code}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.2.0
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_25)
Type in expressions to have them evaluated.
Type :help for more information.

scala> spark.sql(""select '\\\\' like '%\\%'"").show
+----------+
|\ LIKE %\%|
+----------+
|     false|
+----------+
{code}
same in spark-sql
{code}
spark-sql> select '\\\\' like '%\\%';
false
Time taken: 2.296 seconds, Fetched 1 row(s)
{code}
;;;","19/Dec/17 01:13;maropu;I'm looking  into the code and I'll make a follow-up pr.;;;","19/Dec/17 05:29;maropu;Probably, is it okay to set `spark.sql.parser.escapedStringLiterals` to true?;;;","20/Dec/17 13:04;cloud_fan;{code}
spark.sql(""select '\\\\' like '%\\%'"").show
{code}
actually is
{code}
select '\\' like '%\%'
{code}
as SQL statement, after the java string escaping.;;;","03/Jul/19 07:44;swiegleb;I have tested this behavior with spark 2.4.2 and it seems to be that the bug is still there.

With: 
{code:java}
spark.sql.parser.escapedStringLiterals=true
{code}
backslash it is working, but I run into failures with escaping "" and ' .
{code:java}
the escape character is not allowed to precede '\""'
{code}
I have tested the behavior with the following special characters and their escaping:
{code:java}
_ % \ ' ""
{code}
So both modes will not cover the full spectrum of escaping special characters.

 ;;;","13/Jan/21 22:02;noahkawasakigoogle;I can also confirm that this issue is not fully resolved. Like what [~swiegleb] has shown, escape characters are not fully supported. 

I have tested Spark versions 2.1, 2.2, 2.3, 2.4, and 3.0 and they all experience the issue:
{code:java}
# These do not return the expected backslash
SET spark.sql.parser.escapedStringLiterals=false;
SELECT '\\';
> \
(should return \\)

SELECT 'hi\hi';
> hihi
(should return hi\hi) 


# These are correctly escaped
SELECT '\""';
> ""

 SELECT '\'';
> '{code}
If I switch this: 
{code:java}
# These now work
SET spark.sql.parser.escapedStringLiterals=true;
SELECT '\\';
> \\

SELECT 'hi\hi';
> hi\hi


# These are now not correctly escaped
SELECT '\""';
> \""
(should return "")

SELECT '\'';
> \'
(should return ' ){code}
 So basically we have to choose:

SET spark.sql.parser.escapedStringLiterals=false; if we want backslashes correctly escaped but not other special characters


SET spark.sql.parser.escapedStringLiterals=true; if we want other special characters correctly escaped but not backslashes;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
The failed stage never resubmitted due to abort stage in another thread,SPARK-17644,13007071,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,scwf,scwf,scwf,23/Sep/16 07:43,28/Sep/16 23:30,14/Jul/23 06:29,28/Sep/16 23:30,1.6.0,2.0.0,,,,,,,2.0.1,2.1.0,,,Scheduler,Spark Core,,,,,,,0,,,,,,"there is a race condition when FetchFailed and resubmit failed stage:
job1, job2 run in different threads, if job 1 failed 4 times due to fetchfailed and aborted, then job2 can not post ResubmitFailedStages becase the failedStages in DAGScheduler is not empty now.",,apachespark,emlyn,irashid,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 23 08:04:04 UTC 2016,,,,,,,,,,"0|i33zqv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/16 08:04;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/15213;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove comparable requirement from Offset,SPARK-17643,13007011,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,23/Sep/16 01:34,05/Oct/16 23:49,14/Jul/23 06:29,23/Sep/16 19:18,2.0.0,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,,,,,,"For some sources, it can be hard to define a strict ordering that is based only on the data in the offset.  Since we don't really utilize this comparison, lets remove it.",,apachespark,freiss,marmbrus,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 05 21:50:06 UTC 2016,,,,,,,,,,"0|i33zdr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/16 01:37;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/15207;;;","23/Sep/16 19:18;tdas;Issue resolved by pull request 15207
[https://github.com/apache/spark/pull/15207];;;","05/Oct/16 21:50;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/15362;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
collect_set should ignore null values,SPARK-17641,13006991,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,23/Sep/16 00:09,28/Sep/16 23:25,14/Jul/23 06:29,28/Sep/16 23:25,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"`collect_set` throws the following exception when there are null values. It should ignore null values to be consistent with other aggregation methods.

{code}
select collect_set(null) from (select 1) tmp;

java.lang.IllegalArgumentException: Flat hash tables cannot contain null elements.
	at scala.collection.mutable.FlatHashTable$HashUtils$class.elemHashCode(FlatHashTable.scala:390)
	at scala.collection.mutable.HashSet.elemHashCode(HashSet.scala:41)
	at scala.collection.mutable.FlatHashTable$class.addEntry(FlatHashTable.scala:136)
	at scala.collection.mutable.HashSet.addEntry(HashSet.scala:41)
	at scala.collection.mutable.HashSet.$plus$eq(HashSet.scala:60)
	at scala.collection.mutable.HashSet.$plus$eq(HashSet.scala:41)
	at org.apache.spark.sql.catalyst.expressions.aggregate.Collect.update(collect.scala:64)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1$$anonfun$applyOrElse$1.apply(AggregationIterator.scala:170)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1$$anonfun$applyOrElse$1.apply(AggregationIterator.scala:170)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$generateProcessRow$1.apply(AggregationIterator.scala:186)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$generateProcessRow$1.apply(AggregationIterator.scala:180)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.processCurrentSortedGroup(SortBasedAggregationIterator.scala:115)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:150)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:29)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$3.apply(SparkPlan.scala:232)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$3.apply(SparkPlan.scala:225)
{code}

cc: [~yhuai]",,apachespark,mengxr,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 23 01:49:05 UTC 2016,,,,,,,,,,"0|i33z9b:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"23/Sep/16 01:49;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/15208;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to add jce.jar to bootclasspath,SPARK-17639,13006981,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,22/Sep/16 22:52,23/Sep/16 04:36,14/Jul/23 06:29,23/Sep/16 04:36,2.1.0,,,,,,,,2.1.0,,,,Build,,,,,,,,0,,,,,,"The following PR is failing because jce.jar is missing from the classpath when compiling the code:

https://github.com/apache/spark/pull/15172",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 22 22:58:04 UTC 2016,,,,,,,,,,"0|i33z73:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/16 22:58;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/15204;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove hardcode ""agg_plan"" in HashAggregateExec",SPARK-17635,13006857,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yucai,yucai,yucai,22/Sep/16 14:52,23/Sep/16 00:32,14/Jul/23 06:29,23/Sep/16 00:32,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,correctness,,,,,"""agg_plan"" is hardcoded in HashAggregateExec, which has potential issue.
{code}
        ctx.addMutableState(fastHashMapClassName, fastHashMapTerm,
          s""$fastHashMapTerm = new $fastHashMapClassName("" +
            s""agg_plan.getTaskMemoryManager(), agg_plan.getEmptyAggregationBuffer());"")
{code}

I faced this issue when I work on sort agg's codegen support. Below codes will trigger bug:
{code}
  private def variablePrefix: String = this match {
 -    case _: HashAggregateExec => ""agg""		 
 +    case _: HashAggregateExec => ""hagg""
 +    case _: SortAggregateExec => ""sagg""
{code}",,apachespark,joshrosen,kiszk,yucai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 22 14:57:07 UTC 2016,,,,,,,,,,"0|i33yfj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/16 14:57;apachespark;User 'yucai' has created a pull request for this issue:
https://github.com/apache/spark/pull/15199;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming Providers should be labeled Experimental,SPARK-17627,13006682,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,21/Sep/16 22:32,22/Sep/16 03:59,14/Jul/23 06:29,22/Sep/16 03:59,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"All of structured streaming is experimental, but we missed the annotation on two of the APIs.",,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 21 22:36:05 UTC 2016,,,,,,,,,,"0|i33xcn:",9223372036854775807,,,,,,,,,,,,,2.0.1,,,,,,,,,,,"21/Sep/16 22:36;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/15188;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
expectedOutputAttributes should be set when converting SimpleCatalogRelation to LogicalRelation,SPARK-17625,13006581,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ZenWzh,ZenWzh,ZenWzh,21/Sep/16 17:48,22/Sep/16 06:49,14/Jul/23 06:29,22/Sep/16 06:49,2.0.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"expectedOutputAttributes should be set when converting SimpleCatalogRelation to LogicalRelation, otherwise the outputs of LogicalRelation are different from outputs of SimpleCatalogRelation - they have different exprId's.",,apachespark,cloud_fan,ZenWzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 22 06:49:30 UTC 2016,,,,,,,,,,"0|i33wrb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/16 17:52;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/15182;;;","22/Sep/16 06:49;cloud_fan;Issue resolved by pull request 15182
[https://github.com/apache/spark/pull/15182];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hive.default.fileformat=orc does not set OrcSerde,SPARK-17620,13006400,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dkbiswal,chobrian,chobrian,21/Sep/16 06:41,18/Oct/16 03:47,14/Jul/23 06:29,18/Oct/16 03:47,,,,,,,,,2.1.0,,,,SQL,,,,,,,,1,,,,,,"Setting {{hive.default.fileformat=orc}} does not set OrcSerde. This behavior is inconsistent with {{STORED AS ORC}}. This means we cannot set a default behavior for creating tables using orc.

The behavior using stored as:

{noformat}
scala> spark.sql(""CREATE TABLE tmp_stored_as(id INT) STORED AS ORC"")
res0: org.apache.spark.sql.DataFrame = []

scala> spark.sql(""DESC FORMATTED tmp_stored_as"").collect.foreach(println)
...
[# Storage Information,,]
[SerDe Library:,org.apache.hadoop.hive.ql.io.orc.OrcSerde,]
[InputFormat:,org.apache.hadoop.hive.ql.io.orc.OrcInputFormat,]
[OutputFormat:,org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat,]
...
{noformat}

Behavior setting default conf (SerDe Library is not set properly):

{noformat}
scala> spark.sql(""SET hive.default.fileformat=orc"")
res2: org.apache.spark.sql.DataFrame = [key: string, value: string]

scala> spark.sql(""CREATE TABLE tmp_default(id INT)"")
res3: org.apache.spark.sql.DataFrame = []

scala> spark.sql(""DESC FORMATTED tmp_default"").collect.foreach(println)
...
[# Storage Information,,]
[SerDe Library:,org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe,]
[InputFormat:,org.apache.hadoop.hive.ql.io.orc.OrcInputFormat,]
[OutputFormat:,org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat,]
...
{noformat}",,apachespark,chobrian,dkbiswal,dongjoon,smilegator,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 18 03:47:19 UTC 2016,,,,,,,,,,"0|i33vn3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/16 21:37;dkbiswal;fix it now. Thanks!;;;","21/Sep/16 23:02;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/15190;;;","14/Oct/16 20:23;smilegator;Issue resolved by pull request 15190
[https://github.com/apache/spark/pull/15190];;;","14/Oct/16 21:11;yhuai;The PR somehow breaks the build and it has been reverted.;;;","14/Oct/16 22:49;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/15495;;;","18/Oct/16 03:47;smilegator;Issue resolved by pull request 15495
[https://github.com/apache/spark/pull/15495];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dataframe except returns incorrect results when combined with coalesce,SPARK-17618,13006362,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,gedwards,gedwards,21/Sep/16 02:51,28/Sep/16 23:27,14/Jul/23 06:29,27/Sep/16 18:01,1.6.1,1.6.2,,,,,,,1.6.3,2.0.1,2.1.0,,SQL,,,,,,,,0,correctness,,,,,"We were getting incorrect results from the DataFrame except method - all rows were being returned instead of the ones that intersected. Calling subtract on the underlying RDD returned the correct result.

We tracked it down to the use of coalesce - the following is the simplest example case we created that reproduces the issue:

{code}
val schema = new StructType().add(""test"", types.IntegerType )
val t1 = sql.createDataFrame(sql.sparkContext.parallelize(1 to 100).map(i=> Row(i)), schema)
val t2 = sql.createDataFrame(sql.sparkContext.parallelize(5 to 10).map(i=> Row(i)), schema)
val t3 = t1.join(t2, t1.col(""test"").equalTo(t2.col(""test"")), ""leftsemi"")
println(""Count using normal except = "" + t1.except(t3).count())
println(""Count using coalesce = "" + t1.coalesce(8).except(t3.coalesce(8)).count())
{code}

We should get the same result from both uses of except, but the one using coalesce returns 100 instead of 94.",,apachespark,gedwards,hvanhovell,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 27 18:33:05 UTC 2016,,,,,,,,,,"0|i33ven:",9223372036854775807,,,,,,,,,,,,,1.6.3,,,,,,,,,,,"21/Sep/16 19:11;joshrosen;It looks like this affects 1.6.2 as well, but I was unable to reproduce in 2.x.

Comparing the two physical plans, I wonder if the issue has to do with Tungsten vs. regular internal row formats.

For {{t1.except(t3).explain(true)}}:

{code}
== Physical Plan ==
Except
:- Scan ExistingRDD[test#35] 
+- ConvertToSafe
   +- LeftSemiJoinHash [test#35], [test#36], None
      :- TungstenExchange hashpartitioning(test#35,200), None
      :  +- ConvertToUnsafe
      :     +- Scan ExistingRDD[test#35] 
      +- TungstenExchange hashpartitioning(test#36,200), None
         +- ConvertToUnsafe
            +- Scan ExistingRDD[test#36]
{code}

whereas {{t1.coalesce(8).except(t3.coalesce(8)).explain(true)}} produces

{code}
Except
:- Coalesce 8
:  +- Scan ExistingRDD[test#35] 
+- Coalesce 8
   +- LeftSemiJoinHash [test#35], [test#36], None
      :- TungstenExchange hashpartitioning(test#35,200), None
      :  +- ConvertToUnsafe
      :     +- Scan ExistingRDD[test#35] 
      +- TungstenExchange hashpartitioning(test#36,200), None
         +- ConvertToUnsafe
            +- Scan ExistingRDD[test#36]
{code}

My hunch is that Except is inappropriately mixing Tungsten and non-Tungsten row formats due to a bug in the row format conversion rules.;;;","21/Sep/16 19:35;joshrosen;Yep, the problem is that {{Coalesce}} advertises that it accepts Unsafe rows but misdeclares its row output format as being regular rows. Comparing an UnsafeRow to any other row type for equality always returns false (its {{equals()}} implementation is compatible with Java universal equality, so it doesn't throw when performing a comparison against a different type). As a result, the Except compares safe and unsafe rows, causing the comparisons to be incorrect and leading to the wrong answer that you saw here.

I'm marking this as a blocker for 1.6.3 and am working on a fix which will fix this issue.;;;","21/Sep/16 20:15;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15185;;;","21/Sep/16 22:49;gedwards;Thanks Josh, that perfectly explains what we saw. Thanks for the quick response!;;;","27/Sep/16 18:01;hvanhovell;Resolved per Josh's PR.

Follow-up for 2.0/master will follow.;;;","27/Sep/16 18:33;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15265;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remainder(%) expression.eval returns incorrect result,SPARK-17617,13006340,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,clockfly,clockfly,clockfly,21/Sep/16 00:02,21/Sep/16 08:58,14/Jul/23 06:29,21/Sep/16 08:54,,,,,,,,,1.6.3,2.0.1,2.1.0,,SQL,,,,,,,,0,correctness,,,,,"h2.Problem

Remainder(%) expression returns incorrect result when using expression.eval to calculate the result. expression.eval is called in case like constant folding.

{code}
scala> -5083676433652386516D  % 10
res19: Double = -6.0

// Wrong answer with eval!!!
scala> Seq(""-5083676433652386516D"").toDF.select($""value"" % 10).show
|(value % 10)|
+------------+
|         0.0|
+------------+

// Triggers codegen, will  not do constant folding
scala> sc.makeRDD(Seq(""-5083676433652386516D"")).toDF.select($""value"" % 10).show
+------------+
|(value % 10)|
+------------+
|        -6.0|
+------------+
{code}

Behavior of postgres:
{code}
seanzhong=# select -5083676433652386516.0  % 10;
 ?column? 
----------
     -6.0
(1 row)
{code}

",,apachespark,clockfly,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 21 08:54:47 UTC 2016,,,,,,,,,,"0|i33v9r:",9223372036854775807,,,,,,,,,,,,,1.6.3,2.0.1,2.1.0,,,,,,,,,"21/Sep/16 00:33;apachespark;User 'clockfly' has created a pull request for this issue:
https://github.com/apache/spark/pull/15171;;;","21/Sep/16 08:54;cloud_fan;Issue resolved by pull request 15171
[https://github.com/apache/spark/pull/15171];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Getting ""java.lang.RuntimeException: Distinct columns cannot exist in Aggregate """,SPARK-17616,13006339,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,hvanhovell,epahomov,epahomov,20/Sep/16 23:49,23/Sep/16 00:41,14/Jul/23 06:29,23/Sep/16 00:41,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"I execute:

{code}
select platform, 
        collect_set(user_auth) as paid_types,
        count(distinct sessionid) as sessions
    from non_hss.session
    where
        event = 'stop' and platform != 'testplatform' and
        not (month = MONTH(current_date()) AND year = YEAR(current_date()) and day = day(current_date())) and
        (
            (month >= MONTH(add_months(CURRENT_DATE(), -5)) AND year = YEAR(add_months(CURRENT_DATE(), -5)))
            OR
            (month <= MONTH(add_months(CURRENT_DATE(), -5)) AND year > YEAR(add_months(CURRENT_DATE(), -5)))
        )
    group by platform
{code}

I get:

{code}
java.lang.RuntimeException: Distinct columns cannot exist in Aggregate operator containing aggregate functions which don't support partial aggregation.
{code}

IT WORKED IN 1.6.2. I've read error 5 times, and read code once. I still don't understand what I do incorrectly.",,apachespark,epahomov,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17615,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 23 00:41:20 UTC 2016,,,,,,,,,,"0|i33v9j:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"21/Sep/16 22:24;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/15187;;;","23/Sep/16 00:41;joshrosen;Resolving as fixed by Herma's PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PartitioningAwareFileCatalog.allFiles doesn't handle URI specified path at parent,SPARK-17613,13006290,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,20/Sep/16 19:59,22/Sep/16 20:12,14/Jul/23 06:29,22/Sep/16 20:12,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"Consider you have a bucket as 
{code}
s3a://some-bucket
{code}
and under it you have files:
{code}
s3a://some-bucket/file1.parquet
s3a://some-bucket/file2.parquet
{code}

Getting the parent path of {code}s3a://some-bucket/file1.parquet{code}
yields
{code}s3a://some-bucket/{code}
and the ListingFileCatalog uses this as the key in the hash map.
When catalog.allFiles is called, we use {code}s3a://some-bucket{code} (no slash at the end) to get the list of files, and we're left with an empty list!
",,apachespark,brkyvz,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 22 20:12:39 UTC 2016,,,,,,,,,,"0|i33uyn:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"20/Sep/16 21:49;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/15169;;;","22/Sep/16 20:12;joshrosen;Issue resolved by pull request 15169
[https://github.com/apache/spark/pull/15169];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support `DESCRIBE table PARTITION` SQL syntax,SPARK-17612,13006289,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,20/Sep/16 19:56,28/Oct/16 21:04,14/Jul/23 06:29,29/Sep/16 22:33,,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,,,,,,"This issue implements `DESC PARTITION` SQL Syntax again. It was dropped since Spark 2.0.0.

h4. Spark 2.0.0
{code}
scala> sql(""CREATE TABLE partitioned_table (a STRING, b INT) PARTITIONED BY (c STRING, d STRING)"")
res0: org.apache.spark.sql.DataFrame = []

scala> sql(""ALTER TABLE partitioned_table ADD PARTITION (c='Us', d=1)"")
res1: org.apache.spark.sql.DataFrame = []

scala> sql(""DESC partitioned_table PARTITION (c='Us', d=1)"").show(false)
org.apache.spark.sql.catalyst.parser.ParseException:
Unsupported SQL statement
== SQL ==
DESC partitioned_table PARTITION (c='Us', d=1)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply(ParseDriver.scala:58)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply(ParseDriver.scala:53)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:82)
  at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:45)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:53)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:573)
  ... 48 elided
{code}

h4. Spark 1.6.2
{code}
scala> sql(""CREATE TABLE partitioned_table (a STRING, b INT) PARTITIONED BY (c STRING, d STRING)"")
res1: org.apache.spark.sql.DataFrame = [result: string]

scala> sql(""ALTER TABLE partitioned_table ADD PARTITION (c='Us', d=1)"")
res2: org.apache.spark.sql.DataFrame = [result: string]

scala> sql(""DESC partitioned_table PARTITION (c='Us', d=1)"").show(false)
16/09/20 12:48:36 WARN LazyStruct: Extra bytes detected at the end of the row! Ignoring similar problems.
+----------------------------------------------------------------+
|result                                                          |
+----------------------------------------------------------------+
|a                      string                                        |
|b                      int                                           |
|c                      string                                        |
|d                      string                                        |
|                                                                            |
|# Partition Information                                                      |
|# col_name             data_type               comment             |
|                                                                            |
|c                      string                                        |
|d                      string                                        |
+----------------------------------------------------------------+
{code}
",,apachespark,dongjoon,hvanhovell,tafranky@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17452,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 28 21:04:22 UTC 2016,,,,,,,,,,"0|i33uyf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/16 20:24;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/15168;;;","29/Sep/16 22:33;hvanhovell;Resolved per Dongjoon's PR;;;","04/Oct/16 22:44;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/15351;;;","28/Oct/16 19:58;tafranky@gmail.com;Hi  
Basically  I have an issue where I am performing the following operations.

Partitioned Large  Hive Table (hive table 1)  --  filter   ---    join 
                                                                                      /    
                                 Non Partitioned  Large Hive Table

Basically I am join 2 large tables .  Both table raw size exceed the  broadcast join threshold.
The filter filter a specific partition . This partition is small enough so that its size is smaller than the broadcast join threshold.

With Spark 2.0 and Spark 2.0.1 , I do not see  a broadcast join . I see a  sort merge join.  
Which is really  surprising to me given that this could be a really common  case. You can imagine a user who has a large log table partitioned by date and he filters on a specific date. We should be able to do a broadcast join in that case. 

The question now is the following .  

I do not think this Spark Issue addresses the cited problem but I could be wrong  . I tried incorporating the change in the spark 2.0 PR but I see the same behavior . That is no broadcast join.  

Question :  Is this spark issue supposed to address the problem that I mentioned ?  

- If not  , which i think is the case , do you know if spark currently has a fix for the cited issue.  
I also tried the fix under   SPARK-15616 but I hit a runtime failure .

There has got to be a solution to this problem somewhere.



;;;","28/Oct/16 20:09;dongjoon;Hi, [~tafranky@gmail.com].
I think you are asking about *join* problem. Is it related to this `DESC PARTITION`?
{code}
Hi 
Basically I have an issue where I am performing the following operations.
Partitioned Large Hive Table (hive table 1) – filter — join 
/ 
Non Partitioned Large Hive Table
Basically I am join 2 large tables . Both table raw size exceed the broadcast join threshold.
The filter filter a specific partition . This partition is small enough so that its size is smaller than the broadcast join threshold.
With Spark 2.0 and Spark 2.0.1 , I do not see a broadcast join . I see a sort merge join. 
Which is really surprising to me given that this could be a really common case. You can imagine a user who has a large log table partitioned by date and he filters on a specific date. We should be able to do a broadcast join in that case.
The question now is the following .
I do not think this Spark Issue addresses the cited problem but I could be wrong . I tried incorporating the change in the spark 2.0 PR but I see the same behavior . That is no broadcast join.
Question : Is this spark issue supposed to address the problem that I mentioned ?
If not , which i think is the case , do you know if spark currently has a fix for the cited issue.
I also tried the fix under SPARK-15616 but I hit a runtime failure .
There has got to be a solution to this problem somewhere.
{code};;;","28/Oct/16 20:56;tafranky@gmail.com;Hi 
Thanks for the quick reply. So I was only trying a long shot , because I wanted to know if the join issue that i described was related somehow to this issue . I know it is not probable but just wanted to check . ;;;","28/Oct/16 21:04;dongjoon;Ah, I see. For that question, this issue isn't related to that. I think so. :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"YarnShuffleServiceSuite swallows exceptions, doesn't really test a few things",SPARK-17611,13006239,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,20/Sep/16 17:19,17/May/20 18:14,14/Jul/23 06:29,20/Sep/16 21:18,,,,,,,,,2.1.0,,,,Spark Core,Tests,YARN,,,,,,0,,,,,,"If you look at the logs, you'll see exceptions like this:

{noformat}
16/09/20 10:03:45.081 pool-1-thread-1-ScalaTest-running-YarnShuffleServiceSuite ERROR YarnShuffleService: Exception when initializing application application_0_0001
java.lang.NullPointerException
        at io.netty.buffer.Unpooled.wrappedBuffer(Unpooled.java:183)
        at org.apache.spark.network.util.JavaUtils.bytesToString(JavaUtils.java:80)
        at org.apache.spark.network.sasl.ShuffleSecretManager.registerApp(ShuffleSecretManager.java:62)
        at org.apache.spark.network.yarn.YarnShuffleService.initializeApplication(YarnShuffleService.java:254)
        at org.apache.spark.network.yarn.YarnShuffleServiceSuite$$anonfun$1.apply$mcV$sp(YarnShuffleServiceSuite.scala:89)
        at org.apache.spark.network.yarn.YarnShuffleServiceSuite$$anonfun$1.apply(YarnShuffleServiceSuite.scala:81)
        at org.apache.spark.network.yarn.YarnShuffleServiceSuite$$anonfun$1.apply(YarnShuffleServiceSuite.scala:81)
{noformat}

The underlying reason is that the suite is testing with auth on, but providing a null auth secret. But because of catch blocks in the shuffle service code, and failure to properly check things in the suite, the tests pass even though the underlying code is failing.",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 20 17:23:05 UTC 2016,,,,,,,,,,"0|i33unb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/16 17:23;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/15161;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Folder deletion after globbing may fail StructuredStreaming jobs,SPARK-17599,13006027,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,19/Sep/16 20:15,10/May/19 19:31,14/Jul/23 06:29,21/Sep/16 09:09,2.0.0,,,,,,,,2.0.1,2.1.0,,,Structured Streaming,,,,,,,,0,,,,,,"The FileStreamSource used by StructuredStreaming first resolves globs, and then creates a ListingFileCatalog which listFiles with the resolved glob patterns. If a folder is deleted after glob resolution but before the ListingFileCatalog can list the files, we can run into a 'FileNotFoundException'.
This should not be a fatal exception for a streaming job. However we should include a warn message.
",,apachespark,brkyvz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19187,,,,,,,,,SPARK-27676,,,,,,,,,SPARK-24364,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 22 21:24:06 UTC 2016,,,,,,,,,,"0|i33tcf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/16 20:37;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/15153;;;","22/Sep/16 21:24;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/15202;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
list files on s3 very slow,SPARK-17593,13005876,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,gaurav24,gaurav24,19/Sep/16 11:09,15/May/18 10:49,14/Jul/23 06:29,20/Apr/17 13:25,2.0.0,,,,,,,,,,,,,,,,,,,,0,,,,,,"lets say we have following partitioned data:
{code}
events_v3
-- event_date=2015-01-01
---- event_hour=0
------ verb=follow
--------part10000.parquet.gz 
---- event_hour=1
------ verb=click
--------part10000.parquet.gz 
-- event_date=2015-01-02
---- event_hour=5
------ verb=follow
--------part10000.parquet.gz 
---- event_hour=10
------ verb=click
--------part10000.parquet.gz 
{code}
To read (or write ) parquet partitioned data via spark it makes call to `ListingFileCatalog.listLeafFiles` .  Which recursively tries to list all files and folders.

In this case if we had 300 dates, we would have created 300 jobs each trying to get filelist from date_directory. This process takes about 10 minutes to finish ( with 2 executors). vs if I use a ruby script to get list of all files recursively in the same folder it takes about 1 minute, on the same machine with just 1 thread. 

I am confused as to why it would take so much time extra for listing files.
spark code:
{code:scala}
val sparkSession = org.apache.spark.sql.SparkSession.builder
.config(""spark.sql.hive.metastorePartitionPruning"",true)
.config(""spark.sql.parquet.filterPushdown"", true)
.config(""spark.sql.hive.verifyPartitionPath"", false)
.config(""spark.sql.hive.convertMetastoreParquet.mergeSchema"",false)
.config(""parquet.enable.summary-metadata"",false)
.config(""spark.sql.sources.partitionDiscovery.enabled"",false)

.getOrCreate()
val df = sparkSession.read.option(""mergeSchema"",""false"").format(""parquet"").load(""s3n://bucket_name/events_v3"")
    df.createOrReplaceTempView(""temp_events"")
    sparkSession.sql(
      """"""
        |select verb,count(*) from temp_events where event_date = ""2016-08-05"" group by verb
      """""".stripMargin).show()
{code}

ruby code:
{code:ruby}
gem 'aws-sdk', '~> 2'
require 'aws-sdk'
client = Aws::S3::Client.new(:region=>'us-west-1')
next_continuation_token = nil
total = 0
loop do
a= client.list_objects_v2({
  bucket: ""bucket"", # required
  max_keys: 1000,
  prefix: ""events_v3/"",
  continuation_token: next_continuation_token ,
  fetch_owner: false,
})
puts a.contents.last.key
total += a.contents.size
next_continuation_token = a.next_continuation_token
break unless a.is_truncated
end

puts ""total""
puts total
{code}

tried looking into following bug:
https://issues.apache.org/jira/browse/HADOOP-12810
but hadoop 2.7.3 doesn't solve that problem
stackoverflow reference:
http://stackoverflow.com/questions/39525288/spark-parquet-write-gets-slow-as-partitions-grow","spark 2.0.0, hadoop 2.7.2 ( hadoop 2.7.3)",gaurav24,ndimiduk,schlosna,stevel@apache.org,Steven Rand,WeiqingYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-13208,SPARK-24280,,,,,,,,,,,HADOOP-13208,HADOOP-13345,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 10 18:31:35 UTC 2017,,,,,,,,,,"0|i33sf3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/16 11:14;srowen;I'm not sure this is a Spark problem. It seems S3 specific. Try using {{s3n://bucket_name/events_v3/}} as I seem to recall that it does matter in some cases whether you end with a slash, or glob pattern. CC [~stevel@apache.org];;;","19/Sep/16 11:59;gaurav24;Thanks [~srowen] tried after your comment, but that didn't help.
{code}
val df = sparkSession.read.option(""mergeSchema"",""false"").format(""parquet"").load(""s3n://bucket/events_v3/"")
    df.createOrReplaceTempView(""temp_events"")
    sparkSession.sql(
      """"""
        |select verb,count(*) from temp_events where event_date = ""2016-08-05"" group by verb
      """""".stripMargin).show()
{code};;;","19/Sep/16 14:02;stevel@apache.org;Sean is right: this is primarily S3, or more specifically, how S3 is made to look like a filesystem,  but isn't really —what you are seeing   is the cost of doing recursive tree walks (many, many list operations)

For a start, use S3A URLs rather than S3; its where all optimisation work is going.

This isn't going to help you immediately, as it really needs Spark to move to listFiles(recursive) along with the move to Hadoop 2.8 and so pick up the HADOOP-13208. I'll look at the codepath here to see if it's easy to do

Otherwise  try to partition date more heirarchically, and then select under that (e.g. have separate dirs for year, month, etc). Alternatively, go for a flat structure: all events in one single directory. List time drops to O(entries/5000). 


One thing that would be good would be if you could stick up on the JIRA email me direct what your full directory tree looks like, along. That won't fix the problem, but it will give me another example data structure to use when testing performance speedups. We use the TCP-DS layout —it's good to have more examples. The output of your ruby command is enough;;;","19/Sep/16 14:38;stevel@apache.org;Looking at the dir tree, anything you could do to flatten things by putting them in the names would make a difference.

incidentally, .gz files aren't great for parallel data analysis as they aren't splittable. Have you considered snappy?;;;","19/Sep/16 14:40;gaurav24;Thanks [~stevel@apache.org] S3 is definitely slower than hdfs I would agree. But then if list via that ruby script can happen in 1 minute and spark takes 10 minute, then it definitely sounds wrong. 

Hadoop 2.8/2.9 with https://issues.apache.org/jira/browse/HADOOP-13208 will definitely help. 

Updated directory structure and have sent you an email;;;","19/Sep/16 14:44;gaurav24;I definitely agree that flattening out will help, ( not sure how I could because of the way we have partitions) but even if it is not flattened 10 minutes sounds high given that the listing is not actually slow.

I agree, from spark 2.0.0 we have moved to snappy ( from about a month) .;;;","10/Oct/16 12:28;gaurav24;added detail explanation and solution here http://stackoverflow.com/questions/39513505/spark-lists-all-leaf-node-even-in-partitioned-data/39946236#39946236;;;","12/Dec/16 17:09;stevel@apache.org;Marking as a dependency of HADOOP-13208, which fixes it for all code that uses this API. Anything which implements their own treewalk will suffer. In tests over long-haul links, a single {{getFileStatus()}} call can take [~1500millis|https://steveloughran.blogspot.co.uk/2016/12/how-long-does-filesystemexists-take.html]

Note also that HADOOP-13345 delivers faster listing performance for all API calls by caching the metadata in dynamoDB; this will also give you the consistency needed to use s3 as a direct destination of work. ;;;","20/Apr/17 13:25;stevel@apache.org;closing as fixed now that Hadoop 2.8.0 is out the door. Upgrade your hadoop libraries. Thanks;;;","08/Nov/17 22:47;ndimiduk;So the fix in Hadoop 2.8 is for any variant of the s3* FileSystem? Or is it only for s3a?

bq. as it really needs Spark to move to listFiles(recursive)

Do we still need this change to be shipped in Spark? Thanks.;;;","10/Nov/17 18:31;stevel@apache.org;Hey nick, 

yes, need to move to FileSystem.list(path, recursive=true) & then iterate through the results. This actually scales better to the many thousands of files, but you'd know that. Not done a patch for that myself.

Moving to that won't be any worse for spark on older hadoop versions, but will get the read time speedup on Hadoop 2.8+. Write performance a separate issue, which is really ""commit algorithms for blob storage"". ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparseVector __getitem__ should follow __getitem__ contract,SPARK-17587,13005785,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zero323,zero323,zero323,18/Sep/16 20:18,04/Oct/16 12:43,14/Jul/23 06:29,04/Oct/16 00:58,1.6.2,2.0.0,,,,,,,2.0.2,2.1.0,,,ML,MLlib,PySpark,,,,,,0,,,,,,"According to {{\_\_getitem\_\_}} [contract|https://docs.python.org/3/reference/datamodel.html#object.__getitem__]:

{quote}
if of a value outside the set of indexes for the sequence (after any special interpretation of negative values), {{IndexError}} should be raised.
{quote}

This required for example for correct iteration over the structure.

Right now it throws {{ValueError}} what results in a quite confusing behavior when attempt to iterate over a vector results in a {{ValueError}} due to unterminated iteration:

{code}

In [1]: from pyspark.mllib.linalg import SparseVector

In [2]: list(SparseVector(4, [0], [0]))
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-2-147f3bb0a47d> in <module>()
----> 1 list(SparseVector(4, [0], [0]))

/opt/spark-2.0/python/pyspark/mllib/linalg/__init__.py in __getitem__(self, index)
    803 
    804         if index >= self.size or index < -self.size:
--> 805             raise ValueError(""Index %d out of bounds."" % index)
    806         if index < 0:
    807             index += self.size

ValueError: Index 4 out of bounds.
{code}

",,apachespark,josephkb,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 04 12:43:37 UTC 2016,,,,,,,,,,"0|i33ruv:",9223372036854775807,,,,,,,,,,,,,2.0.2,2.1.0,,,,,,,,,,"18/Sep/16 20:24;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/15144;;;","04/Oct/16 00:55;josephkb;I set the target versions to 2.1.0 and 2.0.2 (since it will miss 2.0.1).  I figure it's probably OK not to change this in 1.6, but let me know if you disagree.  I'm hesitant about 1.6 since this is sort of a breaking API change, as you pointed out.;;;","04/Oct/16 00:58;josephkb;Issue resolved by pull request 15144
[https://github.com/apache/spark/pull/15144];;;","04/Oct/16 12:43;zero323;I would probably go with 2.1.0 alone and definitely wouldn't bother with 1.6.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertOnQuery.condition should be consistent in requiring Boolean return type,SPARK-17571,13005618,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petermaxlee,petermaxlee,petermaxlee,17/Sep/16 07:34,01/Nov/16 22:26,14/Jul/23 06:29,18/Sep/16 22:21,,,,,,,,,2.0.1,2.1.0,,,Structured Streaming,,,,,,,,0,,,,,,"AssertOnQuery has two apply constructor: one that accepts a closure that returns boolean, and another that accepts a closure that returns Unit. This is actually very confusing because developers could mistakenly think that AssertOnQuery always require a boolean return type and verifies the return result, when indeed the value of the last statement is ignored in one of the constructors.",,apachespark,petermaxlee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 17 07:38:07 UTC 2016,,,,,,,,,,"0|i33qtz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/16 07:38;apachespark;User 'petermaxlee' has created a pull request for this issue:
https://github.com/apache/spark/pull/15127;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken link to Spark paper,SPARK-17567,13005545,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,iamshrek,ondronr,ondronr,16/Sep/16 20:30,17/Sep/16 11:31,14/Jul/23 06:29,17/Sep/16 11:30,2.0.0,,,,,,,,2.0.1,2.1.0,,,Documentation,,,,,,,,0,,,,,,"Documentation (http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.rdd.RDD) contains broken link to Spark paper (http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf). I found it elsewhere (https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf) and I hope it is the same one. It should be uploaded to and linked from some Apache controlled storage, so it won't break again.",,apachespark,ondronr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 17 11:30:53 UTC 2016,,,,,,,,,,"0|i33qdr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/16 23:02;apachespark;User 'keypointt' has created a pull request for this issue:
https://github.com/apache/spark/pull/15121;;;","17/Sep/16 11:30;srowen;Issue resolved by pull request 15121
[https://github.com/apache/spark/pull/15121];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PeriodicGraphCheckpointer did not persist edges as expected in some cases,SPARK-17559,13005336,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dingding,ding,ding,16/Sep/16 04:49,04/Oct/16 17:07,14/Jul/23 06:29,04/Oct/16 07:00,,,,,,,,,2.0.2,2.1.0,,,MLlib,,,,,,,,0,,,,,,"When use PeriodicGraphCheckpointer to persist graph, sometimes the edge isn't persisted. As currently only when vertices's storage level is none, graph is persisted. However there is a chance vertices's storage level is not none while edges's is none. Eg. graph created by a outerJoinVertices operation, vertices is automatically cached while edges is not. In this way, edges will not be persisted if we use PeriodicGraphCheckpointer do persist.

See below minimum example:
   val graphCheckpointer = new PeriodicGraphCheckpointer[Array[String], Int](2, sc)
    val users = sc.textFile(""data/graphx/users.txt"")
      .map(line => line.split("","")).map(parts => (parts.head.toLong, parts.tail))
    val followerGraph = GraphLoader.edgeListFile(sc, ""data/graphx/followers.txt"")

    val graph = followerGraph.outerJoinVertices(users) {
      case (uid, deg, Some(attrList)) => attrList
      case (uid, deg, None) => Array.empty[String]
    }
    graphCheckpointer.update(graph)    ",,apachespark,ding,josephkb,michaelmalak,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 04 17:07:04 UTC 2016,,,,,,,,,,"0|i33p3b:",9223372036854775807,,,,,,,,,,,,,2.0.2,2.1.0,,,,,,,,,,"16/Sep/16 04:50;apachespark;User 'dding3' has created a pull request for this issue:
https://github.com/apache/spark/pull/15116;;;","17/Sep/16 01:22;apachespark;User 'dding3' has created a pull request for this issue:
https://github.com/apache/spark/pull/15124;;;","04/Oct/16 07:00;josephkb;Issue resolved by pull request 15124
[https://github.com/apache/spark/pull/15124];;;","04/Oct/16 07:02;josephkb;Did I use the correct JIRA username for the ""Assignee?""  I tried ""ding"" but it wasn't working with JIRA admin, but ""dingding"" worked and has the same email.;;;","04/Oct/16 17:07;ding;Yes, it's the correct username. Thank you for your reviewing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InMemoryRelation doesn't scale to large tables,SPARK-17549,13005028,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,14/Sep/16 23:29,04/Oct/16 16:40,14/Jul/23 06:29,04/Oct/16 16:40,1.6.0,2.0.0,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,,,,,,"An {{InMemoryRelation}} is created when you cache a table; but if the table is large, defined by either having a really large amount of columns, or a really large amount of partitions (in the file split sense, not the ""table partition"" sense), or both, it causes an immense amount of memory to be used in the driver.

The reason is that it uses an accumulator to collect statistics about each partition, and instead of summarizing the data in the driver, it keeps *all* entries in memory.

I'm attaching a script I used to create a parquet file with 20,000 columns and a single row, which I then copied 500 times so I'd have 500 partitions.

When doing the following:

{code}
sqlContext.read.parquet(...).count()
{code}

Everything works fine, both in Spark 1.6 and 2.0. (It's super slow with the settings I used, but it works.)

I ran spark-shell like this:

{code}
./bin/spark-shell --master 'local-cluster[4,1,4096]' --driver-memory 2g --conf spark.executor.memory=2g
{code}

And ran:

{code}
sqlContext.read.parquet(...).cache().count()
{code}

You'll see the results in screenshot {{example_1.6_pre_patch.png}}. After 40 partitions were processed, there were 40 GenericInternalRow objects with
100,000 items each (5 stat info fields * 20,000 columns). So, memory usage was:

{code}
  40 * 100000 * (4 * 20 + 24) = 416000000 =~ 400MB
{code}

(Note: Integer = 20 bytes, Long = 24 bytes.)

If I waited until the end, there would be 500 partitions, so ~ 5GB of memory to hold the stats.


I'm also attaching a patch I made on top of 1.6 that uses just a long accumulator to capture the table size; with that patch memory usage on the driver doesn't keep growing. Also note in the patch that I'm multiplying the column size by the row count, which I think is a different bug in the existing code (those stats should be for the whole batch, not just a single row, right?). I also added {{example_1.6_post_patch.png}} to show the {{InMemoryRelation}} with the patch.


I also applied a very similar patch on top of Spark 2.0. But there things blow up even more spectacularly when I try to run the count on the cached table. It starts with this error:

{noformat}
14:19:43 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 1.0 (TID 2, vanzin-st1-3.gce.cloudera.com): java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: java.lang.IndexOutOfBoundsException: Index: 63235, Size: 1
(lots of generated code here...)
Caused by: java.lang.IndexOutOfBoundsException: Index: 63235, Size: 1
	at java.util.ArrayList.rangeCheck(ArrayList.java:635)
	at java.util.ArrayList.get(ArrayList.java:411)
	at org.codehaus.janino.util.ClassFile.getConstantPoolInfo(ClassFile.java:556)
	at org.codehaus.janino.util.ClassFile.getConstantUtf8(ClassFile.java:572)
	at org.codehaus.janino.util.ClassFile.loadAttribute(ClassFile.java:1513)
	at org.codehaus.janino.util.ClassFile.loadAttributes(ClassFile.java:644)
	at org.codehaus.janino.util.ClassFile.loadFields(ClassFile.java:623)
	at org.codehaus.janino.util.ClassFile.<init>(ClassFile.java:280)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anonfun$recordCompilationStats$1.apply(CodeGenerator.scala:913)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anonfun$recordCompilationStats$1.apply(CodeGenerator.scala:911)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.recordCompilationStats(CodeGenerator.scala:911)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	... 54 more
{noformat}

And basically a lot of that going on making the output unreadable, so I just killed the shell. Anyway, I believe the same fix should work there, but I can't be sure because the test doesn't work for different reasons, it seems.
",,apachespark,dongjoon,kiszk,robert3005,vanzin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/16 23:29;vanzin;create_parquet.scala;https://issues.apache.org/jira/secure/attachment/12828543/create_parquet.scala","14/Sep/16 23:29;vanzin;example_1.6_post_patch.png;https://issues.apache.org/jira/secure/attachment/12828544/example_1.6_post_patch.png","14/Sep/16 23:29;vanzin;example_1.6_pre_patch.png;https://issues.apache.org/jira/secure/attachment/12828545/example_1.6_pre_patch.png","14/Sep/16 23:48;vanzin;spark-1.6-2.patch;https://issues.apache.org/jira/secure/attachment/12828549/spark-1.6-2.patch","14/Sep/16 23:29;vanzin;spark-1.6.patch;https://issues.apache.org/jira/secure/attachment/12828546/spark-1.6.patch","15/Sep/16 00:06;vanzin;spark-2.0.patch;https://issues.apache.org/jira/secure/attachment/12828554/spark-2.0.patch",,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 29 21:40:24 UTC 2016,,,,,,,,,,"0|i33n6v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/16 23:48;vanzin;Just noticed there's already a more accurate count of the batch size in the code, so uploading an updated patch.

I'll try to figure out what's wrong in Spark 2 but not really familiar with that area of the code where the exceptions are coming from.;;;","15/Sep/16 00:06;vanzin;Attaching a Spark 2 patch that silences the error (looks like a Janino bug and is in an area that should not affect functionality from what I understand).

It'd be great if someone more familiar with this area could take a look at whether my changes are sane, before I go out and file a PR.;;;","15/Sep/16 16:58;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/15112;;;","16/Sep/16 21:03;yhuai;Issue resolved by pull request 15112
[https://github.com/apache/spark/pull/15112];;;","19/Sep/16 22:59;vanzin;[~yhuai]

There was something that was bothering me about my fix and the test below (modified version of the test in the patch) shows it:

{code}
  test(""SPARK-17549: cached table size should be correctly calculated"") {
    val data = spark.sparkContext.parallelize(1 to 10, 5).map { i => (i, i) }.toDF(""col1"", ""col2"")
    val plan = spark.sessionState.executePlan(data.logicalPlan).sparkPlan
    val cached = InMemoryRelation(true, 5, MEMORY_ONLY, plan, None)

    // Materialize the data.
    val expectedAnswer = data.collect()
    checkAnswer(cached, expectedAnswer)

    // Check that the right size was calculated.
    assert(cached.batchStats.value === 2 * expectedAnswer.size * INT.defaultSize)

    // Create a projection of the cached data and make sure the statistics are kept correctly
    // for each plan.
    val projected = cached.withOutput(Seq(plan.output.head))
    val expectedAnswer2 = data.select(""col1"").collect()
    checkAnswer(projected, expectedAnswer2)
    assert(projected.batchStats.value === expectedAnswer.size * INT.defaultSize)
  }
{code}

Basically, my patch has a problem in that now any relations derived from the cached data (both {{newInstance}} and {{withOutput}} methods of {{InMemoryRelation}}) share the same accumulator with the original instance. So if the original table is materialized before those transformations occur, there will be a problem. I'm also not sure how to fix that - going back to the original code is bad; the other option I see is calculating the stats for the new instance by transforming the cached RDD, but that's kinda wasteful (will cause unnecessary tasks to be executed, and will waste cache memory by caching the same blocks twice).

But that assumes that I understand the problem properly; since I'm not that familiar with the SQL code, could you confirm whether the above is a problem or not?;;;","19/Sep/16 23:47;vanzin;Replying to myself: yes, this seems to be a real problem. If you cache some transformation on a cached relation, that will trigger the ""withOutput"" path, and the stats for the new cached relation will be wrong (they'll be the same as the original cached relation), even though the output is different.

So either we should revert my patch, or make another small patch on top of it to use default stats for these child tables, unless someone has some idea of how to properly fix this.

Pinging [~marmbrus] here also in case he has some ideas.;;;","20/Sep/16 02:56;yhuai;[~vanzin] Let's revert this patch for now. So, this part will be the same as 2.0.0. Then, we can see how to fix the problem.;;;","20/Sep/16 03:00;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/15157;;;","20/Sep/16 03:00;yhuai;Forgot to say. Thank you for the investigation! Should we first get a test in to prevent the issue of having wrong stats?;;;","20/Sep/16 15:56;vanzin;Wouldn't hurt.;;;","20/Sep/16 18:55;yhuai;I have merged it to revert the original change.;;;","21/Sep/16 22:59;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/15189;;;","28/Sep/16 01:28;dongjoon;Hi, All.
Could we add this into RC4 or remove `2.0.1` from the fixed version?
I just noticed that this was shown in RC3 VOTE email.

> This release candidate resolves 290 issues:
> https://s.apache.org/spark-2.0.1-jira;;;","28/Sep/16 01:31;vanzin;Done. Forgot to update the bug after the patch was reverted.;;;","28/Sep/16 01:33;dongjoon;Thank you, [~vanzin]!;;;","29/Sep/16 21:40;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/15304;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Word2VecModel.findSynonyms can spuriously reject the best match when invoked with a vector,SPARK-17548,13005020,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,willbenton,willbenton,willbenton,14/Sep/16 22:32,17/Sep/16 11:51,14/Jul/23 06:29,17/Sep/16 11:50,1.4.1,1.5.2,1.6.2,2.0.0,,,,,2.0.1,2.1.0,,,MLlib,,,,,,,,0,,,,,,"The `findSynonyms` method in `Word2VecModel` currently rejects the best match a priori. When `findSynonyms` is invoked with a word, the best match is almost certain to be that word, but `findSynonyms` can also be invoked with a vector, which might not correspond to any of the words in the model's vocabulary.  In the latter case, rejecting the best match is spurious.",any,apachespark,willbenton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 17 11:50:26 UTC 2016,,,,,,,,,,"0|i33n53:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/16 22:34;apachespark;User 'willb' has created a pull request for this issue:
https://github.com/apache/spark/pull/15105;;;","17/Sep/16 11:50;srowen;Issue resolved by pull request 15105
[https://github.com/apache/spark/pull/15105];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Temporary shuffle data files may be leaked following exception in write,SPARK-17547,13005013,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,14/Sep/16 21:51,17/May/20 18:31,14/Jul/23 06:29,15/Sep/16 18:25,1.5.3,1.6.0,2.0.0,,,,,,1.6.3,2.0.1,2.1.0,,Shuffle,Spark Core,,,,,,,0,,,,,,"SPARK-8029 modified shuffle writers to first stage their data to a temporary file in the same directory as the final destination file and then to atomically rename the file at the end of the write job. However, this change introduced the potential for the temporary output file to be leaked if an exception occurs during the write because the shuffle writers' existing error cleanup code doesn't handle this new temp file.

This is easy to fix: we just need to add a {{finally}} block to ensure that the temporary file is guaranteed to be either moved or deleted before existing the shuffle write method.",,andyd88,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 15 18:25:00 UTC 2016,,,,,,,,,,"0|i33n3j:",9223372036854775807,,,,,,,,,,,,,1.6.3,2.0.1,2.1.0,,,,,,,,,"14/Sep/16 21:54;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15104;;;","15/Sep/16 18:25;joshrosen;Issue resolved by pull request 15104
[https://github.com/apache/spark/pull/15104];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
start-* scripts should use hostname -f,SPARK-17546,13005000,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,burtonator,burtonator,14/Sep/16 20:46,18/Sep/16 15:23,14/Jul/23 06:29,18/Sep/16 15:22,2.0.0,,,,,,,,2.0.1,2.1.0,,,,,,,,,,,0,,,,,,"The ./sbin/start-slaves.sh and ./sbin/start-master.sh scripts use just 'hostname' and if /etc/hostname isn't using a fqdn then you don't get the fully qualified domain name.

If you upgrade your script to hostname --fqdn the problem is solved. ",,apachespark,burtonator,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 18 15:22:55 UTC 2016,,,,,,,,,,"0|i33n0n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/16 07:54;srowen;{{hostname --fqdn}} doesn't work on OS X, so may be specific to some versions of Linux et al. However looks like {{hostname -f}} is a synonym on Linux, and works on OS X (though doesn't necessarily give more info). Go for it with {{-f}} I think.;;;","17/Sep/16 12:26;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15129;;;","18/Sep/16 15:22;srowen;Issue resolved by pull request 15129
[https://github.com/apache/spark/pull/15129];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL Catalyst doesn't handle ISO 8601 date without colon in offset,SPARK-17545,13004999,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,nbeyer,nbeyer,14/Sep/16 20:38,12/Dec/22 18:11,14/Jul/23 06:29,21/Sep/16 08:02,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"When parsing a CSV with a date/time column that contains a variant ISO 8601 that doesn't include a colon in the offset, casting to Timestamp fails.

Here's a simple, example CSV content.
{quote}
time
""2015-07-20T15:09:23.736-0500""
""2015-07-20T15:10:51.687-0500""
""2015-11-21T23:15:01.499-0600""
{quote}

Here's the stack trace that results from processing this data.
{quote}
16/09/14 15:22:59 ERROR Utils: Aborting task
java.lang.IllegalArgumentException: 2015-11-21T23:15:01.499-0600
	at org.apache.xerces.jaxp.datatype.XMLGregorianCalendarImpl$Parser.skip(Unknown Source)
	at org.apache.xerces.jaxp.datatype.XMLGregorianCalendarImpl$Parser.parse(Unknown Source)
	at org.apache.xerces.jaxp.datatype.XMLGregorianCalendarImpl.<init>(Unknown Source)
	at org.apache.xerces.jaxp.datatype.DatatypeFactoryImpl.newXMLGregorianCalendar(Unknown Source)
	at javax.xml.bind.DatatypeConverterImpl._parseDateTime(DatatypeConverterImpl.java:422)
	at javax.xml.bind.DatatypeConverterImpl.parseDateTime(DatatypeConverterImpl.java:417)
	at javax.xml.bind.DatatypeConverter.parseDateTime(DatatypeConverter.java:327)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.stringToTime(DateTimeUtils.scala:140)
	at org.apache.spark.sql.execution.datasources.csv.CSVTypeCast$.castTo(CSVInferSchema.scala:287)
{quote}

Somewhat related, I believe Python standard libraries can produce this form of zone offset. The system I got the data from is written in Python.
https://docs.python.org/2/library/datetime.html#strftime-strptime-behavior",,apachespark,nbeyer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 21 08:02:13 UTC 2016,,,,,,,,,,"0|i33n0f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/16 22:11;nbeyer;As a workaround, the following format can be set as an option for dataframe reads:
{code}spark.read.option(""dateFormat"", ""yyyy-MM-dd'T'HH:mm:ss.SSSXX"").csv(path){code};;;","16/Sep/16 05:19;gurwls223;Hi [~nbeyer], the basic ISO format currently follows https://www.w3.org/TR/NOTE-datetime

That says

{quote}
1997-07-16T19:20:30.45+01:00
{quote}

is the right ISO format where timezone is

{quote}
TZD  = time zone designator (Z or +hh:mm or -hh:mm)
{quote}

To make sure, I double-checked the ISO 8601 - 2004 full specification in http://www.uai.cl/images/sitio/biblioteca/citas/ISO_8601_2004en.pdf

That says,

{quote}
...
the expression shall either be completely in basic format, in which case the minimum number of
separators necessary for the required expression is used, or completely in extended format, in which case
additional separators shall be used
...
{quote}

where the basic format is {{20160707T211822+0300}} whereas the extended format is {{2016-07-07T21:18:22+03:00}}.

In addition, basic format seems even discouraged in text format

{quote}
NOTE : The basic format should be avoided in plain text.
{quote}

Therefore, {{2016-07-07T21:18:22+03:00}} Is the right ISO 8601:2004.
whereas {{2016-07-07T21:18:22+0300}} Is not because the zone designator may not be in the basic format when the date and time of day is in the extended format.


;;;","16/Sep/16 05:20;gurwls223;Therefore, IMHO, this is not an issue as we can workaround via `timestampFormat` and `dateFormat` options in current master branch.;;;","16/Sep/16 05:22;gurwls223;FYI - this is related with https://github.com/apache/spark/pull/14279 if you only meant 2.0 but not the master branch.;;;","16/Sep/16 14:52;nbeyer;I agree, the data is quirky and it's not how I'd personally serialize the data, but it is valid ISO 8601 format, regardless of it being discouraged. Also, the code already has precedent for dealing with ""quirks"".

{code}
    val indexOfGMT = s.indexOf(""GMT"")
    if (indexOfGMT != -1) {
      // ISO8601 with a weird time zone specifier (2000-01-01T00:00GMT+01:00)
      val s0 = s.substring(0, indexOfGMT)
      val s1 = s.substring(indexOfGMT + 3)
      // Mapped to 2000-01-01T00:00+01:00
      stringToTime(s0 + s1)
    }
{code}
https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L126

If the converters are going to handle some quirks, why wouldn't it handle this one? If no quirks are going to be handled, then I would suggest that the documentation be made explicit to define that it handles the W3C note's profile of ISO 8601.

FWIW - I'm getting this data via a CSV export from Splunk.

BTW - Thanks for the link to the PR, that's actually another issue that I was wondering about.;;;","16/Sep/16 15:00;srowen;I think it depends on how hard it is to handle the quirk and whether it causes other problems. It's probably not too hard to also handle this one and not that dangerous. If you want to open a PR we can look at it.;;;","16/Sep/16 15:30;gurwls223;Thank you both for your feedback. I am okay with fixing this and supporting some common quirky cases in general.

However, I'd like to note that we might have to avoid supporting other quirky cases being handled in {{DateTimeUtils.stringToTime}}.
More specifically, we should avoid using {{DatatypeConverter.parseDateTime(...)}} because an issue was identified in that - https://github.com/apache/spark/pull/14279#issuecomment-233887751

If we only allow the strict ISO 8601 format by default and other cases by {{timestampFormat}} and {{dateFormat}}, the problematic call above would not be called but if we allow other quirky cases in that, this will introduce potential problems from 2.0 too.

I left the usages only for backward compatibilities and would like to avoid adding a new logic in that personally.

To cut this short, I am okay with adding this case in that if we fix the issue above together or if this case is pretty much common.
Otherwise, I'd like to stay against this (although I am not supposed to decide what should be added into Spark) and rather promote the use of {{timestampFormat}} and {{dateFormat}}.

;;;","17/Sep/16 15:06;gurwls223;I took another look. This might be solved by changing the default time pattern from

{code}
yyyy-MM-dd'T'HH:mm:ss.SSSZZ
{code}

to

{code}
yyyy-MM-dd'T'HH:mm:ss.SSSXXX
{code}

in [CSVOptions.scala#L111|https://github.com/apache/spark/blob/29952ed096fd2a0a19079933ff691671d6f00835/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVOptions.scala#L111]

which (in case of {{SimpleDateFormat}}) will read {{+0800}}, {{+08}} and {{+08:00}} permissively but write {{+08:00}} which does not break current behaviour.

But, it seems {{FastDateFormat}} does not support this {{X}}/{{XX}}/{{XXX}} - https://issues.apache.org/jira/browse/LANG-1267

We might be able to do one of the followings

 - Use {{SimpleDateFormat}} with {{ThreadLocal}} and change the default pattern.
 - Wait for the release and upgrade common-lang to 3.6 after this is fixed for {{FastDateFormat}}
 - Use {{DateTimeFormatter}} after we drop Java 7 - https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html
;;;","18/Sep/16 19:09;nbeyer;Is it safe to assume the preference is to stick with the commons-lang FastDateFormat?;;;","18/Sep/16 19:43;nbeyer;In looking at the javadoc again and what commons-lang seem to have implemented in master/trunk, it doesn't look like use of 'XXX' would support all three types, rather it would just support the value with the colon. Did you try using SimpleDateFormat with three XXX to parse all three values?;;;","18/Sep/16 19:56;nbeyer;Here's what I would suggest (I can build a PR, but want to propose before doing that):

If the string value as a 'T' in it (already identified at this point: https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L139), then test the value against this regex {code}^.*[+-](\d{2,4})${code}. If it matches, then the value ends in the format without a colon or without minutes, so add a colon and two 00, if appropriate.

This would allow everything to work without upgrading commons-lang and would have no impact on the output.

Thoughts?;;;","18/Sep/16 21:26;gurwls223;I left my comment as I am confirmed that the fixed version is set to 3.6 in the JIRA.

Also, Indeed, this was my mistake from the combination of the code below:

{code}
val format = new SimpleDateFormat(""yyyy-MM-dd'T'HH:mm:ss.SSSX"")
format: java.text.SimpleDateFormat = java.text.SimpleDateFormat@8a9df5f9

scala> format.parse(""2015-01-01T12:30:10.000+08"")
res42: java.util.Date = Thu Jan 01 13:30:10 KST 2015

scala> format.parse(""2015-01-01T12:30:10.000+0800"")
res43: java.util.Date = Thu Jan 01 13:30:10 KST 2015

scala> format.parse(""2015-01-01T12:30:10.000+08:00"")
res44: java.util.Date = Thu Jan 01 13:30:10 KST 2015
{code}

and the documentation below:

{quote}
X	Time zone	ISO 8601 time zone	-08; -0800; -08:00
{quote};;;","18/Sep/16 21:35;gurwls223;I think maybe, we should just change this to {{SimpleDateFormat}} because at least an issue was identified. I assumed it is safe because {{FastDateFormat}} advertise this is thread-safe version of {{SImpleDateFormat}} and direct replacement is okay - https://commons.apache.org/proper/commons-lang/apidocs/org/apache/commons/lang3/time/FastDateFormat.html;;;","19/Sep/16 02:30;apachespark;User 'nbeyer' has created a pull request for this issue:
https://github.com/apache/spark/pull/15147;;;","19/Sep/16 02:32;nbeyer;This is a PR based on my suggestion above. It would be nicer to replace things with commons-lang 3.6 or Java 8 SimpleDateFormat, but those will require quite a bit of time and can always overlay this approach.;;;","21/Sep/16 00:37;gurwls223;[~srowen] and [~nbeyer]. Just a gentle reminder that this problem seems already fixed in master branch which the author of this JIRA/PR,  [~nbeyer], also agreed with at the end. I hope any of you takes an action for this JIRA. 

(BTW, [~nbeyer] has another general suggestion about other timestamp/date formats which I guess another JIRA would explain).
;;;","21/Sep/16 00:38;gurwls223;Fixing JIRA : SPARK-16216
Fixing PR : https://github.com/apache/spark/pull/14279;;;","21/Sep/16 07:24;srowen;You are saying you believe this also resolved by https://github.com/apache/spark/pull/14279 ?;;;","21/Sep/16 07:52;gurwls223;Yes. This is because we introduced {{FastDateFormat}} there with default pattern, {{yyyy-MM-dd'T'HH:mm:ss.SSSZZ}}.

{code}
scala> import org.apache.commons.lang3.time.FastDateFormat
import org.apache.commons.lang3.time.FastDateFormat

scala> val f = FastDateFormat.getInstance(""yyyy-MM-dd'T'HH:mm:ss.SSSZZ"")
f: org.apache.commons.lang3.time.FastDateFormat = FastDateFormat[yyyy-MM-dd'T'HH:mm:ss.SSSZZ,ko_KR,Asia/Seoul]

scala> f.parse(""2015-11-21T23:15:01.499-0600"")
res0: java.util.Date = Sun Nov 22 14:15:01 KST 2015

scala> f.parse(""2015-11-21T23:15:01.499-06:00"")
res1: java.util.Date = Sun Nov 22 14:15:01 KST 2015
{code}

It works also at end-to-end test - https://github.com/apache/spark/pull/15147#issuecomment-247903603.

In more details,

the actual conversion is happening in https://github.com/apache/spark/blob/1dbb725dbef30bf7633584ce8efdb573f2d92bca/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVInferSchema.scala#L265-L273

{code}
Try(options.timestampFormat.parse(datum).getTime * 1000L)
  .getOrElse {
    // If it fails to parse, then tries the way used in 2.0 and 1.x for backwards
    // compatibility.
    DateTimeUtils.stringToTime(datum).getTime * 1000L
  }
{code}

Before https://github.com/apache/spark/pull/14279, it was
https://github.com/apache/spark/blob/e1dc853737fc1739fbb5377ffe31fb2d89935b1f/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVInferSchema.scala#L287

{code}
DateTimeUtils.stringToTime(datum).getTime  * 1000L
{code}

It is true {{DateTimeUtils.stringToTime(...)}} does not handle {{+0800}} case but after https://github.com/apache/spark/pull/14279, we are trying {{FastDateFormat}} first which seems covering this case.;;;","21/Sep/16 08:02;srowen;Resolved by https://github.com/apache/spark/pull/14279;;;",,,,,,,,,,,,,,,,,,,,,,,
Missing log4j config file for tests in common/network-shuffle,SPARK-17543,13004949,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,as2,freiss,freiss,14/Sep/16 17:06,16/Sep/16 09:19,14/Jul/23 06:29,16/Sep/16 09:19,,,,,,,,,2.1.0,,,,,,,,,,,,0,starter,,,,,"*This is a small starter task to help new contributors practice the pull request and code review process.*

The Maven module {{common/network-shuffle}} does not have a log4j configuration file for its test cases. Usually these configuration files are located inside each module, in the directory {{src/test/resources}}. The missing configuration file leads to a scary-looking but harmless series of errors and stack traces in Spark build logs:
{noformat}
SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
log4j:ERROR Could not read configuration file from URL [file:src/test/resources/log4j.properties].
java.io.FileNotFoundException: src/test/resources/log4j.properties (No such file or directory)
        at java.io.FileInputStream.open(Native Method)
        at java.io.FileInputStream.<init>(FileInputStream.java:146)
        at java.io.FileInputStream.<init>(FileInputStream.java:101)
        at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
        at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:557)
        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
        at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
        at org.apache.log4j.Logger.getLogger(Logger.java:104)
        at io.netty.util.internal.logging.Log4JLoggerFactory.newInstance(Log4JLoggerFactory.java:29)
        at io.netty.util.internal.logging.InternalLoggerFactory.newDefaultFactory(InternalLoggerFactory.java:46)
        at io.netty.util.internal.logging.InternalLoggerFactory.<clinit>(InternalLoggerFactory.java:34)
...
{noformat}",,apachespark,as2,freiss,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 16 09:19:11 UTC 2016,,,,,,,,,,"0|i33mpb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/16 05:12;as2;Started working on this.;;;","15/Sep/16 06:09;apachespark;User 'jagadeesanas2' has created a pull request for this issue:
https://github.com/apache/spark/pull/15108;;;","16/Sep/16 09:19;srowen;Issue resolved by pull request 15108
[https://github.com/apache/spark/pull/15108];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix some DDL bugs about table management when same-name temp view exists,SPARK-17541,13004944,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,14/Sep/16 16:51,18/Sep/16 13:52,14/Jul/23 06:29,18/Sep/16 13:52,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 18 13:52:18 UTC 2016,,,,,,,,,,"0|i33mo7:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"14/Sep/16 16:58;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/15099;;;","18/Sep/16 13:52;cloud_fan;Issue resolved by pull request 15099
[https://github.com/apache/spark/pull/15099];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't initialize Hive Listeners for the Execution Client,SPARK-17531,13004701,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,13/Sep/16 19:52,13/Sep/16 23:16,14/Jul/23 06:29,13/Sep/16 22:12,2.0.0,,,,,,,,1.6.3,2.0.1,2.1.0,,SQL,,,,,,,,0,,,,,,"If a user provides listeners inside the Hive Conf, the configuration for these listeners are passed to the Hive Execution Client as well. This may cause issues for two reasons:
 1. The Execution Client will actually generate garbage
 2. The listener class needs to be both in the Spark Classpath and Hive Classpath

The configuration can be overwritten within 
{code}
HiveUtils.newTemporaryConfiguration
{code}",,apachespark,brkyvz,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 13 22:12:36 UTC 2016,,,,,,,,,,"0|i33l67:",9223372036854775807,,,,,,,,,,,,,2.0.1,,,,,,,,,,,"13/Sep/16 20:34;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/15086;;;","13/Sep/16 21:33;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/15087;;;","13/Sep/16 22:12;yhuai;Issue resolved by pull request 15086
[https://github.com/apache/spark/pull/15086];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
data should be copied properly before saving into InternalRow,SPARK-17528,13004669,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,13/Sep/16 17:42,22/Jul/17 12:24,14/Jul/23 06:29,01/Jul/17 01:25,,,,,,,,,2.3.0,,,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,hvanhovell,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 22 12:24:03 UTC 2017,,,,,,,,,,"0|i33kz3:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"13/Sep/16 17:55;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/15082;;;","22/Nov/16 17:31;hvanhovell;[~cloud_fan] I am retargeting this to 2.2? Is this still an issue BTW?;;;","23/Nov/16 15:48;cloud_fan;now it's kind of a refactor work, let's put it in 2.2;;;","30/Jun/17 07:24;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/18483;;;","01/Jul/17 01:25;cloud_fan;Issue resolved by pull request 18483
[https://github.com/apache/spark/pull/18483];;;","22/Jul/17 12:24;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/18712;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkContext.clearFiles() still present in the PySpark bindings though the underlying Scala method was removed in Spark 2.0,SPARK-17525,13004578,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,sjakthol,sjakthol,sjakthol,13/Sep/16 12:34,14/Sep/16 08:39,14/Jul/23 06:29,14/Sep/16 08:38,2.0.0,,,,,,,,2.0.1,2.1.0,,,PySpark,,,,,,,,0,,,,,,"STR: In PySpark shell, run sc.clearFiles()

What happens:
{noformat}
py4j.protocol.Py4JError: An error occurred while calling o74.clearFiles. Trace:
py4j.Py4JException: Method clearFiles([]) does not exist
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)
	at py4j.Gateway.invoke(Gateway.java:272)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:211)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

Apparently the old and deprecated SparkContext.clearFiles() was removed from Spark 2.0 but it's still present in the PySpark API. It should be removed from there too.",,apachespark,sjakthol,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 14 08:38:57 UTC 2016,,,,,,,,,,"0|i33kev:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/16 12:45;srowen;Oops, yeah this was missed in https://github.com/apache/spark/commit/8ce645d4eeda203cf5e100c4bdba2d71edd44e6a#diff-364713d7776956cb8b0a771e9b62f82d  I think you can open a PR for this for 2.0.1+;;;","13/Sep/16 17:12;apachespark;User 'sjakthol' has created a pull request for this issue:
https://github.com/apache/spark/pull/15081;;;","14/Sep/16 08:38;srowen;Issue resolved by pull request 15081
[https://github.com/apache/spark/pull/15081];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error when I use sparkContext.makeRDD(Seq()),SPARK-17521,13004537,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,codlife,codlife,codlife,13/Sep/16 09:17,15/Sep/16 08:39,14/Jul/23 06:29,15/Sep/16 08:38,2.0.0,,,,,,,,2.0.1,2.1.0,,,Spark Core,,,,,,,,0,easyfix,,,,,"when i use sc.makeRDD below
```
    val data3 = sc.makeRDD(Seq())
    println(data3.partitions.length)
```
I got an error:
Exception in thread ""main"" java.lang.IllegalArgumentException: Positive number of slices required
We can fix this bug just modify the last line ,do a check of seq.size
````
  def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope {
    assertNotStopped()
    val indexToPrefs = seq.zipWithIndex.map(t => (t._2, t._1._2)).toMap
    new ParallelCollectionRDD[T](this, seq.map(_._1), seq.size, indexToPrefs)
  }
```",,apachespark,codlife,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 15 08:38:35 UTC 2016,,,,,,,,,,"0|i33k5j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/16 09:31;srowen;Wow, I admit I didn't even know this overload existed:

{code}
  def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope {
    assertNotStopped()
    val indexToPrefs = seq.zipWithIndex.map(t => (t._2, t._1._2)).toMap
    new ParallelCollectionRDD[T](this, seq.map(_._1), seq.size, indexToPrefs)
  }
{code}

This is the only place the 'location prefs' is used. It looks like this is a very old API: https://github.com/apache/spark/commit/c36ca10241991d46f2f1513b2c0c5e369d8b34f9

Anyway, your fix is correct and easy, sure. But I also wonder if we should deprecated makeRDD. The other version just calls parallelize anyway.;;;","13/Sep/16 10:35;codlife;Hi @Sean Owen I can do this fix:just depreceted makeRDD and at the same time fix the bug. By the way，I  learn spark by looking the spark source code,and find this problem occasionally, do you have some tips about learning spark? Thank you very much
Best Wishes!
Wangjianfei;;;","13/Sep/16 10:42;srowen;Don't do the deprecation now, that was more of a side comment. But it's OK to fix this bug, even if it's a real corner case.;;;","13/Sep/16 11:25;apachespark;User 'codlife' has created a pull request for this issue:
https://github.com/apache/spark/pull/15077;;;","13/Sep/16 12:33;srowen;Reaching way back to page [~pwendell] -- do you have any view on whether it's probably right to deprecate makeRDD() at this point in favor of parallelize()?;;;","15/Sep/16 08:38;srowen;Issue resolved by pull request 15077
[https://github.com/apache/spark/pull/15077];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Block Users to Specify the Internal Data Source Provider Hive,SPARK-17518,13004509,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,13/Sep/16 06:48,18/Sep/16 07:40,14/Jul/23 06:29,18/Sep/16 07:38,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"In Spark 2.1, we introduced a new internal provider `hive` for telling Hive serde tables from data source tables. This PR is to block users to specify this in `DataFrameWriter` and SQL APIs.  
",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 18 07:38:51 UTC 2016,,,,,,,,,,"0|i33jzb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/16 06:49;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15073;;;","18/Sep/16 07:38;cloud_fan;Issue resolved by pull request 15073
[https://github.com/apache/spark/pull/15073];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CollectLimit.execute() should perform per-partition limits,SPARK-17515,13004478,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,13/Sep/16 02:08,13/Sep/16 11:03,14/Jul/23 06:29,13/Sep/16 11:03,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"{{CollectLimit.execute()}} incorrectly omits per-partition limits, leading to performance regressions in case this case is hit (which should not happen in normal operation, but can occur in some pathological corner-cases",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 13 02:13:06 UTC 2016,,,,,,,,,,"0|i33jsf:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"13/Sep/16 02:13;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15070;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
df.take(1) and df.limit(1).collect() perform differently in Python,SPARK-17514,13004460,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,13/Sep/16 00:36,14/Sep/16 17:10,14/Jul/23 06:29,14/Sep/16 17:10,,,,,,,,,2.0.1,2.1.0,,,PySpark,SQL,,,,,,,0,,,,,,"In PySpark, {{df.take(1)}} ends up running a single-stage job which computes only one partition of {{df}}, while {{df.limit(1).collect()}} ends up computing all partitions of {{df}} and runs a two-stage job. This difference in performance is confusing, so I think that we should generalize the fix from SPARK-10731 so that {{Dataset.collect()}} can be implemented efficiently in Python.",,apachespark,davies,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 14 17:10:21 UTC 2016,,,,,,,,,,"0|i33jof:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/16 00:56;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15068;;;","14/Sep/16 17:10;davies;Issue resolved by pull request 15068
[https://github.com/apache/spark/pull/15068];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Specifying remote files for Python based Spark jobs in Yarn cluster mode not working,SPARK-17512,13004454,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,uditme,uditme,13/Sep/16 00:08,17/May/20 18:14,14/Jul/23 06:29,03/Oct/16 12:54,2.0.0,,,,,,,,2.0.1,2.1.0,,,PySpark,Spark Core,Spark Submit,YARN,,,,,0,,,,,,"When I run a python application, and specify a remote path for the extra files to be included in the PYTHON_PATH using the '--py-files' or 'spark.submit.pyFiles' configuration option in YARN Cluster mode I get the following error:

Exception in thread ""main"" java.lang.IllegalArgumentException: Launching Python applications through spark-submit is currently only supported for local files: s3://xxxx/app.py
at org.apache.spark.deploy.PythonRunner$.formatPath(PythonRunner.scala:104)
at org.apache.spark.deploy.PythonRunner$$anonfun$formatPaths$3.apply(PythonRunner.scala:136)
at org.apache.spark.deploy.PythonRunner$$anonfun$formatPaths$3.apply(PythonRunner.scala:136)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
at org.apache.spark.deploy.PythonRunner$.formatPaths(PythonRunner.scala:136)
at org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$10.apply(SparkSubmit.scala:636)
at org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$10.apply(SparkSubmit.scala:634)
at scala.Option.foreach(Option.scala:257)
at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:634)
at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:158)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) 

Here are sample commands which would throw this error in Spark 2.0 (sparkApp.py requires app.py):

spark-submit --deploy-mode cluster --py-files s3://xxxx/app.py s3://xxxx/sparkApp.py (works fine in 1.6)

spark-submit --deploy-mode cluster --conf spark.submit.pyFiles=s3://xxxx/app.py s3://xxxx/sparkApp1.py (not working in 1.6)

This would work fine if app.py is downloaded locally and specified.

This was working correctly using ‘—py-files’ option in earlier version of Spark, but not using the ‘spark.submit.pyFiles’ configuration option. But now, it does not work through either of the ways.

The following diff shows the comment which states that it should work with ‘non-local’ paths for the YARN cluster mode, and we are specifically doing separate validation to fail if YARN client mode is used with remote paths:

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L309

And then this code gets triggered at the end of each run, irrespective of whether we are using Client or Cluster mode, and internally validates that the paths should be non-local:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L634

This above validation was not getting triggered in earlier version of Spark using ‘—py-files’ option because we were not storing the arguments passed to ‘—py-files’ in the ‘spark.submit.pyFiles’ configuration for YARN. However, the following code was newly added in 2.0 which now stores it and hence this validation gets triggered even if we specify files through ‘—py-files’ option:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L545

Also, we changed the logic in YARN client, to read values directly from ‘spark.submit.pyFiles’ configuration instead of from ‘—py-files’ (earlier):

https://github.com/apache/spark/commit/8ba2b7f28fee39c4839e5ea125bd25f5091a3a1e#diff-b050df3f55b82065803d6e83453b9706R543

So now its broken whether we use ‘—py-files’ or ‘spark.submit.pyFiles’ as the validation gets triggered in both cases irrespective of whether we use Client or Cluster mode with YARN.
",,apachespark,haridsv,jerryshao,jonathak,uditme,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17566,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 03 12:54:42 UTC 2016,,,,,,,,,,"0|i33jn3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/16 03:47;jerryshao;This is due to some behavior changes during submitting spark applications on yarn with client or cluster deloy mode. In 2.0 we convert most of the arguments to use system properties, while in 1.6 for ""--py-files"" we still use command arguments for yarn cluster mode, and for {{PythonRunner}} it only checks system property {{spark.submit.pyFiles}}, so that's why it works under 1.6. It is really a issue should be fixed, let me handle it.;;;","18/Sep/16 07:39;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/15137;;;","03/Oct/16 12:54;srowen;Resolved by https://github.com/apache/spark/pull/15137;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamic allocation race condition: Containers getting marked failed while releasing,SPARK-17511,13004447,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kishorvpatil,kishorvpatil,kishorvpatil,12/Sep/16 22:47,17/May/20 18:13,14/Jul/23 06:29,14/Sep/16 19:35,2.0.0,2.0.1,2.1.0,,,,,,2.0.1,2.1.0,,,Spark Core,YARN,,,,,,,0,,,,,,"While trying to reach launch multiple containers in pool, if running executors count reaches or goes beyond the target running executors, the container is released and marked failed. This can cause many jobs to be marked failed causing overall job failure.

I will have a patch up soon after completing testing.

{panel:title=Typical Exception found in Driver marking the container to Failed}
{code}
java.lang.AssertionError: assertion failed
        at scala.Predef$.assert(Predef.scala:156)
        at org.apache.spark.deploy.yarn.YarnAllocator$$anonfun$runAllocatedContainers$1.org$apache$spark$deploy$yarn$YarnAllocator$$anonfun$$updateInternalState$1(YarnAllocator.scala:489)
        at org.apache.spark.deploy.yarn.YarnAllocator$$anonfun$runAllocatedContainers$1$$anon$1.run(YarnAllocator.scala:519)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{code}
{panel}

",,apachespark,kishorvpatil,roczei,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 13 01:46:08 UTC 2016,,,,,,,,,,"0|i33jlj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/16 01:46;apachespark;User 'kishorvpatil' has created a pull request for this issue:
https://github.com/apache/spark/pull/15069;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory leak in Memory store when unable to cache the whole RDD in memory,SPARK-17503,13004231,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,clockfly,clockfly,clockfly,12/Sep/16 06:53,12/Sep/16 18:42,14/Jul/23 06:29,12/Sep/16 18:42,2.1.0,,,,,,,,2.0.1,2.1.0,,,Spark Core,,,,,,,,0,,,,,,"h2.Problem description:

The following query triggers out of memory error.  

{code}
sc.parallelize(1 to 1000000000, 100).map(x => new Array[Long](1000)).cache().count()
{code}

This is not expected, we should fallback to use disk instead if there is not enough memory for cache.

Stacktrace:
{code}
scala> sc.parallelize(1 to 1000000000, 100).map(x => new Array[Long](1000)).cache().count()
[Stage 0:>                                                          (0 + 5) / 5]16/09/11 17:27:20 WARN MemoryStore: Not enough space to cache rdd_1_4 in memory! (computed 631.5 MB so far)
16/09/11 17:27:20 WARN MemoryStore: Not enough space to cache rdd_1_0 in memory! (computed 631.5 MB so far)
16/09/11 17:27:20 WARN BlockManager: Putting block rdd_1_0 failed
16/09/11 17:27:20 WARN BlockManager: Putting block rdd_1_4 failed
16/09/11 17:27:21 WARN MemoryStore: Not enough space to cache rdd_1_1 in memory! (computed 947.3 MB so far)
16/09/11 17:27:21 WARN BlockManager: Putting block rdd_1_1 failed
16/09/11 17:27:22 WARN MemoryStore: Not enough space to cache rdd_1_3 in memory! (computed 1423.7 MB so far)
16/09/11 17:27:22 WARN BlockManager: Putting block rdd_1_3 failed

java.lang.OutOfMemoryError: Java heap space
Dumping heap to java_pid26528.hprof ...
Heap dump file created [6551021666 bytes in 9.876 secs]
16/09/11 17:28:15 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)
16/09/11 17:28:15 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(driver,[Lscala.Tuple2;@46c9ce96,BlockManagerId(driver, 127.0.0.1, 55360))] in 1 attempts
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:523)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:552)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:552)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:552)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1857)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:552)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:190)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:81)
	... 14 more
16/09/11 17:28:15 ERROR Executor: Exception in task 3.0 in stage 0.0 (TID 3)
java.lang.OutOfMemoryError: Java heap space
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:24)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:23)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$JoinIterator.next(Iterator.scala:232)
	at org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:683)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1684)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1915)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1915)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/09/11 17:28:15 ERROR Executor: Exception in task 4.0 in stage 0.0 (TID 4)
java.lang.OutOfMemoryError: Java heap space
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:24)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:23)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$JoinIterator.next(Iterator.scala:232)
	at org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:683)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1684)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1915)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1915)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/09/11 17:28:15 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-3,5,main]
java.lang.OutOfMemoryError: Java heap space
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:24)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:23)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$JoinIterator.next(Iterator.scala:232)
	at org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:683)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1684)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1915)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1915)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/09/11 17:28:15 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-4,5,main]
java.lang.OutOfMemoryError: Java heap space
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:24)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:23)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$JoinIterator.next(Iterator.scala:232)
	at org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:683)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1684)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1915)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1915)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/09/11 17:28:15 WARN TaskSetManager: Lost task 4.0 in stage 0.0 (TID 4, localhost): java.lang.OutOfMemoryError: Java heap space
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:24)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:23)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$JoinIterator.next(Iterator.scala:232)
	at org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:683)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1684)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1915)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1915)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

h2.Analysis:

When the RDD is too big to cache, Spark returns a PartiallyUnrolledIterator.
{code}
   // line 287, in file MemoryStore.scala 

    } else {
      // We ran out of space while unrolling the values for this block
      logUnrollFailureMessage(blockId, vector.estimateSize())
      Left(new PartiallyUnrolledIterator(
        this, unrollMemoryUsedByThisBlock, unrolled = vector.iterator, rest = values))
    }
{code}

Parameter 'unrolled' points to a vector array buffer, which stores all input values we have read so far when trying to cache the RDD. Parameter 'rest' is a iterator over all unread input values.

For example, if the input RDD partition has 100GB bytes, and Spark executor has a 10GB cache, then parameter 'unrolled' will points to a array of 10GB bytes, the parameter 'rest' iterator points to unread 90GB input data.

We expect the 10GB 'unrolled' memory to be garbage collected immediately after all values in 'unrolled' have been consumed by PartiallyUnrolledIterator. But current Spark code will not collect the 10GB 'unrolled' until all 100GB input data has been processed.  ",,andyd88,apachespark,clockfly,joshrosen,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/16 08:36;clockfly;Screen Shot 2016-09-12 at 4.16.15 PM.png;https://issues.apache.org/jira/secure/attachment/12827994/Screen+Shot+2016-09-12+at+4.16.15+PM.png","12/Sep/16 08:36;clockfly;Screen Shot 2016-09-12 at 4.34.19 PM.png;https://issues.apache.org/jira/secure/attachment/12827995/Screen+Shot+2016-09-12+at+4.34.19+PM.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 12 18:42:30 UTC 2016,,,,,,,,,,"0|i33i9r:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"12/Sep/16 07:14;srowen;cache() means ""cache in memory"" only. There is a persist() call for other levels, like storing on disk only. I am not sure what you mean then?;;;","12/Sep/16 07:40;apachespark;User 'clockfly' has created a pull request for this issue:
https://github.com/apache/spark/pull/15056;;;","12/Sep/16 07:42;clockfly;[~sowen] I have modified the title to mean ""cache in memory"";;;","12/Sep/16 18:42;joshrosen;Fixed for 2.0.1 / 2.1.0 by Sean's PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple Bugs in DDL Statements on Temporary Views ,SPARK-17502,13004226,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,12/Sep/16 06:23,24/Sep/16 22:03,14/Jul/23 06:29,20/Sep/16 16:26,2.0.1,2.1.0,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"- When the permanent tables/views do not exist but the temporary view exists, the expected error should be `NoSuchTableException` for partition-related ALTER TABLE commands. However, it always reports a confusing error message. For example, 
{noformat}
Partition spec is invalid. The spec (a, b) must match the partition spec () defined in table '`testview`';
{noformat}
- When the permanent tables/views do not exist but the temporary view exists, the expected error should be `NoSuchTableException` for `ALTER TABLE ... UNSET TBLPROPERTIES`. However, it reports missing table property. However, the expected error should be `NoSuchTableException`. For example, 
{noformat}
Attempted to unset non-existent property 'p' in table '`testView`';
{noformat}
- When `ANALYZE TABLE` is called on a view or a temporary view, we should issue an error message. However, it reports a strange error:
{noformat}
ANALYZE TABLE is not supported for Project
{noformat}
",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 21 04:28:03 UTC 2016,,,,,,,,,,"0|i33i8n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/16 06:25;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15054;;;","21/Sep/16 04:28;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15174;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Floor/ceil of decimal returns wrong result if it's in compact format,SPARK-17494,13004088,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,gcivan,gcivan,10/Sep/16 18:12,22/Sep/16 04:02,14/Jul/23 06:29,22/Sep/16 04:02,1.6.1,2.0.0,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,correctness,,,,,"If you create tables as follows:

create table a as select 'A' as str, cast(10.5 as decimal(15,6)) as num;
create table b as select 'A' as str;

Then

select floor(num) from a;

returns 10
but

select floor(num) from a join b on a.str = b.str;

returns 11",,apachespark,gcivan,joshrosen,tsuresh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 19 21:54:07 UTC 2016,,,,,,,,,,"0|i33hdz:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"12/Sep/16 18:52;joshrosen;This also seems to affect Spark 2.0, except there it always returns {{11}} when selecting a floor of {{num}}, while taking the floor of a casted literal seems to work as expected.

Specifically,

{code}
select floor(cast(10.5 as decimal(15,6))), num, floor(num) from a
{code}

returns {{10, 10.5, 11}}.;;;","19/Sep/16 21:54;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/15154;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading Cataloged Data Sources without Extending SchemaRelationProvider,SPARK-17492,13004083,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,10/Sep/16 16:38,22/Sep/16 05:20,14/Jul/23 06:29,22/Sep/16 05:19,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"For data sources without extending `SchemaRelationProvider`, we expect users to not specify schemas when they creating tables. If the schema is input from users, an exception is issued. 

Since Spark 2.1, for any data source, to avoid infer the schema every time, we store the schema in the metastore catalog. Thus, when reading a cataloged data source table, the schema could be read from metastore catalog. In this case, we also got an exception. For example, 

{noformat}
sql(
  s""""""
     |CREATE TABLE relationProvierWithSchema
     |USING org.apache.spark.sql.sources.SimpleScanSource
     |OPTIONS (
     |  From '1',
     |  To '10'
     |)
   """""".stripMargin)
spark.table(tableName).show()
{noformat}
{noformat}
org.apache.spark.sql.sources.SimpleScanSource does not allow user-specified schemas.;
{noformat}
",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 22 05:19:49 UTC 2016,,,,,,,,,,"0|i33hcv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/16 16:39;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15046;;;","22/Sep/16 05:19;cloud_fan;Issue resolved by pull request 15046
[https://github.com/apache/spark/pull/15046];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MemoryStore.putIteratorAsBytes() may silently lose values when KryoSerializer is used,SPARK-17491,13004049,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,10/Sep/16 06:47,17/May/20 18:21,14/Jul/23 06:29,17/Sep/16 18:47,2.0.0,,,,,,,,2.0.1,2.1.0,,,Block Manager,Spark Core,,,,,,,0,correctness,,,,,"MemoryStore.putIteratorAsBytes() may silently lose values when used with KryoSerializer because it does not properly close the serialization stream before attempting to deserialize the already-serialized values, which may cause values buffered in Kryo's internal buffers to not be read.

This is the root cause behind a user-reported ""wrong answer"" bug in PySpark caching reported by Ben Leslie on the Spark user mailing list in a thread titled ""pyspark persist MEMORY_ONLY vs MEMORY_AND_DISK"")",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 17 18:47:02 UTC 2016,,,,,,,,,,"0|i33h5b:",9223372036854775807,,,,,,,,,,,,,2.0.1,,,,,,,,,,,"10/Sep/16 06:56;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15043;;;","17/Sep/16 18:47;joshrosen;Issue resolved by pull request 15043
[https://github.com/apache/spark/pull/15043];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python UDF does not work between Sort and Limit,SPARK-17474,13003921,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,davies,davies,09/Sep/16 17:21,12/Sep/16 23:36,14/Jul/23 06:29,12/Sep/16 23:36,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"Because of this bug, Python UDF will not work with ORDER BY and LIMIT.",,apachespark,davies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 12 23:36:02 UTC 2016,,,,,,,,,,"0|i33gcv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/16 19:07;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/15030;;;","12/Sep/16 23:36;davies;Issue resolved by pull request 15030
[https://github.com/apache/spark/pull/15030];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inappropriate memory management in `org.apache.spark.storage.MemoryStore` may lead to memory leak,SPARK-17465,13003767,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,saturday_s,saturday_s,saturday_s,09/Sep/16 06:35,15/Mar/17 01:41,14/Jul/23 06:29,14/Sep/16 20:47,1.6.0,1.6.1,1.6.2,,,,,,1.6.3,2.0.1,2.1.0,,Spark Core,,,,,,,,1,,,,,,"After updating Spark from 1.5.0 to 1.6.0, I found that it seems to have a memory leak on my Spark streaming application.

Here is the head of the heap histogram of my application, which has been running about 160 hours:
{code:borderStyle=solid}
 num     #instances         #bytes  class name
----------------------------------------------
   1:         28094       71753976  [B
   2:       1188086       28514064  java.lang.Long
   3:       1183844       28412256  scala.collection.mutable.DefaultEntry
   4:        102242       13098768  <methodKlass>
   5:        102242       12421000  <constMethodKlass>
   6:          8184        9199032  <constantPoolKlass>
   7:            38        8391584  [Lscala.collection.mutable.HashEntry;
   8:          8184        7514288  <instanceKlassKlass>
   9:          6651        4874080  <constantPoolCacheKlass>
  10:         37197        3438040  [C
  11:          6423        2445640  <methodDataKlass>
  12:          8773        1044808  java.lang.Class
  13:         36869         884856  java.lang.String
  14:         15715         848368  [[I
  15:         13690         782808  [S
  16:         18903         604896  java.util.concurrent.ConcurrentHashMap$HashEntry
  17:            13         426192  [Lscala.concurrent.forkjoin.ForkJoinTask;
{code}
It shows that *scala.collection.mutable.DefaultEntry* and *java.lang.Long* have unexpected big numbers of instances. In fact, the numbers started growing at streaming process began, and keep growing proportional to total number of tasks.



After some further investigation, I found that the problem is caused by some inappropriate memory management in _releaseUnrollMemoryForThisTask_ and _unrollSafely_ method of class [org.apache.spark.storage.MemoryStore|https://github.com/apache/spark/blob/branch-1.6/core/src/main/scala/org/apache/spark/storage/MemoryStore.scala].

In Spark 1.6.x, a _releaseUnrollMemoryForThisTask_ operation will be processed only with the parameter _memoryToRelease_ > 0:
https://github.com/apache/spark/blob/branch-1.6/core/src/main/scala/org/apache/spark/storage/MemoryStore.scala#L530-L537
But in fact, if a task successfully unrolled all its blocks in memory by _unrollSafely_ method, the memory saved in _unrollMemoryMap_ would be set to zero:
https://github.com/apache/spark/blob/branch-1.6/core/src/main/scala/org/apache/spark/storage/MemoryStore.scala#L322

So the result is, the memory saved in _unrollMemoryMap_ will be released, but the key of that part of memory will never be removed from the hash map. The hash table will keep increasing, while new tasks keep incoming. Although the speed of increase is comparatively slow (about dozens of bytes per task), it is possible that result into OOM after weeks or months.",,agateaaa,andyd88,apachespark,huasanyelao,joshrosen,peterccp,saturday_s,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 15 01:41:05 UTC 2017,,,,,,,,,,"0|i33fen:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/16 08:36;apachespark;User 'saturday-shi' has created a pull request for this issue:
https://github.com/apache/spark/pull/15022;;;","13/Sep/16 08:43;saturday_s;It seems that the issue has some negative side effects on processing time.

When I ran a streaming application using _updateStateByKey_ on Spark 1.6.0, it showed a strange gradual increasing of processing time: the processing time of a 2s-interval-batch would increase from < 0.1s to 2 ~ 3s after 10 ~ 20 days. (similar to [this issue|https://issues.apache.org/jira/browse/SPARK-13288])

But after I fixed the issue described in this JIRA, the problem seems disappeared. The processing time stably keeping at < 0.1s, and shows no tendency to increase.

I have no idea of why it happens, but the issue - described in this JIRA - seems to have an actual effect on processing time of applications that using _updateStateByKey_ (or checkpoint).
;;;","14/Sep/16 20:47;joshrosen;Issue resolved by pull request 15022
[https://github.com/apache/spark/pull/15022];;;","03/Oct/16 09:16;saturday_s;Resolved.

In every task, method _currentUnrollMemory_ will be called several times. It will scan all keys of _unrollMemoryMap_ and _pendingUnrollMemoryMap_, so the processing time is proportional to the map size.
https://github.com/apache/spark/blob/v1.6.0/core/src/main/scala/org/apache/spark/storage/MemoryStore.scala#L540-L542

I have checked the processing time of _currentUnrollMemory_. It just equals to the time increased from before.

Hope this will help someone who has a similar issue of increasing processing time when upgrade Spark to 1.6.0 :);;;","15/Mar/17 01:41;agateaaa;[~saturday_s] Thanks for this fix! It really helped us. We were using 1.6.1 and were seeing processing times increase gradually over period of several days. With 1.6.3 this increase does not happen. Thank you!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR spark.als arguments reg should be 0.1 by default,SPARK-17464,13003746,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yanboliang,yanboliang,yanboliang,09/Sep/16 04:07,09/Sep/16 12:48,14/Jul/23 06:29,09/Sep/16 12:48,,,,,,,,,2.1.0,,,,ML,SparkR,,,,,,,0,,,,,,"SparkR spark.als arguments {{reg}} should be 0.1 by default, which need to be consistent with ML.",,apachespark,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 09 04:10:09 UTC 2016,,,,,,,,,,"0|i33f9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/16 04:10;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/15021;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Serialization of accumulators in heartbeats is not thread-safe,SPARK-17463,13003731,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,zsxwing,joshrosen,joshrosen,09/Sep/16 01:15,17/May/17 23:23,14/Jul/23 06:29,06/Oct/16 20:46,2.0.0,,,,,,,,2.0.1,2.1.0,,,Spark Core,,,,,,,,0,,,,,,"Check out the following {{ConcurrentModificationException}}:

{code}

16/09/06 16:10:29 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@66e7b6e7,BlockManagerId(2, HOST, 57743))] in 1 attempts
org.apache.spark.SparkException: Exception thrown in awaitResult
    at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77)
    at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:75)
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
    at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
    at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
    at scala.PartialFunction$OrElse.apply(PartialFunction.scala:162)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
    at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
    at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:518)
    at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547)
    at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547)
    at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1862)
    at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.ConcurrentModificationException
    at java.util.ArrayList.writeObject(ArrayList.java:766)
    at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
    at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
    at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
    at org.apache.spark.rpc.netty.NettyRpcEnv.serialize(NettyRpcEnv.scala:253)
    at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:227)
    at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:508)
    at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
    ... 13 more
{code}

Even though accumulators aren't thread-safe they can be concurrently read while serializing executor heartbeats and modified while tasks are running, leading to ConcurrentModificationException errors (thereby leading to missing heartbeats) or leading to inconsistent data (since individual fields of a multi-field object might be serialized at different points in time, leading to inconsistencies in accumulators like LongAccum).

This seems like a pretty serious issue but I'm not sure what's the best way to fix this. An obvious fix would be to properly synchronize all accesses to the fields of our accumulators and to synchronize the writeObject and writeKryo methods, but this may have an adverse performance impact",,allengeorge,apachespark,bryanc,emlyn,eseyfe,harishk15,joshrosen,sunil.rangwani,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 17 23:23:55 UTC 2017,,,,,,,,,,"0|i33f6n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/16 01:16;joshrosen;[~zsxwing], FYI, since you're good at these types of RPC thread-safety issues.;;;","12/Sep/16 17:33;zsxwing;[~joshrosen] I think we can just leave LongAccum as it is. It's no worse than Spark 1.6. In Spark 1.6, `sum` and `count` are different accumulators and have the same inconsistent issue.

We definitely should fix CollectionAccumulator and SetAccumulator. I will submit a PR to add the necessary `synchronized` for them.

By the way, I didn't notice that AccumulatorV2 sends the whole object back to the driver. Do you know any special reason? I remember previously we only send the values of accumulators back to driver.;;;","12/Sep/16 17:49;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/15063;;;","12/Sep/16 21:59;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/15065;;;","14/Sep/16 20:34;joshrosen;Issue resolved by pull request 15063
[https://github.com/apache/spark/pull/15063];;;","06/Oct/16 00:23;eseyfe;Hi [~zsxwing],

I think this bug is not fully fixed. I still get ConcurrentModificationException. 

JsonProtocol.scala:314 has below statement which is causing the problem:
          JArray(v.asInstanceOf[java.util.List[(BlockId, BlockStatus)]].asScala.toList.map {

Adding synchronized keyword should fix the issue. What do you think?

This is the stack trace:
java.util.ConcurrentModificationException
        at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901)
        at java.util.ArrayList$Itr.next(ArrayList.java:851)
        at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:43)
        at scala.collection.Iterator$class.foreach(Iterator.scala:893)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
        at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:183)
        at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:45)
        at scala.collection.TraversableLike$class.to(TraversableLike.scala:590)
        at scala.collection.AbstractTraversable.to(Traversable.scala:104)
        at scala.collection.TraversableOnce$class.toList(TraversableOnce.scala:294)
        at scala.collection.AbstractTraversable.toList(Traversable.scala:104)
        at org.apache.spark.util.JsonProtocol$.accumValueToJson(JsonProtocol.scala:314)
        at org.apache.spark.util.JsonProtocol$$anonfun$accumulableInfoToJson$5.apply(JsonProtocol.scala:291)
        at org.apache.spark.util.JsonProtocol$$anonfun$accumulableInfoToJson$5.apply(JsonProtocol.scala:291)
        at scala.Option.map(Option.scala:146)
        at org.apache.spark.util.JsonProtocol$.accumulableInfoToJson(JsonProtocol.scala:291)
        at org.apache.spark.util.JsonProtocol$$anonfun$taskInfoToJson$12.apply(JsonProtocol.scala:283)
        at org.apache.spark.util.JsonProtocol$$anonfun$taskInfoToJson$12.apply(JsonProtocol.scala:283)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)
        at scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:45)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
        at scala.collection.AbstractTraversable.map(Traversable.scala:104)
        at org.apache.spark.util.JsonProtocol$.taskInfoToJson(JsonProtocol.scala:283)
        at org.apache.spark.util.JsonProtocol$.taskEndToJson(JsonProtocol.scala:145)
        at org.apache.spark.util.JsonProtocol$.sparkEventToJson(JsonProtocol.scala:76)
        at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:137)
        at org.apache.spark.scheduler.EventLoggingListener.onTaskEnd(EventLoggingListener.scala:157)
        at org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent(SparkListenerBus.scala:45)
        at org.apache.spark.scheduler.LiveListenerBus.doPostEvent(LiveListenerBus.scala:35)
        at org.apache.spark.scheduler.LiveListenerBus.doPostEvent(LiveListenerBus.scala:35)
        at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:63)
        at org.apache.spark.scheduler.LiveListenerBus.postToAll(LiveListenerBus.scala:35)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(LiveListenerBus.scala:81)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:66)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:66)
        at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:65)
        at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1244)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:64);;;","06/Oct/16 01:39;eseyfe;Reopening since it still  throw java.util.ConcurrentModificationException;;;","06/Oct/16 01:41;apachespark;User 'seyfe' has created a pull request for this issue:
https://github.com/apache/spark/pull/15371;;;","06/Oct/16 20:45;zsxwing;[~eseyfe] It's a different issue. Could you create a new ticket, please?;;;","06/Oct/16 20:46;zsxwing;It's already fixed. The issue in the comment is another one.;;;","11/Oct/16 14:02;harishk15;It looks like a show stopper for my current project. Can you please let me know the 2.1.0 release date or do we have same issue in 1.6.0/1/2 ? So that i can revert back to 1.6.;;;","11/Oct/16 18:01;srowen;What do you mean? this has been released already in 2.0.1.;;;","11/Oct/16 18:37;harishk15;Is this fix is part of the https://github.com/apache/spark/pull/15371 pull request?. I have 2.0.1 in my cluster but i am getting both the errors.

16/10/11 00:53:42 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@43f45f95,BlockManagerId(2, HOST, 43256))] in 1 attempts
org.apache.spark.SparkException: Exception thrown in awaitResult
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:75)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:518)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1877)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.ConcurrentModificationException
	at java.util.ArrayList.writeObject(ArrayList.java:766)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:441)
	at java.util.Collections$SynchronizedCollection.writeObject(Collections.java:2081)
	at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.rpc.netty.NettyRpcEnv.serialize(NettyRpcEnv.scala:253)
	at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:227)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:508)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	... 13 more;;;","11/Oct/16 19:09;srowen;No, that change came after, and is part of a different JIRA that addresses another part of the same problem. It is not in 2.0.1;;;","11/Oct/16 19:10;srowen;No, that change came after, and is part of a different JIRA that addresses another part of the same problem. It is not in 2.0.1;;;","11/Oct/16 19:48;zsxwing;Do you have a reproducer? I saw `at java.util.Collections$SynchronizedCollection.writeObject(Collections.java:2081)` in the stack trace, so I think the internal ArrayList is accessed in some place. Did you use `collectionAccumulator` in your codes?

FYI,  https://github.com/apache/spark/pull/15371 is for SPARK-17816 which fixes an issue in driver.;;;","11/Oct/16 19:56;harishk15;Ok. thanks for the update. Do we have any work around for the second part of the issue? I tried this set(""spark.rpc.netty.dispatcher.numThreads"",""2"") but no luck
;;;","11/Oct/16 20:03;harishk15;No i dont have any code like that. I use pyspark .. Please find my code snippet
df1 with 60 columns (70M records)
df2  with 3000-7000 (varies) columns (10M)
join df1 and df2 with key columns (please note df1 is more granular data and df2 one level above. So data set will grow

df3 = df1.join(df2, [keys])
aggList = [func.mean(col).alias(col + '_m') for col in df2.columns]

Last part is i do -- df4 = df3.groupBy(keys).agg(*aggList) --I applying mean to each column of the df3 data frame which might be 3000-10000 columns.
Let me know if you need entire stack trace of this issue.

PS: We still have issue https://issues.apache.org/jira/browse/SPARK-16845 -- So i have to break number of columns 500 chunks

     ;;;","11/Oct/16 20:33;harishk15;My second approach was:
def testfunc(keys, vals, columnsToStandardize):
           df= pd.DataFrame(vals, columns = keys)
           df[columnsToStandardize] = df[columnsToStandardize] - df[columnsToStandardize].mean() 

df3.rdd.map(keys).groupByKey().flatMap(lambda keyval: testfunc(keys[0], keys[1], columnsToStandardize));;;","19/Oct/16 20:05;zsxwing;I could not figure out the cause. Is it easy to reproduce in your query? Hope you can create a reproducer for us to debug.;;;","08/Nov/16 04:26;harishk15;I was able to figure out the issue, its  not related to this bug. ;;;","30/Dec/16 14:52;sunil.rangwani;Hi 
I get the same error with Spark 2.0.2 on EMR 5.2.0
I am using the CollectionAccumulator and I get the same stack trace. Hard to say from the stack trace what is the root cause. 

{noformat}
16/12/30 13:49:11 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(1,[Lscala.Tuple2;@63f6e6d6,BlockManagerId(1, <Hostname snipped>, 45163))] in 3 attempts
org.apache.spark.SparkException: Exception thrown in awaitResult
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:75)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:518)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.ConcurrentModificationException
	at java.util.ArrayList.writeObject(ArrayList.java:766)
	at sun.reflect.GeneratedMethodAccessor55.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:441)
	at java.util.Collections$SynchronizedCollection.writeObject(Collections.java:2081)
	at sun.reflect.GeneratedMethodAccessor54.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.rpc.netty.NettyRpcEnv.serialize(NettyRpcEnv.scala:253)
	at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:227)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:508)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	... 13 more
{noformat};;;","30/Dec/16 18:48;zsxwing;[~sunil.rangwani] Could you provide the codes that use CollectionAccumulator?;;;","01/Jan/17 18:20;sunil.rangwani;Hi [~zsxwing]

Below is how I am using the Collection accumulator. 

{code}
// val spark: SparkSession
// val dataFrame: DataFrame
val updatedRecordKeysAcc = spark.sparkContext.collectionAccumulator[String]

dataFrame.toJSON.foreach(processRecord(_, updatedRecordKeysAcc))

import collection.JavaConverters._
val updatedRecordKeys: String = updatedRecordKeysAcc.value.asScala.mkString(""\n"")
// write `updatedRecordKeys` to an S3 file using aws-sdk... 

def processRecord(recordJSON: String, updatedRecordKeysAcc: CollectionAccumulator) {
	// do Stuff with recordJSON... 
	// val recordKey: String = getKeyFromJSON(recordJSON)
	updatedRecordKeysAcc.add(recordKey)
}

{code}

It works when I am working with smaller datasets of 10s of 1000s but when I try to run it with a dataset of 10s of millions, it fails with the exception above. I tried to scale up the cluster to 5 to 6 nodes but it still gives the same error and the cluster remains underutilized.  
It also works alright when I comment out the line {noformat}updatedRecordKeysAcc.add(recordKey){noformat};;;","05/Jan/17 10:11;sunil.rangwani;[~zsxwing] Do you agree this bug should be reopened?;;;","09/Jan/17 23:31;zsxwing;[~sunil.rangwani] could you have a simple reproducer? I ran the following codes on both 2.0.2 and the latest master and could not reproduce it.
{code}
import org.apache.spark.util.CollectionAccumulator

val updatedRecordKeysAcc = spark.sparkContext.collectionAccumulator[String]

def processRecord(recordJSON: String, updatedRecordKeysAcc: CollectionAccumulator[String]) {
	updatedRecordKeysAcc.add((recordJSON.toInt percent 100000).toString)
}

for (_ <- 0 until 100) {
  spark.range(1, 10000000).map(_.toString).foreach(processRecord(_, updatedRecordKeysAcc))
}
{code}
This may be an EMR Spark only issue.;;;","12/Jan/17 14:25;sunil.rangwani;Hi [~zsxwing] 
My recordKey that I add to updatedRecordKeysAcc is a string that is 102 characters long. Other than that its similar to you are doing in your code. I currently don't have a way to reproduce this on a non-EMR environment. Are you able to try this out please with a longer recordKey to see if you're able to reproduce it. ;;;","17/May/17 18:31;allengeorge;[~zsxwing] So, we're hitting this problem on spark 2.0.2. One thing I noticed while poking around {{AccumulatorsV2}} is that not all accesses are locked. For example [setValue|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala#L473]
 and [add|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala#L461]. Would that be the cause at all? ;;;","17/May/17 21:24;zsxwing;[~allengeorge] it's protected by `Collections.synchronizedList(new ArrayList[T]())`.;;;","17/May/17 23:23;allengeorge;[~zsxwing] You're absolutely right: my apologies for the PEBCAK.;;;",,,,,,,,,,,,,
"Dataset.joinWith broadcasts gigabyte sized table, causes OOM Exception",SPARK-17460,13003724,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,huaxing,tradersancho,tradersancho,09/Sep/16 00:03,10/Dec/16 14:44,14/Jul/23 06:29,10/Dec/16 14:43,2.0.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Dataset.joinWith is performing a BroadcastJoin on a table that is gigabytes in size due to the dataset.logicalPlan.statistics.sizeInBytes < 0.

The issue is that org.apache.spark.sql.types.ArrayType.defaultSize is of datatype Int.  In my dataset, there is an Array column whose data size exceeds the limits of an Int and so the data size becomes negative.

The issue can be repeated by running this code in REPL:
val ds = (0 to 10000).map( i => (i, Seq((i, Seq((i, ""This is really not that long of a string"")))))).toDS()

// You might have to remove private[sql] from Dataset.logicalPlan to get this to work
val stats = ds.logicalPlan.statistics

yields

stats: org.apache.spark.sql.catalyst.plans.logical.Statistics = Statistics(-1890686892,false)

This causes joinWith to performWith to perform a broadcast join even tho my data is gigabytes in size, which of course causes the executors to run out of memory.

Setting spark.sql.autoBroadcastJoinThreshold=-1 does not help because the logicalPlan.statistics.sizeInBytes is a large negative number and thus it is less than the join threshold of -1.

I've been able to work around this issue by setting autoBroadcastJoinThreshold to a very large negative number.",Spark 2.0 in local mode as well as on GoogleDataproc,apachespark,cloud_fan,tradersancho,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 10 14:43:19 UTC 2016,,,,,,,,,,"0|i33f53:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/16 18:35;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/16175;;;","10/Dec/16 14:43;cloud_fan;Issue resolved by pull request 16175
[https://github.com/apache/spark/pull/16175];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Alias specified for aggregates in a pivot are not honored,SPARK-17458,13003711,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,a1ray,ravisomepalli,ravisomepalli,08/Sep/16 22:38,15/Jan/17 07:40,14/Jul/23 06:29,15/Sep/16 19:49,2.0.0,,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,,,,,,"When using pivot and multiple aggregations we need to alias to avoid special characters, but alias does not help because 

df.groupBy(""C"").pivot(""A"").agg(avg(""D"").as(""COLD""), max(""B"").as(""COLB"")).show

||    C || bar_avg(`D`) AS `COLD` || bar_max(`B`) AS `COLB` || foo_avg(`D`) AS `COLD` || foo_max(`B`) AS `COLB` ||
|small|                   5.5|                   two|    2.3333333333333335|   two|
|large|                   5.5|                   two|                   2.0|                   one|

Expected Output
||    C || bar_COLD || bar_COLB || foo_COLD || foo_COLB ||
|small|                   5.5|                   two|    2.3333333333333335|   two|
|large|                   5.5|                   two|                   2.0|                   one|


One approach you can fix this issue is to change the class
sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala
 and change the outputName method in 
{code}
object ResolvePivot extends Rule[LogicalPlan] {
    def apply(plan: LogicalPlan): LogicalPlan = plan transform {
{code}


{code}
def outputName(value: Literal, aggregate: Expression): String = {
          val suffix = aggregate match {
             case n: NamedExpression => aggregate.asInstanceOf[NamedExpression].name
             case _ => aggregate.sql
           }
          if (singleAgg) value.toString else value + ""_"" + suffix
        }
{code}

Version : 2.0.0
{code}
def outputName(value: Literal, aggregate: Expression): String = {
          if (singleAgg) value.toString else value + ""_"" + aggregate.sql
        }
{code}",,a1ray,apachespark,hvanhovell,ravisomepalli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18393,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 15 02:15:03 UTC 2017,,,,,,,,,,"0|i33f27:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/16 16:09;apachespark;User 'aray' has created a pull request for this issue:
https://github.com/apache/spark/pull/15111;;;","15/Sep/16 19:49;hvanhovell;I cannot find the JIRA username of the assignee (Andrew Ray). I'd would be great if someone can provide this.
;;;","15/Sep/16 21:40;a1ray;[~hvanhovell]: My JIRA username is a1ray.;;;","15/Jan/17 02:15;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/16565;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IsotonicRegression takes non-polynomial time for some inputs,SPARK-17455,13003666,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nseggert,nseggert,nseggert,08/Sep/16 19:07,23/Jan/17 21:22,14/Jul/23 06:29,23/Jan/17 21:21,1.3.1,1.4.1,1.5.2,1.6.2,2.0.0,,,,2.2.0,,,,MLlib,,,,,,,,0,,,,,,"The Pool Adjacent Violators Algorithm (PAVA) implementation that's currently in MLlib can take O(N!) time for certain inputs, when it should have worst-case complexity of O(N^2).

To reproduce this, I pulled the private method poolAdjacentViolators out of mllib.regression.IsotonicRegression and into a benchmarking harness.

Given this input
{code}
val x = (1 to length).toArray.map(_.toDouble)
val y = x.reverse.zipWithIndex.map{ case (yi, i) => if (i % 2 == 1) yi - 1.5 else yi}
val w = Array.fill(length)(1d)

val input: Array[(Double, Double, Double)] = (y zip x zip w) map{ case ((y, x), w) => (y, x, w)}
{code}

I vary the length of the input to get these timings:

|| Input Length || Time (us) ||
| 100 | 1.35 |
| 200 | 3.14 | 
| 400 | 116.10 |
| 800 | 2134225.90 |

(tests were performed using https://github.com/sirthias/scala-benchmarking-template)

I can also confirm that I run into this issue on a real dataset I'm working on when trying to calibrate random forest probability output. Some partitions take > 12 hours to run. This isn't a skew issue, since the largest partitions finish in minutes. I can only assume that some partitions cause something approaching this worst-case complexity.

I'm working on a patch that borrows the implementation that is used in scikit-learn and the R ""iso"" package, both of which handle this particular input in linear time and are quadratic in the worst case.",,apachespark,dougb,josephkb,nseggert,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 23 21:21:30 UTC 2017,,,,,,,,,,"0|i33es7:",9223372036854775807,,,,,josephkb,,,,,,,,2.2.0,,,,,,,,,,,"08/Sep/16 22:02;apachespark;User 'neggert' has created a pull request for this issue:
https://github.com/apache/spark/pull/15018;;;","05/Jan/17 23:05;josephkb;I'm changing the target version since the final patch is pretty involved.  Let's not backport it.;;;","05/Jan/17 23:07;nseggert;Fine by me;;;","23/Jan/17 21:21;josephkb;Issue resolved by pull request 15018
[https://github.com/apache/spark/pull/15018];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
no total size for data source tables in InMemoryCatalog,SPARK-17446,13003478,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,ZenWzh,ZenWzh,08/Sep/16 07:30,08/Dec/16 22:58,14/Jul/23 06:29,08/Nov/16 08:51,2.0.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"For data source table in InMemoryCatalog, it's catalogTable.storage.locationUri is None, so total size can't be calculated. But we can use the path parameter in catalogTable.storage.properties to calculate size.",,apachespark,ZenWzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 08 08:48:13 UTC 2016,,,,,,,,,,"0|i33dmn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/16 07:37;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/15012;;;","08/Sep/16 16:48;srowen;[~ZenWzh] there is no detail at all here. Please describe what you mean in the description. See https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark;;;","08/Sep/16 21:01;ZenWzh;Ok, I've added the description.
Thanks.
;;;","08/Nov/16 08:48;ZenWzh;This issue is resolved after [SPARK-17470|https://issues.apache.org/jira/browse/SPARK-17470] is resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Additional arguments in write.df are not passed to data source,SPARK-17442,13003400,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,felixcheung,falaki,falaki,07/Sep/16 23:03,08/Sep/16 15:23,14/Jul/23 06:29,08/Sep/16 15:23,2.0.0,,,,,,,,2.0.1,2.1.0,,,SparkR,,,,,,,,0,,,,,,"{{write.df}} passes everything in its arguments to underlying data source in 1.x, but it is not passing header = ""true"" in Spark 2.0. For example the following code snippet produces a header line in older versions of Spark but not in 2.0.

{code}
df <- createDataFrame(iris)
write.df(df, source = ""com.databricks.spark.csv"", path = ""/tmp/iris"", header = ""true"")
{code}",,apachespark,falaki,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 08 15:23:20 UTC 2016,,,,,,,,,,"0|i33d5b:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"07/Sep/16 23:52;shivaram;I think we missed passing the options through when we converted to the new writer API in https://github.com/apache/spark/commit/cc4d5229c98a589da76a4d5e5fdc5ea92385183b

The fix is probably just to add a line `write <- callJMethod(write, ""options"", options)` -- Feel free to send a PR if you get a chance or I'll send one later tonight

cc [~felixcheung];;;","08/Sep/16 00:16;apachespark;User 'falaki' has created a pull request for this issue:
https://github.com/apache/spark/pull/15007;;;","08/Sep/16 04:22;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/15010;;;","08/Sep/16 15:23;shivaram;Issue resolved by pull request 15010
[https://github.com/apache/spark/pull/15010];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issue Exceptions when ALTER TABLE RENAME PARTITION tries to alter a data source table,SPARK-17441,13003395,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,07/Sep/16 22:36,15/Sep/16 06:45,14/Jul/23 06:29,15/Sep/16 06:45,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"`ALTER TABLE RENAME PARTITION` is unable to handle data source tables, just like the other `ALTER PARTITION` commands. We should issue an exception instead. 
",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 15 06:45:14 UTC 2016,,,,,,,,,,"0|i33d47:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/16 22:38;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15004;;;","15/Sep/16 06:45;cloud_fan;Issue resolved by pull request 15004
[https://github.com/apache/spark/pull/15004];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issue Exception when ALTER TABLE commands try to alter a VIEW,SPARK-17440,13003393,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,07/Sep/16 22:35,15/Sep/16 06:44,14/Jul/23 06:29,15/Sep/16 06:44,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"For the following `ALTER TABLE` DDL, we should issue an exception when the target table is a `VIEW`:
{code}
 ALTER TABLE viewName SET LOCATION '/path/to/your/lovely/heart'

 ALTER TABLE viewName SET SERDE 'whatever'

 ALTER TABLE viewName SET SERDEPROPERTIES ('x' = 'y')

 ALTER TABLE viewName PARTITION (a=1, b=2) SET SERDEPROPERTIES ('x' = 'y')

 ALTER TABLE viewName ADD IF NOT EXISTS PARTITION (a='4', b='8')

 ALTER TABLE viewName DROP IF EXISTS PARTITION (a='2')

 ALTER TABLE viewName RECOVER PARTITIONS

 ALTER TABLE viewName PARTITION (a='1', b='q') RENAME TO PARTITION (a='100', b='p')
{code}",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 15 06:44:34 UTC 2016,,,,,,,,,,"0|i33d3r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/16 22:38;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15004;;;","15/Sep/16 06:44;cloud_fan;Issue resolved by pull request 15004
[https://github.com/apache/spark/pull/15004];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QuantilesSummaries returns the wrong result after compression,SPARK-17439,13003376,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,thunterdb,thunterdb,thunterdb,07/Sep/16 21:03,11/Sep/16 08:58,14/Jul/23 06:29,11/Sep/16 08:58,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,,0,correctness,,,,,"[~clockfly] found the following corner case that returns the wrong quantile (off by 1):

{code}
test(""test QuantileSummaries compression"") {
    var left = new QuantileSummaries(10000, 0.0001)
    System.out.println(""LEFT      RIGHT"")
    System.out.println(""===================="")
    (0 to 10).foreach { index =>
      left = left.insert(index)
      left = left.compress()

      var right = new QuantileSummaries(10000, 0.0001)
      (0 to index).foreach(right.insert(_))
      right = right.compress()
      System.out.println(s""${left.query(0.5)}   ${right.query(0.5)}"")
    }
  }
{code}

The result is:
{code}
LEFT      RIGHT
====================
0.0   0.0
0.0   1.0
0.0   1.0
0.0   1.0
1.0   2.0
1.0   2.0
2.0   3.0
2.0   3.0
3.0   4.0
3.0   4.0
4.0   5.0
{code}


The value of the ""LEFT"" column represents the output when using QuantileSummaries in Window function, the value on the ""RIGHT"" column represents the expected result. The different between ""LEFT"" and ""RIGHT"" column is that the ""LEFT"" column does intermediate compression on the storage of QuantileSummaries.
",,apachespark,thunterdb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 11 08:58:24 UTC 2016,,,,,,,,,,"0|i33czz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/16 21:04;thunterdb;I have a patch for that. It should be merged after SPARK-17306;;;","07/Sep/16 21:15;apachespark;User 'thunterdb' has created a pull request for this issue:
https://github.com/apache/spark/pull/15002;;;","11/Sep/16 08:58;srowen;Resolved by https://github.com/apache/spark/pull/15002;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Master UI should show the correct core limit when `ApplicationInfo.executorLimit` is set,SPARK-17438,13003373,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,07/Sep/16 20:44,19/Sep/16 17:56,14/Jul/23 06:29,19/Sep/16 17:56,,,,,,,,,2.0.1,2.1.0,,,Web UI,,,,,,,,0,,,,,,"The core info of an application in Master UI doesn't consider `ApplicationInfo.executorLimit`. It's pretty confusing that UI says ""Unlimited"" when `executorLimit` is set.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 07 20:45:09 UTC 2016,,,,,,,,,,"0|i33czb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/16 20:45;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/15001;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YarnShuffleService doesn't handle moving credentials levelDb,SPARK-17433,13003238,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,07/Sep/16 13:42,17/May/20 18:14,14/Jul/23 06:29,09/Sep/16 18:44,2.1.0,,,,,,,,2.1.0,,,,Spark Core,YARN,,,,,,,0,,,,,,"In SPARK-16711, I added a leveldb to store credentials to fix an issue with NM restart.  I missed that getRecoveryPath also handles moving the DB from the old local dirs to the yarn recoverypath. This routine is hardcoded for the registeredexecutors db and needs to be updated to handle the new credentials db I added.",,apachespark,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 07 18:57:08 UTC 2016,,,,,,,,,,"0|i33c5b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/16 13:49;tgraves;will have patch up shortly.;;;","07/Sep/16 18:57;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/14999;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PreprocessDDL should respect case sensitivity when checking duplicated columns,SPARK-17432,13003200,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,07/Sep/16 11:41,08/Sep/16 12:48,14/Jul/23 06:29,08/Sep/16 11:42,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,codlife,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 08 12:48:05 UTC 2016,,,,,,,,,,"0|i33bwv:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"07/Sep/16 11:55;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/14994;;;","08/Sep/16 11:42;cloud_fan;Issue resolved by pull request 14994
[https://github.com/apache/spark/pull/14994];;;","08/Sep/16 12:48;codlife;hello ,please check issues the https://issues.apache.org/jira/browse/SPARK-17447?filter=-2
thinks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark sql length(1) return error,SPARK-17429,13003171,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cenyuhai,cenyuhai,cenyuhai,07/Sep/16 09:46,15/Sep/16 18:45,14/Jul/23 06:29,15/Sep/16 18:45,1.6.2,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"select length(11);
select length(2.0);
these sql will return errors, but hive is ok.


Error in query: cannot resolve 'length(11)' due to data type mismatch: argument 1 requires (string or binary) type, however, '11' is of int type.; line 1 pos 14

Error in query: cannot resolve 'length(2.0)' due to data type mismatch: argument 1 requires (string or binary) type, however, '2.0' is of double type.; line 1 pos 14",,apachespark,cenyuhai,dongjoon,hvanhovell,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 08 17:44:11 UTC 2016,,,,,,,,,,"0|i33bqf:",9223372036854775807,,,,,liancheng,,,,,,,,,,,,,,,,,,,"07/Sep/16 11:16;hvanhovell;TBH this seems like something which is very debatable. Why would anyone want this?

You can fix this by adding {{ImplicitCastInputTypes}} to the {{Length}} expression.;;;","07/Sep/16 18:32;dongjoon;Ya. I agree with [~hvanhovell]. It seems to be debatable. PostgreSQL gives error like Spark while MySQL work like Hive.
{code}
ERROR:  function length(integer) does not exist
LINE 1: select length(1)
{code}
[~cenyuhai], could you make a PR for this. At least, I think this should be discussed and make a conclusion in any way.;;;","08/Sep/16 05:13;cenyuhai;ok, I wil make a PR later;;;","08/Sep/16 17:44;apachespark;User 'cenyuhai' has created a pull request for this issue:
https://github.com/apache/spark/pull/15014;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
function SIZE should return -1 when parameter is null,SPARK-17427,13003116,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,adrian-wang,adrian-wang,adrian-wang,07/Sep/16 05:05,23/Jun/18 17:44,14/Jul/23 06:29,07/Sep/16 11:02,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,api,releasenotes,,,,"`select size(null)` returns -1 in Hive. In order to be compatible, we need to return -1 also.",,adrian-wang,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 07 05:07:06 UTC 2016,,,,,,,,,,"0|i33be7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/16 05:07;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/14991;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Current TreeNode.toJSON may trigger OOM under some corner cases,SPARK-17426,13003111,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,clockfly,clockfly,clockfly,07/Sep/16 04:44,16/Sep/16 11:40,14/Jul/23 06:29,16/Sep/16 11:39,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"In SPARK-17356, we fix the OOM issue when Metadata is super big. There are other cases that may also trigger OOM. Current implementation of TreeNode.toJSON will recursively search and print all fields of current TreeNode, even if the field's type is of type Seq or type Map. 

This is not safe because:
1. the Seq or Map can be very big. Converting them to JSON make take huge memory, which may trigger out of memory error.
2. Some user space input may also be propagated to the Plan. The user space input can be of arbitrary type, and may also be self-referencing. Trying to print user space input to JSON is very risky.

The following example triggers a StackOverflowError when calling toJSON on a plan with user defined UDF.
{code}

case class SelfReferenceUDF(
    var config: Map[String, Any] = Map.empty[String, Any]) extends Function1[String, Boolean] {
  config += ""self"" -> this
  def apply(key: String): Boolean = config.contains(key)
}

test(""toJSON should not throws java.lang.StackOverflowError"") {
  val udf = ScalaUDF(SelfReferenceUDF(), BooleanType, Seq(""col1"".attr))
  // triggers java.lang.StackOverflowError
  udf.toJSON
}

{code}",,apachespark,clockfly,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 16 11:39:34 UTC 2016,,,,,,,,,,"0|i33bd3:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"07/Sep/16 05:02;apachespark;User 'clockfly' has created a pull request for this issue:
https://github.com/apache/spark/pull/14990;;;","16/Sep/16 11:39;cloud_fan;Issue resolved by pull request 14990
[https://github.com/apache/spark/pull/14990];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Override sameResult in HiveTableScanExec to make ReuseExchange work in text format table,SPARK-17425,13003092,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,waterman,waterman,waterman,07/Sep/16 01:44,22/Sep/16 05:06,14/Jul/23 06:29,22/Sep/16 05:05,2.0.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"When I run the below SQL(table src is text format):
{code:sql}
SELECT * FROM src t1
JOIN src t2 ON t1.key = t2.key
JOIN src t3 ON t1.key = t3.key;
{code}
The PhysicalPlan doesn't contain *ReuseExchange*. And I use src_pqt(parquet format) instead of src(text format), PhysicalPlan contain *ReuseExchange* in PhysicalPlan.
I found the *sameResult* in *FileSourceScanExec* has already overrided, but *HiveTableScanExec* didn't.",,apachespark,cloud_fan,waterman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 22 05:05:53 UTC 2016,,,,,,,,,,"0|i33b8v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/16 02:04;apachespark;User 'watermen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14988;;;","22/Sep/16 05:05;cloud_fan;Issue resolved by pull request 14988
[https://github.com/apache/spark/pull/14988];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dataset job fails from unsound substitution in ScalaReflect,SPARK-17424,13003076,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rdblue,rdblue,rdblue,06/Sep/16 23:10,12/May/17 12:41,14/Jul/23 06:29,12/May/17 12:40,1.6.1,2.0.0,,,,,,,2.0.3,2.1.2,2.2.0,,Spark Core,,,,,,,,0,,,,,,"I have a job that uses datasets in 1.6.1 and is failing with this error:

{code}
16/09/02 17:02:56 ERROR Driver ApplicationMaster: User class threw exception: java.lang.AssertionError: assertion failed: Unsound substitution from List(type T, type U) to List()
java.lang.AssertionError: assertion failed: Unsound substitution from List(type T, type U) to List()
    at scala.reflect.internal.Types$SubstMap.<init>(Types.scala:4644)
    at scala.reflect.internal.Types$SubstTypeMap.<init>(Types.scala:4761)
    at scala.reflect.internal.Types$Type.subst(Types.scala:796)
    at scala.reflect.internal.Types$TypeApiImpl.substituteTypes(Types.scala:321)
    at scala.reflect.internal.Types$TypeApiImpl.substituteTypes(Types.scala:298)
    at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$getConstructorParameters$1.apply(ScalaReflection.scala:769)
    at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$getConstructorParameters$1.apply(ScalaReflection.scala:768)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
    at scala.collection.immutable.List.foreach(List.scala:318)
    at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
    at scala.collection.AbstractTraversable.map(Traversable.scala:105)
    at org.apache.spark.sql.catalyst.ScalaReflection$class.getConstructorParameters(ScalaReflection.scala:768)
    at org.apache.spark.sql.catalyst.ScalaReflection$.getConstructorParameters(ScalaReflection.scala:30)
    at org.apache.spark.sql.catalyst.ScalaReflection$.getConstructorParameters(ScalaReflection.scala:610)
    at org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$argNames$lzycompute(TreeNode.scala:418)
    at org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$argNames(TreeNode.scala:418)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$argsMap$1.apply(TreeNode.scala:415)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$argsMap$1.apply(TreeNode.scala:414)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at scala.collection.Iterator$class.foreach(Iterator.scala:727)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
    at scala.collection.TraversableOnce$class.toMap(TraversableOnce.scala:279)
    at scala.collection.AbstractIterator.toMap(Iterator.scala:1157)
    at org.apache.spark.sql.catalyst.trees.TreeNode.argsMap(TreeNode.scala:416)
    at org.apache.spark.sql.execution.SparkPlanInfo$.fromSparkPlan(SparkPlanInfo.scala:46)
    at org.apache.spark.sql.execution.SparkPlanInfo$$anonfun$2.apply(SparkPlanInfo.scala:44)
    at org.apache.spark.sql.execution.SparkPlanInfo$$anonfun$2.apply(SparkPlanInfo.scala:44)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
    at scala.collection.immutable.List.foreach(List.scala:318)
    at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
    at scala.collection.AbstractTraversable.map(Traversable.scala:105)
    at org.apache.spark.sql.execution.SparkPlanInfo$.fromSparkPlan(SparkPlanInfo.scala:44)
    at org.apache.spark.sql.execution.SparkPlanInfo$$anonfun$2.apply(SparkPlanInfo.scala:44)
    at org.apache.spark.sql.execution.SparkPlanInfo$$anonfun$2.apply(SparkPlanInfo.scala:44)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
    at scala.collection.immutable.List.foreach(List.scala:318)
    at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
    at scala.collection.AbstractTraversable.map(Traversable.scala:105)
    at org.apache.spark.sql.execution.SparkPlanInfo$.fromSparkPlan(SparkPlanInfo.scala:44)
    at org.apache.spark.sql.execution.SparkPlanInfo$$anonfun$2.apply(SparkPlanInfo.scala:44)
    at org.apache.spark.sql.execution.SparkPlanInfo$$anonfun$2.apply(SparkPlanInfo.scala:44)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
    at scala.collection.immutable.List.foreach(List.scala:318)
    at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
    at scala.collection.AbstractTraversable.map(Traversable.scala:105)
    at org.apache.spark.sql.execution.SparkPlanInfo$.fromSparkPlan(SparkPlanInfo.scala:44)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:51)
    at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:56)
    at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55)
    at org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:193)
    at org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:166)
    at com.netflix.jobs.main(Processing.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:557)
{code}

I think this is the same bug as SPARK-13067. It looks like that issue wasn't fixed, there was just a work-around added to get the test passing.

The problem is that the reflection code is trying to substitute concrete types for type parameters of {{MapPartitions[T, U]}}, but the concrete types aren't known. So Spark ends up calling {{substituteTypes}} to substitute {{T}} and {{U}} with {{Nil}} (which gets shown as {{List()}}).

An easy fix that works for me is this:

{code:lang=scala}
    // if there are type variables to fill in, do the substitution (SomeClass[T] -> SomeClass[Int])
    if (actualTypeArgs.nonEmpty) {
      params.map { p =>
        p.name.toString -> p.typeSignature.substituteTypes(formalTypeArgs, actualTypeArgs)
      }
    } else {
      params.map { p =>
        p.name.toString -> p.typeSignature
      }
    }
{code}

Does this sound like a reasonable solution?

Edit: I think this affects 2.0.0 because the call to [{{substituteTypes}} is unchanged|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala#L788-L790]",,apachespark,cloud_fan,rdblue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-13067,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 12 12:40:44 UTC 2017,,,,,,,,,,"0|i33b5b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/16 17:31;apachespark;User 'rdblue' has created a pull request for this issue:
https://github.com/apache/spark/pull/15062;;;","12/Sep/16 17:32;rdblue;I'm adding the above fix in a PR. This fix works for us (the job succeeds) and doesn't change the behavior of cases where the concrete type arguments are known.;;;","12/May/17 12:40;cloud_fan;Issue resolved by pull request 15062
[https://github.com/apache/spark/pull/15062];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark release must NOT distribute Kinesis related assembly artifact,SPARK-17418,13003023,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,lresende,lresende,06/Sep/16 19:46,21/Sep/16 18:43,14/Jul/23 06:29,21/Sep/16 18:43,1.6.2,2.0.0,,,,,,,1.6.3,2.0.1,2.1.0,,Build,DStreams,,,,,,,0,,,,,,"The Kinesis streaming connector is based on the Amazon Software License, and based on the Apache Legal resolved issues (http://www.apache.org/legal/resolved.html#category-x) it's not allowed to be distributed by Apache projects.

More details is available in LEGAL-198",,apachespark,fielding,joshrosen,lresende,rmetzger,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17422,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 21 18:43:35 UTC 2016,,,,,,,,,,"0|i33atj:",9223372036854775807,,,,,,,,,,,,,2.0.1,,,,,,,,,,,"06/Sep/16 19:48;lresende;I am going to create a PR for this, basically removing from the release publish process. Will have to see what are the side effects on the overall build as there is embedded support for it over python and samples (which seems to convey that this is not really an optional package) ;;;","06/Sep/16 19:54;apachespark;User 'lresende' has created a pull request for this issue:
https://github.com/apache/spark/pull/14981;;;","06/Sep/16 20:05;srowen;I don't see any Kinesis artifacts in the Spark distribution. Where do you see them? I grepped the whole project's source and all the .jar files contents. ;;;","06/Sep/16 20:24;srowen;Aha. The problem is this: https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kinesis-asl-assembly_2.10 It's not part of the source/binary distribution but an optional module published to Maven, and the assembly here actually does include the Kinesis binary code.;;;","07/Sep/16 12:42;stevel@apache.org;Which dependency is being discussed here? The full AWS Java SDK is ASF licensed: https://github.com/aws/aws-sdk-java/blob/master/LICENSE.txt;;;","07/Sep/16 13:01;srowen;Yeah this is the Kinesis producer/client library, which is not Apache licensed. https://github.com/awslabs/amazon-kinesis-client/blob/master/LICENSE.txt;;;","20/Sep/16 20:23;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15167;;;","20/Sep/16 21:47;fielding;""Kinesis related"" is not a useful term. The Amazon license says

bq. The terms “reproduce,” “reproduction,” “derivative works,” and “distribution” have the meaning as provided under U.S. copyright law; provided, however, that for the purposes of this License, derivative works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work.

What that means is that works that merely call the interfaces of the Amazon work are not a derivative work and not (in any way) subject to the limitations of the Amazon license.  In other words, we can distribute our own code under the Apache License even if it has a strong dependency on Amazon licensed code, provided that the two are separable and distributed separately.;;;","20/Sep/16 22:01;lresende;Thanks [~fielding]. 

It seems we are ok to publish the code that depend on the Amazon licensed library and [~joshrosen] PullRequest takes care of the other aspect which is removing the assembly jar (only) publishing.;;;","21/Sep/16 18:43;joshrosen;Fixed by my PR for master, branch-2.0, and branch-1.6.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix sorting of part files while reconstructing RDD/partition from checkpointed files.,SPARK-17417,13003017,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Dhruve Ashar,Dhruve Ashar,Dhruve Ashar,06/Sep/16 19:19,10/Oct/16 15:57,14/Jul/23 06:29,10/Oct/16 15:57,1.6.0,2.0.0,,,,,,,2.0.2,2.1.0,,,Spark Core,,,,,,,,0,,,,,,"Spark currently assumes # of partitions to be less than 100000 and uses %05d padding. 

If we exceed this no., the sort logic in ReliableCheckpointRDD gets messed up and fails. This is because of part-files are sorted and compared as strings. 

This leads filename order to be part-10000, part-100000, ... instead of part-10000, part-10001, ..., part-100000 and while reconstructing the checkpointed RDD the job fails. 

Possible solutions: 
- Bump the padding to allow more partitions or
- Sort the part files extracting a sub-portion as string and then verify the RDD",,apachespark,Dhruve Ashar,qqsun8819,tgraves,umesh9794@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 06 01:40:05 UTC 2016,,,,,,,,,,"0|i33as7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/16 20:48;srowen;I'd bump the padding to allow 10 digits, because that would accommodate a 32-bit int, and having that many partitions would cause other things to fail. As long as the parsing code can read the 'old format' too, should work fine.;;;","06/Sep/16 21:26;Dhruve Ashar;Thanks for the suggestion. I'll work on the changes and submit a PR.;;;","04/Oct/16 21:59;Dhruve Ashar;[~srowen] AFAIU the checkpointing mechanism in spark core, the recovery of an RDD from a checkpoint is limited to an application attempt. Spark streaming mentions that it can recover metadata/rdd from checkpointed data across application attempts. Please correct me if I have missed something here. With this understanding it wouldn't be necessary to parse the code for the old format as the recovery would be done using the same spark jar which was used to launch it. 

Also why is it that we are not cleaning up the checkpointed directory on sc.close ?;;;","05/Oct/16 06:40;srowen;I think you're right, but it should also be no extra work to read a 5-digit number as well as 10-digit anyway.
I'm not sure about the cleanup logic though that would be a different question. The dir should probably be left as-is, but perhaps not its contents.;;;","06/Oct/16 01:40;apachespark;User 'dhruve' has created a pull request for this issue:
https://github.com/apache/spark/pull/15370;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move Hive-generated Stats Info to HiveClientImpl,SPARK-17410,13002847,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,06/Sep/16 07:21,23/May/17 00:29,14/Jul/23 06:29,23/May/17 00:29,2.1.0,,,,,,,,2.3.0,,,,SQL,,,,,,,,0,,,,,,"After we adding a new field `stats` into `CatalogTable`, we should not expose Hive-specific Stats metadata to `MetastoreRelation`. It complicates all the related codes. We should handle Hive-specific Stats metadata in `HiveClientImpl`. ",,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2016-09-06 07:21:00.0,,,,,,,,,,"0|i339qf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Query in CTAS is Optimized Twice,SPARK-17409,13002830,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,smilegator,smilegator,smilegator,06/Sep/16 04:44,12/Dec/16 18:26,14/Jul/23 06:29,14/Sep/16 15:11,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,correctness,,,,,"The query in CTAS is optimized twice, as reported in the PR: https://github.com/apache/spark/pull/14797

{quote}
Some analyzer rules have assumptions on logical plans, optimizer may break these assumption, we should not pass an optimized query plan into QueryExecution (will be analyzed again), otherwise we may some weird bugs.
{quote}
",,apachespark,cloud_fan,joshrosen,smilegator,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17892,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 12 18:26:05 UTC 2016,,,,,,,,,,"0|i339mn:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"06/Sep/16 04:46;smilegator;Working on it. Thanks!;;;","09/Sep/16 17:36;yhuai;Note: according to https://github.com/apache/spark/pull/14797#issuecomment-244848780, this issue only affects current master (spark 2.1).;;;","11/Sep/16 04:48;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15048;;;","14/Sep/16 15:11;cloud_fan;Issue resolved by pull request 15048
[https://github.com/apache/spark/pull/15048];;;","13/Oct/16 07:58;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15459;;;","12/Dec/16 17:47;joshrosen;This was actually fixed in branch-2.x as well, via SPARK-17892.;;;","12/Dec/16 18:26;smilegator;Sure, will follow it. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: org.apache.spark.sql.hive.StatisticsSuite,SPARK-17408,13002829,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,yhuai,yhuai,06/Sep/16 04:18,07/Sep/16 00:14,14/Jul/23 06:29,07/Sep/16 00:14,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/64956/testReport/junit/org.apache.spark.sql.hive/StatisticsSuite/test_statistics_of_LogicalRelation_converted_from_MetastoreRelation/

{code}
org.apache.spark.sql.hive.StatisticsSuite.test statistics of LogicalRelation converted from MetastoreRelation

Failing for the past 1 build (Since Failed#64956 )
Took 1.4 sec.
Error Message

org.scalatest.exceptions.TestFailedException: 6871 did not equal 4236
Stacktrace

sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedException: 6871 did not equal 4236
	at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:500)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:466)
	at org.apache.spark.sql.hive.StatisticsSuite$$anonfun$14.applyOrElse(StatisticsSuite.scala:247)
	at org.apache.spark.sql.hive.StatisticsSuite$$anonfun$14.applyOrElse(StatisticsSuite.scala:241)
	at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223)
	at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$collect$1.apply(TreeNode.scala:158)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$collect$1.apply(TreeNode.scala:158)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:117)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreach$1.apply(TreeNode.scala:118)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreach$1.apply(TreeNode.scala:118)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:118)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreach$1.apply(TreeNode.scala:118)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreach$1.apply(TreeNode.scala:118)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:118)
	at org.apache.spark.sql.catalyst.trees.TreeNode.collect(TreeNode.scala:158)
	at org.apache.spark.sql.hive.StatisticsSuite.org$apache$spark$sql$hive$StatisticsSuite$$checkLogicalRelationStats(StatisticsSuite.scala:241)
	at org.apache.spark.sql.hive.StatisticsSuite$$anonfun$6$$anonfun$apply$mcV$sp$3$$anonfun$apply$mcV$sp$4.apply$mcV$sp(StatisticsSuite.scala:271)
	at org.apache.spark.sql.test.SQLTestUtils$class.withSQLConf(SQLTestUtils.scala:99)
	at org.apache.spark.sql.hive.StatisticsSuite.withSQLConf(StatisticsSuite.scala:35)
	at org.apache.spark.sql.hive.StatisticsSuite$$anonfun$6$$anonfun$apply$mcV$sp$3.apply$mcV$sp(StatisticsSuite.scala:268)
	at org.apache.spark.sql.test.SQLTestUtils$class.withTable(SQLTestUtils.scala:168)
	at org.apache.spark.sql.hive.StatisticsSuite.withTable(StatisticsSuite.scala:35)
	at org.apache.spark.sql.hive.StatisticsSuite$$anonfun$6.apply$mcV$sp(StatisticsSuite.scala:260)
	at org.apache.spark.sql.hive.StatisticsSuite$$anonfun$6.apply(StatisticsSuite.scala:257)
	at org.apache.spark.sql.hive.StatisticsSuite$$anonfun$6.apply(StatisticsSuite.scala:257)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:57)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:29)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:29)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:357)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:502)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,cloud_fan,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 07 00:14:01 UTC 2016,,,,,,,,,,"0|i339mf:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"06/Sep/16 04:20;yhuai;cc [~cloud_fan] [~ZenWzh];;;","06/Sep/16 18:47;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14978;;;","07/Sep/16 00:14;cloud_fan;Issue resolved by pull request 14978
[https://github.com/apache/spark/pull/14978];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simple aggregation query OOMing after SPARK-16525,SPARK-17405,13002813,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ekhliang,joshrosen,joshrosen,05/Sep/16 23:42,08/Sep/16 23:48,14/Jul/23 06:29,08/Sep/16 23:48,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,1,,,,,,"Prior to SPARK-16525 / https://github.com/apache/spark/pull/14176, the following query ran fine via Beeline / Thrift Server and the Spark shell, but after that patch it is consistently OOMING:

{code}
CREATE TEMPORARY VIEW table_1(double_col_1, boolean_col_2, timestamp_col_3, smallint_col_4, boolean_col_5, int_col_6, timestamp_col_7, varchar0008_col_8, int_col_9, string_col_10) AS (
  SELECT * FROM (VALUES
    (CAST(-147.818640624 AS DOUBLE), CAST(NULL AS BOOLEAN), TIMESTAMP('2012-10-19 00:00:00.0'), CAST(9 AS SMALLINT), false, 77, TIMESTAMP('2014-07-01 00:00:00.0'), '-945', -646, '722'),
    (CAST(594.195125271 AS DOUBLE), false, TIMESTAMP('2016-12-04 00:00:00.0'), CAST(NULL AS SMALLINT), CAST(NULL AS BOOLEAN), CAST(NULL AS INT), TIMESTAMP('1999-12-26 00:00:00.0'), '250', -861, '55'),
    (CAST(-454.171126363 AS DOUBLE), false, TIMESTAMP('2008-12-13 00:00:00.0'), CAST(NULL AS SMALLINT), false, -783, TIMESTAMP('2010-05-28 00:00:00.0'), '211', -959, CAST(NULL AS STRING)),
    (CAST(437.670945524 AS DOUBLE), true, TIMESTAMP('2011-10-16 00:00:00.0'), CAST(952 AS SMALLINT), true, 297, TIMESTAMP('2013-01-13 00:00:00.0'), '262', CAST(NULL AS INT), '936'),
    (CAST(-387.226759334 AS DOUBLE), false, TIMESTAMP('2019-10-03 00:00:00.0'), CAST(-496 AS SMALLINT), CAST(NULL AS BOOLEAN), -925, TIMESTAMP('2028-06-27 00:00:00.0'), '-657', 948, '18'),
    (CAST(-306.138230875 AS DOUBLE), true, TIMESTAMP('1997-10-07 00:00:00.0'), CAST(332 AS SMALLINT), false, 744, TIMESTAMP('1990-09-22 00:00:00.0'), '-345', 566, '-574'),
    (CAST(675.402140308 AS DOUBLE), false, TIMESTAMP('2017-06-26 00:00:00.0'), CAST(972 AS SMALLINT), true, CAST(NULL AS INT), TIMESTAMP('2026-06-10 00:00:00.0'), '518', 683, '-320'),
    (CAST(734.839647174 AS DOUBLE), true, TIMESTAMP('1995-06-01 00:00:00.0'), CAST(-792 AS SMALLINT), CAST(NULL AS BOOLEAN), CAST(NULL AS INT), TIMESTAMP('2021-07-11 00:00:00.0'), '-318', 564, '142')
  ) as t);

CREATE TEMPORARY VIEW table_3(string_col_1, float_col_2, timestamp_col_3, boolean_col_4, timestamp_col_5, decimal3317_col_6) AS (
  SELECT * FROM (VALUES
    ('88', CAST(191.92508 AS FLOAT), TIMESTAMP('1990-10-25 00:00:00.0'), false, TIMESTAMP('1992-11-02 00:00:00.0'), CAST(NULL AS DECIMAL(33,17))),
    ('-419', CAST(-13.477915 AS FLOAT), TIMESTAMP('1996-03-02 00:00:00.0'), true, CAST(NULL AS TIMESTAMP), -653.51000000000000000BD),
    ('970', CAST(-360.432 AS FLOAT), TIMESTAMP('2010-07-29 00:00:00.0'), false, TIMESTAMP('1995-09-01 00:00:00.0'), -936.48000000000000000BD),
    ('807', CAST(814.30756 AS FLOAT), TIMESTAMP('2019-11-06 00:00:00.0'), false, TIMESTAMP('1996-04-25 00:00:00.0'), 335.56000000000000000BD),
    ('-872', CAST(616.50525 AS FLOAT), TIMESTAMP('2011-08-28 00:00:00.0'), false, TIMESTAMP('2003-07-19 00:00:00.0'), -951.18000000000000000BD),
    ('-167', CAST(-875.35675 AS FLOAT), TIMESTAMP('1995-07-14 00:00:00.0'), false, TIMESTAMP('2005-11-29 00:00:00.0'), 224.89000000000000000BD)
  ) as t);

SELECT
CAST(MIN(t2.smallint_col_4) AS STRING) AS char_col,
LEAD(MAX((-387) + (727.64)), 90) OVER (PARTITION BY COALESCE(t2.int_col_9, t2.smallint_col_4, t2.int_col_9) ORDER BY COALESCE(t2.int_col_9, t2.smallint_col_4, t2.int_col_9) DESC, CAST(MIN(t2.smallint_col_4) AS STRING)) AS decimal_col,
COALESCE(t2.int_col_9, t2.smallint_col_4, t2.int_col_9) AS int_col
FROM table_3 t1
INNER JOIN table_1 t2 ON (((t2.timestamp_col_3) = (t1.timestamp_col_5)) AND ((t2.string_col_10) = (t1.string_col_1))) AND ((t2.string_col_10) = (t1.string_col_1))
WHERE
(t2.smallint_col_4) IN (t2.int_col_9, t2.int_col_9)
GROUP BY
COALESCE(t2.int_col_9, t2.smallint_col_4, t2.int_col_9);
{code}

Here's the OOM:

{code}
org.apache.hive.service.cli.HiveSQLException: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 9, localhost): java.lang.OutOfMemoryError: Unable to acquire 262144 bytes of memory, got 0
    at org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:100)
    at org.apache.spark.unsafe.map.BytesToBytesMap.allocate(BytesToBytesMap.java:783)
    at org.apache.spark.unsafe.map.BytesToBytesMap.<init>(BytesToBytesMap.java:204)
    at org.apache.spark.unsafe.map.BytesToBytesMap.<init>(BytesToBytesMap.java:219)
    at org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.<init>(UnsafeFixedWidthAggregationMap.java:104)
    at org.apache.spark.sql.execution.aggregate.HashAggregateExec.createHashMap(HashAggregateExec.scala:305)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
    at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
    at org.apache.spark.scheduler.Task.run(Task.scala:86)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,jlaskowski,joshrosen,nezihyigitbasi,qifan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 08 23:48:08 UTC 2016,,,,,,,,,,"0|i339iv:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"06/Sep/16 07:15;qifan;[~joshrosen] Thanks for reporting. I haven't been able to reproduce this because of a catalyst bug I have now `Error: org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to foldable on unresolved object, tree: 'TIMESTAMP(2012-10-19 00:00:00.0) (state=,code=0)`
I will look more into this.
How much memory is configured for this specific test? One thing is that we added memory management through MemoryConsumer for the generated hashmap, so it correctly accounts that part of memory usage and is more likely to throw OOM.;;;","06/Sep/16 16:15;joshrosen;[~qifan], I believe that you may be able to work around the UnresolvedException by checkout out Spark as of your commit (03d77af9ec4ce9a42affd6ab4381ae5bd3c79a5a) rather than using the current master.

I'm running this query through the Spark ThriftServer, started using the script in {{sbin}}, with default settings on my Macbook Pro. Perhaps the default resource requirements are too high for the amount of {{local\[*]|| task parallelism? In either case, I think we need to fix this so that the out of the box experience works correctly.;;;","06/Sep/16 21:52;joshrosen;On the Spark Dev list, [~jlaskowski] found a simpler example which triggers this issue:

{quote}
{code}
scala> val intsMM = 1 to math.pow(10, 3).toInt
intsMM: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4,
5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,
24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,
41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57,
58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74,
75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91,
92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106,
107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,
121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134,
135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148,
149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162,
163, 164, 165, 166, 167, 168, 169, 1...
scala> val df = intsMM.toDF(""n"").withColumn(""m"", 'n % 2)
df: org.apache.spark.sql.DataFrame = [n: int, m: int]

scala> df.groupBy('m).agg(sum('n)).show
...
16/09/06 22:28:02 ERROR Executor: Exception in task 6.0 in stage 0.0 (TID 6)
java.lang.OutOfMemoryError: Unable to acquire 262144 bytes of memory, got 0
...
{code}

Please see https://gist.github.com/jaceklaskowski/906d62b830f6c967a7eee5f8eb6e9237
{quote};;;","07/Sep/16 04:23;jlaskowski;It definitely got better with the build today Sept, 7th. Yesterday, even such a simple query died {{Seq(1).toDF.groupBy('value).count.show}}.;;;","07/Sep/16 20:00;qifan;[~joshrosen][~jlaskowski]Thanks for the comments and suggestions. I have run both of your queries on 03d77af9ec4ce9a42affd6ab4381ae5bd3c79a5a and was able to finish both of them without any exceptions. 
I'll do some static code analysis based on the log from [~jlaskowski];;;","07/Sep/16 20:24;joshrosen;My hunch is that this is affected by the default number of cores in local mode: I think that my MBP uses 16 tasks by default, while I think that the default parallelism is lower in Jenkins (and perhaps on your machine). If you have trouble reproducing this issue then I'd try explicitly running {{local\[16]}} or {{local\[32]}} to see if that can reproduce the issue.;;;","07/Sep/16 20:47;qifan;[~joshrosen]
Yes likely. The new hashmap asks for 64MB per task, and the default single-node setting uses only hundreds of memory in total.
We decided on 64MB due to our single memory page design for simplicity and performance, and that in production it should hold 64MB * cores << memory_capacity.
Maybe we should increase default memory a bit? Or is it bad in general to have such upfront cost of 64MB?;;;","07/Sep/16 22:39;qifan;One quick fix is to set memory capacity in configuration to make sure memory_capacity > x*cores (x being some number > 64MB);;;","07/Sep/16 22:48;qifan;[~joshrosen] Yes, running local[32] will reproduce the exception. ;;;","08/Sep/16 21:16;apachespark;User 'ericl' has created a pull request for this issue:
https://github.com/apache/spark/pull/15016;;;","08/Sep/16 23:48;joshrosen;Issue resolved by pull request 15016
[https://github.com/apache/spark/pull/15016];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[BRANCH-1.6] Broken test: showDF in test_sparkSQL.R,SPARK-17404,13002787,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sunrui,yhuai,yhuai,05/Sep/16 21:03,06/Sep/16 02:03,14/Jul/23 06:29,06/Sep/16 01:25,1.6.3,,,,,,,,1.6.3,,,,SparkR,,,,,,,,0,,,,,,"https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test/job/spark-branch-1.6-test-sbt-hadoop-2.2/286/console

Seems showDF in test_sparkSQL.R is broken. 

{code}
Failed -------------------------------------------------------------------------
1. Failure: showDF() (@test_sparkSQL.R#1400) -----------------------------------
`s` produced no output
{code}",,shivaram,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 06 02:03:50 UTC 2016,,,,,,,,,,"0|i339d3:",9223372036854775807,,,,,,,,,,,,,1.6.3,,,,,,,,,,,"05/Sep/16 21:04;yhuai;cc [~felixcheung] and [~shivaram];;;","05/Sep/16 22:52;shivaram;Hmm - we upgraded the version of testthat on the cluster last week. This could be related to that. I'm going to try and backport
https://github.com/apache/spark/pull/12867 to branch-1.6 and see if it fixes things;;;","05/Sep/16 23:02;shivaram;I've kicked off https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test/job/spark-branch-1.6-test-sbt-hadoop-2.2/288/ after the cherry-pick. Lets watch this and see if it fixes things;;;","06/Sep/16 01:25;shivaram;Fixed by backporting https://github.com/apache/spark/pull/12867;;;","06/Sep/16 02:03;yhuai;Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
separate the management of temp views and metastore tables/views in SessionCatalog,SPARK-17402,13002731,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,05/Sep/16 13:41,02/Nov/16 05:32,14/Jul/23 06:29,02/Nov/16 05:32,,,,,,,,,,,,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 05 14:05:08 UTC 2016,,,,,,,,,,"0|i3390n:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"05/Sep/16 14:05;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/14962;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to query on external JSon Partitioned table,SPARK-17398,13002665,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wypoon,pin_zhang,pin_zhang,05/Sep/16 05:27,20/Dec/19 23:23,14/Jul/23 06:29,20/Dec/19 18:53,2.0.0,,,,,,,,2.4.5,3.0.0,,,SQL,,,,,,,,0,,,,,,"1. Create External Json partitioned table 
with SerDe in hive-hcatalog-core-1.2.1.jar, download fom
https://mvnrepository.com/artifact/org.apache.hive.hcatalog/hive-hcatalog-core/1.2.1
2. Query table meet exception, which works in spark1.5.2
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task
 0.0 in stage 1.0 (TID 1, localhost): java.lang.ClassCastException: java.util.ArrayList cannot be cast to org.apache.hive.hcatalog.data.HCatRecord
        at org.apache.hive.hcatalog.data.HCatRecordObjectInspector.getStructFieldData(HCatRecordObjectInspector.java:45)
        at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:430)
        at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:426)
 

3. Test Code

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

object JsonBugs {

  def main(args: Array[String]): Unit = {
    val table = ""test_json""
    val location = ""file:///g:/home/test/json""
    val create = s""""""CREATE   EXTERNAL  TABLE  ${table}
             (id string,  seq string )
              PARTITIONED BY(index int)
              ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
              LOCATION ""${location}"" 
          """"""
    val add_part = s""""""
         ALTER TABLE ${table} ADD 
         PARTITION (index=1)LOCATION '${location}/index=1'
    """"""

    val conf = new SparkConf().setAppName(""scala"").setMaster(""local[2]"")
    conf.set(""spark.sql.warehouse.dir"", ""file:///g:/home/warehouse"")
    val ctx = new SparkContext(conf)

    val hctx = new HiveContext(ctx)
    val exist = hctx.tableNames().map { x => x.toLowerCase() }.contains(table)
    if (!exist) {
      hctx.sql(create)
      hctx.sql(add_part)
    } else {
      hctx.sql(""show partitions "" + table).show()
    }
    hctx.sql(""select * from test_json"").show()
  }
}
",,angerszhuuu,bianqi,pin_zhang,wypoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-15773,HIVE-21752,,,,,,,,,"28/Jun/19 04:03;bianqi;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12973134/screenshot-1.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 14 02:02:46 UTC 2019,,,,,,,,,,"0|i338lz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/16 17:58;srowen;Changing resolution because I don't see evidence of what resolved it, even if it may indeed be resolved;;;","28/Jun/19 04:03;bianqi; !screenshot-1.png!  ;;;","06/Nov/19 02:48;angerszhuuu;[~bianqi]
Hi, I meet this problem too, have you know what's the problem? ;;;","14/Dec/19 02:02;wypoon;This issue was never actually fixed. Evidently the problem still exists.
I'll create a PR with a fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Threads number keep increasing when query on external CSV partitioned table,SPARK-17396,13002653,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rdblue,pin_zhang,pin_zhang,05/Sep/16 01:50,10/Oct/16 08:43,14/Jul/23 06:29,10/Sep/16 09:21,2.0.0,,,,,,,,2.0.1,2.1.0,,,Spark Core,,,,,,,,0,,,,,,"1. Create a external partitioned table row format CSV
2. Add 16 partitions to the table
3. Run SQL ""select count(*) from test_csv""
4. ForkJoinThread number keep increasing 
This happend when table partitions number greater than 10.
5. Test Code

{code:lang=java}
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

object Bugs {

  def main(args: Array[String]): Unit = {

    val location = ""file:///g:/home/test/csv""
    val create = s""""""CREATE   EXTERNAL  TABLE  test_csv
             (ID string,  SEQ string )
              PARTITIONED BY(index int)
              ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
              LOCATION ""${location}"" 
          """"""
    val add_part = s""""""
  ALTER TABLE test_csv ADD 
  PARTITION (index=1)LOCATION '${location}/index=1'
  PARTITION (index=2)LOCATION '${location}/index=2'
  PARTITION (index=3)LOCATION '${location}/index=3'
  PARTITION (index=4)LOCATION '${location}/index=4'
  PARTITION (index=5)LOCATION '${location}/index=5'
  PARTITION (index=6)LOCATION '${location}/index=6'
  PARTITION (index=7)LOCATION '${location}/index=7'
  PARTITION (index=8)LOCATION '${location}/index=8'
  PARTITION (index=9)LOCATION '${location}/index=9'
  PARTITION (index=10)LOCATION '${location}/index=10'
  PARTITION (index=11)LOCATION '${location}/index=11'
  PARTITION (index=12)LOCATION '${location}/index=12'
  PARTITION (index=13)LOCATION '${location}/index=13'
  PARTITION (index=14)LOCATION '${location}/index=14'
  PARTITION (index=15)LOCATION '${location}/index=15'
  PARTITION (index=16)LOCATION '${location}/index=16'
    """"""

    val conf = new SparkConf().setAppName(""scala"").setMaster(""local[2]"")
    conf.set(""spark.sql.warehouse.dir"", ""file:///g:/home/warehouse"")
    val ctx = new SparkContext(conf)
    val hctx = new HiveContext(ctx)
    hctx.sql(create)
    hctx.sql(add_part)
     for (i <- 1 to 6) {
      new Query(hctx).start()
    }
  }

  class Query(htcx: HiveContext) extends Thread {

    setName(""Query-Thread"")

    override def run = {
      while (true) {
        htcx.sql(""select count(*) from test_csv"").show()
        Thread.sleep(100)
      }

    }
  }
}
{code}",,apachespark,joshrosen,pin_zhang,rdblue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17842,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 10 09:21:10 UTC 2016,,,,,,,,,,"0|i338jb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/16 09:07;srowen;Can you be more specific? what pool, and what problem do you observe? what are the threads doing?;;;","05/Sep/16 09:34;pin_zhang;1.Thousand of thread created look like
ForkJoinPool-20-worker-9"" #329 daemon prio=5 os_prio=0 tid=0x000000000ac87000 nid=0x3d43 waiting on condition [0x000000005069f000]
""ForkJoinPool-19-worker-3"" #324 daemon prio=5 os_prio=0 tid=0x000000000ae60000 nid=0x3c2a waiting on condition [0x000000005039c000]

2.The thread should be created by UnionRDD
;;;","05/Sep/16 09:47;srowen;[~rdblue] what do you think of this -- I think there's a good point here that ForkJoinPool won't limit the number of threads it uses? if that's true then this would probably need to switch to a normal Java executor service that can cap threads.;;;","05/Sep/16 16:37;rdblue;I'm not sure that the ForkJoinPool is to blame. Each partition in a Hive table becomes a partition RDD in the union. Each UnionRDD creates a new ForkJoinPool (parallelism=8) to list the files in a partition in parallel. I think it is more likely that the problem is that we don't use a shared ForkJoinPool, but instead create a new one for each RDD that doesn't get cleaned up until the UnionRDD is cleaned. Even if the ForkJoinPool can't reuse threads and we're getting more than 8, it wouldn't grow to thousands if we weren't creating so many pools. But, I agree that we should consider a different executor service. I've been reading up on ForkJoinPool and it looks like it's not a great implementation; just trying to determine the maximum number of threads it will use was painfully undocumented.

[~pin_zhang], can you post a list of the thread names? We can use the names to determine how many threads there are in each pool. That should tell us whether we should use a different executor service in addition to using a shared pool.;;;","06/Sep/16 02:21;pin_zhang;""Thread-1902"" daemon prio=6 tid=0x0000000014078800 nid=0x3a6c runnable [0x0000000038d5e000]
""Thread-1901"" daemon prio=6 tid=0x000000000c64f800 nid=0x32fc runnable [0x00000000191ef000]
""Thread-1900"" daemon prio=6 tid=0x0000000014249800 nid=0x263c runnable [0x000000004c73e000]
""Thread-1899"" daemon prio=6 tid=0x0000000014244000 nid=0x189c runnable [0x0000000017c7e000]
""Thread-1898"" daemon prio=6 tid=0x000000000d96a800 nid=0x3e54 runnable [0x000000004c5ef000]
""ForkJoinPool-120-worker-1"" daemon prio=6 tid=0x000000001407d000 nid=0x2234 waiting for monitor entry [0x000000004c31e000]
""ForkJoinPool-120-worker-3"" daemon prio=6 tid=0x0000000013a64000 nid=0x1f0c waiting for monitor entry [0x000000004c0de000]
""ForkJoinPool-120-worker-5"" daemon prio=6 tid=0x0000000013a75800 nid=0x1660 waiting for monitor entry [0x000000004241e000]
""ForkJoinPool-120-worker-7"" daemon prio=6 tid=0x0000000013d6c000 nid=0x117c waiting for monitor entry [0x000000004bece000]
""ForkJoinPool-120-worker-9"" daemon prio=6 tid=0x0000000014233800 nid=0x2a20 waiting for monitor entry [0x000000004bd3e000]
""ForkJoinPool-120-worker-11"" daemon prio=6 tid=0x000000001423f800 nid=0x3568 waiting for monitor entry [0x000000004afae000]
""ForkJoinPool-120-worker-13"" daemon prio=6 tid=0x000000001424e000 nid=0x378c waiting for monitor entry [0x000000004bc0e000]
""ForkJoinPool-120-worker-15"" daemon prio=6 tid=0x0000000014238000 nid=0x1b8c waiting for monitor entry [0x0000000018dfd000]
""ForkJoinPool-119-worker-1"" daemon prio=6 tid=0x0000000013d74800 nid=0x29a0 waiting for monitor entry [0x000000004bade000]
""ForkJoinPool-119-worker-3"" daemon prio=6 tid=0x0000000012cd4000 nid=0x18a0 in Object.wait() [0x000000004b9ae000]
""ForkJoinPool-119-worker-7"" daemon prio=6 tid=0x0000000012cd3000 nid=0x15ec waiting for monitor entry [0x000000004b87d000]
""ForkJoinPool-119-worker-5"" daemon prio=6 tid=0x0000000013bbd800 nid=0x2c24 waiting for monitor entry [0x000000004b76d000]
""ForkJoinPool-119-worker-9"" daemon prio=6 tid=0x0000000013bc9800 nid=0x3d78 waiting for monitor entry [0x000000002acae000]
""ForkJoinPool-119-worker-11"" daemon prio=6 tid=0x000000000d9eb000 nid=0x3f40 waiting for monitor entry [0x000000004b57e000]
""ForkJoinPool-119-worker-13"" daemon prio=6 tid=0x000000000d9e4800 nid=0x286c waiting for monitor entry [0x000000004b40e000]
""ForkJoinPool-119-worker-15"" daemon prio=6 tid=0x000000000d9e9000 nid=0x2304 in Object.wait() [0x00000000194de000]
""ForkJoinPool-118-worker-1"" daemon prio=6 tid=0x0000000014077000 nid=0x3a50 runnable [0x00000000393dd000]
""ForkJoinPool-118-worker-3"" daemon prio=6 tid=0x000000001407a000 nid=0x1dc0 runnable [0x000000002331d000]
""ForkJoinPool-118-worker-5"" daemon prio=6 tid=0x000000000d2f9000 nid=0x2990 runnable [0x000000001b6fd000]
""ForkJoinPool-118-worker-7"" daemon prio=6 tid=0x000000000d2df800 nid=0x3bb4 runnable [0x000000004a9dd000]
""ForkJoinPool-118-worker-9"" daemon prio=6 tid=0x000000000d2f7800 nid=0x37e4 waiting for monitor entry [0x000000002bf5e000]
""ForkJoinPool-118-worker-11"" daemon prio=6 tid=0x0000000012648000 nid=0x2878 runnable [0x000000002b26d000]
""ForkJoinPool-118-worker-13"" daemon prio=6 tid=0x0000000012646000 nid=0x4cc waiting for monitor entry [0x00000000183de000]
""ForkJoinPool-118-worker-15"" daemon prio=6 tid=0x0000000012647800 nid=0x30c8 waiting for monitor entry [0x000000002bd3d000]
""ForkJoinPool-117-worker-5"" daemon prio=6 tid=0x0000000012b5c800 nid=0x3510 waiting for monitor entry [0x000000004b2be000]
""ForkJoinPool-117-worker-1"" daemon prio=6 tid=0x0000000012b5d000 nid=0x36b8 waiting for monitor entry [0x000000004b11e000]
""ForkJoinPool-117-worker-3"" daemon prio=6 tid=0x0000000012eac800 nid=0x32d4 in Object.wait() [0x000000004acae000]
""ForkJoinPool-117-worker-7"" daemon prio=6 tid=0x0000000012ea9800 nid=0x16c4 waiting for monitor entry [0x000000004ab1e000]
""ForkJoinPool-117-worker-9"" daemon prio=6 tid=0x0000000012e9b000 nid=0x1e44 waiting for monitor entry [0x000000002162e000]
""ForkJoinPool-117-worker-11"" daemon prio=6 tid=0x0000000013bcc000 nid=0x37f4 waiting for monitor entry [0x0000000040dee000]
""ForkJoinPool-117-worker-13"" daemon prio=6 tid=0x0000000013bcb000 nid=0x361c in Object.wait() [0x0000000035dbe000]
""ForkJoinPool-117-worker-15"" daemon prio=6 tid=0x0000000013bca800 nid=0x3344 in Object.wait() [0x000000002c0ce000]
""ForkJoinPool-116-worker-1"" daemon prio=6 tid=0x0000000013bc9000 nid=0x3a34 runnable [0x000000004867d000]
""ForkJoinPool-116-worker-3"" daemon prio=6 tid=0x0000000013bc8000 nid=0x1c10 in Object.wait() [0x000000004a8be000]
""ForkJoinPool-116-worker-7"" daemon prio=6 tid=0x0000000013bc7800 nid=0x2910 waiting on condition [0x0000000045e7f000]
""ForkJoinPool-116-worker-5"" daemon prio=6 tid=0x0000000013bc6800 nid=0x3b1c waiting for monitor entry [0x000000003a6fe000]
""ForkJoinPool-116-worker-9"" daemon prio=6 tid=0x0000000013bbf000 nid=0x1e5c waiting on condition [0x000000002a5ce000]
""ForkJoinPool-116-worker-11"" daemon prio=6 tid=0x0000000013bc0000 nid=0x294c waiting for monitor entry [0x00000000263cd000]
""ForkJoinPool-116-worker-13"" daemon prio=6 tid=0x0000000013bbe800 nid=0x35b0 in Object.wait() [0x000000001d9de000]
""ForkJoinPool-116-worker-15"" daemon prio=6 tid=0x0000000013bc6000 nid=0x1c1c in Object.wait() [0x000000001a33e000]
""ForkJoinPool-115-worker-1"" daemon prio=6 tid=0x0000000013bc5000 nid=0x39e0 waiting on condition [0x000000004a6df000]
""ForkJoinPool-115-worker-3"" daemon prio=6 tid=0x0000000013bc4800 nid=0x18ec waiting on condition [0x000000004a55f000]
""ForkJoinPool-115-worker-5"" daemon prio=6 tid=0x0000000013bc3800 nid=0x1780 waiting on condition [0x000000001f8ff000]
""ForkJoinPool-115-worker-7"" daemon prio=6 tid=0x0000000013bc3000 nid=0xb74 waiting on condition [0x000000004a35f000]
""ForkJoinPool-115-worker-9"" daemon prio=6 tid=0x0000000013bc2000 nid=0x374c waiting on condition [0x000000004a18e000]
""ForkJoinPool-115-worker-11"" daemon prio=6 tid=0x0000000013bc1800 nid=0x3d00 waiting on condition [0x000000003063f000]
""ForkJoinPool-115-worker-13"" daemon prio=6 tid=0x0000000013bc0800 nid=0x1928 waiting for monitor entry [0x0000000038b0d000]
""ForkJoinPool-115-worker-15"" daemon prio=6 tid=0x0000000013bbd000 nid=0x1b2c in Object.wait() [0x0000000049fde000]
""ForkJoinPool-114-worker-7"" daemon prio=6 tid=0x0000000013bbc000 nid=0x3fc0 waiting on condition [0x000000003fa5f000]
""ForkJoinPool-114-worker-1"" daemon prio=6 tid=0x0000000013bbb800 nid=0x1a04 waiting on condition [0x000000001e34e000]
""ForkJoinPool-114-worker-3"" daemon prio=6 tid=0x0000000013bb3000 nid=0x3ea4 waiting on condition [0x0000000036dae000]
""ForkJoinPool-114-worker-5"" daemon prio=6 tid=0x0000000013bba000 nid=0x1e2c waiting on condition [0x0000000037e9f000]
""ForkJoinPool-114-worker-9"" daemon prio=6 tid=0x0000000013bba800 nid=0x420 waiting on condition [0x000000002b50f000]
""ForkJoinPool-114-worker-11"" daemon prio=6 tid=0x0000000013bb9000 nid=0x16c8 waiting on condition [0x00000000295cf000]
""ForkJoinPool-114-worker-13"" daemon prio=6 tid=0x0000000013bb2800 nid=0x2874 waiting on condition [0x000000001dd5f000]
""ForkJoinPool-114-worker-15"" daemon prio=6 tid=0x0000000013bb1000 nid=0x3b98 waiting on condition [0x0000000025def000]
""ForkJoinPool-113-worker-1"" daemon prio=6 tid=0x0000000013bb8800 nid=0x39e4 waiting on condition [0x0000000029e2f000]
""ForkJoinPool-113-worker-3"" daemon prio=6 tid=0x0000000013bb7800 nid=0x2f78 waiting on condition [0x000000003929e000]
""ForkJoinPool-113-worker-5"" daemon prio=6 tid=0x0000000013bb7000 nid=0xfd0 waiting on condition [0x0000000049eaf000]
""ForkJoinPool-113-worker-7"" daemon prio=6 tid=0x0000000013bb6000 nid=0x369c waiting on condition [0x0000000028d8f000]
""ForkJoinPool-113-worker-11"" daemon prio=6 tid=0x0000000013bb5800 nid=0x3f50 waiting on condition [0x000000002651f000]
""ForkJoinPool-113-worker-9"" daemon prio=6 tid=0x0000000013bb4800 nid=0x3c9c waiting on condition [0x00000000487df000]
""ForkJoinPool-113-worker-13"" daemon prio=6 tid=0x0000000013bb4000 nid=0x147c waiting on condition [0x0000000049d5f000]
""ForkJoinPool-113-worker-15"" daemon prio=6 tid=0x0000000013bb1800 nid=0x138c waiting on condition [0x000000003104f000]
""ForkJoinPool-112-worker-1"" daemon prio=6 tid=0x0000000013bb0000 nid=0x33d8 waiting on condition [0x000000004997f000]
""ForkJoinPool-112-worker-5"" daemon prio=6 tid=0x0000000013baf800 nid=0x3f44 waiting on condition [0x0000000049c5f000]
""ForkJoinPool-112-worker-3"" daemon prio=6 tid=0x0000000013bae800 nid=0x39d0 waiting on condition [0x000000001772f000]
""ForkJoinPool-112-worker-7"" daemon prio=6 tid=0x0000000013bae000 nid=0x374 waiting on condition [0x0000000049a7f000]
""ForkJoinPool-112-worker-9"" daemon prio=6 tid=0x0000000013bad000 nid=0x3e3c waiting on condition [0x0000000048a8f000]
""ForkJoinPool-112-worker-11"" daemon prio=6 tid=0x0000000014188000 nid=0x329c waiting on condition [0x000000004982f000]
""ForkJoinPool-112-worker-13"" daemon prio=6 tid=0x000000001368f800 nid=0x267c waiting on condition [0x000000004968e000]
""ForkJoinPool-112-worker-15"" daemon prio=6 tid=0x0000000014ed4000 nid=0x3e9c waiting on condition [0x0000000048d4f000]
""ForkJoinPool-111-worker-1"" daemon prio=6 tid=0x0000000013ce8800 nid=0x958 waiting on condition [0x00000000494ef000]
""ForkJoinPool-111-worker-3"" daemon prio=6 tid=0x00000000121ed800 nid=0x2078 waiting on condition [0x00000000492ff000]
""ForkJoinPool-111-worker-5"" daemon prio=6 tid=0x0000000013a7a800 nid=0x2940 waiting on condition [0x00000000491fe000]
""ForkJoinPool-111-worker-7"" daemon prio=6 tid=0x00000000134ca800 nid=0x11e4 waiting on condition [0x00000000490ee000]
""ForkJoinPool-111-worker-9"" daemon prio=6 tid=0x000000000d96b000 nid=0x2fa4 waiting on condition [0x00000000433ae000]
""ForkJoinPool-111-worker-11"" daemon prio=6 tid=0x00000000134a1800 nid=0x1d4c waiting on condition [0x0000000048fcf000]
""ForkJoinPool-111-worker-13"" daemon prio=6 tid=0x00000000141a0800 nid=0x251c waiting on condition [0x000000004844f000]
""ForkJoinPool-111-worker-15"" daemon prio=6 tid=0x0000000012ea9000 nid=0x17d4 waiting on condition [0x0000000048e5e000]
""ForkJoinPool-110-worker-3"" daemon prio=6 tid=0x0000000014070800 nid=0x2720 waiting on condition [0x0000000048baf000]
""ForkJoinPool-110-worker-1"" daemon prio=6 tid=0x000000000f320800 nid=0x326c waiting on condition [0x000000004894f000]
""ForkJoinPool-110-worker-5"" daemon prio=6 tid=0x000000000d9eb800 nid=0x2768 waiting on condition [0x000000002860f000]
""ForkJoinPool-110-worker-7"" daemon prio=6 tid=0x00000000129d5000 nid=0x182c waiting on condition [0x0000000039a3e000]
""ForkJoinPool-110-worker-9"" daemon prio=6 tid=0x000000000d5ce000 nid=0x1d00 waiting on condition [0x00000000275ff000]
""ForkJoinPool-110-worker-11"" daemon prio=6 tid=0x00000000129d4000 nid=0x2f10 waiting on condition [0x000000004855f000]
""ForkJoinPool-110-worker-13"" daemon prio=6 tid=0x00000000143b1000 nid=0x3094 waiting on condition [0x0000000026cee000]
""ForkJoinPool-110-worker-15"" daemon prio=6 tid=0x000000001439a000 nid=0x3224 waiting on condition [0x000000001760f000]
""ForkJoinPool-109-worker-1"" daemon prio=6 tid=0x0000000014395800 nid=0x1ab0 waiting on condition [0x000000004830f000]
""ForkJoinPool-109-worker-3"" daemon prio=6 tid=0x0000000012d73800 nid=0x38bc waiting on condition [0x000000004814e000]
""ForkJoinPool-109-worker-5"" daemon prio=6 tid=0x0000000012d6e000 nid=0x2884 waiting on condition [0x0000000047fdf000]
""ForkJoinPool-109-worker-7"" daemon prio=6 tid=0x0000000013a6e800 nid=0x2684 waiting on condition [0x0000000039eee000]
""ForkJoinPool-109-worker-9"" daemon prio=6 tid=0x0000000013a7b800 nid=0x1b78 waiting on condition [0x0000000047adf000]
""ForkJoinPool-109-worker-11"" daemon prio=6 tid=0x0000000013a5c800 nid=0x3e88 waiting on condition [0x0000000034eef000]
""ForkJoinPool-109-worker-13"" daemon prio=6 tid=0x0000000013a70000 nid=0xfcc waiting on condition [0x0000000035a0f000]
""ForkJoinPool-109-worker-15"" daemon prio=6 tid=0x0000000012af8000 nid=0x100 waiting on condition [0x0000000032d3e000]
""ForkJoinPool-108-worker-1"" daemon prio=6 tid=0x00000000129ce000 nid=0xd64 waiting on condition [0x000000002f45f000]
""ForkJoinPool-108-worker-3"" daemon prio=6 tid=0x00000000129d0800 nid=0x644 waiting on condition [0x000000002613f000]
""ForkJoinPool-108-worker-5"" daemon prio=6 tid=0x000000000d2f3000 nid=0x1924 waiting on condition [0x00000000248be000]
""ForkJoinPool-108-worker-7"" daemon prio=6 tid=0x000000000d2e5800 nid=0xc0c waiting on condition [0x0000000034a8e000]
""ForkJoinPool-108-worker-9"" daemon prio=6 tid=0x000000000d2e0000 nid=0x3de0 waiting on condition [0x000000002d69e000]
""ForkJoinPool-108-worker-11"" daemon prio=6 tid=0x000000000d2f1800 nid=0x2714 waiting on condition [0x0000000027e5f000]
""ForkJoinPool-108-worker-13"" daemon prio=6 tid=0x000000000d2eb000 nid=0x3464 waiting on condition [0x000000001e07f000]
""ForkJoinPool-108-worker-15"" daemon prio=6 tid=0x0000000012ea7800 nid=0x2dac waiting on condition [0x0000000016e9e000]
""ForkJoinPool-107-worker-1"" daemon prio=6 tid=0x0000000013a6f800 nid=0x2c64 waiting on condition [0x000000002e40f000]
""ForkJoinPool-107-worker-9"" daemon prio=6 tid=0x0000000013a7a000 nid=0x1ed8 waiting on condition [0x0000000047e6e000]
""ForkJoinPool-107-worker-3"" daemon prio=6 tid=0x0000000013a79000 nid=0x387c waiting on condition [0x0000000047cae000]
""ForkJoinPool-107-worker-5"" daemon prio=6 tid=0x0000000013a78800 nid=0x3fd4 waiting on condition [0x000000004584e000]
""ForkJoinPool-107-worker-7"" daemon prio=6 tid=0x0000000013a77800 nid=0x1bc0 waiting on condition [0x0000000043d7f000]
""ForkJoinPool-107-worker-11"" daemon prio=6 tid=0x0000000013a77000 nid=0x32b8 waiting on condition [0x0000000024d3f000]
""ForkJoinPool-107-worker-13"" daemon prio=6 tid=0x0000000013a76000 nid=0x3fe4 waiting on condition [0x0000000026def000]
""ForkJoinPool-106-worker-1"" daemon prio=6 tid=0x0000000013a74800 nid=0x3048 waiting on condition [0x000000004797f000]
""ForkJoinPool-106-worker-3"" daemon prio=6 tid=0x0000000013a74000 nid=0x25c8 waiting on condition [0x000000003e46f000]
""ForkJoinPool-106-worker-5"" daemon prio=6 tid=0x0000000013a73000 nid=0x2e90 waiting on condition [0x000000004786f000]
""ForkJoinPool-106-worker-7"" daemon prio=6 tid=0x0000000013a72800 nid=0x3e18 waiting on condition [0x0000000017b4f000]
""ForkJoinPool-106-worker-9"" daemon prio=6 tid=0x0000000013a71800 nid=0x33e0 waiting on condition [0x0000000041e7e000]
""ForkJoinPool-106-worker-11"" daemon prio=6 tid=0x0000000013a71000 nid=0x1f68 waiting on condition [0x00000000338ef000]
""ForkJoinPool-106-worker-13"" daemon prio=6 tid=0x0000000013a6e000 nid=0x1260 waiting on condition [0x0000000032aef000]
""ForkJoinPool-105-worker-1"" daemon prio=6 tid=0x0000000013a6d000 nid=0x3690 waiting on condition [0x00000000476af000]
""ForkJoinPool-105-worker-3"" daemon prio=6 tid=0x0000000013a6c800 nid=0x261c waiting on condition [0x0000000031d9f000]
""ForkJoinPool-105-worker-5"" daemon prio=6 tid=0x0000000013a6b800 nid=0x2a24 waiting on condition [0x00000000474ff000]
""ForkJoinPool-105-worker-7"" daemon prio=6 tid=0x0000000013a6a800 nid=0x2e8 waiting on condition [0x00000000473af000]
""ForkJoinPool-105-worker-9"" daemon prio=6 tid=0x0000000013a6a000 nid=0x349c waiting on condition [0x000000004727f000]
""ForkJoinPool-105-worker-11"" daemon prio=6 tid=0x0000000013a5e800 nid=0xf5c waiting on condition [0x0000000046ebe000]
""ForkJoinPool-105-worker-13"" daemon prio=6 tid=0x0000000012e95000 nid=0x3938 waiting on condition [0x000000003d39e000]
""ForkJoinPool-104-worker-3"" daemon prio=6 tid=0x0000000013a69000 nid=0x33b4 waiting on condition [0x000000004715f000]
""ForkJoinPool-104-worker-1"" daemon prio=6 tid=0x0000000013a68800 nid=0x31b8 waiting on condition [0x000000004701f000]
""ForkJoinPool-104-worker-5"" daemon prio=6 tid=0x0000000013a67800 nid=0x2518 waiting on condition [0x0000000046daf000]
""ForkJoinPool-104-worker-7"" daemon prio=6 tid=0x0000000013a67000 nid=0x1470 waiting on condition [0x00000000311af000]
""ForkJoinPool-104-worker-9"" daemon prio=6 tid=0x0000000013a66000 nid=0x1ec8 waiting on condition [0x0000000046c4f000]
""ForkJoinPool-104-worker-11"" daemon prio=6 tid=0x0000000013a65800 nid=0x26f4 waiting on condition [0x0000000046acf000]
""ForkJoinPool-104-worker-13"" daemon prio=6 tid=0x0000000013a64800 nid=0x2648 waiting on condition [0x00000000468ff000]
""ForkJoinPool-103-worker-3"" daemon prio=6 tid=0x0000000013a63000 nid=0x336c waiting on condition [0x000000004660f000]
""ForkJoinPool-103-worker-1"" daemon prio=6 tid=0x0000000013a62800 nid=0x19c4 waiting on condition [0x000000004673f000]
""ForkJoinPool-103-worker-5"" daemon prio=6 tid=0x0000000013a61800 nid=0x2c04 waiting on condition [0x000000004650f000]
""ForkJoinPool-103-worker-7"" daemon prio=6 tid=0x0000000013a61000 nid=0x35a8 waiting on condition [0x00000000463ce000]
""ForkJoinPool-103-worker-9"" daemon prio=6 tid=0x0000000013a60000 nid=0xdf8 waiting on condition [0x00000000249ce000]
""ForkJoinPool-103-worker-11"" daemon prio=6 tid=0x0000000013a5f800 nid=0x3444 waiting on condition [0x00000000462bf000]
""ForkJoinPool-103-worker-13"" daemon prio=6 tid=0x0000000013a5e000 nid=0x3d8 waiting on condition [0x000000004617f000]
""ForkJoinPool-102-worker-7"" daemon prio=6 tid=0x0000000013a5d000 nid=0x3d1c waiting on condition [0x0000000045ffe000]
""ForkJoinPool-102-worker-1"" daemon prio=6 tid=0x00000000143a9000 nid=0x2384 waiting on condition [0x0000000015e3f000]
""ForkJoinPool-102-worker-3"" daemon prio=6 tid=0x0000000012b5b800 nid=0x22b4 waiting on condition [0x00000000451ef000]
""ForkJoinPool-102-worker-5"" daemon prio=6 tid=0x000000000d963800 nid=0x2eb4 waiting on condition [0x0000000045caf000]
""ForkJoinPool-102-worker-9"" daemon prio=6 tid=0x0000000012ccf800 nid=0x32f0 waiting on condition [0x0000000045b2f000]
""ForkJoinPool-102-worker-11"" daemon prio=6 tid=0x000000000c653000 nid=0x2960 waiting on condition [0x000000001d5af000]
""ForkJoinPool-102-worker-13"" daemon prio=6 tid=0x0000000014273000 nid=0x2a2c waiting on condition [0x000000004597f000]
""ForkJoinPool-101-worker-1"" daemon prio=6 tid=0x0000000012e99800 nid=0x2898 waiting on condition [0x0000000022f4f000]
""ForkJoinPool-101-worker-3"" daemon prio=6 tid=0x0000000012eb1000 nid=0x3bb0 waiting on condition [0x00000000386af000]
""ForkJoinPool-101-worker-5"" daemon prio=6 tid=0x0000000013cda800 nid=0x339c waiting on condition [0x000000004573e000]
""ForkJoinPool-101-worker-7"" daemon prio=6 tid=0x0000000013ce1000 nid=0x3738 waiting on condition [0x00000000210bf000]
""ForkJoinPool-101-worker-9"" daemon prio=6 tid=0x000000000f321800 nid=0x271c waiting on condition [0x000000004561f000]
""ForkJoinPool-101-worker-11"" daemon prio=6 tid=0x000000000f33a000 nid=0x3478 waiting on condition [0x00000000318ef000]
""ForkJoinPool-101-worker-13"" daemon prio=6 tid=0x000000000f327800 nid=0x3610 waiting on condition [0x000000002418f000]
""ForkJoinPool-100-worker-3"" daemon prio=6 tid=0x000000001423f000 nid=0x3e34 waiting on condition [0x00000000454ff000]
""ForkJoinPool-100-worker-1"" daemon prio=6 tid=0x0000000014246800 nid=0x2ec4 waiting on condition [0x0000000023d8f000]
""ForkJoinPool-100-worker-5"" daemon prio=6 tid=0x000000000d00d800 nid=0x366c waiting on condition [0x000000004530e000]
""ForkJoinPool-100-worker-7"" daemon prio=6 tid=0x000000000d012000 nid=0x23f0 waiting on condition [0x000000002397e000]
""ForkJoinPool-100-worker-9"" daemon prio=6 tid=0x0000000012e9a800 nid=0x3a5c waiting on condition [0x000000002779e000]
""ForkJoinPool-100-worker-11"" daemon prio=6 tid=0x0000000012e9c000 nid=0x1808 waiting on condition [0x00000000450ef000]
""ForkJoinPool-100-worker-15"" daemon prio=6 tid=0x00000000143b2000 nid=0x3594 waiting on condition [0x00000000197cf000]
""ForkJoinPool-99-worker-1"" daemon prio=6 tid=0x0000000013d6e800 nid=0x1704 waiting on condition [0x000000003a81f000]
""ForkJoinPool-99-worker-3"" daemon prio=6 tid=0x0000000013d73000 nid=0x3964 waiting on condition [0x000000001a4af000]
""ForkJoinPool-99-worker-5"" daemon prio=6 tid=0x0000000014186000 nid=0x3208 waiting on condition [0x000000004445f000]
""ForkJoinPool-99-worker-7"" daemon prio=6 tid=0x0000000014184800 nid=0x1fa0 waiting on condition [0x0000000044f8f000]
""ForkJoinPool-99-worker-9"" daemon prio=6 tid=0x000000000f324800 nid=0x3860 waiting on condition [0x0000000044e2f000]
""ForkJoinPool-99-worker-11"" daemon prio=6 tid=0x000000000f334000 nid=0x342c waiting on condition [0x0000000040c7f000]
""ForkJoinPool-99-worker-13"" daemon prio=6 tid=0x00000000143a7800 nid=0x808 waiting on condition [0x0000000043c3f000]
""ForkJoinPool-98-worker-1"" daemon prio=6 tid=0x00000000143b0800 nid=0xb88 waiting on condition [0x000000003a92f000]
""ForkJoinPool-98-worker-3"" daemon prio=6 tid=0x00000000143af800 nid=0x171c waiting on condition [0x0000000044cfe000]
""ForkJoinPool-98-worker-5"" daemon prio=6 tid=0x00000000143af000 nid=0x3ef8 waiting on condition [0x0000000044b5f000]
""ForkJoinPool-98-worker-7"" daemon prio=6 tid=0x00000000143ae000 nid=0x27bc waiting on condition [0x000000004498f000]
""ForkJoinPool-98-worker-9"" daemon prio=6 tid=0x00000000143ad800 nid=0x1b90 waiting on condition [0x000000004485f000]
""ForkJoinPool-98-worker-11"" daemon prio=6 tid=0x00000000143a8000 nid=0x2088 waiting on condition [0x000000004472f000]
""ForkJoinPool-98-worker-13"" daemon prio=6 tid=0x0000000012649800 nid=0x39b8 waiting on condition [0x00000000342ae000]
""ForkJoinPool-97-worker-1"" daemon prio=6 tid=0x00000000143ac800 nid=0x3dcc waiting on condition [0x000000004456f000]
""ForkJoinPool-97-worker-5"" daemon prio=6 tid=0x00000000143ac000 nid=0x2544 waiting on condition [0x00000000410df000]
""ForkJoinPool-97-worker-3"" daemon prio=6 tid=0x00000000143a6800 nid=0x3bd0 waiting on condition [0x000000004423f000]
""ForkJoinPool-97-worker-7"" daemon prio=6 tid=0x00000000143aa800 nid=0x31dc waiting on condition [0x00000000434af000]
""ForkJoinPool-97-worker-9"" daemon prio=6 tid=0x00000000143ab000 nid=0x19b4 waiting on condition [0x000000002262f000]
""ForkJoinPool-97-worker-11"" daemon prio=6 tid=0x00000000143a9800 nid=0x30e0 waiting on condition [0x00000000208ef000]
""ForkJoinPool-97-worker-13"" daemon prio=6 tid=0x0000000012d74000 nid=0x28c8 waiting on condition [0x00000000335ae000]
""ForkJoinPool-96-worker-1"" daemon prio=6 tid=0x0000000014399000 nid=0x3c08 waiting on condition [0x00000000394fe000]
""ForkJoinPool-96-worker-3"" daemon prio=6 tid=0x0000000014397000 nid=0x398c waiting on condition [0x000000001a5ae000]
""ForkJoinPool-96-worker-5"" daemon prio=6 tid=0x0000000014398800 nid=0x3a40 waiting on condition [0x00000000336ce000]
""ForkJoinPool-96-worker-7"" daemon prio=6 tid=0x00000000143a2000 nid=0x32b4 waiting on condition [0x000000003075f000]
""ForkJoinPool-96-worker-9"" daemon prio=6 tid=0x00000000143a6000 nid=0x35ec waiting on condition [0x000000002f34f000]
""ForkJoinPool-96-worker-11"" daemon prio=6 tid=0x0000000014397800 nid=0x3dec waiting on condition [0x0000000019baf000]
""ForkJoinPool-96-worker-13"" daemon prio=6 tid=0x00000000134a7000 nid=0x2044 waiting on condition [0x000000002eeef000]
""ForkJoinPool-95-worker-1"" daemon prio=6 tid=0x00000000143a5000 nid=0x3554 waiting on condition [0x000000003770f000]
""ForkJoinPool-95-worker-3"" daemon prio=6 tid=0x00000000143a4800 nid=0x33dc waiting on condition [0x000000004434f000]
""ForkJoinPool-95-worker-5"" daemon prio=6 tid=0x00000000143a3800 nid=0x3334 waiting on condition [0x000000002068f000]
""ForkJoinPool-95-worker-7"" daemon prio=6 tid=0x00000000143a3000 nid=0x38c0 waiting on condition [0x0000000040f7e000]
""ForkJoinPool-95-worker-9"" daemon prio=6 tid=0x00000000143a1800 nid=0x2f44 waiting on condition [0x00000000440bf000]
""ForkJoinPool-95-worker-11"" daemon prio=6 tid=0x00000000143a0800 nid=0x3b20 waiting on condition [0x0000000043eff000]
""ForkJoinPool-95-worker-13"" daemon prio=6 tid=0x00000000143a0000 nid=0x16d0 waiting on condition [0x000000002e81f000]
""ForkJoinPool-94-worker-3"" daemon prio=6 tid=0x000000001439f000 nid=0x1eac waiting on condition [0x00000000439ff000]
""ForkJoinPool-94-worker-1"" daemon prio=6 tid=0x000000001439e800 nid=0x3220 waiting on condition [0x0000000043b2e000]
""ForkJoinPool-94-worker-5"" daemon prio=6 tid=0x000000001439d800 nid=0x183c waiting on condition [0x00000000438cf000]
""ForkJoinPool-94-worker-9"" daemon prio=6 tid=0x000000001439d000 nid=0x38dc waiting on condition [0x00000000437af000]
""ForkJoinPool-94-worker-7"" daemon prio=6 tid=0x000000001439c000 nid=0x1f30 waiting on condition [0x000000004327e000]
""ForkJoinPool-94-worker-11"" daemon prio=6 tid=0x000000001439b800 nid=0x37c0 waiting on condition [0x0000000042e5f000]
""ForkJoinPool-94-worker-13"" daemon prio=6 tid=0x000000001439a800 nid=0x2c10 waiting on condition [0x000000004366f000]
""ForkJoinPool-93-worker-5"" daemon prio=6 tid=0x0000000014396000 nid=0x7f4 waiting on condition [0x000000004315f000]
""ForkJoinPool-93-worker-1"" daemon prio=6 tid=0x0000000014394800 nid=0x18a8 waiting on condition [0x0000000042fef000]
""ForkJoinPool-93-worker-3"" daemon prio=6 tid=0x0000000014393800 nid=0x151c waiting on condition [0x0000000042d3f000]
""ForkJoinPool-93-worker-7"" daemon prio=6 tid=0x0000000014393000 nid=0x3d10 waiting on condition [0x000000001e47f000]
""ForkJoinPool-93-worker-9"" daemon prio=6 tid=0x0000000013946000 nid=0x10d0 waiting on condition [0x0000000042b9f000]
""ForkJoinPool-93-worker-11"" daemon prio=6 tid=0x00000000141a3800 nid=0x3c60 waiting on condition [0x000000001eeef000]
""ForkJoinPool-93-worker-13"" daemon prio=6 tid=0x000000000d013800 nid=0x3eb0 waiting on condition [0x000000001f7ee000]
""ForkJoinPool-92-worker-5"" daemon prio=6 tid=0x0000000012299000 nid=0x2a14 waiting on condition [0x000000001fc2f000]
""ForkJoinPool-92-worker-1"" daemon prio=6 tid=0x0000000013692000 nid=0x38f0 waiting on condition [0x00000000429ef000]
""ForkJoinPool-92-worker-3"" daemon prio=6 tid=0x000000001422f800 nid=0x3aec waiting on condition [0x000000001bb8f000]
""ForkJoinPool-92-worker-7"" daemon prio=6 tid=0x000000000d2de000 nid=0x2748 waiting on condition [0x000000004287f000]
""ForkJoinPool-92-worker-9"" daemon prio=6 tid=0x000000001418d800 nid=0x3024 waiting on condition [0x000000004270f000]
""ForkJoinPool-92-worker-11"" daemon prio=6 tid=0x000000000d647800 nid=0x279c waiting on condition [0x00000000421cf000]
""ForkJoinPool-91-worker-1"" daemon prio=6 tid=0x000000000d969800 nid=0x2250 waiting on condition [0x000000004253f000]
""ForkJoinPool-91-worker-3"" daemon prio=6 tid=0x0000000012b62800 nid=0x2e88 waiting on condition [0x00000000422df000]
""ForkJoinPool-91-worker-7"" daemon prio=6 tid=0x0000000012213800 nid=0x1134 waiting on condition [0x0000000017f5f000]
""ForkJoinPool-91-worker-9"" daemon prio=6 tid=0x000000000f33e000 nid=0x3d90 waiting on condition [0x0000000016c5f000]
""ForkJoinPool-91-worker-11"" daemon prio=6 tid=0x000000000d2ee800 nid=0x3278 waiting on condition [0x000000001a0af000]
""ForkJoinPool-91-worker-13"" daemon prio=6 tid=0x0000000012ea2000 nid=0x23f4 waiting on condition [0x00000000420af000]
""ForkJoinPool-90-worker-1"" daemon prio=6 tid=0x000000001423b000 nid=0x3aac waiting on condition [0x00000000382af000]
""ForkJoinPool-90-worker-3"" daemon prio=6 tid=0x0000000012afa000 nid=0x27d4 waiting on condition [0x0000000028ecf000]
""ForkJoinPool-90-worker-5"" daemon prio=6 tid=0x0000000013d6f000 nid=0x3a08 waiting on condition [0x000000003760f000]
""ForkJoinPool-90-worker-7"" daemon prio=6 tid=0x0000000013d72000 nid=0x1754 waiting on condition [0x000000003615f000]
""ForkJoinPool-90-worker-9"" daemon prio=6 tid=0x0000000013d6d800 nid=0x1fd8 waiting on condition [0x0000000029f6e000]
""ForkJoinPool-90-worker-11"" daemon prio=6 tid=0x000000000d2de800 nid=0x2950 waiting on condition [0x000000001dafe000]
""ForkJoinPool-89-worker-1"" daemon prio=6 tid=0x0000000012140800 nid=0x39ac waiting on condition [0x0000000041f8f000]
""ForkJoinPool-89-worker-3"" daemon prio=6 tid=0x0000000012143800 nid=0x3814 waiting on condition [0x000000003f94f000]
""ForkJoinPool-89-worker-5"" daemon prio=6 tid=0x0000000014ed5800 nid=0x3628 waiting on condition [0x000000003c46f000]
""ForkJoinPool-89-worker-7"" daemon prio=6 tid=0x0000000014ed7000 nid=0x3260 waiting on condition [0x000000003ce8f000]
""ForkJoinPool-89-worker-9"" daemon prio=6 tid=0x000000000d962000 nid=0x1e88 waiting on condition [0x000000002c1cf000]
""ForkJoinPool-89-worker-11"" daemon prio=6 tid=0x0000000012b58000 nid=0x16bc waiting on condition [0x000000003335f000]
""ForkJoinPool-88-worker-5"" daemon prio=6 tid=0x0000000013941800 nid=0x2b14 waiting on condition [0x0000000041d6f000]
""ForkJoinPool-88-worker-3"" daemon prio=6 tid=0x0000000012646800 nid=0x29a4 waiting on condition [0x0000000041bae000]
""ForkJoinPool-88-worker-1"" daemon prio=6 tid=0x0000000012643800 nid=0x2138 waiting on condition [0x000000002aa7f000]
""ForkJoinPool-88-worker-7"" daemon prio=6 tid=0x0000000012649000 nid=0x3b78 waiting on condition [0x000000002c7ef000]
""ForkJoinPool-88-worker-9"" daemon prio=6 tid=0x0000000013d75000 nid=0x31ec waiting on condition [0x000000002428f000]
""ForkJoinPool-88-worker-13"" daemon prio=6 tid=0x0000000013d73800 nid=0x28a8 waiting on condition [0x0000000041a1f000]
""ForkJoinPool-87-worker-7"" daemon prio=6 tid=0x0000000013d71800 nid=0x2aa8 waiting on condition [0x000000004190f000]
""ForkJoinPool-87-worker-1"" daemon prio=6 tid=0x0000000013d70800 nid=0x3040 waiting on condition [0x0000000040a3f000]
""ForkJoinPool-87-worker-3"" daemon prio=6 tid=0x000000000f329000 nid=0x1788 waiting on condition [0x000000003f6df000]
""ForkJoinPool-87-worker-5"" daemon prio=6 tid=0x000000000f339800 nid=0x39b0 waiting on condition [0x000000001c3fe000]
""ForkJoinPool-87-worker-9"" daemon prio=6 tid=0x000000000f322000 nid=0x2b9c waiting on condition [0x000000003fede000]
""ForkJoinPool-87-worker-13"" daemon prio=6 tid=0x0000000013d70000 nid=0x3420 waiting on condition [0x0000000029c1f000]
""ForkJoinPool-86-worker-1"" daemon prio=6 tid=0x0000000013d6d000 nid=0x2b8c waiting on condition [0x00000000407cf000]
""ForkJoinPool-86-worker-5"" daemon prio=6 tid=0x0000000013d6b800 nid=0x3c30 waiting on condition [0x000000004066f000]
""ForkJoinPool-86-worker-7"" daemon prio=6 tid=0x0000000013d6a800 nid=0x2ca4 waiting on condition [0x000000004002e000]
""ForkJoinPool-86-worker-9"" daemon prio=6 tid=0x0000000013d6a000 nid=0x2ca0 waiting on condition [0x000000004055e000]
""ForkJoinPool-86-worker-11"" daemon prio=6 tid=0x0000000013d69000 nid=0x2324 waiting on condition [0x0000000023fff000]
""ForkJoinPool-86-worker-13"" daemon prio=6 tid=0x0000000013d68800 nid=0x2210 waiting on condition [0x000000003fdaf000]
""ForkJoinPool-85-worker-1"" daemon prio=6 tid=0x0000000013d67800 nid=0x121c waiting on condition [0x000000004172f000]
""ForkJoinPool-85-worker-5"" daemon prio=6 tid=0x0000000013d67000 nid=0x3300 waiting on condition [0x000000004157f000]
""ForkJoinPool-85-worker-3"" daemon prio=6 tid=0x0000000013d66000 nid=0x3870 waiting on condition [0x000000004091e000]
""ForkJoinPool-85-worker-7"" daemon prio=6 tid=0x0000000012cd4800 nid=0x249c waiting on condition [0x0000000040b4f000]
""ForkJoinPool-85-worker-9"" daemon prio=6 tid=0x0000000014245800 nid=0x1fe8 waiting on condition [0x00000000413cf000]
""ForkJoinPool-85-worker-11"" daemon prio=6 tid=0x0000000012eb3800 nid=0x1f58 waiting on condition [0x00000000411ef000]
""ForkJoinPool-84-worker-1"" daemon prio=6 tid=0x0000000012298000 nid=0x3488 waiting on condition [0x000000004043f000]
""ForkJoinPool-84-worker-3"" daemon prio=6 tid=0x0000000012d6f800 nid=0x2410 waiting on condition [0x000000004017f000]
""ForkJoinPool-84-worker-5"" daemon prio=6 tid=0x000000000d5d4000 nid=0x1948 waiting on condition [0x000000004027f000]
""ForkJoinPool-84-worker-7"" daemon prio=6 tid=0x00000000134cb000 nid=0x178c waiting on condition [0x000000002225e000]
""ForkJoinPool-84-worker-9"" daemon prio=6 tid=0x00000000134cc000 nid=0x3cc4 waiting on condition [0x000000003fc9f000]
""ForkJoinPool-84-worker-11"" daemon prio=6 tid=0x0000000014072800 nid=0x25dc waiting on condition [0x000000003fb9f000]
""ForkJoinPool-83-worker-1"" daemon prio=6 tid=0x0000000012ccb800 nid=0x2e6c waiting on condition [0x000000002307f000]
""ForkJoinPool-83-worker-3"" daemon prio=6 tid=0x0000000012cd0000 nid=0x3ac4 waiting on condition [0x000000003324e000]
""ForkJoinPool-83-worker-5"" daemon prio=6 tid=0x000000000d2fa800 nid=0x26e4 waiting on condition [0x000000001c1ef000]
""ForkJoinPool-83-worker-7"" daemon prio=6 tid=0x000000000d2fa000 nid=0x3f48 waiting on condition [0x00000000298bf000]
""ForkJoinPool-83-worker-11"" daemon prio=6 tid=0x000000000d2f8800 nid=0x3448 waiting on condition [0x0000000023c8f000]
""ForkJoinPool-83-worker-13"" daemon prio=6 tid=0x000000000d2f4000 nid=0x3f30 waiting on condition [0x000000001ec4f000]
""ForkJoinPool-82-worker-1"" daemon prio=6 tid=0x000000000d2f7000 nid=0x3384 waiting on condition [0x000000003f7fe000]
""ForkJoinPool-82-worker-5"" daemon prio=6 tid=0x000000000d2f6000 nid=0x37c4 waiting on condition [0x000000003f4cf000]
""ForkJoinPool-82-worker-3"" daemon prio=6 tid=0x000000000d2f5800 nid=0x2d04 waiting on condition [0x000000003f5cf000]
""ForkJoinPool-82-worker-7"" daemon prio=6 tid=0x000000000d2eb800 nid=0x3514 waiting on condition [0x000000003f3bf000]
""ForkJoinPool-82-worker-11"" daemon prio=6 tid=0x000000000d2f4800 nid=0x2b18 waiting on condition [0x000000003f1df000]
""ForkJoinPool-82-worker-13"" daemon prio=6 tid=0x000000000d2ec800 nid=0x2610 waiting on condition [0x000000003ee2f000]
""ForkJoinPool-81-worker-3"" daemon prio=6 tid=0x000000000d2f2800 nid=0x3618 waiting on condition [0x000000003f09f000]
""ForkJoinPool-81-worker-5"" daemon prio=6 tid=0x000000000d2f1000 nid=0x1564 waiting on condition [0x000000003ef3e000]
""ForkJoinPool-81-worker-1"" daemon prio=6 tid=0x000000000d2f0000 nid=0x283c waiting on condition [0x000000003e69e000]
""ForkJoinPool-81-worker-7"" daemon prio=6 tid=0x000000000d2ef800 nid=0x2098 waiting on condition [0x000000003ed0f000]
""ForkJoinPool-81-worker-9"" daemon prio=6 tid=0x000000000d2ee000 nid=0x3714 waiting on condition [0x00000000329cf000]
""ForkJoinPool-81-worker-11"" daemon prio=6 tid=0x000000000d2ed000 nid=0x3b70 waiting on condition [0x000000001938f000]
""ForkJoinPool-80-worker-3"" daemon prio=6 tid=0x000000000d2ea000 nid=0x234c waiting on condition [0x000000003eb9e000]
""ForkJoinPool-80-worker-1"" daemon prio=6 tid=0x000000000d2e9800 nid=0x3124 waiting on condition [0x000000002dbef000]
""ForkJoinPool-80-worker-5"" daemon prio=6 tid=0x000000000d2e8800 nid=0x3eec waiting on condition [0x0000000019f8f000]
""ForkJoinPool-80-worker-9"" daemon prio=6 tid=0x000000000d2e8000 nid=0x231c waiting on condition [0x000000002811f000]
""ForkJoinPool-80-worker-11"" daemon prio=6 tid=0x000000000d2e7000 nid=0x220c waiting on condition [0x000000001c0de000]
""ForkJoinPool-80-worker-13"" daemon prio=6 tid=0x000000000d2e6800 nid=0x3f14 waiting on condition [0x0000000027c2f000]
""ForkJoinPool-79-worker-7"" daemon prio=6 tid=0x000000000d2e5000 nid=0xe2c waiting on condition [0x000000003ea9f000]
""ForkJoinPool-79-worker-3"" daemon prio=6 tid=0x000000000d2e4000 nid=0x3d80 waiting on condition [0x000000003e97f000]
""ForkJoinPool-79-worker-1"" daemon prio=6 tid=0x000000000d2e3800 nid=0x1650 waiting on condition [0x000000003e79f000]
""ForkJoinPool-79-worker-5"" daemon prio=6 tid=0x000000000d2e2800 nid=0x3aa4 waiting on condition [0x00000000317bf000]
""ForkJoinPool-79-worker-9"" daemon prio=6 tid=0x000000000d2e1800 nid=0x393c waiting on condition [0x000000003e56f000]
""ForkJoinPool-79-worker-11"" daemon prio=6 tid=0x000000000d2e1000 nid=0x2344 waiting on condition [0x000000003cc0f000]
""ForkJoinPool-78-worker-3"" daemon prio=6 tid=0x000000000d2dd000 nid=0x3914 waiting on condition [0x000000003e32e000]
""ForkJoinPool-78-worker-1"" daemon prio=6 tid=0x000000000d2dc800 nid=0x229c waiting on condition [0x00000000182af000]
""ForkJoinPool-78-worker-5"" daemon prio=6 tid=0x000000000d2db800 nid=0x1ebc waiting on condition [0x000000003e20e000]
""ForkJoinPool-78-worker-7"" daemon prio=6 tid=0x0000000012146000 nid=0x311c waiting on condition [0x000000002ca9f000]
""ForkJoinPool-78-worker-9"" daemon prio=6 tid=0x00000000134ad000 nid=0x1530 waiting on condition [0x000000003e00f000]
""ForkJoinPool-78-worker-11"" daemon prio=6 tid=0x0000000012eaf000 nid=0x37b0 waiting on condition [0x000000002536f000]
""ForkJoinPool-77-worker-1"" daemon prio=6 tid=0x000000001423a000 nid=0x3014 waiting on condition [0x000000003d4ef000]
""ForkJoinPool-77-worker-3"" daemon prio=6 tid=0x0000000012cd1800 nid=0x21b4 waiting on condition [0x000000003e10f000]
""ForkJoinPool-77-worker-5"" daemon prio=6 tid=0x00000000129d1000 nid=0x38b4 waiting on condition [0x000000003deaf000]
""ForkJoinPool-77-worker-7"" daemon prio=6 tid=0x0000000012216800 nid=0x13d8 waiting on condition [0x000000003ddaf000]
""ForkJoinPool-77-worker-9"" daemon prio=6 tid=0x00000000121ee000 nid=0x2254 waiting on condition [0x000000003dc7e000]
""ForkJoinPool-77-worker-13"" daemon prio=6 tid=0x0000000012b61000 nid=0x1d04 waiting on condition [0x000000003da9f000]
""ForkJoinPool-76-worker-1"" daemon prio=6 tid=0x0000000012d78000 nid=0x2edc waiting on condition [0x000000003d98e000]
""ForkJoinPool-76-worker-3"" daemon prio=6 tid=0x0000000012ea0800 nid=0x52c waiting on condition [0x000000003d72f000]
""ForkJoinPool-76-worker-5"" daemon prio=6 tid=0x0000000013ce7000 nid=0x4d4 waiting on condition [0x000000003d84e000]
""ForkJoinPool-76-worker-7"" daemon prio=6 tid=0x000000000d961800 nid=0xa00 waiting on condition [0x000000003d5ef000]
""ForkJoinPool-76-worker-11"" daemon prio=6 tid=0x000000000d9e6000 nid=0x19c0 waiting on condition [0x000000003d26f000]
""ForkJoinPool-76-worker-13"" daemon prio=6 tid=0x00000000134a6000 nid=0x34e8 waiting on condition [0x000000003d15f000]
""ForkJoinPool-75-worker-1"" daemon prio=6 tid=0x0000000012b5a000 nid=0x29d0 waiting on condition [0x000000003d01f000]
""ForkJoinPool-75-worker-5"" daemon prio=6 tid=0x0000000014180000 nid=0x2030 waiting on condition [0x000000003cd8f000]
""ForkJoinPool-75-worker-3"" daemon prio=6 tid=0x000000001418b000 nid=0x2978 waiting on condition [0x000000001f42f000]
""ForkJoinPool-75-worker-7"" daemon prio=6 tid=0x0000000013ce2800 nid=0x2e38 waiting on condition [0x000000003085f000]
""ForkJoinPool-75-worker-9"" daemon prio=6 tid=0x0000000013ce5800 nid=0x2754 waiting on condition [0x000000003c30f000]
""ForkJoinPool-75-worker-11"" daemon prio=6 tid=0x0000000012e9d800 nid=0x3d24 waiting on condition [0x0000000026faf000]
""ForkJoinPool-74-worker-1"" daemon prio=6 tid=0x000000000f33d000 nid=0x2aa0 waiting on condition [0x000000003cacf000]
""ForkJoinPool-74-worker-3"" daemon prio=6 tid=0x000000000f33c800 nid=0x23ac waiting on condition [0x0000000037adf000]
""ForkJoinPool-74-worker-5"" daemon prio=6 tid=0x000000000f33b800 nid=0x3bb8 waiting on condition [0x000000002fd1f000]
""ForkJoinPool-74-worker-7"" daemon prio=6 tid=0x000000000f33b000 nid=0x3a94 waiting on condition [0x000000002439f000]
""ForkJoinPool-74-worker-9"" daemon prio=6 tid=0x000000000f333800 nid=0x1700 waiting on condition [0x00000000219ce000]
""ForkJoinPool-74-worker-11"" daemon prio=6 tid=0x000000000f328000 nid=0x3668 waiting on condition [0x000000002505e000]
""ForkJoinPool-73-worker-1"" daemon prio=6 tid=0x000000000f338800 nid=0x2048 waiting on condition [0x000000003c98f000]
""ForkJoinPool-73-worker-5"" daemon prio=6 tid=0x000000000f338000 nid=0x1140 waiting on condition [0x000000003bfaf000]
""ForkJoinPool-73-worker-3"" daemon prio=6 tid=0x000000000f337000 nid=0x10b0 waiting on condition [0x000000003c82f000]
""ForkJoinPool-73-worker-7"" daemon prio=6 tid=0x000000000f336800 nid=0x299c waiting on condition [0x000000003c68e000]
""ForkJoinPool-73-worker-9"" daemon prio=6 tid=0x000000000f335800 nid=0x30cc waiting on condition [0x000000003c58f000]
""ForkJoinPool-73-worker-11"" daemon prio=6 tid=0x000000000f335000 nid=0x1814 waiting on condition [0x0000000020dbf000]
""ForkJoinPool-72-worker-1"" daemon prio=6 tid=0x000000000f332800 nid=0x3998 waiting on condition [0x000000003c1fe000]
""ForkJoinPool-72-worker-3"" daemon prio=6 tid=0x000000000f332000 nid=0x2cc8 waiting on condition [0x000000003c0bf000]
""ForkJoinPool-72-worker-5"" daemon prio=6 tid=0x000000000f331000 nid=0x3c6c waiting on condition [0x0000000024e3f000]
""ForkJoinPool-72-worker-7"" daemon prio=6 tid=0x000000000f330800 nid=0x30a4 waiting on condition [0x000000002a38f000]
""ForkJoinPool-72-worker-9"" daemon prio=6 tid=0x000000000f32f800 nid=0x29d8 waiting on condition [0x000000003a3ff000]
""ForkJoinPool-72-worker-11"" daemon prio=6 tid=0x000000000f32f000 nid=0x3ed8 waiting on condition [0x000000002356f000]
""ForkJoinPool-71-worker-5"" daemon prio=6 tid=0x000000000f32e000 nid=0x33d0 waiting on condition [0x000000003beaf000]
""ForkJoinPool-71-worker-1"" daemon prio=6 tid=0x000000000f32d800 nid=0x1488 waiting on condition [0x000000003bd8f000]
""ForkJoinPool-71-worker-3"" daemon prio=6 tid=0x000000000f32c800 nid=0x3da8 waiting on condition [0x000000003bbcf000]
""ForkJoinPool-71-worker-7"" daemon prio=6 tid=0x000000000f32b000 nid=0x3f2c waiting on condition [0x000000003ba2f000]
""ForkJoinPool-71-worker-9"" daemon prio=6 tid=0x000000000f32a800 nid=0x221c waiting on condition [0x000000003b52e000]
""ForkJoinPool-71-worker-11"" daemon prio=6 tid=0x000000000f329800 nid=0x30bc waiting on condition [0x000000002387f000]
""ForkJoinPool-70-worker-1"" daemon prio=6 tid=0x000000000f326800 nid=0x21e0 waiting on condition [0x000000003b88f000]
""ForkJoinPool-70-worker-3"" daemon prio=6 tid=0x000000000f326000 nid=0x2fe4 waiting on condition [0x000000003b74f000]
""ForkJoinPool-70-worker-5"" daemon prio=6 tid=0x000000000f325000 nid=0x3df4 waiting on condition [0x000000003b63e000]
""ForkJoinPool-70-worker-7"" daemon prio=6 tid=0x000000000f320000 nid=0x2780 waiting on condition [0x000000003b41f000]
""ForkJoinPool-70-worker-11"" daemon prio=6 tid=0x000000000f323800 nid=0x2b5c waiting on condition [0x000000003b31f000]
""ForkJoinPool-70-worker-13"" daemon prio=6 tid=0x000000000f323000 nid=0x3ce0 waiting on condition [0x000000003b1ff000]
""ForkJoinPool-69-worker-3"" daemon prio=6 tid=0x000000000c64e800 nid=0x1cdc waiting on condition [0x0000000039b9f000]
""ForkJoinPool-69-worker-7"" daemon prio=6 tid=0x000000000f31f000 nid=0x2f98 waiting on condition [0x000000003af2f000]
""ForkJoinPool-69-worker-1"" daemon prio=6 tid=0x000000000d010800 nid=0x34fc waiting on condition [0x000000003adaf000]
""ForkJoinPool-69-worker-5"" daemon prio=6 tid=0x000000000d5d1000 nid=0x332c waiting on condition [0x000000003abde000]
""ForkJoinPool-69-worker-9"" daemon prio=6 tid=0x0000000013cdf000 nid=0x3a84 waiting on condition [0x000000003aa7e000]
""ForkJoinPool-69-worker-11"" daemon prio=6 tid=0x0000000012af9800 nid=0x180c waiting on condition [0x000000003a59f000]
""ForkJoinPool-68-worker-5"" daemon prio=6 tid=0x000000001424b000 nid=0x13fc waiting on condition [0x000000003a2ff000]
""ForkJoinPool-68-worker-3"" daemon prio=6 tid=0x000000001424a000 nid=0x3258 waiting on condition [0x0000000039d5f000]
""ForkJoinPool-68-worker-7"" daemon prio=6 tid=0x0000000012d71000 nid=0x13d4 waiting on condition [0x000000003a1bf000]
""ForkJoinPool-68-worker-9"" daemon prio=6 tid=0x0000000012d72000 nid=0x15a4 waiting on condition [0x0000000032c3f000]
""ForkJoinPool-68-worker-11"" daemon prio=6 tid=0x0000000012210000 nid=0x33a0 waiting on condition [0x00000000373ef000]
""ForkJoinPool-68-worker-13"" daemon prio=6 tid=0x0000000012216000 nid=0x31a0 waiting on condition [0x0000000039ffe000]
""ForkJoinPool-67-worker-3"" daemon prio=6 tid=0x000000001424e800 nid=0x3c54 waiting on condition [0x000000003991f000]
""ForkJoinPool-67-worker-7"" daemon prio=6 tid=0x000000001424d000 nid=0x20f4 waiting on condition [0x00000000397af000]
""ForkJoinPool-67-worker-5"" daemon prio=6 tid=0x000000001424c800 nid=0x3d04 waiting on condition [0x000000003969e000]
""ForkJoinPool-67-worker-9"" daemon prio=6 tid=0x000000001424b800 nid=0x252c waiting on condition [0x0000000038c0e000]
""ForkJoinPool-67-worker-11"" daemon prio=6 tid=0x0000000014241000 nid=0x1fdc waiting on condition [0x00000000202bf000]
""ForkJoinPool-66-worker-3"" daemon prio=6 tid=0x0000000014248800 nid=0x217c waiting on condition [0x000000003919e000]
""ForkJoinPool-66-worker-5"" daemon prio=6 tid=0x0000000014248000 nid=0x350c waiting on condition [0x0000000038fdf000]
""ForkJoinPool-66-worker-7"" daemon prio=6 tid=0x0000000014247000 nid=0x37b4 waiting on condition [0x00000000229ef000]
""ForkJoinPool-66-worker-9"" daemon prio=6 tid=0x0000000014245000 nid=0x23a8 waiting on condition [0x000000002bc2f000]
""ForkJoinPool-66-worker-11"" daemon prio=6 tid=0x0000000014242000 nid=0x3324 waiting on condition [0x0000000038ede000]
""ForkJoinPool-65-worker-3"" daemon prio=6 tid=0x0000000014243800 nid=0x21b8 waiting on condition [0x00000000389ae000]
""ForkJoinPool-65-worker-5"" daemon prio=6 tid=0x0000000014162000 nid=0xb78 waiting on condition [0x00000000212ff000]
""ForkJoinPool-65-worker-7"" daemon prio=6 tid=0x0000000014242800 nid=0x3cc8 waiting on condition [0x000000003817f000]
""ForkJoinPool-65-worker-9"" daemon prio=6 tid=0x0000000014234000 nid=0x3d38 waiting on condition [0x0000000037ffe000]
""ForkJoinPool-65-worker-11"" daemon prio=6 tid=0x0000000014238800 nid=0x310c waiting on condition [0x00000000369bf000]
""ForkJoinPool-64-worker-1"" daemon prio=6 tid=0x0000000014240800 nid=0x3440 waiting on condition [0x00000000388af000]
""ForkJoinPool-64-worker-5"" daemon prio=6 tid=0x000000001423e000 nid=0x34ac waiting on condition [0x00000000387af000]
""ForkJoinPool-64-worker-7"" daemon prio=6 tid=0x000000001423d800 nid=0x3494 waiting on condition [0x00000000385af000]
""ForkJoinPool-64-worker-9"" daemon prio=6 tid=0x000000001423c800 nid=0x1b68 waiting on condition [0x00000000383fe000]
""ForkJoinPool-64-worker-11"" daemon prio=6 tid=0x000000001423b800 nid=0x3f3c waiting on condition [0x0000000036fcf000]
""ForkJoinPool-63-worker-3"" daemon prio=6 tid=0x0000000014239800 nid=0x25c4 waiting on condition [0x000000001fd7e000]
""ForkJoinPool-63-worker-1"" daemon prio=6 tid=0x0000000014237000 nid=0x2900 waiting on condition [0x0000000037d7e000]
""ForkJoinPool-63-worker-5"" daemon prio=6 tid=0x0000000014236800 nid=0x3348 waiting on condition [0x00000000201bf000]
""ForkJoinPool-63-worker-9"" daemon prio=6 tid=0x0000000014235800 nid=0x22c8 waiting on condition [0x00000000323ef000]
""ForkJoinPool-63-worker-11"" daemon prio=6 tid=0x0000000014235000 nid=0x33f0 waiting on condition [0x0000000028c8e000]
""ForkJoinPool-62-worker-3"" daemon prio=6 tid=0x0000000014232800 nid=0x38b8 waiting on condition [0x00000000374ff000]
""ForkJoinPool-62-worker-5"" daemon prio=6 tid=0x0000000014232000 nid=0x2bcc waiting on condition [0x0000000037bde000]
""ForkJoinPool-62-worker-7"" daemon prio=6 tid=0x0000000014231000 nid=0x26b8 waiting on condition [0x00000000379df000]
""ForkJoinPool-62-worker-9"" daemon prio=6 tid=0x0000000014230800 nid=0x24cc waiting on condition [0x000000003780f000]
""ForkJoinPool-62-worker-13"" daemon prio=6 tid=0x0000000012299800 nid=0x3708 waiting on condition [0x00000000372cf000]
""block-manager-slave-async-thread-pool-4"" daemon prio=6 tid=0x0000000012b5b000 nid=0x36ac waiting on condition [0x00000000299ce000]
""ForkJoinPool-61-worker-3"" daemon prio=6 tid=0x00000000141a5000 nid=0x2a68 waiting on condition [0x00000000370df000]
""ForkJoinPool-61-worker-1"" daemon prio=6 tid=0x00000000134ce000 nid=0xcf4 waiting on condition [0x0000000036ecf000]
""ForkJoinPool-61-worker-7"" daemon prio=6 tid=0x0000000013ce0800 nid=0x27a8 waiting on condition [0x0000000036c8f000]
""ForkJoinPool-61-worker-9"" daemon prio=6 tid=0x0000000014ed9800 nid=0x3b74 waiting on condition [0x0000000036ace000]
""ForkJoinPool-61-worker-11"" daemon prio=6 tid=0x0000000014160000 nid=0x19f0 waiting on condition [0x00000000368af000]
""ForkJoinPool-60-worker-1"" daemon prio=6 tid=0x000000000d649000 nid=0x3f0 waiting on condition [0x000000001be4e000]
""ForkJoinPool-60-worker-5"" daemon prio=6 tid=0x000000000d64b800 nid=0x1ec0 waiting on condition [0x000000003673e000]
""ForkJoinPool-60-worker-3"" daemon prio=6 tid=0x0000000012b64000 nid=0x1f8c waiting on condition [0x00000000365bf000]
""ForkJoinPool-60-worker-7"" daemon prio=6 tid=0x0000000012b63000 nid=0x1504 waiting on condition [0x000000003604e000]
""ForkJoinPool-60-worker-13"" daemon prio=6 tid=0x0000000012b61800 nid=0x2a4c waiting on condition [0x00000000274ff000]
""ForkJoinPool-59-worker-1"" daemon prio=6 tid=0x0000000012b60000 nid=0x2808 waiting on condition [0x000000003645e000]
""ForkJoinPool-59-worker-7"" daemon prio=6 tid=0x0000000012b5f800 nid=0x3f74 waiting on condition [0x00000000362af000]
""ForkJoinPool-59-worker-3"" daemon prio=6 tid=0x0000000012b5e800 nid=0x11f8 waiting on condition [0x000000001f07e000]
""ForkJoinPool-59-worker-5"" daemon prio=6 tid=0x0000000012b5e000 nid=0x27e0 waiting on condition [0x0000000017a3e000]
""ForkJoinPool-59-worker-13"" daemon prio=6 tid=0x0000000012b59800 nid=0x2d74 waiting on condition [0x000000002235f000]
""ForkJoinPool-58-worker-1"" daemon prio=6 tid=0x0000000012b58800 nid=0x1d44 waiting on condition [0x0000000035ecf000]
""ForkJoinPool-58-worker-5"" daemon prio=6 tid=0x0000000012b56800 nid=0x39cc waiting on condition [0x0000000035c6f000]
""ForkJoinPool-58-worker-7"" daemon prio=6 tid=0x0000000012b55800 nid=0x32dc waiting on condition [0x000000003515e000]
""ForkJoinPool-58-worker-9"" daemon prio=6 tid=0x0000000012b55000 nid=0x1c9c waiting on condition [0x0000000035b5f000]
""ForkJoinPool-58-worker-11"" daemon prio=6 tid=0x0000000012d7b000 nid=0x38d4 waiting on condition [0x000000003588f000]
""ForkJoinPool-57-worker-5"" daemon prio=6 tid=0x0000000012ccc800 nid=0x14f8 waiting on condition [0x000000003546f000]
""ForkJoinPool-57-worker-3"" daemon prio=6 tid=0x00000000141a6800 nid=0x2508 waiting on condition [0x000000003576e000]
""ForkJoinPool-57-worker-7"" daemon prio=6 tid=0x0000000012d70800 nid=0x1fcc waiting on condition [0x00000000355be000]
""ForkJoinPool-57-worker-9"" daemon prio=6 tid=0x0000000013693800 nid=0x3684 waiting on condition [0x000000003533e000]
""ForkJoinPool-57-worker-13"" daemon prio=6 tid=0x000000000d9e7800 nid=0x3f18 waiting on condition [0x000000003314f000]
""ForkJoinPool-56-worker-3"" daemon prio=6 tid=0x00000000134a5800 nid=0x32ec waiting on condition [0x000000003503f000]
""ForkJoinPool-56-worker-7"" daemon prio=6 tid=0x00000000134a4000 nid=0x9bc waiting on condition [0x0000000034d8f000]
""ForkJoinPool-56-worker-1"" daemon prio=6 tid=0x000000000d9ea000 nid=0x1a3c waiting on condition [0x0000000034b9e000]
""ForkJoinPool-56-worker-5"" daemon prio=6 tid=0x000000000d9f1800 nid=0x2d6c waiting on condition [0x000000002d44f000]
""ForkJoinPool-56-worker-9"" daemon prio=6 tid=0x0000000012eaf800 nid=0x2f00 waiting on condition [0x000000003406f000]
""ForkJoinPool-55-worker-5"" daemon prio=6 tid=0x0000000012d7b800 nid=0x3d14 waiting on condition [0x000000003493f000]
""ForkJoinPool-55-worker-3"" daemon prio=6 tid=0x0000000012d7a000 nid=0x1a2c waiting on condition [0x000000002d58f000]
""ForkJoinPool-55-worker-9"" daemon prio=6 tid=0x0000000012d79800 nid=0xa20 waiting on condition [0x00000000339ff000]
""ForkJoinPool-55-worker-7"" daemon prio=6 tid=0x0000000012d78800 nid=0x3e4c waiting on condition [0x00000000337cf000]
""ForkJoinPool-55-worker-13"" daemon prio=6 tid=0x0000000012d6f000 nid=0x2700 waiting on condition [0x000000001c7be000]
""ForkJoinPool-54-worker-1"" daemon prio=6 tid=0x0000000012d77000 nid=0x37a8 waiting on condition [0x000000003455f000]
""ForkJoinPool-54-worker-3"" daemon prio=6 tid=0x0000000012d76800 nid=0x13f8 waiting on condition [0x000000003480f000]
""ForkJoinPool-54-worker-5"" daemon prio=6 tid=0x0000000012d75800 nid=0x23ec waiting on condition [0x00000000346bf000]
""ForkJoinPool-54-worker-7"" daemon prio=6 tid=0x0000000012d75000 nid=0xa3c waiting on condition [0x000000003445f000]
""ForkJoinPool-54-worker-13"" daemon prio=6 tid=0x0000000012d72800 nid=0x23dc waiting on condition [0x000000003418f000]
""ForkJoinPool-53-worker-5"" daemon prio=6 tid=0x0000000012d6d800 nid=0x2c38 waiting on condition [0x0000000033f1f000]
""ForkJoinPool-53-worker-1"" daemon prio=6 tid=0x000000000d968000 nid=0x3eac waiting on condition [0x0000000033e1e000]
""ForkJoinPool-53-worker-3"" daemon prio=6 tid=0x00000000121ef000 nid=0x2aa4 waiting on condition [0x0000000033cfe000]
""ForkJoinPool-53-worker-7"" daemon prio=6 tid=0x000000001407c000 nid=0x10c8 waiting on condition [0x0000000033b1f000]
""ForkJoinPool-53-worker-13"" daemon prio=6 tid=0x0000000013943000 nid=0x343c waiting on condition [0x00000000334ae000]
""ForkJoinPool-52-worker-1"" daemon prio=6 tid=0x0000000013cdc800 nid=0x39c0 waiting on condition [0x000000003304f000]
""ForkJoinPool-52-worker-3"" daemon prio=6 tid=0x0000000013cdd800 nid=0x3f0c waiting on condition [0x000000001b81e000]
""ForkJoinPool-52-worker-5"" daemon prio=6 tid=0x0000000014161800 nid=0x39ec waiting on condition [0x0000000032e8f000]
""ForkJoinPool-52-worker-7"" daemon prio=6 tid=0x0000000014160800 nid=0x3154 waiting on condition [0x000000002d94f000]
""ForkJoinPool-52-worker-13"" daemon prio=6 tid=0x0000000012eae000 nid=0x3748 waiting on condition [0x00000000291ef000]
""ForkJoinPool-51-worker-5"" daemon prio=6 tid=0x00000000134ae800 nid=0x2d98 waiting on condition [0x000000003280f000]
""ForkJoinPool-51-worker-3"" daemon prio=6 tid=0x00000000134ac000 nid=0x2f94 waiting on condition [0x00000000325cf000]
""ForkJoinPool-51-worker-7"" daemon prio=6 tid=0x00000000134a4800 nid=0x2d90 waiting on condition [0x00000000326cf000]
""ForkJoinPool-51-worker-9"" daemon prio=6 tid=0x00000000134ad800 nid=0x38d0 waiting on condition [0x000000002579e000]
""ForkJoinPool-51-worker-11"" daemon prio=6 tid=0x0000000012eb4000 nid=0x2b7c waiting on condition [0x000000003204f000]
""ForkJoinPool-50-worker-1"" daemon prio=6 tid=0x0000000012eb2800 nid=0x36e0 waiting on condition [0x00000000322ef000]
""ForkJoinPool-50-worker-3"" daemon prio=6 tid=0x0000000012eb2000 nid=0x3844 waiting on condition [0x000000003215e000]
""ForkJoinPool-50-worker-7"" daemon prio=6 tid=0x0000000012eb0800 nid=0x1894 waiting on condition [0x0000000031edf000]
""ForkJoinPool-50-worker-11"" daemon prio=6 tid=0x0000000012eab000 nid=0x392c waiting on condition [0x0000000031c4f000]
""ForkJoinPool-50-worker-13"" daemon prio=6 tid=0x0000000012ead800 nid=0x10bc waiting on condition [0x0000000031b0e000]
""ForkJoinPool-49-worker-1"" daemon prio=6 tid=0x0000000012eac000 nid=0x3294 waiting on condition [0x0000000031a0e000]
""ForkJoinPool-49-worker-7"" daemon prio=6 tid=0x0000000012eaa800 nid=0x103c waiting on condition [0x000000003169f000]
""ForkJoinPool-49-worker-5"" daemon prio=6 tid=0x0000000012ea6800 nid=0x3058 waiting on condition [0x0000000019dae000]
""ForkJoinPool-49-worker-9"" daemon prio=6 tid=0x0000000012ea8000 nid=0x3b4c waiting on condition [0x000000002f90f000]
""ForkJoinPool-49-worker-11"" daemon prio=6 tid=0x0000000012ea1800 nid=0x2060 waiting on condition [0x000000001d6af000]
""ForkJoinPool-48-worker-3"" daemon prio=6 tid=0x0000000012ea6000 nid=0x2318 waiting on condition [0x0000000022e4f000]
""ForkJoinPool-48-worker-1"" daemon prio=6 tid=0x0000000012ea5000 nid=0x1888 waiting on condition [0x00000000314bf000]
""ForkJoinPool-48-worker-5"" daemon prio=6 tid=0x0000000012ea4800 nid=0x2790 waiting on condition [0x000000002465e000]
""ForkJoinPool-48-worker-7"" daemon prio=6 tid=0x0000000012ea3800 nid=0x2d10 waiting on condition [0x0000000019a5f000]
""ForkJoinPool-48-worker-9"" daemon prio=6 tid=0x0000000012ea3000 nid=0x3b40 waiting on condition [0x000000002be5e000]
""ForkJoinPool-47-worker-1"" daemon prio=6 tid=0x0000000012ea0000 nid=0x10b8 waiting on condition [0x00000000313bf000]
""ForkJoinPool-47-worker-3"" daemon prio=6 tid=0x0000000012e9f000 nid=0x315c waiting on condition [0x0000000030e3f000]
""ForkJoinPool-47-worker-5"" daemon prio=6 tid=0x0000000012e9e800 nid=0x2a54 waiting on condition [0x00000000312ae000]
""ForkJoinPool-47-worker-7"" daemon prio=6 tid=0x0000000014163000 nid=0xba8 waiting on condition [0x000000002ab9f000]
""ForkJoinPool-47-worker-11"" daemon prio=6 tid=0x0000000012e9c800 nid=0x2a84 waiting on condition [0x000000002907f000]
""ForkJoinPool-46-worker-1"" daemon prio=6 tid=0x0000000012e99000 nid=0x1ae4 waiting on condition [0x0000000030f4f000]
""ForkJoinPool-46-worker-3"" daemon prio=6 tid=0x0000000012e98000 nid=0x2f58 waiting on condition [0x0000000030cee000]
""ForkJoinPool-46-worker-7"" daemon prio=6 tid=0x0000000012e97800 nid=0x2ee4 waiting on condition [0x000000003096e000]
""ForkJoinPool-46-worker-9"" daemon prio=6 tid=0x0000000012e96800 nid=0x2ffc waiting on condition [0x0000000030bcf000]
""ForkJoinPool-46-worker-11"" daemon prio=6 tid=0x0000000012e96000 nid=0x3adc waiting on condition [0x0000000030abf000]
""ForkJoinPool-45-worker-1"" daemon prio=6 tid=0x0000000014182000 nid=0x2d48 waiting on condition [0x000000003053f000]
""ForkJoinPool-45-worker-3"" daemon prio=6 tid=0x000000001418a800 nid=0x3564 waiting on condition [0x000000003041e000]
""ForkJoinPool-45-worker-7"" daemon prio=6 tid=0x000000000d9f0000 nid=0x4e8 waiting on condition [0x00000000301ef000]
""ForkJoinPool-45-worker-9"" daemon prio=6 tid=0x000000000d9f1000 nid=0x2564 waiting on condition [0x000000003003f000]
""ForkJoinPool-45-worker-11"" daemon prio=6 tid=0x00000000134ab800 nid=0x2f38 waiting on condition [0x000000002fe5f000]
""ForkJoinPool-44-worker-3"" daemon prio=6 tid=0x00000000134aa800 nid=0x1d80 waiting on condition [0x000000002fb7f000]
""ForkJoinPool-44-worker-1"" daemon prio=6 tid=0x00000000134aa000 nid=0x3954 waiting on condition [0x000000002fa3e000]
""ForkJoinPool-44-worker-5"" daemon prio=6 tid=0x00000000134a9000 nid=0x265c waiting on condition [0x0000000021adf000]
""ForkJoinPool-44-worker-7"" daemon prio=6 tid=0x00000000134a8800 nid=0x3dd0 waiting on condition [0x000000002f7ff000]
""ForkJoinPool-44-worker-9"" daemon prio=6 tid=0x00000000134a7800 nid=0x2994 waiting on condition [0x000000002f61f000]
""ForkJoinPool-43-worker-1"" daemon prio=6 tid=0x00000000134a3000 nid=0x3198 waiting on condition [0x000000002f1ae000]
""ForkJoinPool-43-worker-3"" daemon prio=6 tid=0x00000000134a2800 nid=0x235c waiting on condition [0x000000002f0ae000]
""ForkJoinPool-43-worker-7"" daemon prio=6 tid=0x00000000134a1000 nid=0x23cc waiting on condition [0x000000002ed3f000]
""ForkJoinPool-43-worker-9"" daemon prio=6 tid=0x00000000134a0000 nid=0x1708 waiting on condition [0x000000002ec0f000]
""ForkJoinPool-43-worker-11"" daemon prio=6 tid=0x000000001349f800 nid=0x3138 waiting on condition [0x000000002e5bf000]
""ForkJoinPool-42-worker-3"" daemon prio=6 tid=0x00000000134cc800 nid=0x257c waiting on condition [0x000000002eace000]
""ForkJoinPool-42-worker-1"" daemon prio=6 tid=0x000000000d5d0800 nid=0x1388 waiting on condition [0x000000002e2ff000]
""ForkJoinPool-42-worker-7"" daemon prio=6 tid=0x0000000013942800 nid=0x3be4 waiting on condition [0x000000002e95f000]
""ForkJoinPool-42-worker-9"" daemon prio=6 tid=0x000000000d9e7000 nid=0x2e64 waiting on condition [0x000000001c2ff000]
""ForkJoinPool-42-worker-11"" daemon prio=6 tid=0x000000000d9e5800 nid=0x2240 waiting on condition [0x000000001eadf000]
""ForkJoinPool-41-worker-1"" daemon prio=6 tid=0x0000000014180800 nid=0x3de4 waiting on condition [0x000000002e1af000]
""ForkJoinPool-41-worker-3"" daemon prio=6 tid=0x000000000d00e800 nid=0x1734 waiting on condition [0x000000002c5df000]
""ForkJoinPool-41-worker-5"" daemon prio=6 tid=0x000000000d9f3000 nid=0x3634 waiting on condition [0x000000002e6bf000]
""ForkJoinPool-41-worker-9"" daemon prio=6 tid=0x000000000d9f2800 nid=0x10f4 waiting on condition [0x000000002d7af000]
""ForkJoinPool-41-worker-11"" daemon prio=6 tid=0x000000000d9e8800 nid=0x1510 waiting on condition [0x000000002de1f000]
""ForkJoinPool-40-worker-1"" daemon prio=6 tid=0x000000000d9ef800 nid=0x39bc waiting on condition [0x000000002e09f000]
""ForkJoinPool-40-worker-3"" daemon prio=6 tid=0x000000000d9ee800 nid=0x2260 waiting on condition [0x00000000172cf000]
""ForkJoinPool-40-worker-7"" daemon prio=6 tid=0x000000000d9ee000 nid=0x34dc waiting on condition [0x000000002df1f000]
""ForkJoinPool-40-worker-5"" daemon prio=6 tid=0x000000000d9ed000 nid=0x1f98 waiting on condition [0x000000002dcef000]
""ForkJoinPool-40-worker-9"" daemon prio=6 tid=0x000000000d9ec800 nid=0x22c4 waiting on condition [0x000000002daaf000]
""ForkJoinPool-39-worker-1"" daemon prio=6 tid=0x000000000d9e4000 nid=0x29f4 waiting on condition [0x0000000029aff000]
""ForkJoinPool-39-worker-5"" daemon prio=6 tid=0x00000000129d3800 nid=0xbc0 waiting on condition [0x000000002d2ee000]
""ForkJoinPool-39-worker-3"" daemon prio=6 tid=0x000000000d646000 nid=0x2d08 waiting on condition [0x000000002d17e000]
""ForkJoinPool-39-worker-9"" daemon prio=6 tid=0x000000000d5cf000 nid=0x25ec waiting on condition [0x000000002cbae000]
""ForkJoinPool-39-worker-11"" daemon prio=6 tid=0x000000000c64e000 nid=0x32d8 waiting on condition [0x00000000173ff000]
""block-manager-slave-async-thread-pool-3"" daemon prio=6 tid=0x0000000013691000 nid=0x2a58 waiting on condition [0x00000000207df000]
""ForkJoinPool-38-worker-1"" daemon prio=6 tid=0x000000000c651800 nid=0xd7c waiting on condition [0x000000002d04f000]
""ForkJoinPool-38-worker-3"" daemon prio=6 tid=0x000000000c651000 nid=0x3588 waiting on condition [0x000000002ce8f000]
""ForkJoinPool-38-worker-7"" daemon prio=6 tid=0x000000000c650000 nid=0x22c0 waiting on condition [0x000000002ccef000]
""ForkJoinPool-38-worker-13"" daemon prio=6 tid=0x000000000c64d000 nid=0x3c90 waiting on condition [0x00000000171ce000]
""ForkJoinPool-37-worker-1"" daemon prio=6 tid=0x000000000c64c800 nid=0x3b00 waiting on condition [0x000000002c8ef000]
""ForkJoinPool-37-worker-7"" daemon prio=6 tid=0x00000000121ec800 nid=0x254c waiting on condition [0x000000002c6df000]
""ForkJoinPool-37-worker-3"" daemon prio=6 tid=0x0000000012141800 nid=0x191c waiting on condition [0x000000002c4cf000]
""ForkJoinPool-37-worker-5"" daemon prio=6 tid=0x0000000013cde000 nid=0x3614 waiting on condition [0x000000002c2de000]
""ForkJoinPool-36-worker-5"" daemon prio=6 tid=0x000000001415e800 nid=0x2be8 waiting on condition [0x000000002b91e000]
""ForkJoinPool-36-worker-7"" daemon prio=6 tid=0x000000001415d800 nid=0x1fbc waiting on condition [0x000000002bb2e000]
""ForkJoinPool-36-worker-9"" daemon prio=6 tid=0x000000001415d000 nid=0x15c8 waiting on condition [0x000000002ba2f000]
""ForkJoinPool-36-worker-11"" daemon prio=6 tid=0x000000001415c000 nid=0x3374 waiting on condition [0x000000002aebf000]
""ForkJoinPool-35-worker-1"" daemon prio=6 tid=0x0000000012af8800 nid=0x31d8 waiting on condition [0x000000002b80f000]
""ForkJoinPool-35-worker-3"" daemon prio=6 tid=0x000000001229b000 nid=0x1e28 waiting on condition [0x000000002b61f000]
""ForkJoinPool-35-worker-7"" daemon prio=6 tid=0x000000001229a800 nid=0x3508 waiting on condition [0x000000001c68f000]
""ForkJoinPool-35-worker-9"" daemon prio=6 tid=0x000000001229c000 nid=0xe68 waiting on condition [0x000000002b3df000]
""ForkJoinPool-34-worker-1"" daemon prio=6 tid=0x0000000012645000 nid=0x2b88 waiting on condition [0x000000002b15e000]
""ForkJoinPool-34-worker-3"" daemon prio=6 tid=0x0000000012644800 nid=0x3b08 waiting on condition [0x000000002b00f000]
""ForkJoinPool-34-worker-7"" daemon prio=6 tid=0x0000000012643000 nid=0x2da4 waiting on condition [0x000000002adaf000]
""ForkJoinPool-34-worker-9"" daemon prio=6 tid=0x000000000d5d3800 nid=0x3fb4 waiting on condition [0x000000001e85f000]
""ForkJoinPool-33-worker-1"" daemon prio=6 tid=0x0000000012afe000 nid=0x2350 waiting on condition [0x000000002a8df000]
""ForkJoinPool-33-worker-3"" daemon prio=6 tid=0x0000000012afd000 nid=0x3468 waiting on condition [0x000000002821e000]
""ForkJoinPool-33-worker-5"" daemon prio=6 tid=0x0000000012afc800 nid=0x3d94 waiting on condition [0x000000002a72f000]
""ForkJoinPool-33-worker-9"" daemon prio=6 tid=0x0000000012afb000 nid=0x28c0 waiting on condition [0x000000002a4cf000]
""ForkJoinPool-32-worker-1"" daemon prio=6 tid=0x0000000012af7000 nid=0x3830 waiting on condition [0x0000000019caf000]
""ForkJoinPool-32-worker-3"" daemon prio=6 tid=0x000000001418c800 nid=0x2b74 waiting on condition [0x000000002a26f000]
""ForkJoinPool-32-worker-5"" daemon prio=6 tid=0x00000000134cd800 nid=0x36e4 waiting on condition [0x000000002a0af000]
""ForkJoinPool-32-worker-11"" daemon prio=6 tid=0x000000000d5cf800 nid=0x1ce0 waiting on condition [0x0000000029d2f000]
""ForkJoinPool-31-worker-1"" daemon prio=6 tid=0x0000000012297800 nid=0x108c waiting on condition [0x000000002943f000]
""ForkJoinPool-31-worker-3"" daemon prio=6 tid=0x0000000012296800 nid=0x3f90 waiting on condition [0x000000002970e000]
""ForkJoinPool-31-worker-9"" daemon prio=6 tid=0x0000000012295000 nid=0x32bc waiting on condition [0x000000002344e000]
""ForkJoinPool-31-worker-11"" daemon prio=6 tid=0x00000000134cf000 nid=0x1b74 waiting on condition [0x00000000288ae000]
""ForkJoinPool-30-worker-5"" daemon prio=6 tid=0x00000000134c9800 nid=0x1f6c waiting on condition [0x000000001750f000]
""ForkJoinPool-30-worker-7"" daemon prio=6 tid=0x00000000134c9000 nid=0x3460 waiting on condition [0x00000000258af000]
""ForkJoinPool-30-worker-9"" daemon prio=6 tid=0x00000000134c8000 nid=0x2338 waiting on condition [0x0000000028b6f000]
""ForkJoinPool-30-worker-11"" daemon prio=6 tid=0x00000000129d2800 nid=0x26a0 waiting on condition [0x0000000028a0e000]
""ForkJoinPool-29-worker-1"" daemon prio=6 tid=0x0000000012213000 nid=0x3ad4 waiting on condition [0x000000002879f000]
""ForkJoinPool-29-worker-7"" daemon prio=6 tid=0x00000000129d2000 nid=0x2248 waiting on condition [0x000000002848f000]
""ForkJoinPool-29-worker-5"" daemon prio=6 tid=0x000000000d5d5000 nid=0x2524 waiting on condition [0x000000002838f000]
""ForkJoinPool-29-worker-11"" daemon prio=6 tid=0x000000000d5d2000 nid=0x1884 waiting on condition [0x000000002800f000]
""ForkJoinPool-28-worker-3"" daemon prio=6 tid=0x0000000014275000 nid=0x32f4 waiting on condition [0x0000000027d4f000]
""ForkJoinPool-28-worker-7"" daemon prio=6 tid=0x0000000013945800 nid=0x24dc waiting on condition [0x0000000027b0f000]
""ForkJoinPool-28-worker-9"" daemon prio=6 tid=0x0000000013944800 nid=0xcf8 waiting on condition [0x00000000279ef000]
""ForkJoinPool-28-worker-11"" daemon prio=6 tid=0x0000000013944000 nid=0x3af8 waiting on condition [0x00000000278ef000]
""ForkJoinPool-27-worker-5"" daemon prio=6 tid=0x0000000013941000 nid=0x1550 waiting on condition [0x00000000273ff000]
""ForkJoinPool-27-worker-3"" daemon prio=6 tid=0x0000000013940000 nid=0x23e0 waiting on condition [0x000000002720f000]
""ForkJoinPool-27-worker-7"" daemon prio=6 tid=0x000000001393f800 nid=0x16b4 waiting on condition [0x00000000270cf000]
""ForkJoinPool-27-worker-13"" daemon prio=6 tid=0x000000000d966800 nid=0x1e54 waiting on condition [0x000000002042f000]
""ForkJoinPool-26-worker-7"" daemon prio=6 tid=0x000000000d965000 nid=0x22a0 waiting on condition [0x00000000268ef000]
""ForkJoinPool-26-worker-3"" daemon prio=6 tid=0x000000000d967800 nid=0x2bf0 waiting on condition [0x0000000016d8e000]
""ForkJoinPool-26-worker-5"" daemon prio=6 tid=0x000000000d969000 nid=0xfe8 waiting on condition [0x0000000024b9f000]
""ForkJoinPool-26-worker-11"" daemon prio=6 tid=0x000000000d966000 nid=0x2f88 waiting on condition [0x000000002676f000]
""ForkJoinPool-25-worker-5"" daemon prio=6 tid=0x00000000129cf800 nid=0xe30 waiting on condition [0x0000000026baf000]
""ForkJoinPool-25-worker-7"" daemon prio=6 tid=0x00000000129cf000 nid=0x1a88 waiting on condition [0x000000002557e000]
""ForkJoinPool-25-worker-11"" daemon prio=6 tid=0x0000000012ccd000 nid=0x1e3c waiting on condition [0x00000000269ff000]
""ForkJoinPool-25-worker-13"" daemon prio=6 tid=0x00000000121ec000 nid=0x30c0 waiting on condition [0x000000002662e000]
""ForkJoinPool-24-worker-1"" daemon prio=6 tid=0x0000000013ce5000 nid=0x39e8 waiting on condition [0x000000002623f000]
""ForkJoinPool-24-worker-3"" daemon prio=6 tid=0x0000000013ce6800 nid=0x2c84 waiting on condition [0x000000002603f000]
""ForkJoinPool-24-worker-7"" daemon prio=6 tid=0x00000000121f0800 nid=0x35bc waiting on condition [0x0000000025f3e000]
""ForkJoinPool-24-worker-9"" daemon prio=6 tid=0x00000000121ef800 nid=0x2ae4 waiting on condition [0x00000000231ef000]
""ForkJoinPool-23-worker-5"" daemon prio=6 tid=0x00000000121eb000 nid=0x35fc waiting on condition [0x0000000025cdf000]
""ForkJoinPool-23-worker-1"" daemon prio=6 tid=0x00000000121ea800 nid=0x2cc4 waiting on condition [0x0000000025b3f000]
""ForkJoinPool-23-worker-11"" daemon prio=6 tid=0x00000000121e9800 nid=0x3ddc waiting on condition [0x00000000259bf000]
""ForkJoinPool-23-worker-7"" daemon prio=6 tid=0x0000000013cdf800 nid=0x1cb8 waiting on condition [0x0000000023eae000]
""ForkJoinPool-22-worker-3"" daemon prio=6 tid=0x0000000014181800 nid=0x2ea8 waiting on condition [0x000000002568f000]
""ForkJoinPool-22-worker-5"" daemon prio=6 tid=0x000000001418c000 nid=0x2be4 waiting on condition [0x00000000251ef000]
""ForkJoinPool-22-worker-7"" daemon prio=6 tid=0x0000000014189800 nid=0x1d34 waiting on condition [0x000000002547f000]
""ForkJoinPool-22-worker-11"" daemon prio=6 tid=0x0000000013ce8000 nid=0x29ec waiting on condition [0x0000000024f3f000]
""ForkJoinPool-21-worker-1"" daemon prio=6 tid=0x0000000013ce4000 nid=0x1da4 waiting on condition [0x00000000247bf000]
""ForkJoinPool-21-worker-5"" daemon prio=6 tid=0x0000000013ce3800 nid=0x28c4 waiting on condition [0x000000002451e000]
""ForkJoinPool-21-worker-7"" daemon prio=6 tid=0x0000000013ce2000 nid=0x3b60 waiting on condition [0x000000001b5cf000]
""ForkJoinPool-21-worker-13"" daemon prio=6 tid=0x0000000013cdc000 nid=0x2514 waiting on condition [0x000000002172f000]
""ForkJoinPool-20-worker-1"" daemon prio=6 tid=0x0000000013cdb000 nid=0x2bec waiting on condition [0x0000000021eaf000]
""ForkJoinPool-20-worker-3"" daemon prio=6 tid=0x0000000013cd9800 nid=0x3b3c waiting on condition [0x000000001d30e000]
""ForkJoinPool-20-worker-7"" daemon prio=6 tid=0x0000000013690800 nid=0x2b20 waiting on condition [0x0000000023b8e000]
""ForkJoinPool-20-worker-9"" daemon prio=6 tid=0x0000000012215000 nid=0x1f10 waiting on condition [0x0000000023a7f000]
""ForkJoinPool-19-worker-7"" daemon prio=6 tid=0x0000000014187800 nid=0x15e0 waiting on condition [0x000000002274f000]
""ForkJoinPool-19-worker-1"" daemon prio=6 tid=0x0000000014189000 nid=0x9cc waiting on condition [0x0000000022d1f000]
""ForkJoinPool-19-worker-5"" daemon prio=6 tid=0x000000000d964800 nid=0x2f14 waiting on condition [0x000000002372e000]
""ForkJoinPool-19-worker-9"" daemon prio=6 tid=0x000000001418e000 nid=0x2408 waiting on condition [0x000000002214f000]
""ForkJoinPool-18-worker-1"" daemon prio=6 tid=0x0000000014186800 nid=0x1bd8 waiting on condition [0x0000000022b5f000]
""ForkJoinPool-18-worker-7"" daemon prio=6 tid=0x0000000014185000 nid=0x2f20 waiting on condition [0x000000002289f000]
""ForkJoinPool-18-worker-9"" daemon prio=6 tid=0x0000000014183800 nid=0x2404 waiting on condition [0x00000000224ef000]
""ForkJoinPool-18-worker-11"" daemon prio=6 tid=0x0000000014183000 nid=0x3ed4 waiting on condition [0x00000000211be000]
""ForkJoinPool-17-worker-1"" daemon prio=6 tid=0x000000001417f000 nid=0x256c waiting on condition [0x0000000021fdf000]
""ForkJoinPool-17-worker-5"" daemon prio=6 tid=0x00000000141a6000 nid=0x3e24 waiting on condition [0x0000000021daf000]
""ForkJoinPool-17-worker-7"" daemon prio=6 tid=0x00000000141a0000 nid=0x34b0 waiting on condition [0x0000000021bef000]
""ForkJoinPool-17-worker-11"" daemon prio=6 tid=0x0000000012214800 nid=0x348c waiting on condition [0x000000002186e000]
""ForkJoinPool-16-worker-5"" daemon prio=6 tid=0x0000000012212000 nid=0x134c waiting on condition [0x000000001d40e000]
""ForkJoinPool-16-worker-1"" daemon prio=6 tid=0x0000000012211800 nid=0x107c waiting on condition [0x00000000214af000]
""ForkJoinPool-16-worker-3"" daemon prio=6 tid=0x0000000012210800 nid=0x259c waiting on condition [0x000000002004f000]
""ForkJoinPool-16-worker-11"" daemon prio=6 tid=0x000000000d013000 nid=0x2d5c waiting on condition [0x0000000020f7f000]
""ForkJoinPool-15-worker-1"" daemon prio=6 tid=0x000000000d011800 nid=0x1d24 waiting on condition [0x0000000020c8f000]
""ForkJoinPool-15-worker-5"" daemon prio=6 tid=0x000000000d010000 nid=0x122c waiting on condition [0x000000002058f000]
""ForkJoinPool-15-worker-7"" daemon prio=6 tid=0x000000000d00f000 nid=0x3fcc waiting on condition [0x0000000020acf000]
""ForkJoinPool-15-worker-13"" daemon prio=6 tid=0x000000000d00d000 nid=0x230c waiting on condition [0x000000001feef000]
""ForkJoinPool-14-worker-3"" daemon prio=6 tid=0x00000000141a4800 nid=0x2934 waiting on condition [0x000000001f6bf000]
""ForkJoinPool-14-worker-7"" daemon prio=6 tid=0x00000000141a3000 nid=0x24f8 waiting on condition [0x000000001700f000]
""ForkJoinPool-14-worker-9"" daemon prio=6 tid=0x00000000141a2000 nid=0x167c waiting on condition [0x000000001fb2f000]
""ForkJoinPool-14-worker-11"" daemon prio=6 tid=0x00000000141a1800 nid=0x3060 waiting on condition [0x000000001fa0e000]
""RMI TCP Connection(3)-10.111.3.137"" daemon prio=6 tid=0x0000000014ed5000 nid=0x3e68 runnable [0x000000001cf6e000]
""RMI TCP Connection(2)-10.111.3.137"" daemon prio=6 tid=0x0000000014274800 nid=0x3d20 runnable [0x000000001d0ce000]
""ForkJoinPool-13-worker-5"" daemon prio=6 tid=0x000000000d64d000 nid=0x1b44 waiting on condition [0x000000001c57f000]
""ForkJoinPool-13-worker-3"" daemon prio=6 tid=0x000000000d64c000 nid=0x202c waiting on condition [0x000000001f5bf000]
""ForkJoinPool-13-worker-9"" daemon prio=6 tid=0x000000000d64a800 nid=0x24ac waiting on condition [0x000000001f30f000]
""ForkJoinPool-13-worker-11"" daemon prio=6 tid=0x000000000d64a000 nid=0x3d2c waiting on condition [0x000000001deff000]
""ForkJoinPool-12-worker-3"" daemon prio=6 tid=0x000000000d648800 nid=0x3744 waiting on condition [0x000000001f1ae000]
""ForkJoinPool-12-worker-7"" daemon prio=6 tid=0x000000000d647000 nid=0x3074 waiting on condition [0x000000001edae000]
""ForkJoinPool-12-worker-11"" daemon prio=6 tid=0x000000000d963000 nid=0x3044 waiting on condition [0x000000001e99f000]
""ForkJoinPool-12-worker-13"" daemon prio=6 tid=0x000000001368f000 nid=0x353c waiting on condition [0x000000001637e000]
""ForkJoinPool-11-worker-1"" daemon prio=6 tid=0x0000000014273800 nid=0x3330 waiting on condition [0x000000001e6ae000]
""ForkJoinPool-11-worker-3"" daemon prio=6 tid=0x0000000014272000 nid=0x2a0c waiting on condition [0x000000001e57f000]
""ForkJoinPool-11-worker-7"" daemon prio=6 tid=0x0000000014271800 nid=0x2d54 waiting on condition [0x000000001d1ff000]
""ForkJoinPool-11-worker-11"" daemon prio=6 tid=0x0000000014270000 nid=0x359c waiting on condition [0x000000001e1ff000]
""ForkJoinPool-10-worker-1"" daemon prio=6 tid=0x0000000012146800 nid=0x3a48 waiting on condition [0x000000001bf5f000]
""ForkJoinPool-10-worker-5"" daemon prio=6 tid=0x0000000014ed8800 nid=0x3e7c waiting on condition [0x000000001cd5f000]
""ForkJoinPool-10-worker-7"" daemon prio=6 tid=0x0000000014ed8000 nid=0x2680 waiting on condition [0x000000001dc0e000]
""ForkJoinPool-10-worker-11"" daemon prio=6 tid=0x0000000014ed6800 nid=0x3f20 waiting on condition [0x000000001d85e000]
""ForkJoinPool-9-worker-3"" daemon prio=6 tid=0x0000000014ed2800 nid=0x3df0 waiting on condition [0x000000001ce5f000]
""ForkJoinPool-9-worker-1"" daemon prio=6 tid=0x0000000013695800 nid=0xc84 waiting on condition [0x000000001cc3f000]
""ForkJoinPool-9-worker-5"" daemon prio=6 tid=0x0000000013695000 nid=0x1e70 waiting on condition [0x000000001ca9f000]
""ForkJoinPool-9-worker-7"" daemon prio=6 tid=0x0000000013694000 nid=0x13ec waiting on condition [0x000000001c90f000]
""ForkJoinPool-8-worker-7"" daemon prio=6 tid=0x0000000012145000 nid=0x2c68 waiting on condition [0x000000001bd4e000]
""ForkJoinPool-8-worker-1"" daemon prio=6 tid=0x0000000012144800 nid=0x3fa4 waiting on condition [0x000000001ba6f000]
""ForkJoinPool-8-worker-9"" daemon prio=6 tid=0x0000000012143000 nid=0x36d0 waiting on condition [0x000000001b33f000]
""ForkJoinPool-8-worker-11"" daemon prio=6 tid=0x0000000012142000 nid=0x22ec waiting on condition [0x000000001b91f000]
""Executor task launch worker-2"" daemon prio=6 tid=0x0000000012140000 nid=0x25e4 waiting on condition [0x000000001b47e000]
""JMX server connection timeout 259"" daemon prio=6 tid=0x0000000012cda800 nid=0x12c8 in Object.wait() [0x000000001b0ef000]
""RMI Scheduler(0)"" daemon prio=6 tid=0x0000000012cd8800 nid=0x290c waiting on condition [0x000000001b1ff000]
""RMI TCP Connection(1)-10.111.3.137"" daemon prio=6 tid=0x0000000012cd6000 nid=0x8e8 runnable [0x000000001afde000]
""RMI TCP Accept-0"" daemon prio=6 tid=0x0000000012cd5800 nid=0x20c4 runnable [0x000000001a70e000]
""task-result-getter-3"" daemon prio=6 tid=0x0000000012cd7800 nid=0x24c8 waiting on condition [0x000000001ae2f000]
""task-result-getter-2"" daemon prio=6 tid=0x0000000012cd7000 nid=0x3558 waiting on condition [0x000000001ac0f000]
""task-result-getter-1"" daemon prio=6 tid=0x0000000012cda000 nid=0xb40 waiting on condition [0x000000001a85e000]
""Executor task launch worker-1"" daemon prio=6 tid=0x0000000012cd9000 nid=0x2ad8 waiting on condition [0x000000001aa3e000]
""ForkJoinPool-4-worker-1"" daemon prio=6 tid=0x0000000012cd2800 nid=0x3ae0 waiting on condition [0x000000001a1af000]
""ForkJoinPool-4-worker-15"" daemon prio=6 tid=0x0000000012cd1000 nid=0x3b34 waiting on condition [0x00000000185df000]
""ForkJoinPool-6-worker-1"" daemon prio=6 tid=0x0000000012cce800 nid=0x10a0 waiting on condition [0x000000001994e000]
""ForkJoinPool-4-worker-7"" daemon prio=6 tid=0x0000000012cce000 nid=0x3840 waiting on condition [0x0000000018a1f000]
""ForkJoinPool-3-worker-15"" daemon prio=6 tid=0x000000001407e800 nid=0x3288 waiting on condition [0x00000000190be000]
""ForkJoinPool-5-worker-1"" daemon prio=6 tid=0x000000001407d800 nid=0x1e04 waiting on condition [0x000000001963f000]
""ForkJoinPool-2-worker-15"" daemon prio=6 tid=0x000000001407b800 nid=0x29ac waiting on condition [0x00000000181af000]
""ForkJoinPool-2-worker-1"" daemon prio=6 tid=0x000000001407a800 nid=0x3a60 waiting on condition [0x0000000018fbe000]
""ForkJoinPool-2-worker-5"" daemon prio=6 tid=0x0000000014079000 nid=0x1758 waiting on condition [0x00000000187df000]
""ForkJoinPool-6-worker-3"" daemon prio=6 tid=0x0000000014077800 nid=0x175c waiting on condition [0x0000000018c1e000]
""ForkJoinPool-3-worker-3"" daemon prio=6 tid=0x0000000014076000 nid=0x1f50 waiting on condition [0x000000001891f000]
""ForkJoinPool-5-worker-7"" daemon prio=6 tid=0x0000000014075800 nid=0x22e4 waiting on condition [0x00000000186de000]
""ForkJoinPool-5-worker-9"" daemon prio=6 tid=0x0000000014074800 nid=0x307c waiting on condition [0x0000000017d9f000]
""ForkJoinPool-6-worker-5"" daemon prio=6 tid=0x0000000014074000 nid=0x3ff0 waiting on condition [0x000000001620f000]
""ForkJoinPool-7-worker-1"" daemon prio=6 tid=0x0000000014073000 nid=0x2ee8 waiting on condition [0x00000000184df000]
""ForkJoinPool-3-worker-9"" daemon prio=6 tid=0x0000000014071800 nid=0x172c waiting on condition [0x000000001806e000]
""ForkJoinPool-7-worker-3"" daemon prio=6 tid=0x0000000014070000 nid=0x11c8 waiting on condition [0x000000001792e000]
""ForkJoinPool-7-worker-5"" daemon prio=6 tid=0x000000001406f000 nid=0x2d7c waiting on condition [0x000000001610f000]
""block-manager-ask-thread-pool-1"" daemon prio=6 tid=0x000000000d960800 nid=0x3160 waiting on condition [0x000000001647f000]
""block-manager-slave-async-thread-pool-2"" daemon prio=6 tid=0x000000000d960000 nid=0x3c2c waiting on condition [0x0000000016aef000]
""block-manager-slave-async-thread-pool-1"" daemon prio=6 tid=0x000000000d95f000 nid=0x137c waiting on condition [0x000000001698f000]
""block-manager-slave-async-thread-pool-0"" daemon prio=6 tid=0x000000000d95e800 nid=0x2154 waiting on condition [0x00000000167bf000]
""block-manager-ask-thread-pool-0"" daemon prio=6 tid=0x000000000d95d800 nid=0x1ea4 waiting on condition [0x000000001660f000]
""DestroyJavaVM"" prio=6 tid=0x000000000d95d000 nid=0x3780 waiting on condition [0x0000000000000000]
""Query-Thread"" prio=6 tid=0x000000000d95c000 nid=0x3eb8 in Object.wait() [0x0000000015f4d000]
""Query-Thread"" prio=6 tid=0x000000000da7c000 nid=0x2c58 in Object.wait() [0x0000000015bcd000]
""Query-Thread"" prio=6 tid=0x000000000da7b800 nid=0xf64 in Object.wait() [0x0000000015d0d000]
""Query-Thread"" prio=6 tid=0x000000000da7a800 nid=0x3a8c in Object.wait() [0x000000001489d000]
""Query-Thread"" prio=6 tid=0x000000000da7a000 nid=0x190c in Object.wait() [0x0000000015a1d000]
""Query-Thread"" prio=6 tid=0x000000000da79000 nid=0x1fb4 in Object.wait() [0x000000001456d000]
""task-result-getter-0"" daemon prio=6 tid=0x000000000da78800 nid=0x10e4 waiting on condition [0x00000000146ef000]
""Executor task launch worker-0"" daemon prio=6 tid=0x000000000da77800 nid=0x1654 waiting on condition [0x0000000011f5f000]
""BoneCP-pool-watch-thread"" daemon prio=6 tid=0x000000000da75800 nid=0x2ff4 waiting on condition [0x000000001343e000]
""BoneCP-keep-alive-scheduler"" daemon prio=6 tid=0x000000000da74800 nid=0x3cfc waiting on condition [0x00000000131de000]
""com.google.common.base.internal.Finalizer"" daemon prio=6 tid=0x000000000da74000 nid=0x3608 in Object.wait() [0x000000001330f000]
""BoneCP-pool-watch-thread"" daemon prio=6 tid=0x000000000da73000 nid=0x1f48 waiting on condition [0x000000001205f000]
""BoneCP-keep-alive-scheduler"" daemon prio=6 tid=0x000000000da72800 nid=0x23e4 waiting on condition [0x0000000011abe000]
""com.google.common.base.internal.Finalizer"" daemon prio=6 tid=0x000000000da71800 nid=0x2ed8 in Object.wait() [0x0000000011dbe000]
""derby.rawStoreDaemon"" daemon prio=6 tid=0x000000000da71000 nid=0x3268 in Object.wait() [0x0000000011c4e000]
""Timer-1"" daemon prio=6 tid=0x000000000da70000 nid=0x24f4 in Object.wait() [0x000000001198f000]
""Abandoned connection cleanup thread"" daemon prio=6 tid=0x000000000da6f800 nid=0x5c4 in Object.wait() [0x00000000114df000]
""SparkListenerBus"" daemon prio=6 tid=0x000000000da6e800 nid=0x195c waiting on condition [0x00000000117ae000]
""context-cleaner-periodic-gc"" daemon prio=6 tid=0x000000000da6e000 nid=0x25a8 waiting on condition [0x000000001161f000]
""Spark Context Cleaner"" daemon prio=6 tid=0x000000000da6d000 nid=0x1f3c in Object.wait() [0x000000001103f000]
""shuffle-server-0"" daemon prio=6 tid=0x000000000cee4800 nid=0x1df8 runnable [0x000000001134e000]
""driver-heartbeater"" daemon prio=6 tid=0x000000000cee3800 nid=0x3390 waiting on condition [0x000000001117f000]
""dag-scheduler-event-loop"" daemon prio=6 tid=0x000000000cee3000 nid=0x21d0 waiting on condition [0x0000000010e9f000]
""netty-rpc-env-timeout"" daemon prio=6 tid=0x000000000cee2000 nid=0x2144 waiting on condition [0x0000000010a4e000]
""Timer-0"" daemon prio=6 tid=0x000000000cee1800 nid=0x37c8 in Object.wait() [0x000000001021f000]
""heartbeat-receiver-event-loop-thread"" daemon prio=6 tid=0x000000000cee0800 nid=0x3b58 waiting on condition [0x000000001011f000]
""SparkUI-62"" daemon prio=6 tid=0x000000000ceda800 nid=0x3244 waiting on condition [0x0000000010d9e000]
""SparkUI-61"" daemon prio=6 tid=0x000000000cedb800 nid=0x28fc waiting on condition [0x0000000010bde000]
""SparkUI-60"" daemon prio=6 tid=0x000000000cedc000 nid=0x1a40 waiting on condition [0x000000001091e000]
""SparkUI-59-acceptor-0@28dfd572-ServerConnector@7f364454{HTTP/1.1}{0.0.0.0:4041}"" daemon prio=6 tid=0x000000000cedd000 nid=0x20d0 runnable [0x00000000107ef000]
""SparkUI-58-selector-ServerConnectorManager@fb3f1f3/3"" daemon prio=6 tid=0x000000000cede800 nid=0x3698 runnable [0x00000000103de000]
""SparkUI-57-selector-ServerConnectorManager@fb3f1f3/2"" daemon prio=6 tid=0x000000000cedf000 nid=0x1608 runnable [0x000000000ebde000]
""SparkUI-56-selector-ServerConnectorManager@fb3f1f3/1"" daemon prio=6 tid=0x000000000cedd800 nid=0x3418 runnable [0x000000001068e000]
""SparkUI-55-selector-ServerConnectorManager@fb3f1f3/0"" daemon prio=6 tid=0x000000000cee0000 nid=0x1784 runnable [0x000000001054e000]
""map-output-dispatcher-7"" daemon prio=6 tid=0x000000000ceda000 nid=0x3db8 waiting on condition [0x000000000e9bf000]
""map-output-dispatcher-6"" daemon prio=6 tid=0x000000000ced9000 nid=0x21d8 waiting on condition [0x000000000efee000]
""map-output-dispatcher-5"" daemon prio=6 tid=0x000000000ced8000 nid=0x2a64 waiting on condition [0x000000000ee4e000]
""map-output-dispatcher-4"" daemon prio=6 tid=0x000000000ced7800 nid=0x2f34 waiting on condition [0x000000000ed0e000]
""map-output-dispatcher-3"" daemon prio=6 tid=0x000000000ced6800 nid=0x381c waiting on condition [0x000000000b9de000]
""map-output-dispatcher-2"" daemon prio=6 tid=0x000000000ced6000 nid=0x3f38 waiting on condition [0x000000000eadf000]
""map-output-dispatcher-1"" daemon prio=6 tid=0x000000000ced5000 nid=0xa0c waiting on condition [0x000000000e8ae000]
""map-output-dispatcher-0"" daemon prio=6 tid=0x000000000cf0c000 nid=0x3180 waiting on condition [0x000000000e7ae000]
""shuffle-server-0"" daemon prio=6 tid=0x000000000d1b4000 nid=0x345c runnable [0x000000000e4fe000]
""dispatcher-event-loop-7"" daemon prio=6 tid=0x000000000d0d6000 nid=0x3d9c waiting on condition [0x000000000e5ff000]
""dispatcher-event-loop-6"" daemon prio=6 tid=0x000000000d0d5800 nid=0x292c waiting on condition [0x000000000dfee000]
""dispatcher-event-loop-5"" daemon prio=6 tid=0x000000000d0ce800 nid=0x15f4 waiting on condition [0x000000000e39e000]
""dispatcher-event-loop-4"" daemon prio=6 tid=0x000000000d1cf000 nid=0x20e8 waiting on condition [0x000000000bc1e000]
""dispatcher-event-loop-3"" daemon prio=6 tid=0x000000000d1ce800 nid=0x2230 waiting on condition [0x000000000e26f000]
""dispatcher-event-loop-2"" daemon prio=6 tid=0x000000000d1cd800 nid=0x1ef8 waiting on condition [0x000000000de1e000]
""dispatcher-event-loop-1"" daemon prio=6 tid=0x000000000d007800 nid=0x24e4 waiting on condition [0x000000000e0ef000]
""dispatcher-event-loop-0"" daemon prio=6 tid=0x000000000d007000 nid=0x1afc waiting on condition [0x000000000dbce000]
""Service Thread"" daemon prio=6 tid=0x00000000095ae000 nid=0x27c8 runnable [0x0000000000000000]
""C2 CompilerThread1"" daemon prio=10 tid=0x00000000095ab800 nid=0x1850 waiting on condition [0x0000000000000000]
""C2 CompilerThread0"" daemon prio=10 tid=0x000000000242e800 nid=0x15f0 waiting on condition [0x0000000000000000]
""Attach Listener"" daemon prio=10 tid=0x00000000094cc000 nid=0x1728 waiting on condition [0x0000000000000000]
""Signal Dispatcher"" daemon prio=10 tid=0x00000000094cb800 nid=0x3e28 runnable [0x0000000000000000]
""Finalizer"" daemon prio=8 tid=0x00000000094b8800 nid=0x2cbc in Object.wait() [0x000000000b8cf000]
""Reference Handler"" daemon prio=10 tid=0x000000000a315000 nid=0x15a0 in Object.wait() [0x000000000b54f000]
""VM Thread"" prio=10 tid=0x000000000a314000 nid=0x3a0c runnable 
""GC task thread#0 (ParallelGC)"" prio=6 tid=0x00000000023c6000 nid=0x3fac runnable 
""GC task thread#1 (ParallelGC)"" prio=6 tid=0x00000000023c7800 nid=0x17d8 runnable 
""GC task thread#2 (ParallelGC)"" prio=6 tid=0x00000000023c9000 nid=0x3890 runnable 
""GC task thread#3 (ParallelGC)"" prio=6 tid=0x00000000023cb000 nid=0x39f4 runnable 
""GC task thread#4 (ParallelGC)"" prio=6 tid=0x00000000023ce800 nid=0x1dac runnable 
""GC task thread#5 (ParallelGC)"" prio=6 tid=0x00000000023cf800 nid=0x1818 runnable 
""GC task thread#6 (ParallelGC)"" prio=6 tid=0x00000000023d1000 nid=0x28ec runnable 
""GC task thread#7 (ParallelGC)"" prio=6 tid=0x00000000023d4000 nid=0x358c runnable 
""VM Periodic Task Thread"" prio=10 tid=0x00000000095af000 nid=0x325c waiting on condition 
;;;","06/Sep/16 08:59;srowen;Yeah [~rdblue] is right on the mark then. I agree, I wasn't clear on how it manages threads. A shared {{ExecutorService}}, a 'cached' version with some reasonable max number of threads like 8 should be good.;;;","06/Sep/16 17:18;rdblue;I'll put together a patch for this with a shared executor service. Although ForkJoinPool isn't to blame, from what I've read about it we probably should be using a different implementation. We don't need the fork/join task pattern and it would be much better to have reliable (and documented) semantics.;;;","06/Sep/16 23:22;apachespark;User 'rdblue' has created a pull request for this issue:
https://github.com/apache/spark/pull/14985;;;","06/Sep/16 23:23;rdblue;I opened a PR with a fix. It still uses a ForkJoinPool because the thread pool task support is marked as deprecated in Scala. So I guess we should use fork/join.;;;","10/Sep/16 09:21;srowen;Issue resolved by pull request 14985
[https://github.com/apache/spark/pull/14985];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
should not allow specify database in table/view name after RENAME TO,SPARK-17394,13002605,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,04/Sep/16 08:29,08/Dec/16 22:57,14/Jul/23 06:29,05/Sep/16 05:10,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 05 05:10:03 UTC 2016,,,,,,,,,,"0|i3388n:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"04/Sep/16 08:34;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/14955;;;","05/Sep/16 05:10;cloud_fan;Issue resolved by pull request 14955
[https://github.com/apache/spark/pull/14955];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error Handling when CTAS Against the Same Data Source Table Using Overwrite Mode,SPARK-17393,13002602,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,04/Sep/16 07:02,05/Sep/16 03:29,14/Jul/23 06:29,05/Sep/16 03:29,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"When we trying to read a table and then write to the same table using the `Overwrite` save mode, we got a very confusing error message:
For example, 
{noformat}
      Seq((1, 2)).toDF(""i"", ""j"").write.saveAsTable(""tab1"")
      table(""tab1"").write.mode(SaveMode.Overwrite).saveAsTable(""tab1"")
{noformat}

{noformat}
Job aborted.
org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp
...
Caused by: org.apache.spark.SparkException: Task failed while writing rows
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:266)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)
	at org.apache.spark.sql.execution.datasources
{noformat}

After the PR, we will issue an `AnalysisException`:
{noformat}
Cannot overwrite table `tab1` that is also being read from
{noformat}",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 05 03:29:05 UTC 2016,,,,,,,,,,"0|i3387z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/16 07:04;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14954;;;","05/Sep/16 03:29;cloud_fan;Issue resolved by pull request 14954
[https://github.com/apache/spark/pull/14954];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Two Test Failures After Backport,SPARK-17391,13002576,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,smilegator,smilegator,smilegator,03/Sep/16 17:38,05/Sep/16 03:19,14/Jul/23 06:29,05/Sep/16 03:19,2.0.1,,,,,,,,2.0.1,,,,SQL,,,,,,,,0,,,,,,"In the latest branch 2.0, we have two test case failure due to backport.

test(""ALTER VIEW AS should keep the previous table properties, comment, create_time, etc."")

test(""SPARK-6212: The EXPLAIN output of CTAS only shows the analyzed plan"")
",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 05 03:19:03 UTC 2016,,,,,,,,,,"0|i33827:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/16 17:42;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14951;;;","05/Sep/16 03:19;cloud_fan;Issue resolved by pull request 14951
[https://github.com/apache/spark/pull/14951];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Creating SparkContext() from python without spark-submit ignores user conf,SPARK-17387,13002510,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zjffdu,vanzin,vanzin,02/Sep/16 23:25,20/Jan/17 11:09,14/Jul/23 06:29,11/Oct/16 21:57,2.0.0,,,,,,,,2.1.0,,,,PySpark,,,,,,,,0,,,,,,"Consider the following scenario: user runs a python application not through spark-submit, but by adding the pyspark module and manually creating a Spark context. Kinda like this:

{noformat}
$ SPARK_HOME=$PWD PYTHONPATH=python:python/lib/py4j-0.10.3-src.zip python
Python 2.7.12 (default, Jul  1 2016, 15:12:24) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from pyspark import SparkContext
>>> from pyspark import SparkConf
>>> conf = SparkConf().set(""spark.driver.memory"", ""4g"")
>>> sc = SparkContext(conf=conf)
{noformat}

If you look at the JVM launched by the pyspark code, it ignores the user's configuration:

{noformat}
$ ps ax | grep $(pgrep -f SparkSubmit)
12283 pts/2    Sl+    0:03 /apps/java7/bin/java -cp ... -Xmx1g -XX:MaxPermSize=256m org.apache.spark.deploy.SparkSubmit pyspark-shell
{noformat}

Note the ""1g"" of memory. If instead you use ""pyspark"", you get the correct ""4g"" in the JVM.

This also affects other configs; for example, you can't really add jars to the driver's classpath using ""spark.jars"".

You can work around this by setting the undocumented env variable Spark itself uses:

{noformat}
$ SPARK_HOME=$PWD PYTHONPATH=python:python/lib/py4j-0.10.3-src.zip python
Python 2.7.12 (default, Jul  1 2016, 15:12:24) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import os
>>> os.environ['PYSPARK_SUBMIT_ARGS'] = ""pyspark-shell --conf spark.driver.memory=4g""
>>> from pyspark import SparkContext
>>> sc = SparkContext()
{noformat}

But it would be nicer if the configs were automatically propagated.

BTW the reason for this is that the {{launch_gateway}} function used to start the JVM does not take any parameters, and the only place where it reads arguments for Spark is that env variable.",,apachespark,bryanc,codlife,gurmukhd,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19307,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 09 03:10:41 UTC 2016,,,,,,,,,,"0|i337nj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/16 23:42;vanzin;A note about the workaround I posted: it doesn't seem to work for ""spark.driver.memory"" in particular:

{noformat}
13669 pts/2    Sl+    0:02 /apps/java7/bin/java -cp /work/apache/spark/conf/:/work/apache/spark/assembly/target/scala-2.11/jars/* -Xmx1g -XX:MaxPermSize=256m org.apache.spark.deploy.SparkSubmit pyspark-shell --conf spark.driver.memory=4g
{noformat}

Note the Xmx says ""1g"" and the conf says ""4g"". So some adjustments might be needed in the launcher code to account for that.
;;;","05/Sep/16 09:33;apachespark;User 'zjffdu' has created a pull request for this issue:
https://github.com/apache/spark/pull/14959;;;","08/Sep/16 22:45;bryanc;[~vanzin] you said if you use PySpark you could get the correct ""4g"" memory option, but I was only able to do this through the command line like this

{noformat}
$> bin/pyspark --conf spark.driver.memory=4g
{noformat}

Is that what you meant?  I think just adding the command line confs to configure the JVM through a plain Python shell would be a simple fix, and still be inline with how the Scala spark-shell works too.;;;","08/Sep/16 22:48;vanzin;Yeah, that's what I mean. Running the pyspark shell, or using spark-submit to run a python script, correctly preserves the user's configuration.

BTW my workaround above is wrong - ""pyspark-shell"" needs to be the last argument in ""PYSPARK_SUBMIT_ARGS"", not the first.;;;","09/Sep/16 00:11;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/15019;;;","09/Sep/16 03:10;codlife;please check https://issues.apache.org/jira/browse/SPARK-17447
thinks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark version should be available in R,SPARK-17376,13002293,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,felixcheung,felixcheung,felixcheung,02/Sep/16 09:05,02/Sep/16 17:30,14/Jul/23 06:29,02/Sep/16 17:12,2.0.0,,,,,,,,2.0.1,2.1.0,,,SparkR,,,,,,,,0,,,,,,,,apachespark,felixcheung,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 02 17:30:08 UTC 2016,,,,,,,,,,"0|i336bj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/16 09:11;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/14935;;;","02/Sep/16 17:12;shivaram;Issue resolved by pull request 14935
[https://github.com/apache/spark/pull/14935];;;","02/Sep/16 17:30;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/14939;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improves the error message when fails to parse some json file lines in DataFrameReader,SPARK-17374,13002251,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,clockfly,clockfly,clockfly,02/Sep/16 06:50,06/Sep/16 14:21,14/Jul/23 06:29,06/Sep/16 14:21,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Demo case:

We silently replace corrupted line with null without any error message.

{code}
import org.apache.spark.sql.types._
val corruptRecords = spark.sparkContext.parallelize(""""""{""a"":{, b:3}"""""" :: Nil)
val schema = StructType(StructField(""a"", StringType, true) :: Nil)
val jsonDF = spark.read.schema(schema).json(corruptRecords)
scala> jsonDF.show
+----+
|   a|
+----+
|null|
+----+
{code}
",,apachespark,clockfly,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 06 14:21:44 UTC 2016,,,,,,,,,,"0|i33627:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"02/Sep/16 06:52;apachespark;User 'clockfly' has created a pull request for this issue:
https://github.com/apache/spark/pull/14929;;;","06/Sep/16 14:21;cloud_fan;Issue resolved by pull request 14929
[https://github.com/apache/spark/pull/14929];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resubmitted stage outputs deleted by zombie map tasks on stop(),SPARK-17371,13002221,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ekhliang,ekhliang,ekhliang,02/Sep/16 01:50,06/Sep/16 23:56,14/Jul/23 06:29,06/Sep/16 23:56,,,,,,,,,2.1.0,,,,Spark Core,,,,,,,,0,,,,,,"It seems that old shuffle map tasks hanging around after a stage resubmit will delete intended shuffle output files on stop(), causing downstream stages to fail even after successful resubmit completion. This can happen easily if the prior map task is waiting for a network timeout when its stage is resubmitted.

This can cause unnecessary stage resubmits, sometimes multiple times, and very confusing FetchFailure messages that report shuffle index files missing from the local disk.

Given that IndexShuffleBlockResolver commits data atomically, it seems unnecessary to ever delete committed task output: even in the rare case that a task is failed after it finishes committing shuffle output, it should be safe to retain that output.",,apachespark,ekhliang,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 06 23:56:05 UTC 2016,,,,,,,,,,"0|i335vj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/16 01:54;apachespark;User 'ericl' has created a pull request for this issue:
https://github.com/apache/spark/pull/14932;;;","06/Sep/16 23:56;joshrosen;Issue resolved by pull request 14932
[https://github.com/apache/spark/pull/14932];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shuffle service files not invalidated when a slave is lost,SPARK-17370,13002220,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ekhliang,ekhliang,ekhliang,02/Sep/16 01:21,22/May/17 03:19,14/Jul/23 06:29,07/Sep/16 19:34,,,,,,,,,2.0.1,2.1.0,,,Spark Core,,,,,,,,0,,,,,,"DAGScheduler invalidates shuffle files when an executor loss event occurs, but not when the external shuffle service is enabled. This is because when shuffle service is on, the shuffle file lifetime can exceed the executor lifetime.

However, it doesn't invalidate shuffle files when the shuffle service itself is lost (due to whole slave loss). This can cause long hangs when slaves are lost since the file loss is not detected until a subsequent stage attempts to read the shuffle files.",,apachespark,ekhliang,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20832,,,,SPARK-17519,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 07 19:34:39 UTC 2016,,,,,,,,,,"0|i335vb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/16 01:24;apachespark;User 'ericl' has created a pull request for this issue:
https://github.com/apache/spark/pull/14931;;;","07/Sep/16 19:34;joshrosen;Issue resolved by pull request 14931
[https://github.com/apache/spark/pull/14931];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MetastoreRelation toJSON throws exception,SPARK-17369,13002199,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,clockfly,clockfly,clockfly,01/Sep/16 23:22,06/Sep/16 07:43,14/Jul/23 06:29,06/Sep/16 07:43,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"MetastoreRelationSuite.toJSON now throws exception.

test case:
{code}
package org.apache.spark.sql.hive

import org.apache.spark.SparkFunSuite
import org.apache.spark.sql.catalyst.TableIdentifier
import org.apache.spark.sql.catalyst.catalog.{CatalogStorageFormat, CatalogTable, CatalogTableType}
import org.apache.spark.sql.types.{IntegerType, StructField, StructType}

class MetastoreRelationSuite extends SparkFunSuite {
  test(""makeCopy and toJSON should work"") {
    val table = CatalogTable(
      identifier = TableIdentifier(""test"", Some(""db"")),
      tableType = CatalogTableType.VIEW,
      storage = CatalogStorageFormat.empty,
      schema = StructType(StructField(""a"", IntegerType, true) :: Nil))
    val relation = MetastoreRelation(""db"", ""test"")(table, null, null)

    // No exception should be thrown
    relation.makeCopy(Array(""db"", ""test""))
    // No exception should be thrown
    relation.toJSON
  }
}
{code}",,apachespark,clockfly,cloud_fan,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 06 05:05:06 UTC 2016,,,,,,,,,,"0|i335qn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/16 23:29;apachespark;User 'clockfly' has created a pull request for this issue:
https://github.com/apache/spark/pull/14928;;;","06/Sep/16 02:51;cloud_fan;Issue resolved by pull request 14928
[https://github.com/apache/spark/pull/14928];;;","06/Sep/16 04:15;yhuai;Let'e create a patch for branch 2.0. I have reverted the current one because it breaks the 2.0 build.;;;","06/Sep/16 05:05;apachespark;User 'clockfly' has created a pull request for this issue:
https://github.com/apache/spark/pull/14968;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala value classes create encoder problems and break at runtime,SPARK-17368,13002196,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jodersky,arisofalaska@gmail.com,arisofalaska@gmail.com,01/Sep/16 23:12,25/Aug/18 09:09,14/Jul/23 06:29,14/Oct/16 00:48,1.6.2,2.0.0,,,,,,,2.1.0,,,,Spark Core,SQL,,,,,,,0,,,,,,"Using Scala value classes as the inner type for Datasets breaks in Spark 2.0 and 1.6.X.

This simple Spark 2 application demonstrates that the code will compile, but will break at runtime with the error. The value class is of course *FeatureId*, as it extends AnyVal.

{noformat}
Exception in thread ""main"" java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: Couldn't find v on int
assertnotnull(input[0, int, true], top level non-flat input object).v AS v#0
+- assertnotnull(input[0, int, true], top level non-flat input object).v
   +- assertnotnull(input[0, int, true], top level non-flat input object)
      +- input[0, int, true]"".
        at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:279)
        at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:421)
        at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:421)
{noformat}

Test code for Spark 2.0.0:

{noformat}
import org.apache.spark.sql.{Dataset, SparkSession}

object BreakSpark {
  case class FeatureId(v: Int) extends AnyVal

  def main(args: Array[String]): Unit = {
    val seq = Seq(FeatureId(1), FeatureId(2), FeatureId(3))
    val spark = SparkSession.builder.getOrCreate()
    import spark.implicits._
    spark.sparkContext.setLogLevel(""warn"")
    val ds: Dataset[FeatureId] = spark.createDataset(seq)
    println(s""BREAK HERE: ${ds.count}"")
  }
}

{noformat}
","JDK 8 on MacOS
Scala 2.11.8
Spark 2.0.0",apachespark,arisofalaska@gmail.com,jodersky,kiszk,marmbrus,maropu,mthai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17367,,,,,,,,,,,,,,SPARK-20384,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 17 23:48:06 UTC 2016,,,,,,,,,,"0|i335pz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/16 23:45;jodersky;FYI the issue also occurs for top-level value classes (i.e. {{FeatureId}} defined outside of {{object BreakSpark}})

Please also be aware that the given example will *not compile* in a spark shell. See this related issue https://issues.apache.org/jira/browse/SPARK-17367 regarding the definition of value classes in the REPL.;;;","01/Sep/16 23:52;arisofalaska@gmail.com;Yes, agreed that this code cannot be pasted into {{spark-shell}}. My assumption was that this would be a compiled JAR and then passed into {{spark-submit}} -- a normal Spark application. When you do so, the code compiles but Spark breaks at runtime.;;;","02/Sep/16 21:04;jodersky;I'm currently taking a look at this but my first analysis is not very positive: considering that value classes are pure compile-time constructs I think it isn't possible to do anything with them through reflection, which Catalyst assumes.
Here's a relevant blog post http://tech.kinja.com/scala-value-classes-and-reflection-here-be-dragons-1527846740
I'll check it out in a bit more detail but I fear that we'll have to resolve this as a won't fix and not support value classes in Datasets :(;;;","02/Sep/16 21:30;arisofalaska@gmail.com;I actually had an identical first thought from my experience of value classes and how they disappear in the JVM byte code. It would be very helpful if in the documentation somewhere that it was said that Scala value classes were explicitly not supported by spark datasets. The error messages are extremely cryptic and very confusing. Even better would be some kind of macro support or whatever else by Spark that would find and call it out of your code, but that's wishful thinking.;;;","02/Sep/16 22:00;jodersky;Yeah macros would be awesome, something with Scala.meta would be neat :) In the mean time it occurred to me that Catalyst uses ClassTags to do reflection in lots of places. These are generated during compile-time, so it might just yet be possible to support value classes.
A quick test showed me that value classes can be detected and their parameters accessed. Getting a Schema for such a case is trivial, I'll see about adding encoders next!;;;","06/Sep/16 22:35;jodersky;So I thought about this a bit more and although it is possible to support value classes, I currently see two main issues that make it cumbersome:

1. Catalyst (the engine behind Datasets) generates and compiles code during runtime, that will represent the actual computation. This code being Java, together with the fact that value classes don't have runtime representations, will require changes in the implementation of Encoders (see my experimental branch [here|https://github.com/apache/spark/compare/master...jodersky:value-classes]).

2. The largest problem of both is how will encoders for value classes be accessible? Currently, encoders are exposed as type classes and there is unfortunately no way to create type classes for classes extending AnyVal (you could create an encoder for AnyVals, however that would also apply to any primitive type and you would get implicit resolution conflicts). Requiring explicit encoders for value classes may work, however you would still have no compile-time safety, as accessing of a value class' inner val will occur during runtime and may hence fail if it is not encodable.

The cleanest solution would be to use meta programming: it would guarantee ""encodability"" during compile-time and could easily complement the current API. Unfortunately however, I don't think it could be included in Spark in the near future as the current meta programming solutions in Scala are either too new (scala.meta) or on their way to being deprecated (the current experimental scala macros). (I have been wanting to experiment with meta encoders for a while though, so maybe I'll try putting together an external library for that)

How inconvenient is it to extract the wrapped value before creating a dataset and re-wrapping your final results?;;;","07/Sep/16 05:04;arisofalaska@gmail.com;It goes from inconvenient to actually prohibitive in a practical sense. I have a Dataset[Something], and inside case class Something I have various other case classes, and somewhere inside there there is a particular value class. It is so crazy to do manual unwrapping and rewrapping that at this point I just decided to eat the performance cost and use a regular class, not value class (I removed the 'extends AnyVal').

More generally, specially accommodating for value classes is *really hard* in a practical setting because if I have a whole bunch of ADTs and other case classes I'm working with, how do I know if anywhere in my domain I used a *value class* and I suddenly have to jump through a bunch of hoops just so Spark doesn't blow up? If I just had a Dataset[ThisIsAValueClass] with the top-level class being a value class, what you're saying is easy, but in practice the value class is one of many things somewhere deeper.;;;","07/Sep/16 07:58;srowen;I get the problem, but is there actually any solution? this is a compile-time construct.;;;","07/Sep/16 18:27;jodersky;Hmm, you're right my assumption was of using only value classes in the beginning and at the end was too naive.

[~srowen], how likely do you think it is that we can include a meta-encoder in Spark? It could be included in the form of an optional import. Since the existing encoders/ScalaReflection framework already use runtime-reflection, my guess is that adding compile-time reflection will not be too difficult.;;;","07/Sep/16 18:35;srowen;This is beyond my knowledge I'm afraid. I'd help take a look if I can but not sure I'd know where to start on it myself!;;;","28/Sep/16 21:52;apachespark;User 'jodersky' has created a pull request for this issue:
https://github.com/apache/spark/pull/15284;;;","14/Oct/16 00:48;marmbrus;Issue resolved by pull request 15284
[https://github.com/apache/spark/pull/15284];;;","17/Oct/16 17:39;jodersky;[~arisofalaska@gmail.com] Let me explain the fix to what I thought was initially impossible.
Value classes do have a class-representation for compatibility with Java, and although this will have a slight overhead compared to the primitive counterpart, catalyst will mostly negate that overhead by proving its own encoders and operators on serialized objects. This means that any operations on datasets that allow user defined functions (e.g. `map`, `filter` etc) will work with the class representation instead of the wrapped value.
Regarding the availability of encoders: while we cannot create type-classes that apply only to value classes (an implicit for `AnyVal` will also be applied to primitive types), without resorting to macros, this fix adds value class support to existing encoders. E.g. you can define your value class as a case class and have a working encoder out-of-the-box.
Unfortunately there is no way to statically verify that the wrapped value is also encodable, but encoders in general will perform ""deep inspection"" during runtime.;;;","17/Oct/16 23:48;arisofalaska@gmail.com;That is great, thank you for the help with this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not query hive table starting with number,SPARK-17364,13002132,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,clockfly,epahomov,epahomov,01/Sep/16 17:39,15/Sep/16 18:55,14/Jul/23 06:29,15/Sep/16 18:55,2.0.1,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"I can do it with spark-1.6.2

{code}
SELECT * from  temp.20160826_ip_list limit 100
{code}

{code}
Error: org.apache.spark.sql.catalyst.parser.ParseException: 
extraneous input '.20160826' expecting {<EOF>, ',', 'SELECT', 'FROM', 'ADD', 'AS', 'ALL', 'DISTINCT', 'WHERE', 'GROUP', 'BY', 'GROUPING', 'SETS', 'CUBE', 'ROLLUP', 'ORDER', 'HAVING', 'LIMIT', 'AT', 'OR', 'AND', 'IN', NOT, 'NO', 'EXISTS', 'BETWEEN', 'LIKE', RLIKE, 'IS', 'NULL', 'TRUE', 'FALSE', 'NULLS', 'ASC', 'DESC', 'FOR', 'INTERVAL', 'CASE', 'WHEN', 'THEN', 'ELSE', 'END', 'JOIN', 'CROSS', 'OUTER', 'INNER', 'LEFT', 'RIGHT', 'FULL', 'NATURAL', 'LATERAL', 'WINDOW', 'OVER', 'PARTITION', 'RANGE', 'ROWS', 'UNBOUNDED', 'PRECEDING', 'FOLLOWING', 'CURRENT', 'ROW', 'WITH', 'VALUES', 'CREATE', 'TABLE', 'VIEW', 'REPLACE', 'INSERT', 'DELETE', 'INTO', 'DESCRIBE', 'EXPLAIN', 'FORMAT', 'LOGICAL', 'CODEGEN', 'CAST', 'SHOW', 'TABLES', 'COLUMNS', 'COLUMN', 'USE', 'PARTITIONS', 'FUNCTIONS', 'DROP', 'UNION', 'EXCEPT', 'INTERSECT', 'TO', 'TABLESAMPLE', 'STRATIFY', 'ALTER', 'RENAME', 'ARRAY', 'MAP', 'STRUCT', 'COMMENT', 'SET', 'RESET', 'DATA', 'START', 'TRANSACTION', 'COMMIT', 'ROLLBACK', 'MACRO', 'IF', 'DIV', 'PERCENT', 'BUCKET', 'OUT', 'OF', 'SORT', 'CLUSTER', 'DISTRIBUTE', 'OVERWRITE', 'TRANSFORM', 'REDUCE', 'USING', 'SERDE', 'SERDEPROPERTIES', 'RECORDREADER', 'RECORDWRITER', 'DELIMITED', 'FIELDS', 'TERMINATED', 'COLLECTION', 'ITEMS', 'KEYS', 'ESCAPED', 'LINES', 'SEPARATED', 'FUNCTION', 'EXTENDED', 'REFRESH', 'CLEAR', 'CACHE', 'UNCACHE', 'LAZY', 'FORMATTED', TEMPORARY, 'OPTIONS', 'UNSET', 'TBLPROPERTIES', 'DBPROPERTIES', 'BUCKETS', 'SKEWED', 'STORED', 'DIRECTORIES', 'LOCATION', 'EXCHANGE', 'ARCHIVE', 'UNARCHIVE', 'FILEFORMAT', 'TOUCH', 'COMPACT', 'CONCATENATE', 'CHANGE', 'CASCADE', 'RESTRICT', 'CLUSTERED', 'SORTED', 'PURGE', 'INPUTFORMAT', 'OUTPUTFORMAT', DATABASE, DATABASES, 'DFS', 'TRUNCATE', 'ANALYZE', 'COMPUTE', 'LIST', 'STATISTICS', 'PARTITIONED', 'EXTERNAL', 'DEFINED', 'REVOKE', 'GRANT', 'LOCK', 'UNLOCK', 'MSCK', 'REPAIR', 'RECOVER', 'EXPORT', 'IMPORT', 'LOAD', 'ROLE', 'ROLES', 'COMPACTIONS', 'PRINCIPALS', 'TRANSACTIONS', 'INDEX', 'INDEXES', 'LOCKS', 'OPTION', 'ANTI', 'LOCAL', 'INPATH', 'CURRENT_DATE', 'CURRENT_TIMESTAMP', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 19)

== SQL ==
SELECT * from  temp.20160826_ip_list limit 100
-------------------^^^

SQLState:  null
ErrorCode: 0
{code}",,apachespark,clockfly,dkbiswal,epahomov,hvanhovell,tsuresh,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 07 23:58:38 UTC 2016,,,,,,,,,,"0|i335bz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/16 07:44;yumwang;{code}
SELECT * from  `temp`.`20160826_ip_list` limit 100
{code};;;","05/Sep/16 20:46;clockfly;[~epahomov] 

Spark 2.0 rewrote the Sql parser with antlr. 
You can use the backticked version like Yuming's example.;;;","05/Sep/16 21:17;hvanhovell;This should work. For instance this works: {{SELECT * from 20160826_ip_list}}
;;;","07/Sep/16 23:53;apachespark;User 'clockfly' has created a pull request for this issue:
https://github.com/apache/spark/pull/15006;;;","07/Sep/16 23:55;clockfly;[~hvanhovell] That is because the antlr4 lexer breaks temp.20160826_ip_list to tokens like
{code}
// temp.20160826_ip_list is break downs to:
temp  // Matches the IDENTIFIER lexer rule
.20160826 // Matches the DECIMAL_VALUE lexer rule.
_ip_list  // Matches the IDENTIFIER lexer rule.
{code}

;;;","07/Sep/16 23:58;clockfly;I have a trial fix at https://github.com/apache/spark/pull/15006

Which will tokenize temp.20160826_ip_list as:
{code}
temp // Matches the IDENTIFIER lexer rule
. // Matches single dot
20160826_ip_list // Matches the IDENTIFIER lexer rule.
{code}

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix MultivariateOnlineSummerizer.numNonZeros,SPARK-17363,13002126,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,weichenxu123,weichenxu123,weichenxu123,01/Sep/16 17:15,18/Nov/16 21:30,14/Jul/23 06:29,03/Sep/16 17:06,2.1.0,,,,,,,,2.1.0,,,,ML,MLlib,,,,,,,0,,,,,,"The MultivariantOnlineSummerizer.numNonZeros method is wrong.

it should return the nnz array, not weightSum array.",,apachespark,weichenxu123,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,SPARK-17362,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 03 17:06:39 UTC 2016,,,,,,,,,,"0|i335an:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/16 17:20;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/14923;;;","03/Sep/16 17:06;srowen;Resolved by https://github.com/apache/spark/pull/14923;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
file-based external table without path should not be created,SPARK-17361,13002075,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,01/Sep/16 14:39,06/Sep/16 06:18,14/Jul/23 06:29,06/Sep/16 06:18,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 06 06:18:24 UTC 2016,,,,,,,,,,"0|i334zb:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"01/Sep/16 14:56;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/14921;;;","06/Sep/16 06:18;cloud_fan;Issue resolved by pull request 14921
[https://github.com/apache/spark/pull/14921];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cached table(parquet/orc) should be shard between beelines,SPARK-17358,13001915,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,waterman,waterman,waterman,01/Sep/16 06:55,06/Sep/16 03:00,14/Jul/23 06:29,06/Sep/16 02:59,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"I cached table(parquet) in Beeline1, and couldn't use cache data in Beeline2. But table in text format is OK.(Cached table(text) can be shard between beelines)

Beeline1
{code:sql}
CACHE TABLE src_pqt;

EXPLAIN SELECT * FROM src_pqt;
| == Physical Plan ==
InMemoryTableScan
   +- InMemoryRelation
         +- *FileScan parquet default.src_pqt
{code}

Beeline2
{code:sql}
EXPLAIN SELECT * FROM src_pqt;

| == Physical Plan ==
*FileScan parquet default.src_pqt
{code}",,apachespark,cloud_fan,kiszk,waterman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 06 02:59:02 UTC 2016,,,,,,,,,,"0|i333zz:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"01/Sep/16 07:43;apachespark;User 'watermen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14913;;;","06/Sep/16 02:59;cloud_fan;Issue resolved by pull request 14913
[https://github.com/apache/spark/pull/14913];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A large Metadata filed in Alias can cause OOM when calling TreeNode.toJSON,SPARK-17356,13001904,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,clockfly,clockfly,clockfly,01/Sep/16 05:57,06/Sep/16 12:08,14/Jul/23 06:29,06/Sep/16 08:08,,,,,,,,,1.6.3,2.0.1,2.1.0,,SQL,,,,,,,,0,,,,,,"When using MLLib, when calling toJSON on a plan with many level of sub-queries, it may cause out of memory exception with stack trace like this
{code}
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at scala.collection.mutable.AbstractSeq.<init>(Seq.scala:47)
	at scala.collection.mutable.AbstractBuffer.<init>(Buffer.scala:48)
	at scala.collection.mutable.ListBuffer.<init>(ListBuffer.scala:46)
	at scala.collection.immutable.List$.newBuilder(List.scala:396)
	at scala.collection.generic.GenericTraversableTemplate$class.newBuilder(GenericTraversableTemplate.scala:64)
	at scala.collection.AbstractTraversable.newBuilder(Traversable.scala:105)
	at scala.collection.TraversableLike$class.filter(TraversableLike.scala:262)
	at scala.collection.AbstractTraversable.filter(Traversable.scala:105)
	at scala.collection.TraversableLike$class.filterNot(TraversableLike.scala:274)
	at scala.collection.AbstractTraversable.filterNot(Traversable.scala:105)
	at org.json4s.jackson.JValueSerializer.serialize(JValueSerializer.scala:25)
	at org.json4s.jackson.JValueSerializer.serialize(JValueSerializer.scala:20)
	at org.json4s.jackson.JValueSerializer.serialize(JValueSerializer.scala:25)
	at org.json4s.jackson.JValueSerializer.serialize(JValueSerializer.scala:25)
	at org.json4s.jackson.JValueSerializer.serialize(JValueSerializer.scala:25)
	at org.json4s.jackson.JValueSerializer.serialize(JValueSerializer.scala:25)
	at org.json4s.jackson.JValueSerializer.serialize(JValueSerializer.scala:20)
	at org.json4s.jackson.JValueSerializer.serialize(JValueSerializer.scala:20)
	at org.json4s.jackson.JValueSerializer.serialize(JValueSerializer.scala:25)
	at org.json4s.jackson.JValueSerializer.serialize(JValueSerializer.scala:20)
	at org.json4s.jackson.JValueSerializer.serialize(JValueSerializer.scala:7)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128)
	at com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue(ObjectMapper.java:2881)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:2338)
	at org.json4s.jackson.JsonMethods$class.compact(JsonMethods.scala:34)
	at org.json4s.jackson.JsonMethods$.compact(JsonMethods.scala:50)
	at org.apache.spark.sql.catalyst.trees.TreeNode.toJSON(TreeNode.scala:566)
{code}

The query plan, stack trace, and jmap distribution is attached.

",,akrim,apachespark,clockfly,mwc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/16 06:20;clockfly;jmap.txt;https://issues.apache.org/jira/secure/attachment/12826562/jmap.txt","01/Sep/16 06:23;clockfly;jstack.txt;https://issues.apache.org/jira/secure/attachment/12826563/jstack.txt","01/Sep/16 06:17;clockfly;queryplan.txt;https://issues.apache.org/jira/secure/attachment/12826561/queryplan.txt",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 06 10:13:08 UTC 2016,,,,,,,,,,"0|i333xj:",9223372036854775807,,,,,,,,,,,,,1.6.3,2.0.1,2.1.0,,,,,,,,,"01/Sep/16 06:34;clockfly;*Analysis*

After looking at the mmap, there is a suspicious line
{code}
  20:             1        8388624  [Lorg.apache.spark.ml.attribute.Attribute;
{code}

This means a single Attribute array takes more than 8388624 bytes, and if each reference takes 8 bytes, it means there are 1 million attributes.

The array probably is used in AttributeGroup, whose signature is:
{code}
class AttributeGroup private (
    val name: String,
    val numAttributes: Option[Int],
    attrs: Option[Array[Attribute]])
{code}

And, in AttributeGroup, there is a toMetaData function which will convert the Attribute array to Meta data
{code}
  def toMetadata(): Metadata = toMetadata(Metadata.empty)
{code}

Finally, the metadata are saved to expression Attribute.
 
For example, in org.apache.spark.ml.feature.Interaction transform function, the meta data is set to attribute of Alias expression, when aliasing the udf function like this:

{code}
  override def transform(dataset: Dataset[_]): DataFrame = {
    ...
    // !NOTE!: This is an attribute group
    val featureAttrs = getFeatureAttrs(inputFeatures)

    def interactFunc = udf { row: Row =>
      ...
    }

    val featureCols = inputFeatures.map { f =>
      f.dataType match {
        case DoubleType => dataset(f.name)
        case _: VectorUDT => dataset(f.name)
        case _: NumericType | BooleanType => dataset(f.name).cast(DoubleType)
      }
    }

    // !NOTE!: The meta data i stored in Alias expresion by function call .as(..., featureAttrs.toMetadata())
    dataset.select(
      col(""*""),
      interactFunc(struct(featureCols: _*)).as($(outputCol), featureAttrs.toMetadata()))
  }

{code}

And, when calling toJSON, the metaData will be converted to JSON.;;;","01/Sep/16 06:37;clockfly;*Root cause:*

1. MLLib heavily leverage MetaData to store a lot of attribute information, in the case here, the metadata may contains tens of thousands of Attribute information. And the meta data may be stored to Alias expression like this:
{code}
case class Alias(child: Expression, name: String)(
    val exprId: ExprId = NamedExpression.newExprId,
    val qualifier: Option[String] = None,
    val explicitMetadata: Option[Metadata] = None,
    override val isGenerated: java.lang.Boolean = false)
{code} 

If we serialize the meta data to JSON, it will take a huge amount of memory.
;;;","01/Sep/16 07:07;clockfly;Reproducer:

{code}
# Trigger OOM
scala> :paste -raw
// Entering paste mode (ctrl-D to finish)

package org.apache.spark.ml.attribute

import org.apache.spark.ml.attribute._
import org.apache.spark.sql.catalyst.expressions.{Alias, Literal}
import org.apache.spark.sql.catalyst.plans.logical.LocalRelation
import org.apache.spark.sql.catalyst.dsl.plans._

object Test {
  def main(args: Array[String]): Unit = {
    val rand = new java.util.Random()
    val attr: Attribute = new BinaryAttribute(Some(""a""), Some(rand.nextInt(100000)), Some(Array(""value1"", ""value2"")))
    val attributeGroup = new AttributeGroup(""group"", Array.fill(1000000)(attr))
    val alias = Alias(Literal(0), ""alias"")(explicitMetadata = Some(attributeGroup.toMetadata()))
    val testRelation = LocalRelation()
    val query = testRelation.select((0 to 100).toSeq.map(_ => alias): _*)
    System.out.print(query.toJSON.length)
  }
}

// Exiting paste mode, now interpreting.

scala> org.apache.spark.ml.attribute.Test.main(null)
{code};;;","01/Sep/16 09:08;apachespark;User 'clockfly' has created a pull request for this issue:
https://github.com/apache/spark/pull/14915;;;","06/Sep/16 10:13;apachespark;User 'clockfly' has created a pull request for this issue:
https://github.com/apache/spark/pull/14973;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Work around exception thrown by HiveResultSetMetaData.isSigned,SPARK-17355,13001903,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,01/Sep/16 05:48,01/Sep/16 23:46,14/Jul/23 06:29,01/Sep/16 23:46,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"Attempting to use Spark SQL's JDBC data source against the Hive ThriftServer results in a {{java.sql.SQLException: Method not supported}} exception from {{org.apache.hive.jdbc.HiveResultSetMetaData.isSigned}}. Here are two user reports of this issue:

- https://stackoverflow.com/questions/34067686/spark-1-5-1-not-working-with-hive-jdbc-1-2-0
- https://stackoverflow.com/questions/32195946/method-not-supported-in-spark

I have filed HIVE-14684 to attempt to fix this in Hive by implementing the {{isSigned}} method, but in the meantime / for compatibility with older JDBC drivers I think we should add special-case error handling to work around this bug.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-14684,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 01 23:46:13 UTC 2016,,,,,,,,,,"0|i333xb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/16 05:55;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14911;;;","01/Sep/16 23:46;joshrosen;Issue resolved by pull request 14911
[https://github.com/apache/spark/pull/14911];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.ClassCastException: java.lang.Integer cannot be cast to java.sql.Date,SPARK-17354,13001897,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,baghelamit,baghelamit,01/Sep/16 05:27,12/Dec/22 18:10,14/Jul/23 06:29,09/Sep/16 21:27,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"Hive database has one table with column type Date. While running select query using Spark 2.0.0 SQL and calling show() function on DF throws ClassCastException. Same code is working fine on Spark 1.6.2. Please see the sample code below.

{code}
import java.util.Calendar
val now = Calendar.getInstance().getTime()
case class Order(id : Int, customer : String, city : String, pdate : java.sql.Date)
val orders = Seq(
      Order(1, ""John S"", ""San Mateo"", new java.sql.Date(now.getTime)),
      Order(2, ""John D"", ""Redwood City"", new java.sql.Date(now.getTime))
	  )	  
orders.toDF.createOrReplaceTempView(""orders1"")

spark.sql(""CREATE TABLE IF NOT EXISTS order(id INT, customer String,city String)PARTITIONED BY (pdate DATE)STORED AS PARQUETFILE"")
spark.sql(""set hive.exec.dynamic.partition.mode=nonstrict"")
spark.sql(""INSERT INTO TABLE order PARTITION(pdate) SELECT * FROM orders1"")
spark.sql(""SELECT * FROM order"").show()
{code}  

Exception details

{code}
16/09/01 10:30:07 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 6)
java.lang.ClassCastException: java.lang.Integer cannot be cast to java.sql.Date
	at org.apache.spark.sql.execution.vectorized.ColumnVectorUtils.populate(ColumnVectorUtils.java:89)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initBatch(VectorizedParquetRecordReader.java:185)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initBatch(VectorizedParquetRecordReader.java:204)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1.apply(ParquetFileFormat.scala:362)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1.apply(ParquetFileFormat.scala:339)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:116)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:91)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code} 

Expected output 

{code} 
+---+--------+------------+----------+
| id|customer|        city|     pdate|
+---+--------+------------+----------+
|  1|  John S|   San Mateo|2016-09-01|
|  2|  John D|Redwood City|2016-09-01|
+---+--------+------------+----------+
{code} 

Workaround for Spark 2.0.0

Setting enableVectorizedReader=false before show() method on DF returns expected result.
{code} 
spark.sql(""set spark.sql.parquet.enableVectorizedReader=false"")
{code} 



",,apachespark,baghelamit,davies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 09 21:27:04 UTC 2016,,,,,,,,,,"0|i333vz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/16 11:27;gurwls223;I see. This seems a bug in `ColumnVectorUtils`. IIUC, the internel representation of `DateType` should be integer but it seems trying to read `Date`. Let me work on this please.;;;","01/Sep/16 13:15;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/14919;;;","09/Sep/16 21:27;davies;Issue resolved by pull request 14919
[https://github.com/apache/spark/pull/14919];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CREATE TABLE LIKE statements when Source is a VIEW,SPARK-17353,13001883,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,01/Sep/16 03:28,06/Sep/16 02:47,14/Jul/23 06:29,01/Sep/16 08:39,2.0.1,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"- Add a support for tempory view

- When the source table is a `VIEW`, the metadata of the generated table contains the original view text and view original text. So far, this does not break anything, but it could cause something wrong in Hive. (For example, https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java#L1405-L1406)

- When the type of source table is a view, the target table is using the default format of data source tables: `spark.sql.sources.default`.

",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 03 06:41:04 UTC 2016,,,,,,,,,,"0|i333sv:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"01/Sep/16 03:30;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14531;;;","03/Sep/16 06:41;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14946;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executor computing time can be negative-number because of calculation error,SPARK-17352,13001872,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,01/Sep/16 01:18,02/Sep/16 09:27,14/Jul/23 06:29,02/Sep/16 09:27,2.0.0,,,,,,,,2.0.1,2.1.0,,,Web UI,,,,,,,,0,,,,,,"In StagePage, executor-computing-time is calculated but calculation error can occur potentially because it's calculated by subtraction of floating numbers.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 02 09:27:08 UTC 2016,,,,,,,,,,"0|i333qf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/16 01:39;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/14908;;;","02/Sep/16 09:27;srowen;Issue resolved by pull request 14908
[https://github.com/apache/spark/pull/14908];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable default use of KryoSerializer in Thrift Server,SPARK-17350,13001860,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,01/Sep/16 00:08,01/Nov/16 23:18,14/Jul/23 06:29,01/Nov/16 23:18,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"In SPARK-4761 (December 2014) we enabled Kryo serialization by default in the Spark Thrift Server. However, I don't think that the original rationale for doing this still holds as all Spark SQL serialization should now be performed via efficient encoders and our UnsafeRow format. In addition, the use of Kryo as the default serializer can introduce performance problems because the creation of new KryoSerializer instances is expensive and we haven't performed instance-reuse optimizations in several code paths (including DirectTaskResult deserialization). Given all of this, I propose to revert back to using JavaSerializer as the default serializer in the Thrift Server.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 01 00:12:07 UTC 2016,,,,,,,,,,"0|i333nr:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"01/Sep/16 00:12;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14906;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update testthat package on Jenkins,SPARK-17349,13001818,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shaneknapp,shivaram,shivaram,31/Aug/16 21:01,12/Dec/22 18:10,14/Jul/23 06:29,31/Aug/16 21:27,,,,,,,,,,,,,Build,SparkR,,,,,,,0,,,,,,"As per https://github.com/apache/spark/pull/14889#issuecomment-243697097 using version 1.0 of testthat will improve the messages printed at the end of a test to include skipped tests etc.

The current package version on Jenkins is 0.11.0 and we can upgrade this to 1.0.0 if there are no conflicts etc.",,shaneknapp,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 31 23:58:21 UTC 2016,,,,,,,,,,"0|i333ef:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/16 21:01;shivaram;cc [~hyukjin.kwon] [~felixcheung];;;","31/Aug/16 21:22;shivaram;[~shaneknapp] Could we upgrade the testthat package on Jenkins ? Something like

{code}
Rscript -e 'install.packages(""testthat"", repos=""http://cran.stat.ucla.edu/"")'
{code}

should do the trick I think;;;","31/Aug/16 21:27;shaneknapp;done!;;;","31/Aug/16 23:58;gurwls223;Cool!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect results from subquery transformation,SPARK-17348,13001810,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nsyca,nsyca,nsyca,31/Aug/16 20:30,15/Nov/16 22:22,14/Jul/23 06:29,14/Nov/16 20:04,2.0.0,,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,0,correctness,,,,,"{noformat}
Seq((1,1)).toDF(""c1"",""c2"").createOrReplaceTempView(""t1"")
Seq((1,1),(2,0)).toDF(""c1"",""c2"").createOrReplaceTempView(""t2"")
sql(""select c1 from t1 where c1 in (select max(t2.c1) from t2 where t1.c2 >= t2.c2)"").show

+---+
| c1|
+---+
|  1|
+---+
{noformat}

The correct result of the above query should be an empty set. Here is an explanation:

Both rows from T2 satisfies the correlated predicate T1.C2 >= T2.C2 when T1.C1 = 1 so both rows needs to be processed in the same group of the aggregation process in the subquery. The result of the aggregation yields MAX(T2.C1) as 2. Therefore, the result of the evaluation of the predicate T1.C1 (which is 1) IN MAX(T2.C1) (which is 2) should be an empty set.",,apachespark,davies,hvanhovell,nsyca,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18455,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 04 17:36:03 UTC 2016,,,,,,,,,,"0|i333cn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/16 20:36;nsyca;The root cause is in the rule {{ResolveSubquery}} in Analysis phase where the correlated predicate {{T1.C2 >= T2.C2}} is pulled up above the Aggregation operator.

 {noformat}
== Parsed Logical Plan ==
'Project ['c1]
+- 'Filter 'c1 IN (list#324)
   :  +- 'Project [unresolvedalias('max('t2.c1), None)]
   :     +- 'Filter ('t1.c2 >= 't2.c2)
   :        +- 'UnresolvedRelation `t2`
   +- 'UnresolvedRelation `t1`

== Analyzed Logical Plan ==
c1: int
Project [c1#334]
+- Filter predicate-subquery#324 [(c1#334 = max(c1)#339) && (c2#335 >= c2#337)]
   :  +- Aggregate [c2#337], [max(c1#336) AS max(c1)#339, c2#337]
   :     +- SubqueryAlias t2
   :        +- Project [_1#299 AS c1#336, _2#300 AS c2#337]
   :           +- LocalRelation [_1#299, _2#300]
   +- SubqueryAlias t1
      +- Project [_1#288 AS c1#334, _2#289 AS c2#335]
         +- LocalRelation [_1#288, _2#289]
{noformat}

The pull up is okay only when the comparison is an equality.;;;","31/Aug/16 22:06;hvanhovell;This is an interesting one. TBH I have never seen such a query being used in practice. Could you tell me what the analyzed plan should look like, because the only solution to me would be to implicitly join {{t1}} to {{t2}} in the subquery. 

For 2.0.1 we should fail analysis in this case. We have an analyzer rule in place to make sure no one uses aggregates in combination with correlated scalar subqueries. We could extend that or move that into analysis. It would be nice to have a fix for 2.1.;;;","01/Sep/16 15:59;nsyca;Here is a use case:

""Find customers who bought items that are higher than the average price of the same items on the period of last two years from the date the items were bought.""

That is replacing from my example above:
- IN with >
- the correlated predicate t1.c2 > t2.c2 with a BETWEEN..AND predicate for the 2-year period of the item bought
- T1 is ITEM table
- T2 is also the same ITEM table

and adding
- perhaps a CUSTOMER table to join with the ITEM on the parent side to pick up customer profiles
- and another correlated equality join on the ITEM_ID between the parent ITEM and the subquery ITEM

The bottom line is Spark 2.0 supports only a subset of SQL subqueries. A good subset of them are blocked. This is, I believe, because of the lack of runtime capability to process subqueries, specifically correlated subqueries. The current implementation aggressively (and, as in this case, incorrectly) converts correlated subqueries to some forms of joins. If we want to support a full class of SQL subqueries, we cannot assume that all forms of subqueries can be ""de-correlated"" and we need to have a fallback plan to execute subqueries in runtime.

This work is not a single PR work. It needs to craft out a design. Areas that we need to work are:

1. (Assume I am right) Extend nested-loop join to support correlation processing, both shallow and deep correlations.
2. Rework on the representation in the Logical Plan to cover all the supported cases.
3. Rework and extend the ""de-correlation"" rewrite code.;;;","02/Nov/16 15:54;nsyca;I propose to use this JIRA to just close off the incorrect result scenario as an error case. Or I could open a new JIRA for it and maintain this JIRA for a general solution of supporting subquery processing in run-time.

I also propose that we should separate the subquery to join transform from Analyzer phase to Optimizer phase. My understanding is the objective of Analyzer phase is to process any unresolved constructs from the LogicalPlan output from Parser phase to resolved LogicalPlan. Then Optimizer phase will take the resolved LogicalPlan from Analyzer and make it an optimal LogicalPlan.;;;","04/Nov/16 04:31;apachespark;User 'nsyca' has created a pull request for this issue:
https://github.com/apache/spark/pull/15763;;;","04/Nov/16 04:33;nsyca;[~hvanhovell], would you please review my PR to return an Analysis exception on the scenarios that could produce incorrect results? Thank you.;;;","04/Nov/16 16:24;nsyca;I'd like to a note that a piece of existing code that closes to address this problem but only done for the scalar subquery context can be found by searching this pattern in CheckAnalysis.scala (line 126):
_""The correlated scalar subquery can only contain equality predicates:""_

An existing test coverage for that scenario is at
_SubquerySuite/non-equal correlated scalar subquery_;;;","04/Nov/16 17:36;hvanhovell;Yeah, it would be nice if you can consolidate the two, and put your current fix there as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Style of event timeline is broken,SPARK-17342,13001769,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,31/Aug/16 18:23,02/Sep/16 07:46,14/Jul/23 06:29,02/Sep/16 07:46,2.0.0,,,,,,,,2.0.1,2.1.0,,,Web UI,,,,,,,,0,,,,,,"SPARK-15373 updated the version of vis.js to 4.16.1. As of 4.0.0, some class was renamed like 'timeline to vis-timeline' but that ticket didn't care and now style is broken.",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17216,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 02 07:46:36 UTC 2016,,,,,,,,,,"0|i3333j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/16 18:57;srowen;Go ahead and fix it up. CC [~dongjoon];;;","31/Aug/16 20:13;dongjoon;Oops. Thank you for reporting and going to fix that, [~sarutak]. Thank you for pining me, [~srowen].;;;","31/Aug/16 23:22;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/14900;;;","02/Sep/16 07:46;srowen;Issue resolved by pull request 14900
[https://github.com/apache/spark/pull/14900];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix SparkR tests on Windows,SPARK-17339,13001738,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,shivaram,shivaram,31/Aug/16 16:16,12/Dec/22 18:11,14/Jul/23 06:29,07/Sep/16 10:27,,,,,,,,,2.0.1,2.1.0,,,SparkR,Tests,,,,,,,0,,,,,,"A number of SparkR tests are current failing when run on Windows as discussed in https://github.com/apache/spark/pull/14743

The list of tests that fail right now is at https://gist.github.com/shivaram/7693df7bd54dc81e2e7d1ce296c41134

A full log from a build and test on AppVeyor is at https://ci.appveyor.com/project/HyukjinKwon/spark/build/46-test123",,apachespark,felixcheung,sarutak,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 08 04:11:42 UTC 2016,,,,,,,,,,"0|i332wn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/16 16:17;shivaram;From a cursory look, most of the errors seem to be related to file paths when loading / saving data. I think we can probably fix them in one go, but if we want we can also split this into smaller sub-tasks.;;;","31/Aug/16 18:32;felixcheung;Seems that way. I can help but I don't have Windows to test this..;;;","31/Aug/16 19:28;shivaram;We are discussing automating Windows builds in https://github.com/apache/spark/pull/14859 -- so we'll have some way to test this if we have a proposed fix. But yeah it might still be hard to debug / trace why this is happening;;;","31/Aug/16 20:14;shivaram;Looking at the code my current guess is that calling normalizePath might result in file paths that dont work in Windows. The R function has an option ""winslash: the separator to be used on Windows - ignored elsewhere. Must be one of ‘c(""/"", ""\\"")’."" Right now we are using the default which is ""\\"";;;","05/Sep/16 05:33;shivaram;[~felixcheung] [~hyukjin.kwon] I looked into this a little bit more today and I think the failures are related to SPARK-11227 (or rather the PR at https://github.com/apache/spark/pull/13738/files) -- Specifically the line which calls `FileSystem.get(new URI(path), hadoopConfiguration)` in `hadoopFile` leads to failures with a stack trace that looks like.  
{code}
  java.net.URISyntaxException: Illegal character in opaque part at index 2: C:\Users\shivaram\Downloads\spark-1-sparkr-win-tests-fix\spark-1-sparkr-win-tests-fix\README.md
        at java.net.URI$Parser.fail(URI.java:2829)
        at java.net.URI$Parser.checkChars(URI.java:3002)
        at java.net.URI$Parser.parse(URI.java:3039)
        at java.net.URI.<init>(URI.java:595)
        at org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply(SparkContext.scala:995)
        at org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply(SparkContext.scala:990)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
        at org.apache.spark.SparkContext.withScope(SparkContext.scala:686)
        at org.apache.spark.SparkContext.hadoopFile(SparkContext.scala:990)
        at org.apache.spark.SparkContext$$anonfun$textFile$1.apply(SparkContext.scala:806)
{code}

If I comment out the changes from that PR, the SparkR tests seem to pass. 

[~sarutak] Do you know if there is a better way to fix the issue in SPARK-11227 without trying to construct a URI from the path ?;;;","05/Sep/16 05:41;shivaram;Specifically can we construct a Hadoop `Path` and then get the URI from that Path ? The reason I think this might work for the SparkR case is that textFile is able to read that file after it is converted to a Path;;;","05/Sep/16 06:07;sarutak;[~shivaram] I think more discussion is needed for the better solution of SPARK-11227.
For hot-fix, what you mentioned is one solution and another can be using `org.apache.spark.util.Utils.resolveURI`.;;;","05/Sep/16 06:33;gurwls223;[~sarutak] [~shivaram] Please cc me if any of you submit a PR so that I can run the build automation (as it is not merged yet). Otherwise, I can do this if you tell me which one is preferred.;;;","05/Sep/16 06:38;shivaram;Thanks [~hyukjin.kwon] -- It will be great if you can try the `Utils.resolveURI` change as a PR and run that through the build automation tool. 

Also the reason I was trying to debug this today is I feel like it would be better to make the build green before merging the automation -- otherwise it might confuse other contributors etc.;;;","05/Sep/16 06:39;sarutak;[~hyukjin.kwon] Go ahead and submit a PR. Thanks!;;;","05/Sep/16 06:41;gurwls223;Yeap, I totally agree. Thank you both! Will submit a PR within today.;;;","05/Sep/16 11:44;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/14960;;;","08/Sep/16 00:25;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/15008;;;","08/Sep/16 02:48;hadoopqa;
    [ https://issues.apache.org/jira/browse/SPARK-17339?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15464250#comment-15464250 ] 

Hyukjin Kwon commented on SPARK-17339:
--------------------------------------

Yeap, I totally agree. Thank you both! Will submit a PR within today.




--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

---------------------------------------------------------------------
To unsubscribe, e-mail: issues-unsubscribe@spark.apache.org
For additional commands, e-mail: issues-help@spark.apache.org



---------------------------------------------------------------------
To unsubscribe, e-mail: issues-unsubscribe@spark.apache.org
For additional commands, e-mail: issues-help@spark.apache.org

;;;","08/Sep/16 02:48;hadoopqa;
    [ https://issues.apache.org/jira/browse/SPARK-17339?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15464250#comment-15464250 ] 

Hyukjin Kwon commented on SPARK-17339:
--------------------------------------

Yeap, I totally agree. Thank you both! Will submit a PR within today.




--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

---------------------------------------------------------------------
To unsubscribe, e-mail: issues-unsubscribe@spark.apache.org
For additional commands, e-mail: issues-help@spark.apache.org



---------------------------------------------------------------------
To unsubscribe, e-mail: issues-unsubscribe@spark.apache.org
For additional commands, e-mail: issues-help@spark.apache.org

;;;","08/Sep/16 02:56;hadoopqa;
    [ https://issues.apache.org/jira/browse/SPARK-17339?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15464247#comment-15464247 ] 

Kousuke Saruta commented on SPARK-17339:
----------------------------------------

[~hyukjin.kwon] Go ahead and submit a PR. Thanks!




--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

---------------------------------------------------------------------
To unsubscribe, e-mail: issues-unsubscribe@spark.apache.org
For additional commands, e-mail: issues-help@spark.apache.org



---------------------------------------------------------------------
To unsubscribe, e-mail: issues-unsubscribe@spark.apache.org
For additional commands, e-mail: issues-help@spark.apache.org

;;;","08/Sep/16 02:56;hadoopqa;
    [ https://issues.apache.org/jira/browse/SPARK-17339?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15464247#comment-15464247 ] 

Kousuke Saruta commented on SPARK-17339:
----------------------------------------

[~hyukjin.kwon] Go ahead and submit a PR. Thanks!




--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

---------------------------------------------------------------------
To unsubscribe, e-mail: issues-unsubscribe@spark.apache.org
For additional commands, e-mail: issues-help@spark.apache.org



---------------------------------------------------------------------
To unsubscribe, e-mail: issues-unsubscribe@spark.apache.org
For additional commands, e-mail: issues-help@spark.apache.org

;;;","08/Sep/16 03:27;hadoopqa;
    [ https://issues.apache.org/jira/browse/SPARK-17339?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15464244#comment-15464244 ] 

Shivaram Venkataraman commented on SPARK-17339:
-----------------------------------------------

Thanks [~hyukjin.kwon] -- It will be great if you can try the `Utils.resolveURI` change as a PR and run that through the build automation tool. 

Also the reason I was trying to debug this today is I feel like it would be better to make the build green before merging the automation -- otherwise it might confuse other contributors etc.




--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

---------------------------------------------------------------------
To unsubscribe, e-mail: issues-unsubscribe@spark.apache.org
For additional commands, e-mail: issues-help@spark.apache.org



---------------------------------------------------------------------
To unsubscribe, e-mail: issues-unsubscribe@spark.apache.org
For additional commands, e-mail: issues-help@spark.apache.org

;;;","08/Sep/16 03:28;hadoopqa;
    [ https://issues.apache.org/jira/browse/SPARK-17339?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15464244#comment-15464244 ] 

Shivaram Venkataraman commented on SPARK-17339:
-----------------------------------------------

Thanks [~hyukjin.kwon] -- It will be great if you can try the `Utils.resolveURI` change as a PR and run that through the build automation tool. 

Also the reason I was trying to debug this today is I feel like it would be better to make the build green before merging the automation -- otherwise it might confuse other contributors etc.




--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

---------------------------------------------------------------------
To unsubscribe, e-mail: issues-unsubscribe@spark.apache.org
For additional commands, e-mail: issues-help@spark.apache.org



---------------------------------------------------------------------
To unsubscribe, e-mail: issues-unsubscribe@spark.apache.org
For additional commands, e-mail: issues-help@spark.apache.org

;;;","08/Sep/16 04:11;hadoopqa;
    [ https://issues.apache.org/jira/browse/SPARK-17339?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15464236#comment-15464236 ] 

Hyukjin Kwon commented on SPARK-17339:
--------------------------------------

[~sarutak] [~shivaram] Please cc me if any of you submit a PR so that I can run the build automation (as it is not merged yet). Otherwise, I can do this if you tell me which one is preferred.




--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

---------------------------------------------------------------------
To unsubscribe, e-mail: issues-unsubscribe@spark.apache.org
For additional commands, e-mail: issues-help@spark.apache.org



---------------------------------------------------------------------
To unsubscribe, e-mail: issues-unsubscribe@spark.apache.org
For additional commands, e-mail: issues-help@spark.apache.org

;;;","08/Sep/16 04:11;hadoopqa;
    [ https://issues.apache.org/jira/browse/SPARK-17339?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15464236#comment-15464236 ] 

Hyukjin Kwon commented on SPARK-17339:
--------------------------------------

[~sarutak] [~shivaram] Please cc me if any of you submit a PR so that I can run the build automation (as it is not merged yet). Otherwise, I can do this if you tell me which one is preferred.




--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

---------------------------------------------------------------------
To unsubscribe, e-mail: issues-unsubscribe@spark.apache.org
For additional commands, e-mail: issues-help@spark.apache.org



---------------------------------------------------------------------
To unsubscribe, e-mail: issues-unsubscribe@spark.apache.org
For additional commands, e-mail: issues-help@spark.apache.org

;;;",,,,,,,,,,,,,,,,,,,,,,
Incomplete algorithm for name resolution in Catalyst paser may lead to incorrect result,SPARK-17337,13001680,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,nsyca,nsyca,31/Aug/16 14:40,11/Jan/17 20:43,14/Jul/23 06:29,04/Nov/16 20:36,2.0.0,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,1,correctness,,,,,"While investigating SPARK-16951, I found an incorrect results case from a NOT IN subquery. I thought originally it is an edge case. Further investigation found this is a more general problem.",,apachespark,emlyn,hvanhovell,nsyca,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17154,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 04 20:36:05 UTC 2016,,,,,,,,,,"0|i332jr:",9223372036854775807,,,,,hvanhovell,,,,,,,,,,,,,,,,,,,"31/Aug/16 14:42;nsyca;This problem originated from the Case 1 documented in SPARK-16951. The script illustrates the problem

{noformat}
Seq(1,2).toDF(""c1"").createOrReplaceTempView(""t1"")
Seq(1).toDF(""c2"").createOrReplaceTempView(""t2"")

scala> sql(""select * from (select t2.c2+1 as c3 from t1 left join t2 on t1.c1=t2.c2) t3 where c3 not in (select c2 from t2)"").show
+----+
|  c3|
+----+
|   2|
|null|
+----+
{noformat}

The correct answer is 1 row of (2). From the plan below, the incorrect portion of the plan is the LeftAnti, rewritten from the NOT IN subquery, is pushed down below the (T1 LOJ T2) operation. Because LeftAnti predicate is evaluated to unknown (or null) if any argument of a comparison operator in the predicate is null, e.g. NULL = <value> is evaluated to unknown (which is equivalent to false in the context of a predicate), the LeftAnti predicate cannot be pushed down into a LOJ operation.

{noformat}
scala> sql(""select * from (select t2.c2+1 as c3 from t1 left join t2 on t1.c1=t2.c2) t3 where c3 not in (select c2 from t2)"").explain(true)
== Parsed Logical Plan ==
'Project [*]
+- 'Filter NOT 'c3 IN (list#124)
   :  +- 'SubqueryAlias list#124
   :     +- 'Project ['c2]
   :        +- 'UnresolvedRelation `t2`
   +- 'SubqueryAlias t3
      +- 'Project [('t2.c2 + 1) AS c3#123]
         +- 'Join LeftOuter, ('t1.c1 = 't2.c2)
            :- 'UnresolvedRelation `t1`
            +- 'UnresolvedRelation `t2`

== Analyzed Logical Plan ==
c3: int
Project [c3#123]
+- Filter NOT predicate-subquery#124 [(c3#123 = c2#77)]
   :  +- SubqueryAlias predicate-subquery#124 [(c3#123 = c2#77)]
   :     +- Project [c2#77]
   :        +- SubqueryAlias t2
   :           +- Project [value#75 AS c2#77]
   :              +- LocalRelation [value#75]
   +- SubqueryAlias t3
      +- Project [(c2#77 + 1) AS c3#123]
         +- Join LeftOuter, (c1#3 = c2#77)
            :- SubqueryAlias t1
            :  +- Project [value#1 AS c1#3]
            :     +- LocalRelation [value#1]
            +- SubqueryAlias t2
               +- Project [value#75 AS c2#77]
                  +- LocalRelation [value#75]

== Optimized Logical Plan ==
Project [(c2#77 + 1) AS c3#123]
+- Join LeftOuter, (c1#3 = c2#77)
   :- Project [value#1 AS c1#3]
   :  +- Join LeftAnti, (isnull(((c2#77 + 1) = c2#77)) || ((c2#77 + 1) = c2#77))
   :     :- LocalRelation [value#1]
   :     +- LocalRelation [c2#77]
   +- LocalRelation [c2#77]
{noformat};;;","31/Aug/16 14:43;nsyca;@hvanhovell has observed that by disabling the rule {{PushPredicateThroughJoin}} will solve this problem. The predicate is pushed down because the code thinks all the columns in the LeftAnti predicate reference to the T2, the right table of (T1 LOJ T2). The fact is some of the C2#77 in the LeftAnti predicate are from the T2 in the NOT IN subquery.

However, {{PushPredicateThroughJoin}} is not the root cause of the problem. The problem is the identifier {{[value#75 AS c2#77]}} is used in two places from the two unrelated references of {{SubqueryAlias t2}}. This can be demonstrated by replacing the T2 in the subquery by a different name, SQ in the example below. We will not see the LeftAnti predicate pushed down below the LOJ.

{noformat}
Seq(1).toDF(""cx"").createOrReplaceTempView(""sq"")

scala> sql(""select * from (select t2.c2+1 as c3 from t1 left join t2 on t1.c1=t2.c2) t3 where c3 not in (select cx from sq)"").explain(true)

...

== Optimized Logical Plan ==
Project [(c2#77 + 1) AS c3#137]
+- Join LeftAnti, (isnull(((c2#77 + 1) = cx#133)) || ((c2#77 + 1) = cx#133))
   :- Join LeftOuter, (c1#3 = c2#77)
   :  :- LocalRelation [c1#3]
   :  +- LocalRelation [c2#77]
   +- LocalRelation [cx#133]
...
{noformat};;;","31/Aug/16 15:52;apachespark;User 'nsyca' has created a pull request for this issue:
https://github.com/apache/spark/pull/14899;;;","08/Sep/16 16:01;nsyca;The same problem surfaced in different symptoms was discussed in SPARK-13801, SPARK-14040, and SPARK-17154. The problem reported here is a specific pattern. We shall find a solution that addresses the root cause. I am considering closing this JIRA as a duplicate.;;;","03/Nov/16 23:51;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/15761;;;","04/Nov/16 00:13;nsyca;As commented in the PR, the code was nicely done but the bigger problem remains there, tracked by SPARK-17154. I made a few attempts to fix the root cause of the problem surfaced in many symptoms but it has taken me long and not yet as clean as I want. Also I am seeing [~kousuke] keeps refining his work through a series of PRs for that so I hesitate to try competing with his solution.;;;","04/Nov/16 17:56;hvanhovell;[~nsyca] You are right to say that this is part of a larger set of problems which we should address and which given the amount of attempts made to fix this proves that it it non-trivial. It is not a problem to have different (competing) PRs for this problem, this usually leads to solutions. So please submit a PR is feel that this is a better solution.

However, for this ticket I just want to solve the correctness issue at hand.;;;","04/Nov/16 18:01;nsyca;Totally agreed on your approach. We should close off any potential incorrect results at the soonest possible.

Thanks for the advise on the approach of different/competing PRs. I am relatively new to the community and try to be careful not to break any etiquette of the community.;;;","04/Nov/16 20:36;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/15772;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repeated calls sbin/spark-config.sh file Causes ${PYTHONPATH} Value duplicate,SPARK-17336,13001652,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bryanc,axu4apache,axu4apache,31/Aug/16 13:12,11/Sep/16 09:20,14/Jul/23 06:29,11/Sep/16 09:20,2.0.0,,,,,,,,2.0.1,2.1.0,,,PySpark,,,,,,,,0,,,,,,"On Spark start up by command: sbin/start-all.sh, the sbin/spark-config.sh Repeated calls. In sbin/spark-config.sh code.
{code:title=sbin/spark-config.sh|borderStyle=solid}
# Add the PySpark classes to the PYTHONPATH:
export PYTHONPATH=""${SPARK_HOME}/python:${PYTHONPATH}""
export PYTHONPATH=""${SPARK_HOME}/python/lib/py4j-0.10.3-src.zip:${PYTHONPATH}""
{code}
{color:red}PYTHONPATH{color} has duplicate Value.
example:
{code:borderStyle=solid}
axu4iMac:spark-2.0.0-hadoop2.4 axu$ sbin/start-all.sh  | grep PYTHONPATH
axu.print [Log] [6,16,31] [sbin/spark-config.sh] 定义PYTHONPATH
axu.print [sbin/spark-config.sh] [Define Global] PYTHONPATH(1): [/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python:]
axu.print [Log] [7,17,32] [sbin/spark-config.sh] 再次定义PYTHONPATH
axu.print [sbin/spark-config.sh] [Define Global] PYTHONPATH(2): [/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python/lib/py4j-0.10.1-src.zip:/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python:]
axu.print [Log] [6,16,31] [sbin/spark-config.sh] 定义PYTHONPATH
axu.print [sbin/spark-config.sh] [Define Global] PYTHONPATH(1): [/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python:/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python/lib/py4j-0.10.1-src.zip:/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python:]
axu.print [Log] [7,17,32] [sbin/spark-config.sh] 再次定义PYTHONPATH
axu.print [sbin/spark-config.sh] [Define Global] PYTHONPATH(2): [/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python/lib/py4j-0.10.1-src.zip:/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python:/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python/lib/py4j-0.10.1-src.zip:/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python:]
axu.print [Log] [6,16,31] [sbin/spark-config.sh] 定义PYTHONPATH
axu.print [sbin/spark-config.sh] [Define Global] PYTHONPATH(1): [/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python:/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python/lib/py4j-0.10.1-src.zip:/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python:/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python/lib/py4j-0.10.1-src.zip:/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python:]
axu.print [Log] [7,17,32] [sbin/spark-config.sh] 再次定义PYTHONPATH
axu.print [sbin/spark-config.sh] [Define Global] PYTHONPATH(2): [/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python/lib/py4j-0.10.1-src.zip:/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python:/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python/lib/py4j-0.10.1-src.zip:/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python:/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python/lib/py4j-0.10.1-src.zip:/Users/axu/code/axuProject/spark-2.0.0-hadoop2.4/python:]
{code}",,apachespark,as2,axu4apache,bryanc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 11 09:20:21 UTC 2016,,,,,,,,,,"0|i332dj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/16 13:34;as2;Hi [~axu4apache] I tried to test this in spark-2.0.0-hadoop2.4 as well as latest master. But I am not able to reproduce this issue.
;;;","01/Sep/16 08:22;srowen;[~axu4apache] I assume you modified the scripts to print PYTHONPATH in order to show this behavior.
Yeah it looks like the file that sets it is repeatedly sourced so this path is appended many times.
I don't know of a cleaner way to do this other than to see if the variable already contains the path and only append if it doesn't. Do you want to try that in a PR?;;;","09/Sep/16 17:37;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/15028;;;","11/Sep/16 09:20;srowen;Issue resolved by pull request 15028
[https://github.com/apache/spark/pull/15028];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Creating Hive table from Spark data,SPARK-17335,13001636,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,jupblb,jupblb,31/Aug/16 12:34,03/Sep/16 17:03,14/Jul/23 06:29,03/Sep/16 17:03,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"Recently my team started using Spark for analysis of huge JSON objects. Spark itself handles it well. The problem starts when we try to create a Hive table from it using steps from this part of doc: http://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables

After running command `spark.sql(""CREATE TABLE x AS (SELECT * FROM y)"") we get following exception (sorry for obfuscating, confidential data):
{code}
Exception in thread ""main"" org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.IllegalArgumentException: Error: : expected at the position 993 of 'string:struct<a:boolean,b:array<string>,c:boolean,d:struct<e:boolean,f:boolean,[...(few others)],z:boolean,... 4 more fields>,[...(rest of valid struct string)]>' but ' ' is found.;
{code}

It turned out that the exception was raised because of `... 4 more fields` part as it is not a valid representation of data structure.

An easy workaround is to set `spark.debug.maxToStringFields` to some large value. Nevertheless it shouldn't be required and the stringifying process should use methods targeted at giving valid data structure for Hive.

In my opinion the root problem is here:
https://github.com/apache/spark/blob/9d7a47406ed538f0005cdc7a62bc6e6f20634815/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoder.scala#L318 when calling `simpleString` method instead of `catalogString`. Nevertheless this class is used at many places and I don't feel that experienced with Spark to automatically submit PR.

We believe this issue is indirectly caused by this PR: https://github.com/apache/spark/pull/13537
There has been almost the same issue in the past. You can find it here: https://issues.apache.org/jira/browse/SPARK-16415",,apachespark,hvanhovell,jupblb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 02 15:28:19 UTC 2016,,,,,,,,,,"0|i3329z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/16 12:57;hvanhovell;[~jupblb] Could you open a PR for this?;;;","31/Aug/16 13:18;jupblb;Sure, I just need a while to get my hands on it. ;-);;;","01/Sep/16 12:54;jupblb;It turned out I was a bit wrong, the problem doesn't lie within StructType.simpleString method as ExpressionEncoder.toString is not used (simpleString is only called when initializing the schemaString value).
We do not know why truncation is being used when stringifying the data structure. Please find our stacktrace below:

{code}
java.lang.IllegalArgumentException: Error: : expected at the position 1012 of '[VERY LONG STRUCTTYPE STRING]' but ' ' is found.
    at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.expect(TypeInfoUtils.java:360)
    at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.expect(TypeInfoUtils.java:331)
    at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseType(TypeInfoUtils.java:483)
    at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseType(TypeInfoUtils.java:484)
    at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseType(TypeInfoUtils.java:447)
    at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseType(TypeInfoUtils.java:484)
    at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseTypeInfos(TypeInfoUtils.java:305)
    at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfosFromTypeString(TypeInfoUtils.java:765)
    at org.apache.hadoop.hive.ql.io.orc.OrcSerde.initialize(OrcSerde.java:104)
    at org.apache.spark.sql.hive.orc.OrcSerializer.<init>(OrcFileFormat.scala:178)
    at org.apache.spark.sql.hive.orc.OrcOutputWriter.<init>(OrcFileFormat.scala:220)
    at org.apache.spark.sql.hive.orc.OrcFileFormat$$anon$1.newInstance(OrcFileFormat.scala:93)
    at org.apache.spark.sql.execution.datasources.BaseWriterContainer.newOutputWriter(WriterContainer.scala:131)
    at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:247)
    at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)
    at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
    at org.apache.spark.scheduler.Task.run(Task.scala:85)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
{code};;;","01/Sep/16 14:16;hvanhovell;[~jupblb] Are you using a nested schema? If you do then this line is probably causing you grief: https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/types/DataType.scala#L66

That will mess up the catalog string of any struct.;;;","01/Sep/16 14:54;hvanhovell;Turns out this is not correct, StructType handles this correctly.

[~jupblb] Could you check the log for the following message:
{noformat}
16/09/01 16:41:45 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
{noformat};;;","01/Sep/16 15:07;hvanhovell;{{ArrayType}} and {{MapType}} do not have a proper {{catalogString}} implementation. They call `simpleString` on their child data types, which is problematic if the child data type is a struct.

You can reproduce this with the following code:
{noformat}
import org.apache.spark.sql.types._
def complex = new StructType((0 to 25).map(i => new StructField(('a' + i).toChar.toString, IntegerType, false)).toArray)
val schema = new StructType().add(""elements"", ArrayType(complex))
println(schema.catalogString)

>struct<elements:array<struct<a:int,b:int,c:int,d:int,e:int,f:int,g:int,h:int,i:int,j:int,k:int,l:int,m:int,n:int,o:int,p:int,q:int,r:int,s:int,t:int,u:int,v:int,w:int,x:int,... 2 more fields>>>
{noformat}
;;;","02/Sep/16 15:23;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/14938;;;","02/Sep/16 15:28;jupblb;Cool! I unfortunately was limited by my company open-source policy. Thanks a lot to you Herman!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE with EXPLAIN DESCRIBE TABLE,SPARK-17328,13001583,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,jlaskowski,jlaskowski,31/Aug/16 08:19,05/Oct/16 17:54,14/Jul/23 06:29,05/Oct/16 17:54,2.0.1,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"With today's build:

{code}
scala> sql(""EXPLAIN DESCRIBE TABLE x"").show(truncate = false)
INFO SparkSqlParser: Parsing command: EXPLAIN DESCRIBE TABLE x
java.lang.NullPointerException
  at org.apache.spark.sql.execution.command.ExplainCommand.run(commands.scala:104)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:88)
  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:88)
  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:62)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:569)
  ... 48 elided
{code}

while the following executes fine:

{code}
scala> sql(""describe table x"").explain
INFO SparkSqlParser: Parsing command: describe table x
org.apache.spark.sql.catalyst.parser.ParseException:
Unsupported SQL statement
== SQL ==
describe table x
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply(ParseDriver.scala:58)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply(ParseDriver.scala:53)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:82)
  at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:45)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:53)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:569)
  ... 48 elided
{code}

I think it's related to the condition in https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala#L262.

If guided I'd like to work on it.",,apachespark,dongjoon,hvanhovell,jlaskowski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 05 04:35:25 UTC 2016,,,,,,,,,,"0|i331y7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/16 10:02;hvanhovell;[~jlaskowski] go for it. Ping me if you need any help.;;;","04/Oct/16 23:42;dongjoon;Hi, [~jacek@japila.pl] and [~hvanhovell].
May I create a PR for this?;;;","04/Oct/16 23:54;jlaskowski;Sure! Go for it! Thanks.;;;","04/Oct/16 23:57;dongjoon;Thank YOU, [~jacek@japila.pl]!;;;","04/Oct/16 23:59;hvanhovell;Go ahead. The actual fix is a one liner. just add {{TABLE?}} to the {{#describeTable}} rule.;;;","05/Oct/16 04:30;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/15357;;;","05/Oct/16 04:35;dongjoon;Thank you, [~hvanhovell]. Sorry for missing your comment.
A few minutes ago, I made a PR for this. It fixes two kinds of error in DESCRIBE and EXPLAIN.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ALTER VIEW AS should keep the previous table properties, comment, create_time, etc.",SPARK-17323,13001536,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,31/Aug/16 01:45,01/Sep/16 01:01,14/Jul/23 06:29,31/Aug/16 09:10,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,lingesh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 31 10:29:15 UTC 2016,,,,,,,,,,"0|i331nz:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"31/Aug/16 01:47;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/14874;;;","31/Aug/16 09:10;cloud_fan;Issue resolved by pull request 14874
[https://github.com/apache/spark/pull/14874];;;","31/Aug/16 10:29;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/14893;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARN shuffle service should use good disk from yarn.nodemanager.local-dirs,SPARK-17321,13001498,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,zhaoyunjiong,zhaoyunjiong,30/Aug/16 22:38,17/May/20 18:13,14/Jul/23 06:29,31/Aug/17 01:27,1.6.2,2.0.0,2.1.1,,,,,,2.3.0,,,,Spark Core,YARN,,,,,,,2,,,,,,"We run spark on yarn, after enabled spark dynamic allocation, we notice some spark application failed randomly due to YarnShuffleService.
From log I found
{quote}
2016-08-29 11:33:03,450 ERROR org.apache.spark.network.TransportContext: Error while initializing Netty pipeline
java.lang.NullPointerException
        at org.apache.spark.network.server.TransportRequestHandler.<init>(TransportRequestHandler.java:77)
        at org.apache.spark.network.TransportContext.createChannelHandler(TransportContext.java:159)
        at org.apache.spark.network.TransportContext.initializePipeline(TransportContext.java:135)
        at org.apache.spark.network.server.TransportServer$1.initChannel(TransportServer.java:123)
        at org.apache.spark.network.server.TransportServer$1.initChannel(TransportServer.java:116)
        at io.netty.channel.ChannelInitializer.channelRegistered(ChannelInitializer.java:69)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRegistered(AbstractChannelHandlerContext.java:133)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRegistered(AbstractChannelHandlerContext.java:119)
        at io.netty.channel.DefaultChannelPipeline.fireChannelRegistered(DefaultChannelPipeline.java:733)
        at io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:450)
        at io.netty.channel.AbstractChannel$AbstractUnsafe.access$100(AbstractChannel.java:378)
        at io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:424)
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)
{quote} 
Which caused by the first disk in yarn.nodemanager.local-dirs was broken.
If we enabled spark.yarn.shuffle.stopOnFailure(SPARK-16505) we might lost hundred nodes which is unacceptable.
We have 12 disks in yarn.nodemanager.local-dirs, so why not use other good disks if the first one is broken?",,AnLei,apachespark,Dhruve Ashar,jerryshao,lishuming,rajeshhadoop,roncenzhao,tgraves,xwc3504,zhaoyunjiong,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21660,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 31 01:27:22 UTC 2017,,,,,,,,,,"0|i331fj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/16 23:09;apachespark;User 'zhaoyunjiong' has created a pull request for this issue:
https://github.com/apache/spark/pull/14887;;;","31/Aug/16 09:37;srowen;Duplicate of https://issues.apache.org/jira/browse/SPARK-14963 ?;;;","31/Aug/16 11:05;zhaoyunjiong;Below logs shows that NodeManager already detect the disk failure, but ExternalShuffleBlockResolver still use the failure disk.

2016-08-30 10:16:24,982 INFO org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Disk(s) failed. 3/12 local-dirs turned bad: /hadoop/1/scratch/local,/hadoop/7/scratch/local,/hadoop/10/scratch/local;3/12 log-dirs turned bad: /hadoop/1/scratch/logs,/hadoop/7/scratch/logs,/hadoop/10/scratch/logs
...
2016-08-30 10:16:38,008 INFO org.apache.spark.network.yarn.YarnShuffleService: Initializing YARN shuffle service for Spark
2016-08-30 10:16:38,008 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Adding auxiliary service spark_shuffle, ""spark_shuffle""
2016-08-30 10:16:38,260 ERROR org.apache.spark.network.shuffle.ExternalShuffleBlockResolver: error opening leveldb file /hadoop/1/scratch/local/registeredExecutors.ldb. Creating new file, will not be able to recover state for existing applications
org.fusesource.leveldbjni.internal.NativeDB$DBException: IO error: /hadoop/1/scratch/local/registeredExecutors.ldb/LOCK: No such file or directory
at org.fusesource.leveldbjni.internal.NativeDB.checkStatus(NativeDB.java:200)
at org.fusesource.leveldbjni.internal.NativeDB.open(NativeDB.java:218)
at org.fusesource.leveldbjni.JniDBFactory.open(JniDBFactory.java:168)
at org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.(ExternalShuffleBlockResolver.java:100)
at org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.(ExternalShuffleBlockResolver.java:81)
at org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.(ExternalShuffleBlockHandler.java:56)
at org.apache.spark.network.yarn.YarnShuffleService.serviceInit(YarnShuffleService.java:129)
at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:122)
at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:220)
at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:186)
at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:357)
at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:404)
2016-08-30 10:16:38,262 WARN org.apache.spark.network.shuffle.ExternalShuffleBlockResolver: error deleting /hadoop/1/scratch/local/registeredExecutors.ldb
2016-08-30 10:16:38,262 ERROR org.apache.spark.network.yarn.YarnShuffleService: Failed to initialize external shuffle service
java.io.IOException: Unable to create state store
at org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.(ExternalShuffleBlockResolver.java:129)
at org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.(ExternalShuffleBlockResolver.java:81)
at org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.(ExternalShuffleBlockHandler.java:56)
at org.apache.spark.network.yarn.YarnShuffleService.serviceInit(YarnShuffleService.java:129)
at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:122)
at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:220)
at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:186)
at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:357)
at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:404)
Caused by: org.fusesource.leveldbjni.internal.NativeDB$DBException: IO error: /hadoop/1/scratch/local/registeredExecutors.ldb/LOCK: No such file or directory
at org.fusesource.leveldbjni.internal.NativeDB.checkStatus(NativeDB.java:200)
at org.fusesource.leveldbjni.internal.NativeDB.open(NativeDB.java:218)
at org.fusesource.leveldbjni.JniDBFactory.open(JniDBFactory.java:168)
at org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.(ExternalShuffleBlockResolver.java:127)
... 14 more
2016-08-30 10:16:38,456 INFO org.apache.spark.network.yarn.YarnShuffleService: Started YARN shuffle service for Spark on port 7337. Authentication is not enabled. Registered executor file is /hadoop/1/scratch/local/registeredExecutors.ldb;;;","31/Aug/16 11:13;zhaoyunjiong;If yarn.nodemanager.recovery.enabled = false & the first disk was broken, yarn shuffle service still will fail.;;;","08/Sep/16 16:34;AnLei;We discovered the same issue. It seems the shuffle service has no way of being notified about the failed disk. Only option then is probably to restart the node manager.;;;","08/Sep/16 17:53;tgraves;so there are 2 possible things here:

1) You are using YARN NM recovery.  If this is the case SPARK-14963 should prevent this problem as the recovery path is supposed to be critical to NM and NM should not start if its bad

2) You aren't using NM recovery. If this is the case then you probably don't really care about the levelDB being saved because you aren't expecting things to live across Nm restarts.  In this case if people are having issues like this perhaps we should change the code to be conditionalized on NM recovery or a spark config.

Which case are you running?;;;","09/Sep/16 08:39;AnLei;But NM recovery only kicks in if the NM goes down, right? As I see it, a failing disk will not necessarily trigger a NM restart, but the NM will still know that the disk is not usable and keep working. At this point you can have a working NM with a non-working shuffle service which tries to access its ldb file on a broken disk. Or am I missing something here?;;;","09/Sep/16 13:12;tgraves;it is possible but the point of the recovery dir is it something that is supposed to be critical to the NM or its supposed to be special storage to handle that situation.  NM writes its recovery data there too in addition to the shuffle services so if it goes bad more then likely the nm is going to get the same exception and crash.;;;","09/Sep/16 13:26;tgraves;Note that if we want to fix this without yarn NM recovery enabled I think we should do as mentioned in the Pr and in this jira above.  change the shuffle handler so that if nm recovery/spark config aren't enabled then don't save the data to the DB at all. Then it doesn't rely on the disk at all.;;;","12/Sep/16 07:56;AnLei;I guess then we encountered the 1% where the NM handles the broken disk just fine, but the shuffle service did not.;;;","12/Sep/16 17:42;tgraves;Not sure I follow this comment.  So you are using NM recovery with the recovery path specified?    
And you saw an error in the spark shuffle creating or writing to the DB but the NM stayed up ok writing its recovery data to the same disk?;;;","13/Sep/16 13:10;AnLei;No, we're not using NM recovery. What we observed is the following:
- NM runs with a list of local dirs on various disks
- Shuffle service puts its data into one of these local dirs
- One disk fails making that local dir unuseable, which is incidentally the one where shuffle service put its data
- NM recognizes the disk failure but keeps running happily (did not have any critical data there? no idea...)
- Shuffle service can't access its data and is not working anymore on this node

So in essence, the NM has some way of recognizing that a local dir is not useable anymore and can keep operating. The shuffle service lacks this functionality and becomes unuseable. Does that make sense?;;;","13/Sep/16 13:31;tgraves;yes that makes sense and as I stated I think the fix for this should be that the Spark shuffle services doesn't use the backup database at all if NM recovery (and spark config) aren't enabled.  Thus you wouldn't have any disk errors.  If NM recovery isn't enabled the spark DB isn't going to do you any good because NM is going to shoot any running containers on restart.

If you are up for making those changes please go ahead and put up patch.;;;","28/Jul/17 03:05;roncenzhao;Hi, I encounter this problem too. Any process about this bug? Thanks~;;;","28/Jul/17 16:04;tgraves;Can you clarify?   as stated above you should not be using nodemanager.local-dirs.  If you are you should look at reconfiguring yarn to use the proper NM recovery dirs.  see https://issues.apache.org/jira/browse/SPARK-14963

if you aren't using NM recovery then yes we should fix this so spark doesn't use backup db at all.

;;;","23/Aug/17 13:12;jerryshao;We're facing the same issue. I think YARN shuffle service should be like:

* If NM recovery is not enabled, then Spark will not persist data into leveldb, in that case yarn shuffle service can still be served but lose the ability for recovery, (it is fine because the failure of NM will kill the containers as well as applications).
* If NM recovery is enabled, then user or yarn should guarantee recovery path is reliable. Because recovery path is also crucial for NM to recover.

What do you think [~tgraves] ? 

I'm currently working on the 1st thing to avoid persisting data into leveldb, to see if this is a feasible solution.;;;","23/Aug/17 13:36;tgraves;Yes that sounds good.  It wouldn't hurt to verify the second point. The NM should throw an exception on container launch because it itself can't record the container start thus can't recover.;;;","24/Aug/17 02:55;lishuming;[~jerryshao] I agree with what you said, however there are some questions in my mind:

1. The current chosen strategy is puzzling somehow, because both `yarn.nodemanager.local-dirs` and `NM recovery path` are available to choose to store leveldb now, so we can always pick an available disk to store, avoiding the disk problem(https://github.com/apache/spark/pull/18905#issuecomment-323287272).

2. If as [~jerryshao] said, `yarn.nodemanager.local-dirs` should not be used whenever NM recovery is enabled or not, am I right ?

3. Can someone check that If we don't use leveldb, ShuffleService which uses `Map` will affect NM's memory or something else?;;;","24/Aug/17 03:18;jerryshao;1. if NM recovery is enabled, then yarn will provide a recovery path, this recovery path will be used for any aux-service running on yarn (tez, mr, spark...) and NM itself to store state. So user/yarn should guarantee the availability of this path, if not then NM itself will be failed to restart. So as a conclusion if NM recovery is enabled, then we should always use recovery path.

2. Yes we will never use NM local dirs whether NM recovery is enabled or not. Previously we need to support Hadoop 2.6- which has no recovery path, so we choose a local dir instead. Since now we only support 2.6+, so there's no meaning to still use NM local dir.

3. The memory overhead should not be large, since it only stores some application/executor information. Also when you use external shuffle service in standalone and Mesos, it always use memory, so I don't think it is a big problem.;;;","25/Aug/17 21:26;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/19032;;;","25/Aug/17 21:26;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/19032;;;","31/Aug/17 01:27;jerryshao;Issue resolved by pull request 19032
[https://github.com/apache/spark/pull/19032];;;",,,,,,,,,,,,,,,,,,,,,
ALTER VIEW should throw exception if view not exist,SPARK-17309,13001249,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,30/Aug/16 09:24,01/Sep/16 18:19,14/Jul/23 06:29,31/Aug/16 09:10,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 31 10:29:10 UTC 2016,,,,,,,,,,"0|i32zvz:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"30/Aug/16 09:39;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/14874;;;","31/Aug/16 09:10;cloud_fan;Issue resolved by pull request 14874
[https://github.com/apache/spark/pull/14874];;;","31/Aug/16 10:29;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/14893;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QuantileSummaries doesn't compress,SPARK-17306,13001156,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,clockfly,clockfly,30/Aug/16 00:34,11/Sep/16 08:58,14/Jul/23 06:29,11/Sep/16 08:58,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"compressThreshold was not referenced anywhere

{code}
class QuantileSummaries(
    val compressThreshold: Int,
    val relativeError: Double,
    val sampled: ArrayBuffer[Stats] = ArrayBuffer.empty,
    private[stat] var count: Long = 0L,
    val headSampled: ArrayBuffer[Double] = ArrayBuffer.empty) extends Serializable
{code}

And, it causes memory leak, QuantileSummaries takes unbounded memory
{code}
val summary = new QuantileSummaries(10000, relativeError = 0.001)
// Results in creating an array of size 100000000 !!! 
(1 to 100000000).foreach(summary.insert(_))
{code}",,apachespark,clockfly,thunterdb,tsuresh,yintengfei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 11 08:58:58 UTC 2016,,,,,,,,,,"0|i32zbb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/16 08:46;srowen;[~thunterdb] it looks like you might have added the original implementation. I also don't see that compressThreshold is ever used. Should it be checked vs count in the insert method, where compress() would be invoked if it exceeds the threshold?;;;","30/Aug/16 16:52;thunterdb;[~srowen] yes I had a discussion yesterday with [~clockfly]. The issue is performance, not correctness, by the way. The fix is to add a call to the compression: the compression threshold should be
used in insert() after inserting the head buffer at this line:

https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/QuantileSummaries.scala#L66

Unless someone else wants to step in, I will be happy to fix this issue.;;;","06/Sep/16 13:56;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14976;;;","11/Sep/16 08:58;srowen;Resolved by https://github.com/apache/spark/pull/15002 (based on https://github.com/apache/spark/pull/14976);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskSetManager.abortIfCompletelyBlacklisted is a perf. hotspot in scheduler benchmark,SPARK-17304,13001148,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joshrosen,joshrosen,joshrosen,29/Aug/16 23:37,17/May/20 17:46,14/Jul/23 06:29,30/Aug/16 20:15,2.1.0,,,,,,,,2.1.0,,,,Scheduler,Spark Core,,,,,,,0,,,,,,"If you run

{code}
sc.parallelize(1 to 100000, 100000).map(identity).count()
{code}

then {{TaskSetManager.abortIfCompletelyBlacklisted()}} is the number-one performance hotspot in the scheduler, accounting for over half of the time. This method was introduced in SPARK-15865, so this is a performance regression in 2.1.0-SNAPSHOT.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-15865,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 30 20:15:55 UTC 2016,,,,,,,,,,"0|i32z9j:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"30/Aug/16 00:14;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14871;;;","30/Aug/16 20:15;joshrosen;Issue resolved by pull request 14871
[https://github.com/apache/spark/pull/14871];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dev/run-tests fails if spark-warehouse directory exists,SPARK-17303,13001141,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,freiss,freiss,freiss,29/Aug/16 23:00,30/Aug/16 06:35,14/Jul/23 06:29,30/Aug/16 06:34,,,,,,,,,2.1.0,,,,Build,,,,,,,,0,,,,,,"The script dev/run-tests, which is intended for verifying the correctness of pull requests, runs Apache RAT to check for missing Apache license headers. Later, the script does a full compile/package/test sequence.

The script as currently written works fine the first time. But the second time it runs, the Apache RAT checks fail due to the presence of the directory spark-warehouse, which the script indirectly creates during its regression test run.",,apachespark,freiss,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 29 23:06:07 UTC 2016,,,,,,,,,,"0|i32z7z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/16 23:06;apachespark;User 'frreiss' has created a pull request for this issue:
https://github.com/apache/spark/pull/14870;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TRIM/LTRIM/RTRIM strips characters other than spaces,SPARK-17299,13001094,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,techaddict,jbeard,jbeard,29/Aug/16 19:31,06/Sep/16 21:19,14/Jul/23 06:29,06/Sep/16 21:18,2.0.0,,,,,,,,2.0.1,2.1.0,,,Documentation,SQL,,,,,,,0,,,,,,"TRIM/LTRIM/RTRIM docs state that they only strip spaces:

http://spark.apache.org/docs/2.0.0/api/java/org/apache/spark/sql/functions.html#trim(org.apache.spark.sql.Column)

But the implementation strips all characters of ASCII value 20 or less:

https://github.com/apache/spark/blob/v2.0.0/common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java#L468-L470",,apachespark,chenghao,dongjoon,jbeard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 06 21:18:52 UTC 2016,,,,,,,,,,"0|i32yxj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/16 19:45;srowen;I'm almost certain its intent is to match the behavior of String.trim() in Java. Unless there's a reason to believe it should behave otherwise I think the docs should be fixed to resolve this.;;;","29/Aug/16 20:00;jbeard;What is the priority for compatibility with other SQL dialects? TRIM in (at least) Hive, Impala, Oracle, and Teradata is just for spaces.;;;","29/Aug/16 20:12;srowen;That's probably the intent yeah. If that's how the other engines treat TRIM then this is a bug. I can see it is indeed implemented internally with String.trim().
CC [~chenghao] for https://github.com/apache/spark/commit/0b0b9ceaf73de472198c9804fb7ae61fa2a2e097;;;","30/Aug/16 16:17;dongjoon;Hi, [~jbeard] and [~srowen].
For the compatibility, it seems we had better fix this in SQL.
Could you make a PR for this, [~jbeard]?;;;","31/Aug/16 10:12;chenghao;Yes, that's my bad, I thought it should be the same behavior of `String.trim()`. We should fix this bug. [~jbeard], can you please fix it?;;;","31/Aug/16 10:59;chenghao;Or come after SPARK-14878 ?;;;","31/Aug/16 11:01;srowen;We should fix this now IMHO. I'm not sure about the other change which is both large, and less useful in Spark SQL where you can perform any transformations you want easily in code.;;;","01/Sep/16 19:32;apachespark;User 'techaddict' has created a pull request for this issue:
https://github.com/apache/spark/pull/14924;;;","06/Sep/16 21:18;srowen;Issue resolved by pull request 14924
[https://github.com/apache/spark/pull/14924];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL: cross join + two joins = BUG,SPARK-17296,13001021,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,fpin,fpin,29/Aug/16 15:47,08/Dec/16 22:56,14/Jul/23 06:29,06/Sep/16 22:58,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"In spark shell :

{code}
CREATE TABLE test (col INT) ;
INSERT OVERWRITE TABLE test VALUES (1), (2) ;

SELECT 
COUNT(1)
FROM test T1 
CROSS JOIN test T2
JOIN test T3
ON T3.col = T1.col
JOIN test T4
ON T4.col = T1.col
;
{code}

returns :

{code}
Error in query: cannot resolve '`T1.col`' given input columns: [col, col]; line 6 pos 12
{code}

Apparently, this example is minimal (removing the CROSS or one of the JOIN causes no issue).",,apachespark,dondrake,fpin,hvanhovell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17384,SPARK-17253,,,,,SPARK-17384,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 06 23:22:05 UTC 2016,,,,,,,,,,"0|i32yhb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/16 16:34;srowen;Pardon if I'm missing something, but you are not joining T3 with T1, so I don't think you can use T1.col in the join condition right?;;;","29/Aug/16 16:35;hvanhovell;I think you have found a bug in the parser. Your query produces the following (unresolved) LogicalPlan:
{noformat}
'Project [unresolvedalias('COUNT(1), None)]
+- 'Join Inner, ('T4.col = 'T1.col)
   :- 'Join Inner
   :  :- 'UnresolvedRelation `test`, T1
   :  +- 'Join Inner, ('T3.col = 'T1.col)
   :     :- 'UnresolvedRelation `test`, T2
   :     +- 'UnresolvedRelation `test`, T3
   +- 'UnresolvedRelation `test`, T4
{noformat}

Notice how the the most nested Inner Join references T2 and T3 using a join condition on T1 (which is an unknown relation for that join).;;;","29/Aug/16 16:40;fpin;Yes, this is not critical though, a workaround is to invert the order between T1 and T2.;;;","29/Aug/16 20:35;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/14867;;;","06/Sep/16 23:22;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/14984;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove statistics-related table properties from SHOW CREATE TABLE,SPARK-17284,13000818,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,28/Aug/16 22:46,23/May/17 00:31,14/Jul/23 06:29,23/May/17 00:31,2.0.0,,,,,,,,2.3.0,,,,SQL,,,,,,,,0,,,,,,"{noformat}
CREATE TABLE t1 (
  c1 INT COMMENT 'bla',
  c2 STRING
)
LOCATION '$dir'
TBLPROPERTIES (
  'prop1' = 'value1',
  'prop2' = 'value2'
)
{noformat}

The output of {{SHOW CREATE TABLE t1}} is 

{noformat}
CREATE EXTERNAL TABLE `t1`(`c1` int COMMENT 'bla', `c2` string)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
WITH SERDEPROPERTIES (
  'serialization.format' = '1'
)
STORED AS
  INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION 'file:/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-ee317538-0f8c-42d0-b08c-cf077d94fe75'
TBLPROPERTIES (
  'rawDataSize' = '-1',
  'numFiles' = '0',
  'transient_lastDdlTime' = '1472424052',
  'totalSize' = '0',
  'prop1' = 'value1',
  'prop2' = 'value2',
  'COLUMN_STATS_ACCURATE' = 'false',
  'numRows' = '-1'
)
{noformat}

The statistics-related table properties should be skipped by {{SHOW CREATE TABLE}}, since it could be incorrect in the newly created table. See the Hive JIRA: https://issues.apache.org/jira/browse/HIVE-13792
",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 06 07:22:13 UTC 2016,,,,,,,,,,"0|i32x87:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/16 22:51;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14855;;;","06/Sep/16 07:22;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14971;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite and JavaDirectKafkaStreamSuite.testKafkaStream,SPARK-17280,13000695,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,yhuai,yhuai,27/Aug/16 16:43,22/Feb/17 17:56,14/Jul/23 06:29,22/Feb/17 17:56,,,,,,,,,,,,,DStreams,Tests,,,,,,,0,,,,,,"https://spark-tests.appspot.com/builds/spark-master-test-maven-hadoop-2.2/1793

https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-2.2/1793/

{code}
org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream

Error Message

assertion failed: Partition [topic1, 0] metadata not propagated after timeout
Stacktrace

java.util.concurrent.TimeoutException: assertion failed: Partition [topic1, 0] metadata not propagated after timeout
	at org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.createTopicAndSendData(JavaDirectKafkaStreamSuite.java:176)
	at org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream(JavaDirectKafkaStreamSuite.java:74)
{code}

{code}
org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD

Error Message

Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost): java.lang.AssertionError: assertion failed: Failed to get records for spark-executor-java-test-consumer--363965267-1472280538438 topic2 0 0 after polling for 512
 at scala.Predef$.assert(Predef.scala:170)
 at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.get(CachedKafkaConsumer.scala:74)
 at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:227)
 at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:193)
 at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
 at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1684)
 at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134)
 at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134)
 at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1910)
 at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1910)
 at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
 at org.apache.spark.scheduler.Task.run(Task.scala:86)
 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
Stacktrace

org.apache.spark.SparkException: 
Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost): java.lang.AssertionError: assertion failed: Failed to get records for spark-executor-java-test-consumer--363965267-1472280538438 topic2 0 0 after polling for 512
	at scala.Predef$.assert(Predef.scala:170)
	at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.get(CachedKafkaConsumer.scala:74)
	at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:227)
	at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:193)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1684)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1910)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1910)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD(JavaKafkaRDDSuite.java:115)
Caused by: java.lang.AssertionError: assertion failed: Failed to get records for spark-executor-java-test-consumer--363965267-1472280538438 topic2 0 0 after polling for 512
{code}
",,koeninger,original-brownbear,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 22 17:56:51 UTC 2017,,,,,,,,,,"0|i32whj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Aug/16 16:44;yhuai;[~cody@koeninger.org] Will you have time to take a look at these two kafka 0.10 tests?;;;","27/Aug/16 18:01;koeninger;I can take a look but there's not a lot to go on.;;;","22/Feb/17 17:56;original-brownbear;closing this, can't find any recent examples of this on Jenkins and haven't experienced this locally either as of late.
Also tried reproducing this running 1k+ loops of all the Kafka0.10_2.11 tests with 3 forks in parallel without issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Planner adds un-necessary Sort even if child ordering is semantically same as required ordering,SPARK-17271,13000564,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tejasp,tejasp,tejasp,27/Aug/16 02:37,01/Sep/16 14:25,14/Jul/23 06:29,28/Aug/16 17:16,1.6.2,2.0.0,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Found a case when the planner is adding un-needed SORT operation due to bug in the way comparison for `SortOrder` is done at https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala#L253

`SortOrder` needs to be compared semantically because `Expression` within two `SortOrder` can be ""semantically equal"" but not literally equal objects.

eg. In case of `sql(""SELECT * FROM table1 a JOIN table2 b ON a.col1=b.col1"")`

Expression in required SortOrder:

{code}
      AttributeReference(
        name = ""col1"",
        dataType = LongType,
        nullable = false
      ) (exprId = exprId,
        qualifier = Some(""a"")
      )
{code}

Expression in child SortOrder:

{code}
      AttributeReference(
        name = ""col1"",
        dataType = LongType,
        nullable = false
      ) (exprId = exprId)
{code}

Notice that the output column has a qualifier but the child attribute does not but the inherent expression is the same and hence in this case we can say that the child satisfies the required sort order.",,apachespark,h_o,tejasp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16419,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 01 14:25:07 UTC 2016,,,,,,,,,,"0|i32vof:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Aug/16 03:15;apachespark;User 'tejasapatil' has created a pull request for this issue:
https://github.com/apache/spark/pull/14841;;;","01/Sep/16 05:49;apachespark;User 'tejasapatil' has created a pull request for this issue:
https://github.com/apache/spark/pull/14910;;;","01/Sep/16 14:25;apachespark;User 'tejasapatil' has created a pull request for this issue:
https://github.com/apache/spark/pull/14920;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Long running structured streaming requirements,SPARK-17267,13000536,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,petermaxlee,rxin,rxin,26/Aug/16 23:05,02/Nov/16 06:00,14/Jul/23 06:29,02/Nov/16 05:59,,,,,,,,,2.1.0,,,,Structured Streaming,,,,,,,,0,,,,,,"This is an umbrella ticket to track various things that are required in order to have the engine for structured streaming run non-stop in production.
",,copris,freiss,joaomaiaduarte,lwlin,prashant,rxin,sethah,sks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 31 18:32:00 UTC 2016,,,,,,,,,,"0|i32vi7:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"29/Aug/16 06:42;prashant;I think, SPARK-16963 should be moved under this.;;;","31/Aug/16 18:32;freiss;Converted SPARK-16963 to a subtask of this JIRA.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"PrefixComparatorsSuite's ""String prefix comparator"" failed when both input strings are empty strings",SPARK-17266,13000535,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,26/Aug/16 23:02,27/Aug/16 02:39,14/Jul/23 06:29,27/Aug/16 02:39,,,,,,,,,2.1.0,,,,SQL,Tests,,,,,,,0,,,,,,"https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.4/1620/testReport/junit/org.apache.spark.util.collection.unsafe.sort/PrefixComparatorsSuite/String_prefix_comparator/

{code}
org.scalatest.exceptions.GeneratorDrivenPropertyCheckFailedException: TestFailedException was thrown during property evaluation.   Message: 0 equaled 0, but 1 did not equal 0, and 0 was not less than 0, and 0 was not greater than 0   Location: (PrefixComparatorsSuite.scala:42)   Occurred when passed generated values (     arg0 = """",     arg1 = """"   )
{code}

I could not reproduce it locally. But, let me add this case in the regressionTests to explicitly test it.",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 27 02:39:18 UTC 2016,,,,,,,,,,"0|i32vhz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/16 23:11;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/14837;;;","27/Aug/16 02:39;yhuai;Issue resolved by pull request 14837
[https://github.com/apache/spark/pull/14837];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataStreamWriter should document that it only supports Parquet for now,SPARK-17264,13000381,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,srowen,billreed63@gmail.com,billreed63@gmail.com,26/Aug/16 14:14,12/Dec/22 18:10,14/Jul/23 06:29,30/Aug/16 10:20,2.0.0,,,,,,,,2.0.1,2.1.0,,,Documentation,Input/Output,,,,,,,0,,,,,,"The API documentations for DataStreamWriter.format states ""Specifies the underlying output data source. Built-in options include ""parquet"", ""json"", etc."" but when specifying ""json"" or ""text"" for the format. the following exception is thrown:
Exception in thread ""main"" java.lang.UnsupportedOperationException: Data source json does not support streamed writing
	at org.apache.spark.sql.execution.datasources.DataSource.createSink(DataSource.scala:273)
	at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:291)

The only format that works is .format(""parquet"")",Mac OSX,apachespark,billreed63@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-15472,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 30 10:20:07 UTC 2016,,,,,,,,,,"0|i32ujr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/16 14:28;srowen;(Don't set Fix version)
Yes the docs need fixing, feel free to make a PR. Parquet looks like the only supported format now.;;;","26/Aug/16 14:32;billreed63@gmail.com;so ""parquet"" will be the only supported output format?;;;","26/Aug/16 14:34;srowen;Yeah that's certainly what the code does at the moment.;;;","26/Aug/16 14:38;billreed63@gmail.com;Should I open or change this issue to be a request to support ""json"" and ""text""?;;;","26/Aug/16 14:49;srowen;You could do that. At least we can fix the docs for now here.
I don't know whether writing JSON/Text is a priority or isn't on anybody's radar or what. 
All the better if you intend to work on it.;;;","28/Aug/16 00:44;gurwls223;Is this a duplicate of SPARK-15472?;;;","29/Aug/16 08:51;srowen;It is. I'm going to consider this JIRA to be about fixing up the documentation for now since it implies other formats are supported.;;;","29/Aug/16 09:53;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14860;;;","30/Aug/16 10:20;srowen;Issue resolved by pull request 14860
[https://github.com/apache/spark/pull/14860];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Using HiveContext after re-creating SparkContext in Spark 2.0 throws ""Java.lang.illegalStateException: Cannot call methods on a stopped sparkContext""",SPARK-17261,13000347,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zjffdu,rahuljain788,rahuljain788,26/Aug/16 11:48,11/Mar/17 20:45,14/Jul/23 06:29,02/Sep/16 17:08,2.0.0,,,,,,,,2.0.1,2.1.0,,,PySpark,,,,,,,,0,,,,,,"After stopping SparkSession if we recreate it and use HiveContext in it. it will throw error.

Steps to reproduce:
spark = SparkSession.builder.enableHiveSupport().getOrCreate()
spark.sql(""show databases"")
spark.stop()
spark = SparkSession.builder.enableHiveSupport().getOrCreate()
spark.sql(""show databases"")



""Java.lang.illegalStateException: Cannot call methods on a stopped sparkContext""

Above error occurs only in case of Pyspark not in SparkShell",Amazon AWS EMR 5.0,apachespark,chiragvaya,davies,dongjoon,rahuljain788,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 11 20:45:04 UTC 2017,,,,,,,,,,"0|i32uc7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/16 11:56;srowen;I think that's as intended. The context is actually pretty global. I don't think you are getting a different one. ;;;","26/Aug/16 13:19;rahuljain788;Thanks for replying, i am using the same commands in Spark-Shell and it works pretty fine. 
In case of pyspark i am not actually able to access hive metastore but SQLContext works fine.;;;","26/Aug/16 19:14;dongjoon;Hi, [~dakghar]

For me, those seems not to work even in `spark-shell`. Could you add a `show` at the end? I tested and got the same result in 2.0.0 and current master branch.
{code}
scala> import org.apache.spark.sql.SparkSession
scala> val spark = SparkSession.builder.enableHiveSupport().getOrCreate()
scala> spark.sql(""show databases"").show
+------------+
|databaseName|
+------------+
|     default|
+------------+

scala> spark.stop()
scala> val spark = SparkSession.builder.enableHiveSupport().getOrCreate()
scala> spark.sql(""show databases"").show
16/08/26 12:09:22 ERROR Schema: Failed initialising database.
Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@6b60d99c, see the next exception for details.
{code};;;","29/Aug/16 03:40;apachespark;User 'zjffdu' has created a pull request for this issue:
https://github.com/apache/spark/pull/14857;;;","29/Aug/16 03:42;zjffdu;[~dongjoon] spark-shell works well for me. It seems your case is due to something else. ;;;","29/Aug/16 06:24;rahuljain788;i looked into 'zjffdu' pull request and the changed code. From my understanding, this code change particularly addresses to SparkSession only and doesn't provide backward compatibility for previous spark version codes(Spark Context). I ran the below code in Spark 2.0 and it failed. 

""""""
>>> sc.stop()
>>> sc._instantiatedContext = None
>>> from  pyspark import SparkContext
>>> from pyspark import HiveContext
>>> sc = SparkContext()
16/08/29 06:20:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
16/08/29 06:20:18 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
>>> sqlContext = HiveContext(sc)
>>> sqlContext.sql(""show databases"").collect()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/spark/python/pyspark/sql/context.py"", line 350, in sql
    return self.sparkSession.sql(sqlQuery)
  File ""/usr/lib/spark/python/pyspark/sql/session.py"", line 541, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
  File ""/usr/lib/spark/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py"", line 933, in __call__
  File ""/usr/lib/spark/python/pyspark/sql/utils.py"", line 63, in deco
    return f(*a, **kw)
  File ""/usr/lib/spark/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py"", line 312, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o44.sql.
: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:
"""""";;;","29/Aug/16 06:39;zjffdu;It works if you change 'sc._instantiatedContext = None' to 'SparkSession._instantiatedContext = None'.  The problem here is that SQLContext/HIveContext is created from SparkSession, although you create HiveContext, SparkSession will still be created first underneath. And here the SparkContext is stopped, but SparkSession don't know that, so it still use the stopped SparkContext. ;;;","02/Sep/16 17:08;davies;Issue resolved by pull request 14857
[https://github.com/apache/spark/pull/14857];;;","11/Mar/17 20:16;apachespark;User 'elviento' has created a pull request for this issue:
https://github.com/apache/spark/pull/17261;;;","11/Mar/17 20:45;apachespark;User 'elviento' has created a pull request for this issue:
https://github.com/apache/spark/pull/17262;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performing arithmetic in VALUES can lead to ClassCastException / MatchErrors during query parsing,SPARK-17252,13000260,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sjakthol,joshrosen,joshrosen,26/Aug/16 02:53,08/Oct/16 09:19,14/Jul/23 06:29,26/Aug/16 18:03,2.0.0,,,,,,,,2.0.1,,,,SQL,,,,,,,,0,,,,,,"The following example fails with a ClassCastException:

{code}
create table t(d double);
insert into t VALUES (1 * 1.0);
{code}

 Here's the error:

{code}
java.lang.ClassCastException: org.apache.spark.sql.types.Decimal cannot be cast to java.lang.Integer
	at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106)
	at scala.math.Numeric$IntIsIntegral$.times(Numeric.scala:57)
	at org.apache.spark.sql.catalyst.expressions.Multiply.nullSafeEval(arithmetic.scala:207)
	at org.apache.spark.sql.catalyst.expressions.BinaryExpression.eval(Expression.scala:416)
	at org.apache.spark.sql.catalyst.expressions.CreateStruct$$anonfun$eval$2.apply(complexTypeCreator.scala:198)
	at org.apache.spark.sql.catalyst.expressions.CreateStruct$$anonfun$eval$2.apply(complexTypeCreator.scala:198)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.catalyst.expressions.CreateStruct.eval(complexTypeCreator.scala:198)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:320)
	at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitInlineTable$1$$anonfun$39.apply(AstBuilder.scala:677)
	at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitInlineTable$1$$anonfun$39.apply(AstBuilder.scala:674)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitInlineTable$1.apply(AstBuilder.scala:674)
	at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitInlineTable$1.apply(AstBuilder.scala:658)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:96)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitInlineTable(AstBuilder.scala:658)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitInlineTable(AstBuilder.scala:43)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$InlineTableContext.accept(SqlBaseParser.java:9358)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:57)
	at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitInlineTableDefault1(SqlBaseBaseVisitor.java:608)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$InlineTableDefault1Context.accept(SqlBaseParser.java:7073)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:57)
	at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitQueryTermDefault(SqlBaseBaseVisitor.java:580)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryTermDefaultContext.accept(SqlBaseParser.java:6895)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:47)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.plan(AstBuilder.scala:83)
	at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleInsertQuery$1.apply(AstBuilder.scala:158)
	at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleInsertQuery$1.apply(AstBuilder.scala:162)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:96)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleInsertQuery(AstBuilder.scala:157)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleInsertQuery(AstBuilder.scala:43)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleInsertQueryContext.accept(SqlBaseParser.java:6500)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:47)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.plan(AstBuilder.scala:83)
	at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitQuery$1.apply(AstBuilder.scala:89)
	at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitQuery$1.apply(AstBuilder.scala:88)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:96)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery(AstBuilder.scala:88)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery(AstBuilder.scala:43)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryContext.accept(SqlBaseParser.java:4751)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:57)
	at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitStatementDefault(SqlBaseBaseVisitor.java:48)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$StatementDefaultContext.accept(SqlBaseParser.java:992)
	at org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visit(AbstractParseTreeVisitor.java:42)
	at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleStatement$1.apply(AstBuilder.scala:64)
	at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleStatement$1.apply(AstBuilder.scala:64)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:96)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleStatement(AstBuilder.scala:63)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply(ParseDriver.scala:54)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply(ParseDriver.scala:53)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:82)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:46)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:53)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:682)
{code}

It's surprising to me that this error is occurring during query parsing. My hunch is that we're performing expression evaluation too early and need to run more analysis and type promotion rules prior to trying to evaluate the expressions here. ",,apachespark,hvanhovell,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 13 16:58:40 UTC 2016,,,,,,,,,,"0|i32tsv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/16 15:33;hvanhovell;I cannot reproduce this. I tried both on the latest master and branch-2.0.;;;","26/Aug/16 18:03;joshrosen;Looks like this issue only affects 2.0.0, so I'm going to resolve it as fixed in 2.0.1.;;;","13/Sep/16 16:58;apachespark;User 'sjakthol' has created a pull request for this issue:
https://github.com/apache/spark/pull/15081;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""ClassCastException: OuterReference cannot be cast to NamedExpression"" for correlated subquery on the RHS of an IN operator",SPARK-17251,13000255,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,joshrosen,joshrosen,26/Aug/16 02:25,26/Nov/16 23:11,14/Jul/23 06:29,26/Nov/16 23:11,2.0.0,,,,,,,,2.0.3,2.1.0,,,SQL,,,,,,,,1,,,,,,"The following test case produces a ClassCastException in the analyzer:

{code}
CREATE TABLE t1(a INTEGER);
INSERT INTO t1 VALUES(1),(2);
CREATE TABLE t2(b INTEGER);
INSERT INTO t2 VALUES(1);

SELECT a FROM t1 WHERE a NOT IN (SELECT a FROM t2);
{code}

Here's the exception:

{code}
java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.OuterReference cannot be cast to org.apache.spark.sql.catalyst.expressions.NamedExpression
	at org.apache.spark.sql.catalyst.plans.logical.Project$$anonfun$1.apply(basicLogicalOperators.scala:48)
	at scala.collection.LinearSeqOptimized$class.exists(LinearSeqOptimized.scala:80)
	at scala.collection.immutable.List.exists(List.scala:84)
	at org.apache.spark.sql.catalyst.plans.logical.Project.resolved$lzycompute(basicLogicalOperators.scala:44)
	at org.apache.spark.sql.catalyst.plans.logical.Project.resolved(basicLogicalOperators.scala:43)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQuery(Analyzer.scala:1091)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:1130)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:1116)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:279)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:279)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionDown$1(QueryPlan.scala:156)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:166)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:175)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:175)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:144)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries(Analyzer.scala:1116)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$16.applyOrElse(Analyzer.scala:1148)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$16.applyOrElse(Analyzer.scala:1141)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:60)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:1141)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:909)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:682)
{code}

This bug was discovered while trying to run SQLite bug reports through Spark SQL (see https://www.sqlite.org/src/tktview?name=5e3c886796)
",,apachespark,dongjoon,hvanhovell,joshrosen,nsyca,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 26 23:11:11 UTC 2016,,,,,,,,,,"0|i32trr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/16 16:35;hvanhovell;Ok, I have taken a look at this one. We should make {{OuterReference}} a {{NamedExpression}} and then we are good (have most of the code working locally). 

If we fix this, it will fail analysis because we are using a correlated predicate in a {{Project}}. We could make an exception for IN, but I am just wondering if we support such a weird construct at all.;;;","24/Nov/16 23:06;dongjoon;Hi, [~hvanhovell].
The suggested solution seems to extend `OuterReference` with trait `NamedExpression` and eventually to raise the following correct exception. May I create a PR for this?
{code}
org.apache.spark.sql.AnalysisException: Correlated predicates are not supported outside of WHERE/HAVING clauses
{code};;;","24/Nov/16 23:22;hvanhovell;Yeah, go ahead. The only thing is that it should not throw an exception, it must return an empty set.;;;","24/Nov/16 23:46;dongjoon;Thank you! I see.;;;","25/Nov/16 11:31;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/16012;;;","26/Nov/16 03:34;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/16015;;;","26/Nov/16 23:11;hvanhovell;The current PR fixes the ClassCastException, and we now issue an AnalysisException. We should fix this in 2.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE thrown by ClientWrapper.conf,SPARK-17245,13000179,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,25/Aug/16 21:24,09/Sep/16 06:03,14/Jul/23 06:29,07/Sep/16 13:55,1.6.2,,,,,,,,1.6.3,,,,SQL,,,,,,,,0,,,,,,"This issue has been fixed in Spark 2.0. Seems ClientWrapper.conf is trying to access the ThreadLocal SessionState, which has been set. 
{code}
java.lang.NullPointerException 
at org.apache.spark.sql.hive.client.ClientWrapper.conf(ClientWrapper.scala:225) 
at org.apache.spark.sql.hive.client.ClientWrapper.client(ClientWrapper.scala:279) 
at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1.apply(ClientWrapper.scala:291) 
at org.apache.spark.sql.hive.client.ClientWrapper.liftedTree1$1(ClientWrapper.scala:246) 
at org.apache.spark.sql.hive.client.ClientWrapper.retryLocked(ClientWrapper.scala:245) 
at org.apache.spark.sql.hive.client.ClientWrapper.withHiveState(ClientWrapper.scala:288) 
at org.apache.spark.sql.hive.client.ClientWrapper.runHive(ClientWrapper.scala:493) 
at org.apache.spark.sql.hive.client.ClientWrapper.runSqlHive(ClientWrapper.scala:483) 
at org.apache.spark.sql.hive.client.ClientWrapper.addJar(ClientWrapper.scala:603) 
at org.apache.spark.sql.hive.HiveContext.addJar(HiveContext.scala:654) 
at org.apache.spark.sql.hive.execution.AddJar.run(commands.scala:105)
at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58) 
at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56) 
at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70) 
at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132) 
at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130) 
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150) 
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130) 
at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55) 
at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55) 
at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:145) 
at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:130) 
at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:52) 
at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:816) 
{code}",,apachespark,cloud_fan,codlife,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 07 13:55:53 UTC 2016,,,,,,,,,,"0|i32tav:",9223372036854775807,,,,,,,,,,,,,1.6.3,,,,,,,,,,,"25/Aug/16 22:09;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/14816;;;","07/Sep/16 13:55;cloud_fan;Issue resolved by pull request 14816
[https://github.com/apache/spark/pull/14816];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Joins should not pushdown non-deterministic conditions,SPARK-17244,13000166,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sameerag,sameerag,sameerag,25/Aug/16 20:49,12/Dec/17 06:26,14/Jul/23 06:29,26/Aug/16 23:41,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,correctness,,,,,"Given that non-deterministic expressions can be stateful, pushing them down the query plan during the optimization phase can cause incorrect behavior.",,apachespark,sameerag,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 26 23:41:34 UTC 2016,,,,,,,,,,"0|i32t7z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/16 20:55;apachespark;User 'sameeragarwal' has created a pull request for this issue:
https://github.com/apache/spark/pull/14815;;;","26/Aug/16 23:41;yhuai;Issue resolved by pull request 14815
[https://github.com/apache/spark/pull/14815];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spark 2.0 history server summary page gets stuck at ""loading history summary"" with 10K+ application history",SPARK-17243,13000165,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ajbozarth,wgtmac,wgtmac,25/Aug/16 20:41,31/Aug/16 13:51,14/Jul/23 06:29,30/Aug/16 21:35,2.0.0,,,,,,,,2.0.1,2.1.0,,,Web UI,,,,,,,,0,,,,,,"The summary page of Spark 2.0 history server web UI keep displaying ""Loading history summary..."" all the time and crashes the browser when there are more than 10K application history event logs on HDFS. 

I did some investigation, ""historypage.js"" file sends a REST request to /api/v1/applications endpoint of history server REST endpoint and gets back json response. When there are more than 10K applications inside the event log directory it takes forever to parse them and render the page. When there are only hundreds or thousands of application history it is running fine.",Linux,ajbozarth,apachespark,stevel@apache.org,tgraves,wgtmac,wzheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 30 21:59:08 UTC 2016,,,,,,,,,,"0|i32t7r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/16 20:58;srowen;Related, but not identical: https://issues.apache.org/jira/browse/SPARK-15083;;;","25/Aug/16 21:04;ajbozarth;I'm not sure I agree that this should be a blocker, but I was actually planning on filing a JIRA and starting work on a pr next month (September) that will switch the history server to only load application data when an application ui is opened and only loading application metadata on the initial load of the history server. This is just one of many problems that would be fixed by such a change. I won't have the bandwidth to start working on it for another week or two though.

tl;dr I plan to fix this but not until next month;;;","25/Aug/16 21:06;srowen;Yes, should not be assigned as a Blocker.;;;","25/Aug/16 21:12;ajbozarth;[~wgtmac] until this is fixed you can limit the number of applications available by setting {{spark.history.retainedApplications}} It limits the apps the history server loads;;;","25/Aug/16 21:16;wgtmac;Hi Alex, I think in Spark 1.5 history server obtains all application summary metadata directly from class FsHistoryProvider. You can check in HistoryPage.scala. While in Spark 2.0 it deals with JSON string (in historypage.js) which is MUCH slower than before. It may make sense if the old way is used?;;;","25/Aug/16 21:17;wgtmac;This doesn't work. This is for the cache of WEB UIs not for the application metadata. The default value is 50 which is small enough.;;;","25/Aug/16 21:49;ajbozarth;Thanks, that'll help when I look into it;;;","25/Aug/16 21:50;ajbozarth;Sorry, my misunderstanding of your problem, I will make sure to keep this in mind once I start my work
;;;","26/Aug/16 16:31;stevel@apache.org;The REST API actually lets you set a time range for querying entries coming back, though not a limit.

This problem could presumably be addressed in a couple of ways

# add a {{limit}} argument to the REST API, declaring the max #of responses to return
# leave the REST API alone but tweak the client code to work backwards from now to try and get a range. That's more convoluted and is probably brittle to clocks. 

strategy #1 is simpler and would avoid the server being overloaded from large requests made directly by arbitrary callers —that serialization is going to be expensive too, and an easy to way to bring the history server down.;;;","26/Aug/16 17:58;ajbozarth;Thanks [~stevel@apache.org], this idea is great. [~wgtmac], based on this I might be able to get a small fix for this out next week instead of waiting to include it in my larger update next month.;;;","26/Aug/16 18:22;wgtmac;Thanks [~ajbozarth]! Let me know when it is done.;;;","26/Aug/16 20:43;ajbozarth;So I decided to work on this as a short break from my current work and I have a fix that just requires some final testing before I open a pr, should be open by EOD.;;;","26/Aug/16 21:08;apachespark;User 'ajbozarth' has created a pull request for this issue:
https://github.com/apache/spark/pull/14835;;;","29/Aug/16 21:28;wgtmac;I've test this PR. It indeed reduces the number of application metadata list. I think it intends to restrict only the summary page; jobs that are dropped from summary web ui should still be available via its URL like http://x.x.x.x:18080/history/application_id/jobs. However, those dropped ones cannot be accessed. This may heavily decrease the usability of history server.;;;","29/Aug/16 21:35;ajbozarth;[~wgtmac] I'm not sure which version of the pr you tested, in my initial commit the issue you saw still existed but I updated it EOD Friday to switch to a version that only restricts the summary display, leaving all the applications available via their direct url as you would expect.;;;","29/Aug/16 21:54;wgtmac;I imported the last change. I can get all application list from rest endpoint /api/v1/applications, (without limit parameter). However, the web UI indicates the app_id is not found when I specify the app_id. I can get it using spark 1.5 history server. ;;;","29/Aug/16 22:33;ajbozarth;that's odd, how long did you wait before accessing the app url? because the history server still needs to propagate after starting and that can take a long time, I was testing with a limit of 50 and testing an app in the thousands and it took about 5min to propagate for me to see it;;;","29/Aug/16 22:42;wgtmac;Yup you're right. I finally got some app_ids that were not in the summary page but their urls can be accessed. Our cluster has 100K+ app_ids so it took me a long time to figure it out. Thanks for your help!;;;","30/Aug/16 10:55;stevel@apache.org;One thing to consider here whether there are any ways to improve incremental loading of histories; start at the most recent and work backwards.

There's also the fact that the entire history is loaded just to get the final summary info (success/failure). Once parsed once, this could just be saved in a summary file alongside the original. That'd reduce load time from O(files * events) to O(files);;;","30/Aug/16 15:02;tgraves;I agree, there are a ton of ways to improve the history server. I think these should be separate jiras though. Ideally it is much faster to load all the apps and get the initial list very quickly. Only load the entire application as a user requests or in the background to fill the cache.   Like you mention could have summary file written after loaded.  They could be stored differently so basic data is in dir or file path (like MapReduce history server), etc.  I just haven't had time to do this myself.

Right now this seems like a good workaround and as I mention in Pr  spark.history.retainedApplications used to do this limiting in of the display but things have changed and I guess it broke/wasn't updated.;;;","30/Aug/16 18:43;ajbozarth;[~stevel@apache.org] [~tgraves] The issues you mentioned are what I'm hoping to work on next month (what I mentioned above) when I'm given the bandwidth to do so. When that comes I'll file a JIRA and loop you two in to discuss implementation ideas. (Unless some brave soul decides to give it a try before then);;;","30/Aug/16 21:59;apachespark;User 'ajbozarth' has created a pull request for this issue:
https://github.com/apache/spark/pull/14886;;;",,,,,,,,,,,,,,,,,,,,,
SparkConf is Serializable but contains a non-serializable field,SPARK-17240,13000123,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,mgummelt,mgummelt,25/Aug/16 18:37,25/Aug/16 23:12,14/Jul/23 06:29,25/Aug/16 23:12,2.1.0,,,,,,,,2.1.0,,,,Spark Core,,,,,,,,0,,,,,,"This commit: https://github.com/apache/spark/commit/5da6c4b24f512b63cd4e6ba7dd8968066a9396f5

Added ConfigReader to SparkConf.  SparkConf is Serializable, but ConfigReader is not, which results in the following exception:

{code}
java.io.NotSerializableException: org.apache.spark.internal.config.ConfigReader
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.util.Utils$.serialize(Utils.scala:134)
	at org.apache.spark.scheduler.cluster.mesos.ZookeeperMesosClusterPersistenceEngine.persist(MesosClusterPersistenceEngine.scala:111)
	at org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler.submitDriver(MesosClusterScheduler.scala:170)
	at org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet.handleSubmit(MesosRestServer.scala:126)
	at org.apache.spark.deploy.rest.SubmitRequestServlet.doPost(RestSubmissionServer.scala:265)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.spark_project.jetty.servlet.ServletHolder.handle(ServletHolder.java:812)
	at org.spark_project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:587)
	at org.spark_project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1127)
	at org.spark_project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)
	at org.spark_project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1061)
	at org.spark_project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)
	at org.spark_project.jetty.server.Server.handle(Server.java:499)
	at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:311)
	at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)
	at org.spark_project.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)
	at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
	at org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
	at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,mgummelt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 25 19:53:06 UTC 2016,,,,,,,,,,"0|i32syf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/16 18:37;mgummelt;cc [~vanzin];;;","25/Aug/16 19:53;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14813;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataFrame fill after pivot causing org.apache.spark.sql.AnalysisException,SPARK-17237,12999979,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,tintinlotus,tintinlotus,25/Aug/16 10:42,16/Jun/17 01:57,14/Jul/23 06:29,13/Jan/17 17:26,2.0.0,,,,,,,,2.0.3,2.1.1,2.2.0,,SQL,,,,,,,,0,newbie,,,,,"I am trying to run a pivot transformation which I ran on a spark1.6 cluster, 
namely

sc.parallelize(Seq((2,3,4), (3,4,5))).toDF(""a"", ""b"", ""c"")
res1: org.apache.spark.sql.DataFrame = [a: int, b: int, c: int]

scala> res1.groupBy(""a"").pivot(""b"").agg(count(""c""), avg(""c"")).na.fill(0)
res2: org.apache.spark.sql.DataFrame = [a: int, 3_count(c): bigint, 3_avg(c): double, 4_count(c): bigint, 4_avg(c): double]

scala> res1.groupBy(""a"").pivot(""b"").agg(count(""c""), avg(""c"")).na.fill(0).show
+---+----------+--------+----------+--------+
|  a|3_count(c)|3_avg(c)|4_count(c)|4_avg(c)|
+---+----------+--------+----------+--------+
|  2|         1|     4.0|         0|     0.0|
|  3|         0|     0.0|         1|     5.0|
+---+----------+--------+----------+--------+

after upgrade the environment to spark2.0, got an error while executing .na.fill method

scala> sc.parallelize(Seq((2,3,4), (3,4,5))).toDF(""a"", ""b"", ""c"")
res3: org.apache.spark.sql.DataFrame = [a: int, b: int ... 1 more field]

scala> res3.groupBy(""a"").pivot(""b"").agg(count(""c""), avg(""c"")).na.fill(0)
org.apache.spark.sql.AnalysisException: syntax error in attribute name: `3_count(`c`)`;
  at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute$.e$1(unresolved.scala:103)
  at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute$.parseAttributeName(unresolved.scala:113)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveQuoted(LogicalPlan.scala:168)
  at org.apache.spark.sql.Dataset.resolve(Dataset.scala:218)
  at org.apache.spark.sql.Dataset.col(Dataset.scala:921)
  at org.apache.spark.sql.DataFrameNaFunctions.org$apache$spark$sql$DataFrameNaFunctions$$fillCol(DataFrameNaFunctions.scala:411)
  at org.apache.spark.sql.DataFrameNaFunctions$$anonfun$2.apply(DataFrameNaFunctions.scala:162)
  at org.apache.spark.sql.DataFrameNaFunctions$$anonfun$2.apply(DataFrameNaFunctions.scala:159)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
  at org.apache.spark.sql.DataFrameNaFunctions.fill(DataFrameNaFunctions.scala:159)
  at org.apache.spark.sql.DataFrameNaFunctions.fill(DataFrameNaFunctions.scala:149)
  at org.apache.spark.sql.DataFrameNaFunctions.fill(DataFrameNaFunctions.scala:134)


",,albertofem,apachespark,maropu,michaelmalak,robert3005,tintinlotus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 16 01:57:58 UTC 2017,,,,,,,,,,"0|i32s27:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/16 16:36;maropu;It seems the root cause of this bug is nested backticks in column names.
UnresolvedAttribute#parseAttributeName cannot handle the nested backtics.

;;;","25/Aug/16 19:32;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/14812;;;","13/Jan/17 16:54;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/16565;;;","07/Jun/17 11:17;albertofem;Hi there,

I think this change introduced a breaking change in the way the ""withColumnRenamed"" method works. I can reproduce this breaking change with the following example:

{code}
dataframe = sql(""SELECT * FROM db.table"")
another_dataframe = sql(""SELECT * FROM db.another_table"")

dataframe
	.join(another_dataframe, on=[...])
	.pivot(""column_name"", values=[0, 1])
	.max(""column1"", ""column2"")
	.withColumnRenamed(""0_max(another_table.`column1`)"", ""name1"")
	.withColumnRenamed(""1_max(another_table.`column2`)"", ""name2"")
{code}

With Spark 2.1.0, the behaviour is the expected: columns get renamed.

With Spark 2.1.1, and if this issue was resolved, you wouldn't need to change anything for the renames to work. However, the column doesn't get renamed at all because now you would need to use the following renames:

{code}
dataframe = sql(""SELECT * FROM db.table"")
another_dataframe = sql(""SELECT * FROM db.another_table"")

dataframe
	.join(another_dataframe, on=[...])
	.pivot(""column_name"", values=[0, 1])
	.max(""column1"", ""column2"")
	.withColumnRenamed(""0_max(column1)"", ""name1"")
	.withColumnRenamed(""1_max(column2)"", ""name2"")
{code}

As you can see, it seems that this PR somehow managed to removed the table name from the join context and also removed the backticks, thus introducing a breaking change.

I should also notice that the original issue didn't happen when using JSON as output format. It only happens because Parquet doesn't support () characters in column names, but in JSON they work just fine. Here is an example of the error thrown by Parquet after upgrading to Spark 2.1.1 and not modifying your code.

{code}
Attribute name ""0_max(column1)"" contains invalid character(s) among "" ,;{}()\\n\\t="". Please use alias to rename it.
{code}

I think the original issue was that the parseAttributeName cannot detect ""table.column"" notation, and as I understand this PR still doesn't fix this issue right?

As a workaround, you can change your column renames to accomodate the new format.

Any ideas? Am I missing something?;;;","07/Jun/17 19:03;maropu;Thanks for the report.
I think there are two points you suggestedt: a qualifier and buck-ticks.
Yea, you're right and it seems my pr above wrongly drop a qualifier for aggregated column names(then, it changed the behaviour).
{code}
// Spark-v2.1
scala> Seq((1, 2)).toDF(""id"", ""v1"").createOrReplaceTempView(""s"")

scala> Seq((1, 2)).toDF(""id"", ""v2"").createOrReplaceTempView(""t"")

scala> val df1 = sql(""SELECT * FROM s"")
df1: org.apache.spark.sql.DataFrame = [id: int, v1: int]

scala> val df2 = sql(""SELECT * FROM t"")
df2: org.apache.spark.sql.DataFrame = [id: int, v2: int]

scala> df1.join(df2, ""id"" :: Nil).groupBy(""id"").pivot(""id"").max(""v1"", ""v2"").show
+---+-------------+-------------+                                               
| id|1_max(s.`v1`)|1_max(t.`v2`)|
+---+-------------+-------------+
|  1|            2|            2|
+---+-------------+-------------+

// Master
scala> df1.join(df2, ""id"" :: Nil).groupBy(""id"").pivot(""id"").max(""v1"", ""v2"").show
+---+---------+---------+                                                       
| id|1_max(v1)|1_max(v2)|
+---+---------+---------+
|  1|        2|        2|
+---+---------+---------+
{code}

We could easily fix this, but I'm not 100% sure that we need to fix this. WDYT? cc: [~smilegator]

{code}
// Master with a patch (https://github.com/apache/spark/compare/master...maropu:SPARK-17237-4)
scala> df1.join(df2, ""id"" :: Nil).groupBy(""id"").pivot(""id"").max(""v1"", ""v2"").show
+---+-----------+-----------+                                                       
| id|1_max(s.v1)|1_max(t.v2)|
+---+-----------+-----------+
|  1|          2|          2|
+---+-----------+-----------+
{code}

On the other hand, IIUC back-ticks are not allowed in column names cuz they have special meaning in Spark.;;;","08/Jun/17 10:20;albertofem;Hi there,

thank you for your thorough answer.

I think it would be good to fix it since the patch version bump in Spark may lead to think that you don't need to make changes in your code, and that may not be the case.

Also, why are backticks not allowed since Spark 2.1.1? They seem to work fine in Spark 2.1.0.

Thanks in advance.;;;","08/Jun/17 13:44;maropu;sorry for confusing you, but I originally mean nested buck-ticks are not accepted as written in the description: ""`1_max(t.`v2`)`"". Spark internally add outer buck-ticks in this case.;;;","14/Jun/17 14:30;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/18302;;;","16/Jun/17 01:57;maropu;I checked the other behaviours and I probably think it seems to be correct to use aggregated column names without qualifiers (since other aggregated columns has no qualifier: https://github.com/apache/spark/pull/18302/files). So, it seems the current master behaviour is okay to me for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Existence Checking when Index Table with the Same Name Exists,SPARK-17234,12999925,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,25/Aug/16 06:59,30/Aug/16 09:28,14/Jul/23 06:29,30/Aug/16 09:28,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Hive Index tables are not supported by Spark SQL. Thus, we issue an exception when users try to access Hive Index tables. When the internal function `tableExists` tries to access Hive Index tables, it always gets the same error message: ```Hive index table is not supported```. This message could be confusing to users, since their SQL operations could be completely unrelated to Hive Index tables. For example, when users try to alter a table to a new name and there exists an index table with the same name, the expected exception should be a `TableAlreadyExistsException`.
",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 30 09:28:38 UTC 2016,,,,,,,,,,"0|i32rq7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/16 07:14;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14801;;;","30/Aug/16 09:28;cloud_fan;Issue resolved by pull request 14801
[https://github.com/apache/spark/pull/14801];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Writing decimal to csv will result empty string if the decimal exceeds (20, 18)",SPARK-17230,12999798,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,davies,davies,24/Aug/16 20:49,02/Sep/16 22:14,14/Jul/23 06:29,02/Sep/16 22:14,1.6.2,2.0.0,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"{code}
// file content 
spark.read.csv(""/mnt/djiang/test-case.csv"").show 
// read in as string and create temp view 
spark.read.csv(""/mnt/djiang/test-case.csv"").createOrReplaceTempView(""test"") 
// confirm schema 
spark.table(""test"").printSchema 
// apply decimal calculation, confirm the result is correct 
spark.sql(""select _c0, cast(_c0 as long) * cast('1.0' as decimal(38, 18)) from test"").show(false) 
// run the same query, and write out as csv 
spark.sql(""select _c0, cast(_c0 as long) * cast('1.0' as decimal(38, 18)) from test"").write.csv(""/mnt/djiang/test-case-result"") 
// show the content of the result file, particularly, for number exceeded decimal(20, 18), the csv is not writing anything or failing silently 
spark.read.csv(""/mnt/djiang/test-case-result"").show

+------+ 
| _c0| 
+------+ 
| 1| 
| 10| 
| 100| 
| 1000| 
| 10000| 
|100000| 
+------+

root 
|-- _c0: string (nullable = true)

+------+-------------------------------------------------------------------------------------------------------------------------------------------------+ 
|_c0 |(CAST(CAST(CAST(CAST(_c0 AS DECIMAL(20,0)) AS BIGINT) AS DECIMAL(20,0)) AS DECIMAL(38,18)) * CAST(CAST(1.0 AS DECIMAL(38,18)) AS DECIMAL(38,18)))| 
+------+-------------------------------------------------------------------------------------------------------------------------------------------------+ 
|1 |1.000000000000000000 | 
|10 |10.000000000000000000 | 
|100 |100.000000000000000000 | 
|1000 |1000.000000000000000000 | 
|10000 |10000.000000000000000000 |
|100000|100000.000000000000000000 | 
+------+-------------------------------------------------------------------------------------------------------------------------------------------------+

+------+--------------------+ 
| _c0| _c1| 
+------+--------------------+ 
| 1|1.000000000000000000| 
| 10|10.00000000000000...| 
| 100| | 
| 1000| | 
| 10000| | 
|100000| | 
+------+--------------------+
{code}",,apachespark,davies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 24 21:21:06 UTC 2016,,,,,,,,,,"0|i32qxz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/16 21:21;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/14797;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Postgres JDBC dialect should not widen float and short types during reads,SPARK-17229,12999796,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joshrosen,joshrosen,joshrosen,24/Aug/16 20:45,25/Aug/16 21:23,14/Jul/23 06:29,25/Aug/16 21:23,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"When reading {{float4}} and {{smallint}} columns from PostgreSQL, Spark's Postgres dialect widens these types to Decimal and Integer rather than using the narrower Float and Short types.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 24 20:54:05 UTC 2016,,,,,,,,,,"0|i32qxj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/16 20:54;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14796;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not infer/propagate non-deterministic constraints,SPARK-17228,12999785,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sameerag,sameerag,sameerag,24/Aug/16 20:21,31/Aug/16 21:36,14/Jul/23 06:29,25/Aug/16 04:24,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,correctness,,,,,,,apachespark,sameerag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 24 20:28:06 UTC 2016,,,,,,,,,,"0|i32qv3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/16 20:28;apachespark;User 'sameeragarwal' has created a pull request for this issue:
https://github.com/apache/spark/pull/14795;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Even timeline for a stage doesn't core 100% of the bar timeline bar in chrome,SPARK-17216,12999689,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,robert3005,robert3005,robert3005,24/Aug/16 14:46,02/Sep/16 07:42,14/Jul/23 06:29,27/Aug/16 07:47,,,,,,,,,2.0.1,2.1.0,,,Web UI,,,,,,,,0,,,,,,If you look at event timeline for a stage in chrome you get total time that's bigger than sum of all individual elements. Actually proportions are calculated correctly but due to css issue they're not expanded to the size of the full bar,,apachespark,robert3005,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17342,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 27 07:47:37 UTC 2016,,,,,,,,,,"0|i32q9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/16 14:49;apachespark;User 'robert3005' has created a pull request for this issue:
https://github.com/apache/spark/pull/14791;;;","27/Aug/16 07:47;srowen;Issue resolved by pull request 14791
[https://github.com/apache/spark/pull/14791];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet String Pushdown for Non-Eq Comparisons Broken,SPARK-17213,12999641,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,andreweduffy,andreweduffy,24/Aug/16 11:54,12/Dec/22 18:11,14/Jul/23 06:29,02/Dec/16 06:02,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,1,,,,,,"Spark defines ordering over strings based on comparison of UTF8 byte arrays, which compare bytes as unsigned integers. Currently however Parquet does not respect this ordering. This is currently in the process of being fixed in Parquet, JIRA and PR link below, but currently all filters are broken over strings, with there actually being a correctness issue for {{>}} and {{<}}.

*Repro:*
Querying directly from in-memory DataFrame:
{code}
    > Seq(""a"", ""é"").toDF(""name"").where(""name > 'a'"").count
    1
{code}

Querying from a parquet dataset:
{code}
    > Seq(""a"", ""é"").toDF(""name"").write.parquet(""/tmp/bad"")
    > spark.read.parquet(""/tmp/bad"").where(""name > 'a'"").count
    0
{code}
This happens because Spark sorts the rows to be {{[a, é]}}, but Parquet's implementation of comparison of strings is based on signed byte array comparison, so it will actually create 1 row group with statistics {{min=é,max=a}}, and so the row group will be dropped by the query.

Based on the way Parquet pushes down Eq, it will not be affecting correctness but it will force you to read row groups you should be able to skip.

Link to PARQUET issue: https://issues.apache.org/jira/browse/PARQUET-686
Link to PR: https://github.com/apache/parquet-mr/pull/362",,aash,andreweduffy,apachespark,diederik,djiangxu,findepi,lian cheng,myali,robert3005,schlosna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20958,,,,PARQUET-686,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 04 22:15:05 UTC 2018,,,,,,,,,,"0|i32pzb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/16 14:02;gurwls223;It sounds we should disable the filters for string and binary back for now.;;;","01/Dec/16 18:41;lian cheng;Agree that we should disable string and binary filter push down for now until PARQUET-686 gets fixed.

We turned off Parquet filter pushdown for string and binary columns in 1.6 due to PARQUET-251 (see SPARK-11153). In Spark 2.1, we upgraded to Parquet 1.8.1 to get PARQUET-251 fixed, then this issue pops up due to PARQUET-686. I think this also affects Spark 1.5.1 and prior versions.;;;","01/Dec/16 20:00;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/16106;;;","03/Feb/17 19:09;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/16791;;;","06/Feb/17 08:25;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/16817;;;","04/Jul/18 22:15;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/21716;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TypeCoercion support widening conversion between DateType and TimestampType,SPARK-17212,12999629,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,24/Aug/16 10:50,12/Dec/22 18:10,14/Jul/23 06:29,26/Aug/16 01:00,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Currently, type-widening does not work between {{TimestampType}} and {{DateType}}.

This applies to {{SetOperation}}, {{Union}}, {{In}}, {{CaseWhen}}, {{Greatest}}, {{Leatest}}, {{CreateArray}}, {{CreateMap}} and {{Coalesce}}.

For a simple example, 

{code}
Seq(Tuple2(new Timestamp(0), new Date(0))).toDF(""a"", ""b"").selectExpr(""greatest(a, b)"").show()
{code}

{code}
cannot resolve 'greatest(`a`, `b`)' due to data type mismatch: The expressions should all have the same type, got GREATEST(timestamp, date)
{code}

or Union as below:

{code}
val a = Seq(Tuple1(new Timestamp(0))).toDF()
val b = Seq(Tuple1(new Date(0))).toDF()
a.union(b).show()
{code}

{code}
Union can only be performed on tables with the compatible column types. DateType <> TimestampType at the first column of the second table;
{code}
",,apachespark,cloud_fan,fpin,hvanhovell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 27 14:12:42 UTC 2016,,,,,,,,,,"0|i32pwn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/16 10:56;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/14786;;;","26/Aug/16 01:00;cloud_fan;Issue resolved by pull request 14786
[https://github.com/apache/spark/pull/14786];;;","27/Nov/16 11:57;fpin;Hi,

I have the same issue between DateType and StringType on Sparkv 2.0.2, do you know if this pull request fixes it?

in Spark 2.0.2 :
{code}
spark-sql> select greatest(""2015-02-02"", date(""2015-01-01"")) ;
Error in query: cannot resolve 'greatest('2015-02-02', CAST('2015-01-01' AS DATE))' due to data type mismatch: The expressions should all have the same type, got GREATEST(string, date).; line 1 pos 7
{code}

in Hive 2.0:
{code}
hive (default)> select greatest(""2015-02-02"", date(""2015-01-01"")) ;
OK
_c0
2015-02-02
{code}
;;;","27/Nov/16 13:36;hvanhovell;This fixes union, but not greatest/least. The latter need to implement {{ImplicitCastInputTypes}} for it to work. [~hyukjin.kwon] is this by design?;;;","27/Nov/16 13:56;gurwls223;Oh, [~hvanhovell], I might be wrong but I guess it is because it was not backported into branch 2.0, https://github.com/apache/spark/blob/branch-2.0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TypeCoercion.scala

{{Greatest}}/{{Least}} (and other ones I listed above) will have the same input types which are forced in {{FunctionArgumentConversion}} (and some other rules) and I understood (by myself) this is the reason why it does not extend {{ImplicitCastInputTypes}} if I understood correctly. It uses {{TypeCoercion.findTightestCommonTypeOfTwo}} at the end commonly. Maybe, I should have added some related test codes for sure.;;;","27/Nov/16 14:12;gurwls223;Oh wait, I am sorry. I rushed the comments. The problem specified in the comment seems a bit different with the problem specified in this JIRA (types between {{TimestampType}} and {{DateType}}. The comment says types between {{StringType}} and {{DateType}}. Yes, I just checked it does not still work in the master.

{code}
spark-sql> select greatest(""2015-02-02"", date(""2015-01-01"")) ;
Error in query: cannot resolve 'greatest('2015-02-02', CAST('2015-01-01' AS DATE))' due to data type mismatch: The expressions should all have the same type, got GREATEST(string, date).; line 1 pos 7;
'Project [unresolvedalias(greatest(2015-02-02, cast(2015-01-01 as date)), None)]
+- OneRowRelation$

spark-sql> select least(""2015-02-02"", date(""2015-01-01"")) ;
Error in query: cannot resolve 'least('2015-02-02', CAST('2015-01-01' AS DATE))' due to data type mismatch: The expressions should all have the same type, got LEAST(string, date).; line 1 pos 7;
'Project [unresolvedalias(least(2015-02-02, cast(2015-01-01 as date)), None)]
+- OneRowRelation$

spark-sql> select greatest(timestamp(""2015-02-02""), date(""2015-01-01"")) ;
2015-02-02 00:00:00
Time taken: 1.707 seconds, Fetched 1 row(s)
{code}

Ah, I see what you mean [~hvanhovell]. {{Leatest}}/{{Greatest}} use {{findWiderTypeWithoutStringPromotion}} and when the input type is {{StringType}}, it ends up with different types and that's why you said {{ImplicitCastInputTypes}}. I see. I can take a look for other DBMS implementations and open a JIRA if you are fine (or I am also fine if anyone feels up and takes over this).
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Broadcast join produces incorrect results when compressed Oops differs between driver, executor",SPARK-17211,12999540,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,jseppanen,jseppanen,24/Aug/16 07:53,06/Sep/16 17:47,14/Jul/23 06:29,06/Sep/16 17:47,2.0.0,,,,,,,,2.0.1,2.1.0,,,PySpark,,,,,,,,1,,,,,,"Broadcast join produces incorrect columns in join result, see below for an example. The same join but without using broadcast gives the correct columns.

Running PySpark on YARN on Amazon EMR 5.0.0.

{noformat}

import pyspark.sql.functions as func

keys = [
    (54000000, 0),
    (54000001, 1),
    (54000002, 2),
]

keys_df = spark.createDataFrame(keys, ['key_id', 'value']).coalesce(1)
keys_df.show()
# +--------+-----+
# |  key_id|value|
# +--------+-----+
# |54000000|    0|
# |54000001|    1|
# |54000002|    2|
# +--------+-----+

data = [
    (54000002,    1),
    (54000000,    2),
    (54000001,    3),
]

data_df = spark.createDataFrame(data, ['key_id', 'foo'])
data_df.show()
# +--------+---+                                                                  
# |  key_id|foo|
# +--------+---+
# |54000002|  1|
# |54000000|  2|
# |54000001|  3|
# +--------+---+

### INCORRECT ###

data_df.join(func.broadcast(keys_df), 'key_id').show()
# +--------+---+--------+                                                         
# |  key_id|foo|   value|
# +--------+---+--------+
# |54000002|  1|54000002|
# |54000000|  2|54000000|
# |54000001|  3|54000001|
# +--------+---+--------+

### CORRECT ###

data_df.join(keys_df, 'key_id').show()
# +--------+---+-----+
# |  key_id|foo|value|
# +--------+---+-----+
# |54000000|  2|    0|
# |54000001|  3|    1|
# |54000002|  1|    2|
# +--------+---+-----+
{noformat}
",,apachespark,davies,dongjoon,gurmukhd,himanish,jseppanen,migtor,sameerag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 06 17:47:15 UTC 2016,,,,,,,,,,"0|i32pcv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/16 18:06;dongjoon;Hi, [~jseppanen].

Thank you for the reporting. But, it seems to work correctly like the following in Apache Spark 2.0.0 for me.
{code}
>>> data_df.join(func.broadcast(keys_df), 'key_id').show()
+--------+---+-----+
|  key_id|foo|value|
+--------+---+-----+
|54000002|  1|    2|
|54000000|  2|    0|
|54000001|  3|    1|
+--------+---+-----+

>>> data_df.join(keys_df, 'key_id').show()
+--------+---+-----+
|  key_id|foo|value|
+--------+---+-----+
|54000002|  1|    2|
|54000001|  3|    1|
|54000000|  2|    0|
+--------+---+-----+

>>> spark.version
u'2.0.0'
{code}

I tested with the following three environments.
- Local Mode with spark-2.0.0-bin-hadoop2.7
- [Databricks CE|https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6660119172909095/2828921900907106/5162191866050912/latest.html]
- Yarn Client mode on Hadoop 2.7.2

Is there something for me to reproduce your problem?;;;","24/Aug/16 19:14;himanish;[~dongjoon] [~jseppanen] I am also seeing this issue on EMR using release emr-5.0.0, Amazon Hadoop 2.7.2 and Spark 2.0.0

For a simple join {{df1.join(df2, ""id"")}} the broadcast causes the fields from df2 to either return ""null"" or get assigned an incorrect value. Disabling the broadcast works as expected.

Everything works fine locally through test cases. Could it be something to do with the EMR environment ?;;;","24/Aug/16 19:26;dongjoon;Up to now, this seems to happen in EMR 5.0.0 only since I tested with official Apache Hadoop 2.7.2 and Apache Spark 2.0.0 in yarn-client mode.;;;","24/Aug/16 19:28;dongjoon;Is there any way to meet that situation without EMR? Unfortunately, I don't have EMR environment.;;;","24/Aug/16 21:28;himanish;I ran the following in a Databricks environment with Spark 2.0. Works fine.

{code:java}
import spark.implicits._

val a1 = Array((123,1),(234,2),(432,5))
val a2 = Array((""abc"",1),(""bcd"",2),(""dcb"",5))
val df1 = sc.parallelize(a1).toDF(""gid"",""id"")
val df2 = sc.parallelize(a2).toDF(""gname"",""id"")
df1.join(df2,""id"").show() // WORKS
+---+---+-----+
| id|gid|gname|
+---+---+-----+
|  5|432|  dcb|
|  2|234|  bcd|
|  1|123|  abc|
+---+---+-----+
df1.join(broadcast(df2),""id"").show() // BROADCASTING - DOES NOT WORK on EMR
+---+---+-----+
| id|gid|gname|
+---+---+-----+
|  1|123| null|
|  2|234| null|
|  5|432| null|
+---+---+-----+
broadcast(df1).join(df2,""id"").show() // BROADCASTING - DOES NOT WORK on EMR
{code};;;","25/Aug/16 08:37;gurmukhd;@Himanish - How are you running the spark-shell ?

Are you passing any custom parameters like --driver-memory etc to the spark-shell ?
;;;","25/Aug/16 13:02;himanish;Hi [~gurmukhd] 

I am using this command on a cluster with r3.2xlarge node instances ( with 61G memory) : 
{code}
spark-shell --master yarn --deploy-mode client --num-executors 20 --executor-cores 2 --executor-memory 12g --driver-memory 48g --conf spark.yarn.executor.memoryOverhead=4096 --conf spark.sql.shuffle.partitions=1024 --conf spark.yarn.maxAppAttempts=1
{code}

*One important thing to note*, if I allocate less memory to the driver {{--driver-memory 12g}} , it works as expected and produces the correct results even with broadcasting. Maybe some weird memory issues during broadcasting ?

Hi [~dongjoon] , will it be possible for you to check whether you see similar behavior with high driver memory settings on the three environments you tested on ?

Thanks ;;;","25/Aug/16 13:33;dongjoon;Interesting, but the use case does not consume such a large memory. If that is the real situation, we had better update the description.;;;","30/Aug/16 07:04;jseppanen;Thanks, I've also filed the issue with Amazon, since it seems possible that it's EMR specific.;;;","31/Aug/16 11:08;srowen;I'll keep it open a while but if we can't reproduce in Spark, not sure what we might do here.;;;","31/Aug/16 22:17;gurmukhd;Hi

I can see this in Apache Spark 2.0 as well, running with same node configurations as mentioned above.

Apache Hadoop 2.72., Spark 2.0:
-------------------------------------------

[hadoop@sp1 ~]$ hadoop version
Hadoop 2.7.2
Subversion Unknown -r Unknown
Compiled by root on 2016-05-16T03:56Z
Compiled with protoc 2.5.0
From source with checksum d0fda26633fa762bff87ec759ebe689c
This command was run using /opt/cluster/hadoop-2.7.2/share/hadoop/common/hadoop-common-2.7.2.jar

[hadoop@sp1 ~]$ spark-shell --version
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.0.0
      /_/

Branch
Compiled by user jenkins on 2016-07-19T21:16:09Z
Revision

[hadoop@sp1 hadoop]$ spark-shell --master yarn --deploy-mode client --num-executors 20 --executor-cores 2 --executor-memory 12g --driver-memory 48g --conf spark.yarn.executor.memoryOverhead=4096 --conf spark.sql.shuffle.partitions=1024 --conf spark.yarn.maxAppAttempts=1
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel).
16/08/31 04:29:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/08/31 04:29:49 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
16/08/31 04:30:19 WARN spark.SparkContext: Use an existing SparkContext, some configuration may not take effect.
Spark context Web UI available at http://10.0.0.227:4040
Spark context available as 'sc' (master = yarn, app id = application_1472617754154_0001).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.0.0
      /_/

Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_101)
Type in expressions to have them evaluated.
Type :help for more information.

scala> val a1 = Array((123,1),(234,2),(432,5))
a1: Array[(Int, Int)] = Array((123,1), (234,2), (432,5))

scala> val a2 = Array((""abc"",1),(""bcd"",2),(""dcb"",5))
a2: Array[(String, Int)] = Array((abc,1), (bcd,2), (dcb,5))

scala> val df1 = sc.parallelize(a1).toDF(""gid"",""id"")
df1: org.apache.spark.sql.DataFrame = [gid: int, id: int]

scala> val df2 = sc.parallelize(a2).toDF(""gname"",""id"")
df2: org.apache.spark.sql.DataFrame = [gname: string, id: int]


scala> df1.join(df2,""id"").show()
+---+---+-----+
| id|gid|gname|
+---+---+-----+
|  1|123|  abc|
|  2|234|  bcd|
|  5|432|  dcb|
+---+---+-----+

scala> val df2 = sc.parallelize(a2).toDF(""gname"",""id"")
df2: org.apache.spark.sql.DataFrame = [gname: string, id: int]

scala> df1.join(broadcast(df2),""id"").show()
+---+---+-----+
| id|gid|gname|
+---+---+-----+
|  1|123| null|
|  2|234| null|
|  5|432| null|
+---+---+-----+

scala> broadcast(df1).join(df2,""id"").show()
+---+---+-----+
| id|gid|gname|
+---+---+-----+
|  0|  1|  abc|
|  0|  2|  bcd|
|  0|  5|  dcb|
+---+---+-----+

If I reduce the driver memory, this works as well. 

It works on Apache spark 1.6 

As lot of things have changed in Spark 2.0, it needs to be looked upon. It should give error or OOM, instead of returning NULL or ZERO values.

[~himanish] Although, it will be interesting to understand the use case that on a node with 61 GB, executing with driver memory=48GB, leaving just 12 GB for so many other things, when there are other overheads on the system.

On DataBricks, are you running with same parameters ?
;;;","01/Sep/16 01:13;himanish;[~gurmukhd] You are seeing the errors outside of EMR environment also, right ?  On Databricks I ran it through a scala notebook, not sure what the underlying configuration was. 

I would want to add one more thing , while running the real job (with lots of data), I noticed even with low driver memory settings and broadcasting enabled , some joins works fine but others messes up the data (either assigns null or field values get mixed up).  What are the other things that would use up 12 GB of memory on the node ?;;;","01/Sep/16 03:15;gurmukhd;yes, outside of EMR.

On a node you will have OS, nodemanger, datanode node daemons using memory.

One other we might need to look at is java 1.8. I have used java 1.8 on Apache Spark 2.0, for tests.;;;","01/Sep/16 21:00;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/14927;;;","01/Sep/16 22:52;gurmukhd;Thanks Davies.

Can see the issue with the offsets:

It picks some random values beyond offset and then tries to do a join on rows, which will obviosuly return NULL as those rows are not there:

scala> val bc=sc.broadcast(df1)

scala> bc.value.view.take(10).foreach(println)
(abc,1)
(bcd,2)
(dcb,5)

scala> bc.value.view.toDF
res13: org.apache.spark.sql.DataFrame = [_1: string, _2: int]

scala> bc.value.view.toDF(""gid"", ""id"")
res14: org.apache.spark.sql.DataFrame = [gid: string, id: int]

scala> bc.value.view.toDF(""gid"", ""id"").show()
+---+---+
|gid| id|
+---+---+
|abc|  1|
|bcd|  2|
|dcb|  5|
+---+---+

scala> df1.join(bc.value.view.toDF(""gid"", ""id""), ""id"").show()
+---+---+----+
| id|gid| gid|
+---+---+----+
|  1|123|null|
|  2|234|null|
|  5|432|null|
+---+---+----+


scala> df1.join(bc.value.view.toDF(""gidyy"", ""id""), ""id"").show()
+---+---+-----+
| id|gid|gidyy|
+---+---+-----+
|  1|123| null|
|  2|234| null|
|  5|432| null|
+---+---+-----+


scala> bc.value.view.toDF(""gid"", ""id"").show()
+---+---+
|gid| id|
+---+---+
|abc|  1|
|bcd|  2|
|dcb|  5|
+---+---+


scala> df1.show()
+---+---+
|gid| id|
+---+---+
|123|  1|
|234|  2|
|432|  5|
+---+---+


scala> df1.join(bc.value.view.toDF(""gidyy"", ""id""), ""id"").show()
+---+---+-----+
| id|gid|gidyy|
+---+---+-----+
|  1|123| null|
|  2|234| null|
|  5|432| null|
+---+---+-----+


scala> bc.value.view.toDF(""gid"", ""id"").join(df1, ""id"").show()
+-------+----+---+
|     id| gid|gid|
+-------+----+---+
|6513249|null|123|	<----- Look at the ""id"" where is it picking from ? is is fetching some out of address locations, which it should not be
|6579042|null|234|
|6447972|null|432|
+-------+----+---+


scala> bc.value.view.toDF(""gidy"", ""id"").join(df1, ""id"").show()
+-------+----+---+
|     id|gidy|gid|
+-------+----+---+
|6513249|null|123|
|6579042|null|234|
|6447972|null|432|
+-------+----+---+;;;","02/Sep/16 11:09;migtor;Hi,

I have been testing this and it seems to be related to memory pointers compression. It happens when the JVMs of the executors use different memory pointers than the driver. The JVM by default enables UseCompressedOops for heaps under 32 GB. So if both the driver and executors memory heaps are over 32 GB, or if both are under 32 GB, there is no problem.

As a workaround, disabling UseCompressedOops in the smaller heap seems to work. For example:
spark-shell --driver-memory 30G --executor-memory 40G --driver-java-options ""-XX:-UseCompressedOops"" ...

Or the other case:
spark-shell --driver-memory 45G --executor-memory 30G --conf ""spark.executor.extraJavaOptions=-XX:-UseCompressedOops"" ...

I also tested JVM 1.7 and 1.8, with the same results. On EMR or oustide, on YARN or standalone, it doesn't matter. Always tested with Spark 2.0.0 using Scala 2.11.8.;;;","02/Sep/16 14:17;srowen;Yeah, almost certainly related to compressed Oops. It'd be great to try master if you can, but I don't know if something else would have addressed it. It seems legitimate though.;;;","02/Sep/16 16:18;migtor;I've just tried master from git, exactly the same results.;;;","02/Sep/16 16:58;davies;[~migtor] Could you try this patch ? https://github.com/apache/spark/pull/14927;;;","02/Sep/16 17:26;dongjoon;Oh, now I see the point of this issue.;;;","02/Sep/16 22:34;gurmukhd;Thanks,

I have tested by disabling UseCompressedOops ""-XX:-UseCompressedOops"" and as stated by [~migtor], it works only for heap <32 GB;;;","02/Sep/16 23:01;davies;Could you try the patch ? https://github.com/apache/spark/pull/14927;;;","02/Sep/16 23:26;gurmukhd;Sure, will update soon with my findings.;;;","02/Sep/16 23:55;migtor;[~gurmukhd], you say it works only for heap < 32 GB, which one do you mean? Driver memory or executors? There is only need to disable compressed oops for the heap that is under 32 GB when the other isn't. You can test disabling it for both cases to be on the safe side:

spark-shell ...  --driver-java-options ""-XX:-UseCompressedOops"" --conf ""spark.executor.extraJavaOptions=-XX:-UseCompressedOops"" ...

[~davies] I just tried the patch, it seems to work! I couldn't break it although probably it needs more thorough testing to cover other cases. But it does seem to fix the issue, thanks!

;;;","03/Sep/16 01:08;gurmukhd;[~davies] After applying the patch, tested with various combination of executor and driver memory. The issue seems to have been fixed.

Tested for cases as below:

$ spark-shell --master yarn --deploy-mode client --executor-memory 12g --driver-memory 48g --conf spark.yarn.executor.memoryOverhead=4096 --conf spark.sql.shuffle.partitions=1024 --conf spark.yarn.maxAppAttempts=1

$ spark-shell --master yarn --deploy-mode client --executor-memory 48g --driver-memory 12g --conf spark.yarn.executor.memoryOverhead=4096 --conf spark.sql.shuffle.partitions=1024 --conf spark.yarn.maxAppAttempts=1

$ spark-shell --master yarn --deploy-mode client --executor-memory 12g --driver-memory 12g --conf spark.yarn.executor.memoryOverhead=4096 --conf spark.sql.shuffle.partitions=1024 --conf spark.yarn.maxAppAttempts=1

$ spark-shell --master yarn --deploy-mode client --executor-memory 38g --driver-memory 38g --conf spark.yarn.executor.memoryOverhead=4096 --conf spark.sql.shuffle.partitions=1024 --conf spark.yarn.maxAppAttempts=1;;;","06/Sep/16 17:47;davies;Issue resolved by pull request 14927
[https://github.com/apache/spark/pull/14927];;;",,,,,,,,,,,,,,,,,
sparkr.zip is not distributed to executors when run sparkr in RStudio,SPARK-17210,12999534,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zjffdu,zjffdu,zjffdu,24/Aug/16 07:37,24/Sep/16 22:04,14/Jul/23 06:29,23/Sep/16 18:39,2.0.0,,,,,,,,2.0.1,2.1.0,,,SparkR,,,,,,,,1,,,,,,"Here's the code to reproduce this issue. 
{code}
Sys.setenv(SPARK_HOME=""/Users/jzhang/github/spark"")
.libPaths(c(file.path(Sys.getenv(), ""R"", ""lib""), .libPaths()))
library(SparkR)
sparkR.session(master=""yarn-client"", sparkConfig = list(spark.executor.instances=""1""))
df <- as.DataFrame(mtcars)
head(df)
{code}

And this is the exception in executor log.
{noformat}
16/08/24 15:33:45 INFO BufferedStreamThread: Fatal error: cannot open file '/Users/jzhang/Temp/hadoop_tmp/nm-local-dir/usercache/jzhang/appcache/application_1471846125517_0022/container_1471846125517_0022_01_000002/sparkr/SparkR/worker/daemon.R': No such file or directory
16/08/24 15:33:55 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 6)
java.net.SocketTimeoutException: Accept timed out
    at java.net.PlainSocketImpl.socketAccept(Native Method)
    at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:404)
    at java.net.ServerSocket.implAccept(ServerSocket.java:545)
    at java.net.ServerSocket.accept(ServerSocket.java:513)
    at org.apache.spark.api.r.RRunner$.createRWorker(RRunner.scala:367)
    at org.apache.spark.api.r.RRunner.compute(RRunner.scala:69)
    at org.apache.spark.api.r.BaseRRDD.compute(RRDD.scala:49)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
    at org.apache.spark.scheduler.Task.run(Task.scala:86)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
{noformat}",,apachespark,asukhenko,felixcheung,rxin,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-12239,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 24 06:12:59 UTC 2016,,,,,,,,,,"0|i32pbj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/16 07:44;apachespark;User 'zjffdu' has created a pull request for this issue:
https://github.com/apache/spark/pull/14784;;;","23/Sep/16 18:41;felixcheung;cc [~rxin] - this is merged to master and branch-2.0. If we are rolling another RC for Spark 2.0.1 will be good to include this fix. Thanks;;;","23/Sep/16 18:45;rxin;[~felixcheung] please set the version to 2.0.2 from now on, since rc was already cut. If there is a new RC, I will change all 2.0.2 tickets to 2.0.1 when I cut the next one.
;;;","24/Sep/16 06:12;felixcheung;Got it, sorry about that, I should have noticed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Comparing Vector in relative tolerance or absolute tolerance in UnitTests error ,SPARK-17207,12999507,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,peng.meng@intel.com,peng.meng@intel.com,peng.meng@intel.com,24/Aug/16 05:59,08/Oct/16 09:18,14/Jul/23 06:29,26/Aug/16 18:54,,,,,,,,,2.1.0,,,,ML,MLlib,,,,,,,0,,,,,,"The result of compare two vectors using UnitTests (org.apache.spark.mllib.util.TestingUtils) is not right sometime.
For example:
val a = Vectors.dense(Arrary(1.0, 2.0))
val b = Vectors.zeros(0)
a ~== b absTol 1e-1 // the result is true. 

",,apachespark,dbtsai,peng.meng@intel.com,qhuang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 26 18:54:31 UTC 2016,,,,,,,,,,"0|i32p5j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/16 06:14;peng.meng@intel.com;This is caused by two Vector zip problem:
    def absTol(eps: Double): CompareVectorRightSide = CompareVectorRightSide(
      (x: Vector, y: Vector, eps: Double) => {
        x.toArray.zip(y.toArray).forall(x => x._1 ~= x._2 absTol eps)  
      }, x, eps, ABS_TOL_MSG)

// forall () always return true if x or y is zero element Vector

val a = Vectors.dense(Array(1.0, 2.0))
val b = Vectors.dense(Array(1.0))
a ~== b absTol 1e-1 // this also return true. Because,  a.toArray.zip(b.toArray) = Array((1.0, 1.0))

;;;","24/Aug/16 07:22;qhuang;So we should check if their length is equal first?
cc [~dbtsai];;;","24/Aug/16 08:54;srowen;Oof, yeah the array lengths have to be checked for vectors and matrices, and for absolute and relative tolerance. Good catch.;;;","24/Aug/16 08:59;peng.meng@intel.com;Thanks Owen, I am testing the code with array length check. will submit PR soon. ;;;","24/Aug/16 10:44;apachespark;User 'mpjlu' has created a pull request for this issue:
https://github.com/apache/spark/pull/14785;;;","25/Aug/16 13:41;peng.meng@intel.com;Hi, org.apache.spark.ml.feature.CountVectorizerSuite.CountVectorizer can not pass this PR.
Because org.scalatest.exceptions.TestFailedException: Expected (2,[0,1],[1.0,1.0]) and (3,[0,1],[1.0,1.0]) to be within 1.0E-14 using absolute tolerance for all elements.
The size of this two vectors is not the same, so it throws exception.
Should we fix this PR or fix CountVectorizerSuite?  ;;;","25/Aug/16 13:46;srowen;Sounds like the test needs to be fixed. Yes, you would fix it in this PR, assuming it's clearly just a latent bug in the test uncovered by this fix. I'd look into it more but not sure which test you refer to here.

(Also I see that the error message is a little misleading, because the cause of the failure isn't that the elements mismatch but the sizes. Maybe we can throw a more specific error here.);;;","25/Aug/16 14:31;peng.meng@intel.com;This is the bug information: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/64418/testReport/org.apache.spark.ml.feature/CountVectorizerSuite/CountVectorizer_vocabSize_and_minDF/
For the following case: 
a = Array(1.0, 1.0)
b = Array(1.0, 1.0, 0)
a ~== b absTol 1e-1 the result should be true or false. maybe this can discuss again.  I tend to this should be false.
Another case: 
a = Array (1.0, 1.0)
b = Array (1.0, 1.0, 2.0)
a ~== b absTol 1e-1 should be true or false? 
I tend to both this two cases should be false. ;;;","25/Aug/16 14:37;srowen;I think vectors of different dimensions can never be nearly-equal. In the first case, there is no third dimension to compare on, because one lacks a third dimension at all; it's not implicitly 0, not even in the sparse case if the dimension is < 3. Yes, both should be false.

Yes, in the second part of that test case, it expects counts over just two words, but the expected output is still of dimension 3. I think you just make the dimension 2 in each of the sparse vectors.;;;","25/Aug/16 14:42;peng.meng@intel.com;Ok, thanks. I will fix CountVectorizerSuite test error in this PR. ;;;","26/Aug/16 18:54;dbtsai;Issue resolved by pull request 14785
[https://github.com/apache/spark/pull/14785];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Literal.sql does not properly convert NaN and Infinity literals,SPARK-17205,12999467,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joshrosen,joshrosen,joshrosen,24/Aug/16 00:43,25/Aug/16 22:16,14/Jul/23 06:29,25/Aug/16 22:16,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,{{Literal.sql}} mishandles NaN and Infinity literals: the handling of these needs to be special-cased instead of simply appending a suffix to the string representation of the value,,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 24 00:47:06 UTC 2016,,,,,,,,,,"0|i32own:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"24/Aug/16 00:47;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14777;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 2.0 off heap RDD persistence with replication factor 2 leads to in-memory data corruption,SPARK-17204,12999280,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,michael,michael,michael,23/Aug/16 16:34,24/Mar/17 04:54,14/Jul/23 06:29,21/Mar/17 03:55,2.0.0,,,,,,,,2.0.3,2.1.1,2.2.0,,Spark Core,,,,,,,,0,,,,,,"We use the {{OFF_HEAP}} storage level extensively with great success. We've tried off-heap storage with replication factor 2 and have always received exceptions on the executor side very shortly after starting the job. For example:

{code}
com.esotericsoftware.kryo.KryoException: Encountered unregistered class ID: 9086
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:137)
	at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:670)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:781)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:229)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:169)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

or

{code}
java.lang.IndexOutOfBoundsException: Index: 6, Size: 0
	at java.util.ArrayList.rangeCheck(ArrayList.java:653)
	at java.util.ArrayList.get(ArrayList.java:429)
	at com.esotericsoftware.kryo.util.MapReferenceResolver.getReadObject(MapReferenceResolver.java:60)
	at com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:834)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:788)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:229)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:169)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

or

{code}
java.lang.NullPointerException
	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$doExecute$1$$anonfun$6.apply(InMemoryTableScanExec.scala:141)
	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$doExecute$1$$anonfun$6.apply(InMemoryTableScanExec.scala:140)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

We've tried switching to Java serialization and get a different exception:

{code}
java.io.StreamCorruptedException: invalid stream header: 780000D0
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:808)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:301)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:63)
	at org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:63)
	at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:122)
	at org.apache.spark.serializer.SerializerManager.dataDeserializeStream(SerializerManager.scala:146)
	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:433)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:672)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

This suggest some kind of memory corruption to me.

I've been able to consistently reproduce this problem in local-cluster mode with a very simple code snippet. First, start a spark shell like this:

{noformat}
MASTER=local-cluster[2,1,1024] ./spark-2.1/bin/spark-shell --conf spark.memory.offHeap.enabled=true --conf spark.memory.offHeap.size=1024
{noformat}

Then run the following:

{code}
import org.apache.spark.storage.StorageLevel
val OFF_HEAP_2 = StorageLevel(useDisk = true, useMemory = true, useOffHeap = true, deserialized = false, replication = 2)
sc.range(0, 100).persist(OFF_HEAP_2).count
{code}",,apachespark,jerryshao,joshrosen,kiszk,michael,michaelmalak,paulweiss,rxin,tsuresh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 22 17:22:03 UTC 2017,,,,,,,,,,"0|i32nrb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/16 18:55;rxin;Does this problem still exist on today's master/branch-2.0? 

SPARK-16550 was merged. It might be fixed already.;;;","23/Aug/16 19:05;michael;[~rxin] I'll give it a try. Thanks for the heads up. I missed that Jira/PR.;;;","23/Aug/16 23:51;michael;[~rxin] I rebuilt from master as of commit 8fd63e808e15c8a7e78fef847183c86f332daa91 (which includes https://github.com/apache/spark/commit/8e223ea67acf5aa730ccf688802f17f6fc10907c) and am still experiencing this issue. I'll work on instructions to reproduce next.;;;","25/Aug/16 03:00;jerryshao;It works OK in my local test with latest build:

{code}
val OFF_HEAP_2 = StorageLevel(useDisk = true, useMemory = true, useOffHeap = true, deserialized = false, replication = 2)
sc.range(0, 0).persist(OFF_HEAP_2).count
{code}

Also I'm curious why SparkSQL related code will be involved according to the exception you pasted above, are you using {{SparkSession#range}} instead. Also tested Dataset persist with {{OFF_HEAP_2}}, it also works fine without exception. 

;;;","25/Aug/16 03:22;michael;Hi [~jerryshao]. I wonder if you're testing in local mode? I only see this problem when running with remote executors on a cluster. When I run in local mode, I see a bunch of warnings like:

{code}
...
16/08/25 03:13:55 WARN storage.BlockManager: Block rdd_3_8 replicated to only 0 peer(s) instead of 1 peers
16/08/25 03:13:55 WARN storage.BlockManager: Block rdd_3_15 replicated to only 0 peer(s) instead of 1 peers
16/08/25 03:13:55 WARN storage.BlockManager: Block rdd_3_9 replicated to only 0 peer(s) instead of 1 peers
...
{code}

These messages suggest to me no actual replication is being attempted, and that's why the problem is not manifested. To answer your other question, the test case I provided was something very simple I came up with after discovering this problem. My coworker was reading data from parquet files when I cut-n-pasted those stack traces.

I'll clarify these points in the description.;;;","25/Aug/16 03:24;jerryshao;No, I tested in yarn cluster, not local mode.;;;","25/Aug/16 03:31;jerryshao;I think to reflect the issue {{sc.range(0, 0)}} should be changed to {{sc.range(0, 2)}}, {{range(0, 0)}} actually persist nothing to memory.;;;","25/Aug/16 03:36;michael;I would think that, but {{sc.range(0, 0)}} throws the exception, too. Are you able to reproduce the problem with {{sc.range(0, 2)}}?;;;","25/Aug/16 03:40;jerryshao;Yes, I could reproduce this issue, but not constantly, sometimes it is OK without any exception.;;;","29/Nov/16 18:02;rxin;Can you try repro this using the local-cluster mode?;;;","29/Nov/16 18:02;michael;FYI I've noticed this remains an issue in the Spark 2.1 branch. I don't know if I'll have time to really dig into this any time in the near future. I'm hoping someone with expertise in this subsystem can have a look. [~joshrosen]?;;;","29/Nov/16 18:08;michael;Is your question directed at me? The RDD storage blocks don't seem to be replicated in that mode, and I believe that's why this problem does not manifest itself. Or maybe I misunderstand what you mean by ""local-cluster"" mode.;;;","29/Nov/16 18:11;rxin;local-cluster is different from the local mode. It is a local ""cluster"" with multiple processes.

Try 

{noformat}
> MASTER=local-cluster[2,1,1024] bin/spark-shell
{noformat}
;;;","29/Nov/16 18:33;michael;I'm able to reproduce the problem with this configuration, and I've updated this ticket's description to reflect that. Thanks.;;;","02/Jan/17 00:19;michael;I'm 99% sure I've fixed this. I'll submit a PR in the coming days.;;;","07/Jan/17 21:39;apachespark;User 'mallman' has created a pull request for this issue:
https://github.com/apache/spark/pull/16499;;;","22/Mar/17 17:22;apachespark;User 'mallman' has created a pull request for this issue:
https://github.com/apache/spark/pull/17390;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"When emitting SQL for string literals Spark should use single quotes, not double",SPARK-17194,12999068,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joshrosen,joshrosen,joshrosen,23/Aug/16 00:38,23/Aug/16 20:33,14/Jul/23 06:29,23/Aug/16 20:33,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"When Spark emits SQL for a string literal, it should wrap the string in single quotes, not double quotes. Databases which adhere more strictly to the ANSI SQL standards, such as Postgres, allow only single-quotes to be used for denoting string literals (see http://stackoverflow.com/a/1992331/590203).",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 23 00:41:04 UTC 2016,,,,,,,,,,"0|i32mg7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/16 00:41;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14763;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CollectList and CollectSet should be marked as non-deterministic,SPARK-17182,12998849,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,22/Aug/16 10:26,23/Aug/16 01:15,14/Jul/23 06:29,23/Aug/16 01:15,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,{{CollectList}} and {{CollectSet}} should be marked as non-deterministic since their results depend on the actual order of input rows.,,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 22 10:32:05 UTC 2016,,,,,,,,,,"0|i32l3j:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"22/Aug/16 10:32;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/14749;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to Alter the Temporary View Using ALTER VIEW command,SPARK-17180,12998823,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,smilegator,smilegator,22/Aug/16 06:53,01/Sep/16 01:00,14/Jul/23 06:29,31/Aug/16 09:10,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"In the current master branch, when users do not specify the database name in the `ALTER VIEW AS SELECT` command, we always try to alter the permanent view even if the temporary view exists. 

The expected behavior of `ALTER VIEW AS SELECT` should be like: alters the temporary view if the temp view exists; otherwise, try to alter the permanent view. This order is consistent with another command `DROP VIEW`, since users are unable to specify the keyword TEMPORARY.",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 31 10:29:05 UTC 2016,,,,,,,,,,"0|i32kxr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/16 06:55;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14746;;;","30/Aug/16 09:39;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/14874;;;","31/Aug/16 09:10;cloud_fan;Issue resolved by pull request 14874
[https://github.com/apache/spark/pull/14874];;;","31/Aug/16 10:29;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/14893;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issue Exceptions when Analyze Table on In-Memory Cataloged Tables,SPARK-17167,12998658,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,20/Aug/16 06:52,25/Aug/16 12:40,14/Jul/23 06:29,25/Aug/16 12:40,2.0.0,,,,,,,,2.0.1,,,,SQL,,,,,,,,0,,,,,,"Currently, `Analyze Table` is only for Hive-serde tables. We should issue exceptions in all the other cases. When the tables are data source tables, we issued an exception. However, when tables are In-Memory Cataloged tables, we do not issue any exception.",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 24 05:08:07 UTC 2016,,,,,,,,,,"0|i32jx3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/16 07:02;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14729;;;","24/Aug/16 05:08;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14781;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CTAS lost table properties after conversion to data source tables.,SPARK-17166,12998656,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,smilegator,smilegator,20/Aug/16 05:46,07/Nov/18 00:36,14/Jul/23 06:29,07/Nov/18 00:36,2.0.0,,,,,,,,2.4.0,,,,SQL,,,,,,,,0,,,,,,"CTAS lost table properties after conversion to data source tables. For example, 
{noformat}
CREATE TABLE t TBLPROPERTIES('prop1' = 'c', 'prop2' = 'd') AS SELECT 1 as a, 1 as b
{noformat}
The output of `DESC FORMATTED t` does not have the related properties. 
{noformat}
|Table Parameters:           |                                                                                                              |       |
|  rawDataSize               |-1                                                                                                            |       |
|  numFiles                  |1                                                                                                             |       |
|  transient_lastDdlTime     |1471670983                                                                                                    |       |
|  totalSize                 |496                                                                                                           |       |
|  spark.sql.sources.provider|parquet                                                                                                       |       |
|  EXTERNAL                  |FALSE                                                                                                         |       |
|  COLUMN_STATS_ACCURATE     |false                                                                                                         |       |
|  numRows                   |-1                                                                                                            |       |
|                            |                                                                                                              |       |
|# Storage Information       |                                                                                                              |       |
|SerDe Library:              |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                                                   |       |
|InputFormat:                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat                                                 |       |
|OutputFormat:               |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat                                                |       |
|Compressed:                 |No                                                                                                            |       |
|Storage Desc Parameters:    |                                                                                                              |       |
|  serialization.format      |1                                                                                                             |       |
|  path                      |file:/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/warehouse-f3aa2927-6464-4a35-a715-1300dde6c614/t|       |
{noformat}
",,apachespark,dongjoon,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23355,SPARK-22158,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 07 00:36:36 UTC 2018,,,,,,,,,,"0|i32jwn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/16 05:51;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14727;;;","07/Nov/18 00:36;dongjoon;It is decided to support Spark-related Parquet/ORC properties and implemented in Spark 2.4.0.
The other properties which are unknown to Spark will be ignored intentionally.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Range does not support SQL generation,SPARK-17162,12998618,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ekhliang,ekhliang,ekhliang,19/Aug/16 23:10,08/Dec/16 23:13,14/Jul/23 06:29,22/Aug/16 22:48,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"{code}
scala> sql(""create view a as select * from range(100)"")
16/08/19 21:10:29 INFO SparkSqlParser: Parsing command: create view a as select * from range(100)
java.lang.UnsupportedOperationException: unsupported plan Range (0, 100, splits=8)

  at org.apache.spark.sql.catalyst.SQLBuilder.org$apache$spark$sql$catalyst$SQLBuilder$$toSQL(SQLBuilder.scala:212)
  at org.apache.spark.sql.catalyst.SQLBuilder.org$apache$spark$sql$catalyst$SQLBuilder$$toSQL(SQLBuilder.scala:165)
  at org.apache.spark.sql.catalyst.SQLBuilder.projectToSQL(SQLBuilder.scala:229)
  at org.apache.spark.sql.catalyst.SQLBuilder.org$apache$spark$sql$catalyst$SQLBuilder$$toSQL(SQLBuilder.scala:127)
  at org.apache.spark.sql.catalyst.SQLBuilder.org$apache$spark$sql$catalyst$SQLBuilder$$toSQL(SQLBuilder.scala:165)
  at org.apache.spark.sql.catalyst.SQLBuilder.projectToSQL(SQLBuilder.scala:229)
  at org.apache.spark.sql.catalyst.SQLBuilder.org$apache$spark$sql$catalyst$SQLBuilder$$toSQL(SQLBuilder.scala:127)
  at org.apache.spark.sql.catalyst.SQLBuilder.toSQL(SQLBuilder.scala:97)
  at org.apache.spark.sql.execution.command.CreateViewCommand.prepareTable(views.scala:174)
  at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:138)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
```

{code}",,apachespark,ekhliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 19 23:13:05 UTC 2016,,,,,,,,,,"0|i32jo7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Aug/16 23:13;apachespark;User 'ericl' has created a pull request for this issue:
https://github.com/apache/spark/pull/14724;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"GetExternalRowField does not properly escape field names, causing generated code not to compile",SPARK-17160,12998574,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,joshrosen,joshrosen,19/Aug/16 19:21,20/Sep/16 03:22,14/Jul/23 06:29,20/Sep/16 03:22,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"The following end-to-end test uncovered a bug in {{GetExternalRowField}}:

{code}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.catalyst.encoders._

spark.sql(""set spark.sql.codegen.fallback=false"")

val df = Seq((""100-200"", ""1"", ""300"")).toDF(""a"", ""b"", ""c"")
val df2 = df.select(regexp_replace($""a"", ""(\\d+)"", ""num""))
df2.mapPartitions(x => x)(RowEncoder(df2.schema)).collect()
{code}

This causes

{code}
java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 55, Column 64: Invalid escape sequence
{code}

The generated code is

{code}
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */
/* 005 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 006 */   private Object[] references;
/* 007 */   private scala.collection.Iterator inputadapter_input;
/* 008 */   private java.lang.String serializefromobject_errMsg;
/* 009 */   private java.lang.String serializefromobject_errMsg1;
/* 010 */   private UnsafeRow serializefromobject_result;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder serializefromobject_holder;
/* 012 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter serializefromobject_rowWriter;
/* 013 */
/* 014 */   public GeneratedIterator(Object[] references) {
/* 015 */     this.references = references;
/* 016 */   }
/* 017 */
/* 018 */   public void init(int index, scala.collection.Iterator inputs[]) {
/* 019 */     partitionIndex = index;
/* 020 */     inputadapter_input = inputs[0];
/* 021 */     this.serializefromobject_errMsg = (java.lang.String) references[0];
/* 022 */     this.serializefromobject_errMsg1 = (java.lang.String) references[1];
/* 023 */     serializefromobject_result = new UnsafeRow(1);
/* 024 */     this.serializefromobject_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(serializefromobject_result, 32);
/* 025 */     this.serializefromobject_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(serializefromobject_holder, 1);
/* 026 */   }
/* 027 */
/* 028 */   protected void processNext() throws java.io.IOException {
/* 029 */     while (inputadapter_input.hasNext()) {
/* 030 */       InternalRow inputadapter_row = (InternalRow) inputadapter_input.next();
/* 031 */       org.apache.spark.sql.Row inputadapter_value = (org.apache.spark.sql.Row)inputadapter_row.get(0, null);
/* 032 */
/* 033 */       if (false) {
/* 034 */         throw new RuntimeException(serializefromobject_errMsg);
/* 035 */       }
/* 036 */
/* 037 */       boolean serializefromobject_isNull1 = false || false;
/* 038 */       final boolean serializefromobject_value1 = serializefromobject_isNull1 ? false : inputadapter_value.isNullAt(0);
/* 039 */       boolean serializefromobject_isNull = false;
/* 040 */       UTF8String serializefromobject_value = null;
/* 041 */       if (!serializefromobject_isNull1 && serializefromobject_value1) {
/* 042 */         final UTF8String serializefromobject_value5 = null;
/* 043 */         serializefromobject_isNull = true;
/* 044 */         serializefromobject_value = serializefromobject_value5;
/* 045 */       } else {
/* 046 */         if (false) {
/* 047 */           throw new RuntimeException(serializefromobject_errMsg1);
/* 048 */         }
/* 049 */
/* 050 */         if (false) {
/* 051 */           throw new RuntimeException(""The input external row cannot be null."");
/* 052 */         }
/* 053 */
/* 054 */         if (inputadapter_value.isNullAt(0)) {
/* 055 */           throw new RuntimeException(""The 0th field 'regexp_replace(a, (\d+), num)' of input row "" +
/* 056 */             ""cannot be null."");
/* 057 */         }
/* 058 */
/* 059 */         final Object serializefromobject_value8 = inputadapter_value.get(0);
/* 060 */         java.lang.String serializefromobject_value7 = null;
/* 061 */         if (!false) {
/* 062 */           if (serializefromobject_value8 instanceof java.lang.String) {
/* 063 */             serializefromobject_value7 = (java.lang.String) serializefromobject_value8;
/* 064 */           } else {
/* 065 */             throw new RuntimeException(serializefromobject_value8.getClass().getName() + "" is not a valid "" +
/* 066 */               ""external type for schema of string"");
/* 067 */           }
/* 068 */         }
/* 069 */         boolean serializefromobject_isNull6 = false;
/* 070 */         final UTF8String serializefromobject_value6 = serializefromobject_isNull6 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(serializefromobject_value7);
/* 071 */         serializefromobject_isNull6 = serializefromobject_value6 == null;
/* 072 */         serializefromobject_isNull = serializefromobject_isNull6;
/* 073 */         serializefromobject_value = serializefromobject_value6;
/* 074 */       }
/* 075 */       serializefromobject_holder.reset();
/* 076 */
/* 077 */       serializefromobject_rowWriter.zeroOutNullBytes();
/* 078 */
/* 079 */       if (serializefromobject_isNull) {
/* 080 */         serializefromobject_rowWriter.setNullAt(0);
/* 081 */       } else {
/* 082 */         serializefromobject_rowWriter.write(0, serializefromobject_value);
/* 083 */       }
/* 084 */       serializefromobject_result.setTotalSize(serializefromobject_holder.totalSize());
/* 085 */       append(serializefromobject_result);
/* 086 */       if (shouldStop()) return;
/* 087 */     }
/* 088 */   }
/* 089 */ }
{code}

Here, the problem is that the auto-generated field name contains special characters (including backslashes) and those aren't escaped when being interpolated into the generated code, causing the invalid string literal

{code}
""The 0th field 'regexp_replace(a, (\d+), num)' of input row ""
{code}

to appear in the generated code.

We need to update {{GetExternalRowField}} to escape field names and also need to audit other expressions to make sure that we're not making the same mistake there.",,apachespark,joshrosen,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 20 03:22:22 UTC 2016,,,,,,,,,,"0|i32jef:",9223372036854775807,,,,,,,,,,,,,2.0.1,,,,,,,,,,,"19/Sep/16 22:32;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15156;;;","20/Sep/16 03:22;joshrosen;Issue resolved by pull request 15156
[https://github.com/apache/spark/pull/15156];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve error message for numeric literal parsing,SPARK-17158,12998560,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vssrinath,vssrinath,vssrinath,19/Aug/16 18:23,20/Aug/16 02:54,14/Jul/23 06:29,20/Aug/16 02:54,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"Spark currently gives confusing and inconsistent error messages for numeric literals. For example:
scala> sql(""select 123456Y"")
org.apache.spark.sql.catalyst.parser.ParseException:
Value out of range. Value:""123456"" Radix:10(line 1, pos 7)

== SQL ==
select 123456Y
-------^^^
scala> sql(""select 123456S"")
org.apache.spark.sql.catalyst.parser.ParseException:
Value out of range. Value:""123456"" Radix:10(line 1, pos 7)

== SQL ==
select 123456S
-------^^^
scala> sql(""select 12345623434523434564565L"")
org.apache.spark.sql.catalyst.parser.ParseException:
For input string: ""12345623434523434564565""(line 1, pos 7)

== SQL ==
select 12345623434523434564565L
-------^^^
The problem is that we are relying on JDK's implementations for parsing, and those functions throw different error messages. This code can be found in AstBuilder.numericLiteral function.
The proposal is that instead of using `_.toByte` to turn a string into a byte, we always turn the numeric literal string into a BigDecimal, and then we validate the range before turning it into a numeric value. This way, we have more control over the data.
If BigDecimal fails to parse the number, we should throw a better exception than ""For input string ..."".",,apachespark,vssrinath,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 19 18:26:08 UTC 2016,,,,,,,,,,"0|i32jbb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Aug/16 18:26;apachespark;User 'srinathshankar' has created a pull request for this issue:
https://github.com/apache/spark/pull/14721;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Structured streams] readStream ignores partition columns,SPARK-17153,12998508,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,dcarpov,dcarpov,19/Aug/16 14:58,28/Dec/16 18:13,14/Jul/23 06:29,26/Sep/16 20:07,2.0.0,,,,,,,,2.0.2,2.1.0,,,Structured Streaming,,,,,,,,0,release_notes,releasenotes,,,,"When parquet files are persisted using partitions, spark's `readStream` returns data with all `null`s for the partitioned columns.

For example:

{noformat}
case class A(id: Int, value: Int)

val data = spark.createDataset(Seq(
  A(1, 1), 
  A(2, 2), 
  A(2, 3))
)

val url = ""/mnt/databricks/test""
data.write.partitionBy(""id"").parquet(url)
{noformat}

when data is read as stream:

{noformat}
spark.readStream.schema(spark.read.load(url).schema).parquet(url)
{noformat}

it reads:

{noformat}
id, value
null, 1
null, 2
null, 3
{noformat}

A possible reason is `readStream` reads parquet files directly but when those are stored the columns they are partitioned by are excluded from the file itself. In the given example the parquet files contain `value` information only since `id` is partition.",,apachespark,dcarpov,marmbrus,wm624,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 27 23:54:10 UTC 2016,,,,,,,,,,"0|i32izr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/16 07:26;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/14803;;;","26/Sep/16 20:07;marmbrus;Issue resolved by pull request 14803
[https://github.com/apache/spark/pull/14803];;;","27/Oct/16 23:54;yhuai;This change needs a release note because {{spark.readStream.json('/a.file')}} (create a stream on a single file) will not work anymore (https://github.com/apache/spark/pull/14803/files#diff-e82a44dc550d2a0a92e44d1ec2ecabccR52).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Streaming Kafka 0.10 Consumer Can't Handle Non-consecutive Offsets (i.e. Log Compaction),SPARK-17147,12998372,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,koeninger,robert@crunchbase.com,robert@crunchbase.com,19/Aug/16 01:33,24/Apr/18 11:54,14/Jul/23 06:29,27/Feb/18 14:21,2.0.0,,,,,,,,2.4.0,,,,DStreams,,,,,,,,10,,,,,,"When Kafka does log compaction offsets often end up with gaps, meaning the next requested offset will be frequently not be offset+1. The logic in KafkaRDD & CachedKafkaConsumer has a baked in assumption that the next offset will always be just an increment of 1 above the previous offset. 

I have worked around this problem by changing CachedKafkaConsumer to use the returned record's offset, from:
{{nextOffset = offset + 1}}
to:
{{nextOffset = record.offset + 1}}

and changed KafkaRDD from:
{{requestOffset += 1}}
to:
{{requestOffset = r.offset() + 1}}

(I also had to change some assert logic in CachedKafkaConsumer).

There's a strong possibility that I have misconstrued how to use the streaming kafka consumer, and I'm happy to close this out if that's the case. If, however, it is supposed to support non-consecutive offsets (e.g. due to log compaction) I am also happy to contribute a PR.",,apachespark,caolaner,chao.wu,deanwampler,dougb,fredericschmaljohann,graphex,hakim.acharifi,jlaskowski,jrmiller,koeninger,kristopherkane,lwlin,niezgop,oliver.lockwood,rajkumarmore,robert@crunchbase.com,roczei,rxin,skidank,sparrovv,twasti,,,,,,,,,,,,SPARK-24067,,,,SPARK-19361,,,,,,,,,,SPARK-23685,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 27 14:21:26 UTC 2018,,,,,,,,,,"0|i32i5j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/16 14:15;koeninger;Have you successfully used the 0.8 consumer with compacted topics?
Off the top of my head, i'd expect the count methods and offset ranges to have similar baked in assumptions.;;;","21/Aug/16 18:30;robert@crunchbase.com;Nope, I started the project with kafka 0.10 and spark 2.0.0-preview. I could take a crack at building a small test project that illuminates the issue, or maybe it would be easier if I wrote a test in whichever spark suite hits the streaming connector. ;;;","22/Aug/16 14:33;koeninger;My point is more that this probably isn't just two lines in CachedKafkaConsumer.  There's other code, both within the spark streaming connector and in users of the connector, that assumes an offset range from..until has a number of messages equal to (until - from).  I haven't seen what databricks is coming up with for the structured streaming connector, but I'd imagine that an assumption that offsets are contiguous would certainly simplify that implementation, and might actually be necessary depending on how recovery works.

This might be a simple as your change plus logging a warning when a stream starts on a compacted topic, but we need to think through the issues here.;;;","25/Aug/16 19:02;graphex;Compacted topics in Kafka prior to 0.9 had some issues and using topic compaction in 0.8.x is generally not recommended, from what I've seen.
;;;","25/Aug/16 21:05;graphex;I tried Robert's changes, but the performance for any sizable number of reads is really bad. At least the way I understand it, whenever there is a discontiguous offset, it forces Kafka to do a seek, which is extremely slow.;;;","31/Aug/16 20:09;robert@crunchbase.com;[~graphex] you're absolutely right about the seek, but that's exactly how log compaction is designed. How else could it work?;;;","01/Sep/16 00:44;graphex;I think Kafka's log compaction's design is still intended for sequential reading, even if the offsets are not consecutive for a compacted topic. Kafka's internal log cleaner process copies log segments to new files which have been compacted, so the messages are still stored sequentially even if the offset metadata for them increases by more than one. The typical consumer just does a poll() to get the next records, regardless of their offsets, but this Spark's CachedKafkaConsumer checks the offset of each record before calling poll(), and if that offset isn't the previous record's offset +1, it's going to call consumer.seek() before the next poll(), which I think is producing the dramatic slowdown I've seen.
It is certainly possible, using a non-Spark Kafka consumer, to get equivalent read speeds regardless of whether a topic is compacted.
I think the interplay between the CachedKafkaConsumer and the KafkaRDD might need to be adjusted. I haven't looked to see if more than one KafkaRDD will ever be asking for records from a single CachedKafkaConsumer instance, but since CachedKafkaConsumer was inspecting each offset to see if it was exactly the offset requested, and not just >= the requested offset, I'm guessing there was a reason. 

The main issue here is that it's becoming apparent that Kafka consumers can't assume consecutively increasing offsets. Unfortunately that is an assumption that Spark-Kafka was making, and I think that assumption will need to be removed.
(Edit: changed ""monotonically"" to ""consecutively"" above, since consumers _can_ assume an ever increasing set of offsets, just not consecutively increasing) ;;;","08/Oct/16 19:28;koeninger;I talked with Sean in person about this, and think there's a way to move forward.  I'll start hacking on it.;;;","09/Oct/16 16:21;koeninger;[~graphex] My WIP is at https://github.com/koeninger/spark-1/tree/SPARK-17147  Still really rough, but it's passing the basic test I set up.  Let me know at what point you have time for trying it out & we can iterate.;;;","09/Oct/16 18:50;graphex;I just had time to read through the changeset today, but so far it seems like a good direction. I had initially thought it would be nice if we could have the same code paths for compacted/non-compacted, but there are certainly at least a few places where the separation can't be avoided. I understand the reasoning of having the flag and some distinct paths now. I'll work on vetting it through my use case during the week and let you know what I find. Note that my use case is primarily using the KafkaUtils.createRDD to read from previously computed offset ranges in topic partitions, so that's at least where I'll start.;;;","18/Oct/16 02:15;jrmiller;Could this possibly be related to why I'm seeing the following?

16/10/18 02:11:02 WARN TaskSetManager: Lost task 6.0 in stage 2.0 (TID 5823, ip-172-20-222-162.int.protectwise.net): java.lang.IllegalStateException: This consumer has already been closed.
	at org.apache.kafka.clients.consumer.KafkaConsumer.ensureNotClosed(KafkaConsumer.java:1417)
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:1428)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:929)
	at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.poll(CachedKafkaConsumer.scala:99)
	at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.get(CachedKafkaConsumer.scala:73)
	at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:227)
	at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:193);;;","18/Oct/16 02:17;koeninger;Well, are you using compacted topics?;;;","18/Oct/16 19:15;jrmiller;Log compaction? Only for our offset topics.;;;","18/Oct/16 19:19;koeninger;Then no, this issue is unlikely to affect you unless there's something wrong with your topic.;;;","18/Oct/16 19:21;jrmiller;OK thank you. Might be related to the thousands of partitions we have spread across hundreds of topics. ""Oops"".;;;","18/Oct/16 19:24;koeninger;If that's something you're seeing regularly, probably worth bringing it up on the mailing list, with a full stacktrace and whatever background you have;;;","18/Oct/16 19:25;jrmiller;K I'll try to assemble everything I've seen so far around it and post it to the list. Thanks!;;;","07/Dec/16 09:59;caolaner;Will createDirectStream also be fixed.
I hit the same issue, but I use KafkaUtils.createDirectStream() for compact mode kafka topics.

BTW, I hit another issue recently, this is not a delete mode kafka topic, but it had the similar error log.
-----------------------------------------
User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 501.0 failed 4 times, most recent failure: Lost task 0.3 in stage 501.0 (TID 505, siwu24yarn21xxxxx.com): java.lang.AssertionError: assertion failed: Failed to get records for spark-executor-xxxxxContext..dXa3AOoH 1029.job.dXa3AOoH.data 0 128825069 after polling for 512
at scala.Predef$.assert(Predef.scala:170)
at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.get(CachedKafkaConsumer.scala:74)
at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:227)
at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:193)
at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31)
;;;","07/Dec/16 15:43;koeninger;This ticket is about createDirectStream.  The question of whether it will be fixed is largely down to whether it's important enough to Sean or someone else to help test it thoroughly.

The stack trace you posted more than likely has nothing to do with this ticket, especially if you aren't using log compaction.  It's probably a network issue.  Adjust spark.streaming.kafka.consumer.poll.ms, or do more investigation into what's going on with your network / Kafka.;;;","12/Dec/16 04:08;caolaner;I am using spark 2.0.0 + kafka 0.10 + compact mode topics even in some production environment.  This fix is really important. so the question is how to decide it is important or not. Compact kafka topic should be widely used now, spark 2.0 should support it well.

For the other issue, it did not happen all the time, did not have regular pattern, several times one day or did not happen in several days.
so  I should enlarge the spark.streaming.kafka.consumer.poll.ms, right.;;;","12/Dec/16 15:44;koeninger;If compacted topics are important to you, then you should help test the branch listed above.

Yes, you can try increasing poll.ms;;;","12/Dec/16 20:16;graphex;I apologize for the extended radio silence on this. I've been trying to track down problems I'm seeing after dynamically scaling down Spark Streaming jobs in Mesos. Haven't been able to attribute them to Spark Streaming Kafka (or a version thereof) yet but it's been difficult to pin down. I am hoping to return to testing the compacted consumer modifications soon.;;;","13/Dec/16 06:54;caolaner;Sure, I will give a try on your branch, will let you know the result.;;;","24/Feb/17 14:26;deanwampler;We're interested in this enhancement. Anyone know if and one it will be implemented in Spark?;;;","28/Feb/17 02:22;koeninger;Dean if you guys have any bandwith to help test out https://github.com/koeninger/spark-1/tree/SPARK-17147  I'm happy to iterate on whatever you find.;;;","28/Feb/17 05:55;deanwampler;Cody, thanks for the suggestion. We'll try to test it and also suggest a customer do the same who wants this functionality.;;;","07/Sep/17 13:14;sparrovv;Hey, just wanted to check if there's any progress on this issue?;;;","07/Sep/17 14:22;koeninger;Patch is there, if anyone wants to test it and provide feedback I'm happy to help.  We don't use compacted topics, so I can't really test it against production workloads.;;;","24/Jan/18 06:51;chao.wu;While the  topic clean config is delete ""cleanup.policy=delete"",  the offset  is also  non-consecutive. And it cause the spark streaming failure.
h1.  ;;;","24/Jan/18 18:31;jrmiller;I'm also seeing this behavior on a topic that has cleanup.policy=delete. The volume on this topic is very large, > 10 billion messages per day, and it seems to happen about once per day. Another topic with lower volume but larger messages happens every few days.

18/01/23 18:30:10 WARN TaskSetManager: Lost task 28.0 in stage 26.0 (TID 861, <ip>,executor 15): java.lang.AssertionError: assertion failed: Got wrong record for <consumergroup>-8002 <topic> 124 even after seeking to offset 1769485661 got back record.offset 1769485662
18/01/23 18:30:12 INFO TaskSetManager: Lost task 28.1 in stage 26.0 (TID 865) on <ip>,executor 24: java.lang.AssertionError (assertion failed: Got wrong record for <consumergroup>-8002 <topic> 124 even after seeking to offset 1769485661 got back record.offset 1769485662) [duplicate 1]
18/01/23 18:30:14 INFO TaskSetManager: Lost task 28.2 in stage 26.0 (TID 866) on <ip>,executor 15: java.lang.AssertionError (assertion failed: Got wrong record for <consumergroup>-8002 <topic> 124 even after seeking to offset 1769485661 got back record.offset 1769485662) [duplicate 2]
18/01/23 18:30:15 INFO TaskSetManager: Lost task 28.3 in stage 26.0 (TID 867) on <ip>,executor 15: java.lang.AssertionError (assertion failed: Got wrong record for <consumergroup>-8002 <topic> 124 even after seeking to offset 1769485661 got back record.offset 1769485662) [duplicate 3]
18/01/23 18:30:18 WARN TaskSetManager: Lost task 28.0 in stage 27.0 (TID 898, <ip>,executor 6): java.lang.AssertionError: assertion failed: Got wrong record for <consumergroup>-8002 <topic> 124 even after seeking to offset 1769485661 got back record.offset 1769485662
18/01/23 18:30:19 INFO TaskSetManager: Lost task 28.1 in stage 27.0 (TID 900) on <ip>,executor 15: java.lang.AssertionError (assertion failed: Got wrong record for <consumergroup>-8002 <topic> 124 even after seeking to offset 1769485661 got back record.offset 1769485662) [duplicate 1]
18/01/23 18:30:20 INFO TaskSetManager: Lost task 28.2 in stage 27.0 (TID 901) on <ip>,executor 15: java.lang.AssertionError (assertion failed: Got wrong record for <consumergroup>-8002 <topic> 124 even after seeking to offset 1769485661 got back record.offset 1769485662) [duplicate 2]
18/01/23 18:30:21 INFO TaskSetManager: Lost task 28.3 in stage 27.0 (TID 902) on <ip>,executor 15: java.lang.AssertionError (assertion failed: Got wrong record for <consumergroup>-8002 <topic> 124 even after seeking to offset 1769485661 got back record.offset 1769485662) [duplicate 3]

When checked with kafka-simple-consumer-shell, the offset is in fact missing:

next offset = 1769485661
next offset = 1769485663
next offset = 1769485664
next offset = 1769485665

I'm currently testing out this branch in the persister and will post if it crashes again over the next few days (I currently have the kafka-10 source from the branch with a few extra log lines deployed). We're currently on log format 0.10.2 (upgraded yesterday) but saw the same issue on 0.9.0.0.

chao.wu - Is this behavior similar to what you're seeing?;;;","11/Feb/18 05:28;apachespark;User 'koeninger' has created a pull request for this issue:
https://github.com/apache/spark/pull/20572;;;","27/Feb/18 14:21;srowen;Issue resolved by pull request 20572
[https://github.com/apache/spark/pull/20572];;;",,,,,,,,,,,
Complex query triggers binding error in HashAggregateExec,SPARK-17142,12998323,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,jiangxb1987,joshrosen,joshrosen,18/Aug/16 21:53,14/Sep/16 05:46,14/Jul/23 06:29,13/Sep/16 15:08,2.1.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"The following example runs successfully on Spark 2.0.0 but fails in the current master (as of b72bb62d421840f82d663c6b8e3922bd14383fbb, if not earlier):

{code}
spark.sql(""set spark.sql.crossJoin.enabled=true"")

sc.parallelize(Seq(0)).toDF(""int_col_1"").createOrReplaceTempView(""table_4"")
sc.parallelize(Seq(0)).toDF(""bigint_col_2"").createOrReplaceTempView(""table_2"")

val query = """"""
SELECT
((t2.int_col) + (t1.bigint_col_2)) + ((t2.int_col) + (t1.bigint_col_2)) AS int_col_1
FROM table_2 t1
INNER JOIN (
    SELECT
        LEAST(IF(False, LAG(0) OVER (ORDER BY t2.int_col_1 DESC), -230), -991) AS int_col,
        (t2.int_col_1) + (t1.int_col_1) AS int_col_2,
        (t1.int_col_1) + (t2.int_col_1) AS int_col_3,
        t2.int_col_1
    FROM
        table_4 t1,
        table_4 t2
    GROUP BY
        (t1.int_col_1) + (t2.int_col_1),
        t2.int_col_1
) t2
WHERE (t2.int_col_3) NOT IN (t2.int_col, t2.int_col_1)
GROUP BY (t2.int_col) + (t1.bigint_col_2)
""""""

spark.sql(query).collect()
{code}

This fails with the following exception:

{code}
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: bigint_col_2#65
  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:88)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:87)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:279)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:279)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:278)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:322)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:320)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:284)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:322)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:320)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:284)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:322)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:320)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:284)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:268)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:87)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$32.apply(HashAggregateExec.scala:455)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$32.apply(HashAggregateExec.scala:454)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.immutable.List.map(List.scala:285)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.generateResultCode(HashAggregateExec.scala:454)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:538)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:145)
  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:37)
  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:309)
  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:347)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:226)
  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:273)
  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2226)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
  at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2576)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2225)
  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2230)
  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2230)
  at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2589)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2230)
  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2206)
  ... 48 elided
Caused by: java.lang.RuntimeException: Couldn't find bigint_col_2#65 in [(-991 + bigint_col_2#65)#84]
  at scala.sys.package$.error(package.scala:27)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:94)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:88)
  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
  ... 110 more
{code}

Note that this error occurs during query execution, not during analysis or physical planning.",,apachespark,jiangxb1987,joshrosen,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 14 05:46:06 UTC 2016,,,,,,,,,,"0|i32hun:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"18/Aug/16 21:56;joshrosen;Interestingly, this query executes fine if the repeated addition in the SELECT clause is replaced by {{* 2}} instead.;;;","01/Sep/16 09:06;jiangxb1987;In `ReorderAssociativeOperator` rule, we extract foldable expressions with Add/Multiply arithmetics, and replace with eval literal. For example, (a + 1) + (b + 2) is optimized to (a + b + 3) by this rule.
For aggregate operator, output expressions should be derived from groupingExpressions, current implemenation of `ReorderAssociativeOperator` rule may break this promise. A instance could be:
{code:sql}
SELECT
  ((t1.a + 1) + (t2.a + 2)) AS out_col
FROM
  testdata2 AS t1
INNER JOIN
  testdata2 AS t2
ON
  (t1.a = t2.a)
GROUP BY (t1.a + 1), (t2.a + 2)
{code}
((t1.a + 1) + (t2.a + 2)) is optimized to (t1.a + t2.a + 3), which could not be derived from ExpressionSet((t1.a +1), (t2.a + 2)).

Maybe we should improve the rule of `ReorderAssociativeOperator` by adding a GroupingExpressionSet to keep Aggregate.groupingExpressions, and respect these expressions during the optimize stage.;;;","01/Sep/16 10:58;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/14917;;;","14/Sep/16 05:46;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/15092;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MinMaxScaler behaves weird when min and max have the same value and some values are NaN,SPARK-17141,12998284,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yanboliang,Bonsanto,Bonsanto,18/Aug/16 18:54,19/Aug/16 11:01,14/Jul/23 06:29,19/Aug/16 10:24,1.6.2,2.0.0,,,,,,,2.1.0,,,,ML,,,,,,,,0,,,,,,"When you have a {{DataFrame}} with a column named {{features}}, which is a {{DenseVector}} and the *maximum* and *minimum* are exactly the same value, but some other are {{Double.NaN}} they get replaced by 0.5, and they should remain with the same value, I believe.

I know how to fix it, but I haven't ever made a pull request. You can check the bug in this [notebook|https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2485090270202665/3126465289264547/8589256059752547/latest.html]","Databrick's Community, Spark 2.0 + Scala 2.10",apachespark,Bonsanto,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 19 11:01:09 UTC 2016,,,,,,,,,,"0|i32hlz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/16 18:57;srowen;Summarize the reproduction here? best to put it all here for the record.
If you have a small fix and can describe it then someone else can commit it, though I think making a PR is a useful skill and not that hard. Worth taking a shot at it.;;;","19/Aug/16 00:32;Bonsanto;Crude data.

| id|chicken|jam|roast beef|
|  1|    NaN|2.0|       2.0|
|  2|    2.0|0.0|       2.0|
|  3|    NaN|0.0|       2.0|
|  4|    2.0|1.0|      -2.0|
|  5|    2.0|2.0|       2.0|
|  6|    2.0|2.0|       NaN|

After assemble and normalization, as you can see {{Double.NaN}} are replaced for {{0.5}}.

|id |chicken|jam|roast beef|features      |featuresNorm |
|1  |NaN    |2.0|2.0       |[NaN,2.0,2.0] |[0.5,1.0,1.0]|
|2  |2.0    |0.0|2.0       |[2.0,0.0,2.0] |[0.5,0.0,1.0]|
|3  |NaN    |0.0|2.0       |[NaN,0.0,2.0] |[0.5,0.0,1.0]|
|4  |2.0    |1.0|-2.0      |[2.0,1.0,-2.0]|[0.5,0.5,0.0]|
|5  |2.0    |2.0|2.0       |[2.0,2.0,2.0] |[0.5,1.0,1.0]|
|6  |2.0    |2.0|NaN       |[2.0,2.0,NaN] |[0.5,1.0,NaN]|


;;;","19/Aug/16 08:53;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/14716;;;","19/Aug/16 09:01;yanboliang;In the existing code, {{MinMaxScaler}} handle NaN value indeterminately.
* If a column has identity value, that is max == min, {{MinMaxScalerModel}} transformation will output 0.5 for all rows even the original value is NaN.
* Otherwise, it will remain NaN after transformation.

I think we should unify the behavior by remaining NaN value at any condition, since we don't know how to transform a NaN value. In Python sklearn, it will throw exception when there is NaN in the dataset.;;;","19/Aug/16 11:01;Bonsanto;Just a question, I had to be the pull requester in order to appear as a contributor during the next release right? :(;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RelationalGroupedDataset.agg should be order preserving and allow duplicate column names,SPARK-17124,12998110,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petermaxlee,petermaxlee,petermaxlee,18/Aug/16 06:42,20/Aug/16 16:28,14/Jul/23 06:29,20/Aug/16 16:28,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"See discussion on the mailing list: http://apache-spark-developers-list.1001551.n3.nabble.com/Re-Aggregations-with-scala-pairs-td18666.html

{code}
 def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = {
    agg((aggExpr +: aggExprs).toMap)
  }
{code}

The above implementation is not order preserving, and does not allow duplicate names.
",,apachespark,cloud_fan,petermaxlee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 20 16:28:05 UTC 2016,,,,,,,,,,"0|i32gjb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/16 06:53;apachespark;User 'petermaxlee' has created a pull request for this issue:
https://github.com/apache/spark/pull/14697;;;","20/Aug/16 16:28;cloud_fan;Issue resolved by pull request 14697
[https://github.com/apache/spark/pull/14697];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performing set operations that combine string and date / timestamp columns may result in generated projection code which doesn't compile,SPARK-17123,12998101,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,joshrosen,joshrosen,18/Aug/16 05:43,12/Dec/22 17:35,14/Jul/23 06:29,22/Oct/16 18:10,2.0.0,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,,,,,,"The following example program causes SpecificSafeProjection code generation to produce Java code which doesn't compile:

{code}
import org.apache.spark.sql.types._
spark.sql(""set spark.sql.codegen.fallback=false"")
val dateDF = spark.createDataFrame(sc.parallelize(Seq(Row(new java.sql.Date(0)))), StructType(StructField(""value"", DateType) :: Nil))
val longDF = sc.parallelize(Seq(new java.sql.Date(0).toString)).toDF
dateDF.union(longDF).collect()
{code}

This fails at runtime with the following error:

{code}
failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 28, Column 107: No applicable constructor/method found for actual parameters ""org.apache.spark.unsafe.types.UTF8String""; candidates are: ""public static java.sql.Date org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(int)""
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private org.apache.spark.sql.types.StructType schema;
/* 011 */
/* 012 */
/* 013 */   public SpecificSafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     mutableRow = (MutableRow) references[references.length - 1];
/* 016 */
/* 017 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 018 */   }
/* 019 */
/* 020 */   public java.lang.Object apply(java.lang.Object _i) {
/* 021 */     InternalRow i = (InternalRow) _i;
/* 022 */
/* 023 */     values = new Object[1];
/* 024 */
/* 025 */     boolean isNull2 = i.isNullAt(0);
/* 026 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(0));
/* 027 */     boolean isNull1 = isNull2;
/* 028 */     final java.sql.Date value1 = isNull1 ? null : org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(value2);
/* 029 */     isNull1 = value1 == null;
/* 030 */     if (isNull1) {
/* 031 */       values[0] = null;
/* 032 */     } else {
/* 033 */       values[0] = value1;
/* 034 */     }
/* 035 */
/* 036 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 037 */     if (false) {
/* 038 */       mutableRow.setNullAt(0);
/* 039 */     } else {
/* 040 */
/* 041 */       mutableRow.update(0, value);
/* 042 */     }
/* 043 */
/* 044 */     return mutableRow;
/* 045 */   }
/* 046 */ }
{code}

Here, the invocation of {{DateTimeUtils.toJavaDate}} is incorrect because the generated code tries to call it with a UTF8String while the method expects an int instead.",,apachespark,dongjoon,joshrosen,rxin,wassimdr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 28 16:36:05 UTC 2017,,,,,,,,,,"0|i32ghb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/16 07:10;rxin;[~dongjoon] are you still working on this?
;;;","25/Aug/16 07:20;dongjoon;Not now. Unfortunately, my first attempt failed eventually. So I removed my comment not to make the other people confused.;;;","13/Sep/16 05:55;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/15072;;;","23/Oct/16 09:58;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/15601;;;","28/Dec/17 16:36;wassimdr;Hello, having same issue at runtime in java with spark 2.2.0, could you please suggest a solution 

{{org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 214, Column 114: No applicable constructor/method found for actual parameters ""org.apache.spark.unsafe.types.UTF8String""; candidates are: ""public static java.sql.Date org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(int)""}}

<dependency>
   <groupId>org.apache.spark</groupId>
   <artifactId>spark-core_2.11</artifactId>
   <version>2.2.0</version>
</dependency>
 <dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-sql_2.11</artifactId>
    <version>2.2.0</version>
</dependency>;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to drop database when use the database in Spark 2.0,SPARK-17122,12998096,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adrian-wang,jameszhouyi,jameszhouyi,18/Aug/16 04:13,03/Nov/16 07:19,14/Jul/23 06:29,03/Nov/16 07:18,2.0.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Found below case was broken in Spark 2.0, but it can run successfully in Spark 1.6. BTW, the case can also run successfully in Hive. Please see below reproduces for details.

Spark-SQL CLI:
/usr/lib/spark/bin/spark-sql
spark-sql> use test_db;
spark-sql> DROP DATABASE IF EXISTS test_db CASCADE;
16/08/10 15:13:35 INFO execution.SparkSqlParser: Parsing command: DROP DATABASE IF EXISTS test_db CASCADE
Error in query: Can not drop current database `test_db`;

Hive CLI:
/usr/bin/hive
hive> use test_db;
OK
hive> DROP DATABASE IF EXISTS test_db CASCADE;
OK
Time taken: 0.116 seconds",,apachespark,dongjoon,jameszhouyi,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 03 07:18:53 UTC 2016,,,,,,,,,,"0|i32gg7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/16 05:12;dongjoon;Hi, [~jameszhouyi]
It's intentionally implemented in SPARK-16459. Please refer the comment in PR https://github.com/apache/spark/pull/14115 .;;;","18/Aug/16 05:38;jameszhouyi;Thanks [~dongjoon] for your quick response. But the problem is this break original behavior and caused user confused after upgrade 1.x to 2.0.  BTW, how to keep compatibility with Hive ? This operation passed in Hive but failed in Spark 2.0.;;;","18/Aug/16 06:37;dongjoon;You can see SPARK-17053, too.
Spark does not follow Hive's way for some cases.

For this issue, you can keep this issue open more. Apache Spark committer will response sooner or later.;;;","18/Aug/16 06:59;jameszhouyi;Thanks [~dongjoon] for your clarity..;;;","08/Sep/16 07:05;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/15011;;;","03/Nov/16 07:18;smilegator;Issue resolved by pull request 15011
[https://github.com/apache/spark/pull/15011];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Analyzer incorrectly optimizes plan to empty LocalRelation,SPARK-17120,12998076,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,smilegator,joshrosen,joshrosen,18/Aug/16 00:47,31/Aug/16 21:14,14/Jul/23 06:29,25/Aug/16 12:24,2.1.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,correctness,,,,,"Consider the following query:

{code}
sc.parallelize(Seq(97)).toDF(""int_col_6"").createOrReplaceTempView(""table_3"")
sc.parallelize(Seq(0)).toDF(""int_col_1"").createOrReplaceTempView(""table_4"")

println(sql(""""""
  SELECT
  *
  FROM (
  SELECT
      COALESCE(t2.int_col_1, t1.int_col_6) AS int_col
      FROM table_3 t1
      LEFT JOIN table_4 t2 ON false
  ) t where (t.int_col) is not null
"""""").collect().toSeq)
{code}

In the innermost query, the LEFT JOIN's condition is {{false}} but nevertheless the number of rows produced should equal the number of rows in {{table_3}} (which is non-empty). Since no values are {{null}}, the outer {{where}} should retain all rows, so the overall result of this query should contain a single row with the value '97'.

Instead, the current Spark master (as of 12a89e55cbd630fa2986da984e066cd07d3bf1f7 at least) returns no rows. Looking at {{explain}}, it appears that the logical plan is optimizing to {{LocalRelation <empty>}}, so Spark doesn't even run the query. My suspicion is that there's a bug in constraint propagation or filter pushdown.

This issue doesn't seem to affect Spark 2.0, so I think it's a regression in master. ",,apachespark,hvanhovell,joshrosen,robert3005,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16991,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 24 21:19:15 UTC 2016,,,,,,,,,,"0|i32gbr:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"23/Aug/16 22:42;hvanhovell;TL;DR the {{EliminateOuterJoin}} rule converts the outer join into an Inner join:
{noformat}
16/08/24 00:55:46 TRACE SparkOptimizer: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.EliminateOuterJoin ===
 Project [coalesce(int_col_1#12, int_col_6#4) AS int_col#16]         Project [coalesce(int_col_1#12, int_col_6#4) AS int_col#16]
 +- Filter isnotnull(coalesce(int_col_1#12, int_col_6#4))            +- Filter isnotnull(coalesce(int_col_1#12, int_col_6#4))
!   +- Join LeftOuter, false                                            +- Join Inner, false
       :- Project [value#2 AS int_col_6#4]                                 :- Project [value#2 AS int_col_6#4]
       :  +- SerializeFromObject [input[0, int, true] AS value#2]          :  +- SerializeFromObject [input[0, int, true] AS value#2]
       :     +- ExternalRDD [obj#1]                                        :     +- ExternalRDD [obj#1]
       +- Project [value#10 AS int_col_1#12]                               +- Project [value#10 AS int_col_1#12]
          +- SerializeFromObject [input[0, int, true] AS value#10]            +- SerializeFromObject [input[0, int, true] AS value#10]
             +- ExternalRDD [obj#9]                                              +- ExternalRDD [obj#9]
{noformat}
I correctly assumes that a non-null literal cannot be well... non-null, and then converts the join. 

BTW: set {{spark.sql.crossJoin.enabled}} to {{true}} if you want to run this. Also use {{sc.setLogLevel(""TRACE"")}} to see what the optimizer is doing.

(updated this: my first attempt at diagnoses was way off).;;;","23/Aug/16 23:13;hvanhovell;PR https://github.com/apache/spark/pull/14661 fixes this;;;","24/Aug/16 21:19;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14661;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"'SELECT 1 / NULL` throws AnalysisException, while 'SELECT 1 * NULL` works",SPARK-17117,12998040,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petermaxlee,joshrosen,joshrosen,17/Aug/16 22:20,18/Aug/16 11:47,14/Jul/23 06:29,18/Aug/16 11:47,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"Running {{SELECT 1 / NULL}} fails with

{code}
org.apache.spark.sql.AnalysisException: cannot resolve '(1 / NULL)' due to data type mismatch: differing types in '(1 / NULL)' (int and null).
{code}

but {{SELECT 1 * NULL}} returns {{NULL}}, as expected.",,apachespark,joshrosen,wm624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 18 03:55:07 UTC 2016,,,,,,,,,,"0|i32g3r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/16 03:55;apachespark;User 'petermaxlee' has created a pull request for this issue:
https://github.com/apache/spark/pull/14695;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the performance of UnsafeProjection for wide table,SPARK-17115,12998010,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,davies,davies,17/Aug/16 20:11,22/Aug/16 08:17,14/Jul/23 06:29,22/Aug/16 08:17,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"We increase the threshold for splitting the generate code for expressions to 64K in 2.0 by accident (too optimistic), which could cause bad performance (the huge method may not be JITed), we should decrease that to 16K.",,apachespark,cloud_fan,davies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 22 08:17:13 UTC 2016,,,,,,,,,,"0|i32fx3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/16 22:07;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/14692;;;","22/Aug/16 08:17;cloud_fan;Issue resolved by pull request 14692
[https://github.com/apache/spark/pull/14692];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding a 'GROUP BY 1' where first column is literal results in wrong answer,SPARK-17114,12998009,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,joshrosen,joshrosen,17/Aug/16 20:10,08/Dec/16 22:54,14/Jul/23 06:29,15/Sep/16 18:26,1.6.2,2.0.0,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,correctness,,,,,"Consider the following example:

{code}
sc.parallelize(Seq(128, 256)).toDF(""int_col"").registerTempTable(""mytable"")

// The following query should return an empty result set because the `IN` filter condition is always false for this single-row table.
val withoutGroupBy = sqlContext.sql(""""""
  SELECT 'foo'
  FROM mytable
  WHERE int_col == 0
"""""")
assert(withoutGroupBy.collect().isEmpty, ""original query returned wrong answer"")

// After adding a 'GROUP BY 1' the query result should still be empty because we'd be grouping an empty table:
val withGroupBy = sqlContext.sql(""""""
  SELECT 'foo'
  FROM mytable
  WHERE int_col == 0
  GROUP BY 1
"""""")
assert(withGroupBy.collect().isEmpty, ""adding GROUP BY resulted in wrong answer"")
{code}

Here, this fails the second assertion by returning a single row. It appears that running {{group by 1}} where column 1 is a constant causes filter conditions to be ignored.

Both PostgreSQL and SQLite return empty result sets for the query containing the {{GROUP BY}}. ",,apachespark,cloud_fan,joshrosen,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 14 20:56:07 UTC 2016,,,,,,,,,,"0|i32fwv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/16 22:09;rxin;The problem is that RemoveLiteralFromGroupExpressions eliminates all the grouping expressions that are literals, and then the underlying physical operator assumes there is no grouping key. Basically we are not differentiating Some(Seq.empty) vs None.

There are a few ways we can solve this ... One is to just have a flag tracking whether this is an aggregate with grouping key (or by putting groupingExpressions in an Option).

cc [~cloud_fan]

;;;","18/Aug/16 02:10;cloud_fan;A simple way is to not remove grouping literals if it's the only one, i.e. skip this optimization for `GROUP BY 1`. It's sub-optimal, but should be easier than adding a flag.;;;","13/Sep/16 09:06;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/15076;;;","14/Sep/16 20:56;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/15101;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job failure due to Executor OOM in offheap mode,SPARK-17113,12998002,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sitalkedia@gmail.com,sitalkedia@gmail.com,sitalkedia@gmail.com,17/Aug/16 19:35,08/Dec/16 23:13,14/Jul/23 06:29,19/Aug/16 18:28,1.6.2,2.0.0,,,,,,,2.0.1,2.1.0,,,Spark Core,,,,,,,,0,,,,,,"We have been seeing many job failure due to executor OOM with following stack trace - 

{code}
java.lang.OutOfMemoryError: Unable to acquire 1220 bytes of memory, got 0
	at org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:120)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:341)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:362)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:93)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:170)
	at org.apache.spark.sql.execution.Sort$$anonfun$1.apply(Sort.scala:90)
	at org.apache.spark.sql.execution.Sort$$anonfun$1.apply(Sort.scala:64)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$21.apply(RDD.scala:736)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$21.apply(RDD.scala:736)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:307)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:271)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:307)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:271)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:307)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:271)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:307)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:271)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

Digging into the code, we found out that this is an issue with cooperative memory management for off heap memory allocation.  

In the code https://github.com/sitalkedia/spark/blob/master/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java#L463, when the UnsafeExternalSorter is checking if memory page is being used by upstream, the base object in case of off heap memory is always null so the UnsafeExternalSorter does not spill the memory pages.

",,andyd88,apachespark,sitalkedia@gmail.com,tejasp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 18 01:44:05 UTC 2016,,,,,,,,,,"0|i32fvb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/16 19:36;sitalkedia@gmail.com;cc - [~davies] ;;;","18/Aug/16 01:44;apachespark;User 'sitalkedia' has created a pull request for this issue:
https://github.com/apache/spark/pull/14693;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""select if(true, null, null)"" via JDBC triggers IllegalArgumentException in Thriftserver",SPARK-17112,12997991,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,joshrosen,joshrosen,17/Aug/16 18:28,07/Oct/16 01:27,14/Jul/23 06:29,04/Oct/16 04:28,2.0.0,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,1,,,,,,"Via the Thriftserver beeline client, run

{code}
select if(true, null, null);
{code}

The expected output is {{null}} (this is what's produced when not running through the thrift server), but this instead results in an error in the Thriftserver:

{code}
16/08/17 11:05:38 WARN ThriftCLIService: Error getting result set metadata:
java.lang.RuntimeException: java.lang.IllegalArgumentException: Unrecognized type name: null
       	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:83)
       	at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)
       	at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)
       	at java.security.AccessController.doPrivileged(Native Method)
       	at javax.security.auth.Subject.doAs(Subject.java:422)
       	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
       	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)
       	at com.sun.proxy.$Proxy21.getResultSetMetadata(Unknown Source)
       	at org.apache.hive.service.cli.CLIService.getResultSetMetadata(CLIService.java:436)
       	at org.apache.hive.service.cli.thrift.ThriftCLIService.GetResultSetMetadata(ThriftCLIService.java:607)
       	at org.apache.hive.service.cli.thrift.TCLIService$Processor$GetResultSetMetadata.getResult(TCLIService.java:1533)
       	at org.apache.hive.service.cli.thrift.TCLIService$Processor$GetResultSetMetadata.getResult(TCLIService.java:1518)
       	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
       	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
       	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)
       	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
       	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Unrecognized type name: null
       	at org.apache.hive.service.cli.Type.getType(Type.java:168)
       	at org.apache.hive.service.cli.TypeDescriptor.<init>(TypeDescriptor.java:53)
       	at org.apache.hive.service.cli.ColumnDescriptor.<init>(ColumnDescriptor.java:53)
       	at org.apache.hive.service.cli.TableSchema.<init>(TableSchema.java:52)
       	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.resultSchema$lzycompute(SparkExecuteStatementOperation.scala:66)
       	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.resultSchema(SparkExecuteStatementOperation.scala:58)
       	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getResultSetSchema(SparkExecuteStatementOperation.scala:147)
       	at org.apache.hive.service.cli.operation.OperationManager.getOperationResultSetSchema(OperationManager.java:209)
       	at org.apache.hive.service.cli.session.HiveSessionImpl.getResultSetMetadata(HiveSessionImpl.java:673)
       	at sun.reflect.GeneratedMethodAccessor128.invoke(Unknown Source)
       	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
       	at java.lang.reflect.Method.invoke(Method.java:497)
       	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)
       	... 18 more
{code}",,apachespark,dongjoon,joshrosen,maropu,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17818,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 02 07:18:30 UTC 2016,,,,,,,,,,"0|i32fsv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/16 07:16;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/15325;;;","02/Oct/16 07:18;dongjoon;Hi, [~joshrosen].
After some investigation, I found that all NullType columes do, e.g., `SELECT null`.
Could you review this PR when you have sometime?
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pyspark with locality ANY throw java.io.StreamCorruptedException,SPARK-17110,12997963,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,tomerk,tomerk,17/Aug/16 16:59,07/Sep/16 02:34,14/Jul/23 06:29,06/Sep/16 22:08,2.0.0,,,,,,,,2.0.1,2.1.0,,,PySpark,,,,,,,,0,,,,,,"In Pyspark 2.0.0, any task that accesses cached data non-locally throws a StreamCorruptedException like the stacktrace below:

{noformat}
WARN TaskSetManager: Lost task 7.0 in stage 2.0 (TID 26, 172.31.26.184): java.io.StreamCorruptedException: invalid stream header: 12010A80
        at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:807)
        at java.io.ObjectInputStream.<init>(ObjectInputStream.java:302)
        at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:63)
        at org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:63)
        at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:122)
        at org.apache.spark.serializer.SerializerManager.dataDeserializeStream(SerializerManager.scala:146)
        at org.apache.spark.storage.BlockManager$$anonfun$getRemoteValues$1.apply(BlockManager.scala:524)
        at org.apache.spark.storage.BlockManager$$anonfun$getRemoteValues$1.apply(BlockManager.scala:522)
        at scala.Option.map(Option.scala:146)
        at org.apache.spark.storage.BlockManager.getRemoteValues(BlockManager.scala:522)
        at org.apache.spark.storage.BlockManager.get(BlockManager.scala:609)
        at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:661)
        at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:281)
        at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
        at org.apache.spark.scheduler.Task.run(Task.scala:85)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

The simplest way I have found to reproduce this is by running the following code in the pyspark shell, on a cluster of 2 slaves set to use only one worker core each:

{code}
x = sc.parallelize([1, 1, 1, 1, 1, 1000, 1, 1, 1], numSlices=9).cache()
x.count()

import time
def waitMap(x):
    time.sleep(x)
    return x

x.map(waitMap).count()
{code}

Or by running the following via spark-submit:
{code}
from pyspark import SparkContext
sc = SparkContext()

x = sc.parallelize([1, 1, 1, 1, 1, 1000, 1, 1, 1], numSlices=9).cache()
x.count()

import time
def waitMap(x):
    time.sleep(x)
    return x

x.map(waitMap).count()
{code}","Cluster of 2 AWS r3.xlarge slaves launched via ec2 scripts, Spark 2.0.0, hadoop: yarn, pyspark shell",apachespark,gen,joshrosen,radostyle@gmail.com,tomerk,wm624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 07 02:34:21 UTC 2016,,,,,,,,,,"0|i32fmn:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"17/Aug/16 23:31;wm624;It seems that it is a network I/O timeout, because there is one entry sleeps too long and the connection is reset by remote. Is it a configuration problem? If you run it locally, it works fine.;;;","18/Aug/16 01:18;tomerk;It is with the default configuration settings, and this worked fine with spark 1.6.0. Still, it's not unthinkable to me that the default configurations have changed since 1.6. 
It should happen with any wait time greater than the spark.locality.wait setting (forcing the task to be launched on a different node than the one the data was cached on).

The initial workload we came across this in had each task taking ~20 seconds, and this error would consistently happen near the end of the stage, as soon as tasks began being executed non-locally. That workload also ran fine in spark 1.6.;;;","18/Aug/16 05:35;wm624;The network layer of Spark must have some change for dealing with long running tasks, as you described it should not be cluster network issue. I can try to debug it and find what's going on here. ;;;","22/Aug/16 16:36;radostyle@gmail.com;Is there a workaround for this issue?  I'm affected by it.;;;","24/Aug/16 18:14;tomerk;[~radostyle@gmail.com] Unfortunately we haven't been able to find any for our project other than staying with spark 1.6 for the time being.;;;","25/Aug/16 02:00;gen;[~radostyle@gmail.com], It seems spark scala doesn't have this bug in version 2.0.0;;;","27/Aug/16 05:57;wm624;I set up a two-node cluster, one master, one worker, 48 cores. 1G memory. pyspark run the above code works fine. No exception. It seems that this bug has been fix in latest master branch. Can you upgrade and try again?;;;","27/Aug/16 06:20;tomerk;Hi Miao, 

That setup wouldn't cause this bug to appear (However, I can try the most recent master tomorrow anyway just in case someone else has fixed it)

I should have explicitly specified 2 slaves, not 2 nodes (as I suppose that is too ambiguous. I've updated the description). It is also critical that each slave is set to use *only 1 worker core* (as I did specify above) for this example.

This is because this specific example & setup is designed to cause (non-deterministically, but with high probability) a situation where one of the pyspark workers reads data non-locally, which is what I have observed to cause this error consistently.

To provide a mental model of how this example & code snippet forces this situation:
1. The workers initially cache the data, forcing it to be stored in memory locally. Worker A contains the large number (1000), Worker B contains only small numbers (1).
2. The two workers each process their local numbers one at a time.
3. Once Worker A hits the large wait, Worker B continues on to process all of its local data
4. Because Worker A is taking so long to finish its task (1000 seconds, but it can be set much smaller), the spark.locality.wait setting leads Worker B to begin processing data that is stored on Worker A
5. Worker B attempts to read non-local data not stored on that node, leading the stream corrupted exception to occur. 

The case in which this does not happen is the one run every few times where Worker A processes the large number (1000) last, as then there will be no data remaining on Worker A to attempt to launch on Worker B.;;;","29/Aug/16 07:02;wm624;Can you post a sample configuration? It could be simpler to reproduce this issue.;;;","29/Aug/16 11:26;gen;Hi,

I tried the similar code in python, scala, java and with different StorageLevel. Only pyspark threw the error when spark wanted to launch non node-locality tasks. As In Python, stored objects will always be serialized with the Pickle library. I guess this bug is related to pickle serialization.;;;","29/Aug/16 18:27;tomerk;Hi Miao, all that is needed using the fully default configurations with 2 slaves is to just set the number of worker cores per slave to 1.

That is, putting the following in /spark/conf/spark-env.sh
{code}
SPARK_WORKER_CORES=1
{code}


More concretely (if you’re looking for exact steps), the way I start up the cluster & reproduce this example is as follows.

I use the Spark EC2 scripts from the PR here:
https://github.com/amplab/spark-ec2/pull/46

I launch the cluster on 2 r3.xlarge machines (but any machine should work, though you may need to change a later sed command):

{code}
./spark-ec2 -k ec2_ssh_key -i path_to_key_here -s 2 -t r3.xlarge launch temp-cluster --spot-price=1.00 --spark-version=2.0.0 --region=us-west-2 --hadoop-major-version=yarn
{code}

I update the number of worker cores and launch the pyspark shell:
{code}
sed -i'f' 's/SPARK_WORKER_CORES=4/SPARK_WORKER_CORES=1/g' /root/spark/conf/spark-env.sh
~/spark-ec2/copy-dir ~/spark/conf/
~/spark/sbin/stop-all.sh
~/spark/sbin/start-all.sh
~/spark/bin/pyspark
{code}

And then I run the example I included at the start:
{code}
x = sc.parallelize([1, 1, 1, 1, 1, 1000, 1, 1, 1], numSlices=9).cache()
x.count()

import time
def waitMap(x):
    time.sleep(x)
    return x

x.map(waitMap).count()
{code};;;","03/Sep/16 17:46;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14952;;;","06/Sep/16 22:08;joshrosen;Issue resolved by pull request 14952
[https://github.com/apache/spark/pull/14952];;;","07/Sep/16 02:34;tomerk;Thanks all who helped out with this!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BIGINT and INT comparison failure in spark sql,SPARK-17108,12997938,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WeiqingYang,kbeathanabhotla,kbeathanabhotla,17/Aug/16 15:36,11/Nov/16 01:08,14/Jul/23 06:29,07/Nov/16 20:35,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"I have a Hive table with the following definition:
{noformat}
create table testforerror (
    my_column MAP<BIGINT, ARRAY<String>>
);
{noformat}
The table has the following records
{noformat}
hive> select * from testforerror;
OK
{11001:[""0034111000a4WaAAA2""]}
{11001:[""0034111000orWiWAAU""]}
{11001:["""",""0034111000VgrHdAAJ""]}
{11001:[""0034110000cS4rDAAS""]}
{12001:[""0037110001a7ofsAAA""]}
Time taken: 0.067 seconds, Fetched: 5 row(s)
{noformat}
I have a query which filters records with key of the my_column. The query is as follows
{noformat}
select * from testforerror where my_column[11001] is not null;
{noformat}
This query is executing fine from hive/beeline shell and producing the following records:
{noformat}
hive> select * from testforerror where my_column[11001] is not null;
OK
{11001:[""0034111000a4WaAAA2""]}
{11001:[""0034111000orWiWAAU""]}
{11001:["""",""0034111000VgrHdAAJ""]}
{11001:[""0034110000cS4rDAAS""]}
Time taken: 2.224 seconds, Fetched: 4 row(s)
{noformat}
But however I get an error when trying to execute from spark sqlContext. The following is the error message:
{noformat}
scala> val errorquery = ""select * from testforerror where my_column[11001] is not null""
errorquery: String = select * from testforerror where my_column[11001] is not null

scala> sqlContext.sql(errorquery).show()
org.apache.spark.sql.AnalysisException: cannot resolve 'my_column[11001]' due to data type mismatch: argument 2 requires bigint type, however, '11001' is of int type.; line 1 pos 43
    at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:65)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:57)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:335)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:335)
    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:334)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:332)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:332)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:281)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at scala.collection.Iterator$class.foreach(Iterator.scala:727)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
    at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
{noformat}",,apachespark,h_o,hvanhovell,kbeathanabhotla,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 12 06:23:05 UTC 2016,,,,,,,,,,"0|i32fh3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/16 16:18;hvanhovell;As a work around you could add a cast to bigint or use a bigint literal by appending {{L}} to the key.

We could add implicit casting to the GetMapValue expression to make this work.;;;","12/Oct/16 06:23;apachespark;User 'weiqingy' has created a pull request for this issue:
https://github.com/apache/spark/pull/15448;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogicalRelation.newInstance should follow the semantics of MultiInstanceRelation,SPARK-17104,12997845,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,17/Aug/16 08:51,08/Oct/16 09:18,14/Jul/23 06:29,20/Aug/16 15:31,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"Currently LogicalRelation.newInstance() simply creates another LogicalRelation object with the same parameters. However, the newInstance() method inherited from MultiInstanceRelation should return a copy of unique expression ids. Current LogicalRelation.newInstance() causes failure when doing self-join.",,apachespark,cloud_fan,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 20 15:31:19 UTC 2016,,,,,,,,,,"0|i32ewf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/16 09:04;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/14682;;;","20/Aug/16 15:31;cloud_fan;Issue resolved by pull request 14682
[https://github.com/apache/spark/pull/14682];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark filter on a udf column after join gives java.lang.UnsupportedOperationException,SPARK-17100,12997803,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,tim_s,tim_s,17/Aug/16 03:51,25/Nov/16 21:27,14/Jul/23 06:29,19/Sep/16 20:24,2.0.0,,,,,,,,2.0.1,2.1.0,,,PySpark,,,,,,,,0,,,,,,"In pyspark, when filtering on a udf derived column after some join types,
the optimized logical plan results is a java.lang.UnsupportedOperationException.

I could not replicate this in scala code from the shell, just python. It is a pyspark regression from spark 1.6.2.

This can be replicated with: bin/spark-submit bug.py

{code:python:title=bug.py}
import pyspark.sql.functions as F
from pyspark.sql import Row, SparkSession

if __name__ == '__main__':
    spark = SparkSession.builder.appName(""test"").getOrCreate()
    left = spark.createDataFrame([Row(a=1)])
    right = spark.createDataFrame([Row(a=1)])
    df = left.join(right, on='a', how='left_outer')
    df = df.withColumn('b', F.udf(lambda x: 'x')(df.a))
    df = df.filter('b = ""x""')
    df.explain(extended=True)
{code}

The output is:
{code}
== Parsed Logical Plan ==
'Filter ('b = x)
+- Project [a#0L, <lambda>(a#0L) AS b#8]
   +- Project [a#0L]
      +- Join LeftOuter, (a#0L = a#3L)
         :- LogicalRDD [a#0L]
         +- LogicalRDD [a#3L]

== Analyzed Logical Plan ==
a: bigint, b: string
Filter (b#8 = x)
+- Project [a#0L, <lambda>(a#0L) AS b#8]
   +- Project [a#0L]
      +- Join LeftOuter, (a#0L = a#3L)
         :- LogicalRDD [a#0L]
         +- LogicalRDD [a#3L]

== Optimized Logical Plan ==
java.lang.UnsupportedOperationException: Cannot evaluate expression: <lambda>(input[0, bigint, true])
== Physical Plan ==
java.lang.UnsupportedOperationException: Cannot evaluate expression: <lambda>(input[0, bigint, true])
{code}


It fails when the join is:

* how='outer', on=column expression
* how='left_outer', on=string or column expression
* how='right_outer', on=string or column expression

It passes when the join is:

* how='inner', on=string or column expression
* how='outer', on=string

I made some tests to demonstrate each of these.

Run with bin/spark-submit test_bug.py",spark-2.0.0-bin-hadoop2.7. Python2 and Python3.,apachespark,davies,nchammas,rxin,tim_s,wm624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18589,,,,,,,,,,,,,,"17/Aug/16 03:52;tim_s;bug.py;https://issues.apache.org/jira/secure/attachment/12824066/bug.py","17/Aug/16 03:52;tim_s;test_bug.py;https://issues.apache.org/jira/secure/attachment/12824067/test_bug.py",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 19 20:24:09 UTC 2016,,,,,,,,,,"0|i32en3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/16 05:28;tim_s;I don't know why, but using `dataframe.cache()` before the filter is a workaround. ;;;","14/Sep/16 21:39;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/15103;;;","19/Sep/16 20:24;davies;Issue resolved by pull request 15103
[https://github.com/apache/spark/pull/15103];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect result when HAVING clause is added to group by query,SPARK-17099,12997783,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,smilegator,joshrosen,joshrosen,17/Aug/16 02:17,31/Aug/16 21:15,14/Jul/23 06:29,25/Aug/16 12:23,2.1.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,1,correctness,,,,,"Random query generation uncovered the following query which returns incorrect results when run on Spark SQL. This wasn't the original query uncovered by the generator, since I performed a bit of minimization to try to make it more understandable.

With the following tables:

{code}
val t1 = sc.parallelize(Seq(-234, 145, 367, 975, 298)).toDF(""int_col_5"")
val t2 = sc.parallelize(
  Seq(
    (-769, -244),
    (-800, -409),
    (940, 86),
    (-507, 304),
    (-367, 158))
).toDF(""int_col_2"", ""int_col_5"")

t1.registerTempTable(""t1"")
t2.registerTempTable(""t2"")
{code}

Run

{code}
SELECT
  (SUM(COALESCE(t1.int_col_5, t2.int_col_2))),
     ((COALESCE(t1.int_col_5, t2.int_col_2)) * 2)
FROM t1
RIGHT JOIN t2
  ON (t2.int_col_2) = (t1.int_col_5)
GROUP BY GREATEST(COALESCE(t2.int_col_5, 109), COALESCE(t1.int_col_5, -449)),
         COALESCE(t1.int_col_5, t2.int_col_2)
HAVING (SUM(COALESCE(t1.int_col_5, t2.int_col_2))) > ((COALESCE(t1.int_col_5, t2.int_col_2)) * 2)
{code}

In Spark SQL, this returns an empty result set, whereas Postgres returns four rows. However, if I omit the {{HAVING}} clause I see that the group's rows are being incorrectly filtered by the {{HAVING}} clause:

{code}
+--------------------------------------+---------------------------------------+--+
| sum(coalesce(int_col_5, int_col_2))  | (coalesce(int_col_5, int_col_2) * 2)  |
+--------------------------------------+---------------------------------------+--+
| -507                                 | -1014                                 |
| 940                                  | 1880                                  |
| -769                                 | -1538                                 |
| -367                                 | -734                                  |
| -800                                 | -1600                                 |
+--------------------------------------+---------------------------------------+--+
{code}

Based on this, the output after adding the {{HAVING}} should contain four rows, not zero.

I'm not sure how to further shrink this in a straightforward way, so I'm opening this bug to get help in triaging further.",,apachespark,ewanleith,hvanhovell,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16991,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 24 21:19:07 UTC 2016,,,,,,,,,,"0|i32ein:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"17/Aug/16 10:39;ewanleith;I've done a quick test in Spark 1.6.1 and this produces the expected 4 rows with the same output above, so I this appears to be a regression;;;","17/Aug/16 11:53;hvanhovell;TL;DR: This is caused by an interaction between the optimizer's {{InferFiltersFromConstraints}} and {{EliminateOuterJoin}} rules.

If you look at the (optimized) query plan:
{noformat}
== Analyzed Logical Plan ==
sum(coalesce(int_col_5, int_col_2)): bigint, (coalesce(int_col_5, int_col_2) * 2): int
Project [sum(coalesce(int_col_5, int_col_2))#34L, (coalesce(int_col_5, int_col_2) * 2)#32]
+- Filter (sum(cast(coalesce(int_col_5#4, int_col_2#13) as bigint))#37L > cast((coalesce(int_col_5#4, int_col_2#13)#38 * 2) as bigint))
   +- Aggregate [greatest(coalesce(int_col_5#14, 109), coalesce(int_col_5#4, -449)), coalesce(int_col_5#4, int_col_2#13)], [sum(cast(coalesce(int_col_5#4, int_col_2#13) as bigint)) AS sum(coalesce(int_col_5, int_col_2))#34L, (coalesce(int_col_5#4, int_col_2#13) * 2) AS (coalesce(int_col_5, int_col_2) * 2)#32, sum(cast(coalesce(int_col_5#4, int_col_2#13) as bigint)) AS sum(cast(coalesce(int_col_5#4, int_col_2#13) as bigint))#37L, coalesce(int_col_5#4, int_col_2#13) AS coalesce(int_col_5#4, int_col_2#13)#38]
      +- Join RightOuter, (int_col_2#13 = int_col_5#4)
         :- SubqueryAlias t1
         :  +- Project [value#2 AS int_col_5#4]
         :     +- SerializeFromObject [input[0, int, true] AS value#2]
         :        +- ExternalRDD [obj#1]
         +- SubqueryAlias t2
            +- Project [_1#10 AS int_col_2#13, _2#11 AS int_col_5#14]
               +- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true], top level non-flat input object)._1 AS _1#10, assertnotnull(input[0, scala.Tuple2, true], top level non-flat input object)._2 AS _2#11]
                  +- ExternalRDD [obj#9]

== Optimized Logical Plan ==
Project [sum(coalesce(int_col_5, int_col_2))#34L, (coalesce(int_col_5, int_col_2) * 2)#32]
+- Filter (isnotnull(sum(cast(coalesce(int_col_5#4, int_col_2#13) as bigint))#37L) && (sum(cast(coalesce(int_col_5#4, int_col_2#13) as bigint))#37L > cast((coalesce(int_col_5#4, int_col_2#13)#38 * 2) as bigint)))
   +- Aggregate [greatest(coalesce(int_col_5#14, 109), coalesce(int_col_5#4, -449)), coalesce(int_col_5#4, int_col_2#13)], [sum(cast(coalesce(int_col_5#4, int_col_2#13) as bigint)) AS sum(coalesce(int_col_5, int_col_2))#34L, (coalesce(int_col_5#4, int_col_2#13) * 2) AS (coalesce(int_col_5, int_col_2) * 2)#32, sum(cast(coalesce(int_col_5#4, int_col_2#13) as bigint)) AS sum(cast(coalesce(int_col_5#4, int_col_2#13) as bigint))#37L, coalesce(int_col_5#4, int_col_2#13) AS coalesce(int_col_5#4, int_col_2#13)#38]
      +- Join Inner, (isnotnull(coalesce(int_col_5#4, int_col_2#13)) && (int_col_2#13 = int_col_5#4))
         :- Project [value#2 AS int_col_5#4]
         :  +- Filter (isnotnull(value#2) && isnotnull(coalesce(value#2, value#2)))
         :     +- SerializeFromObject [input[0, int, true] AS value#2]
         :        +- ExternalRDD [obj#1]
         +- Project [_1#10 AS int_col_2#13, _2#11 AS int_col_5#14]
            +- Filter (isnotnull(coalesce(_1#10, _1#10)) && isnotnull(_1#10))
               +- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true], top level non-flat input object)._1 AS _1#10, assertnotnull(input[0, scala.Tuple2, true], top level non-flat input object)._2 AS _2#11]
                  +- ExternalRDD [obj#9]
{noformat}

{{InferFiltersFromConstraints}} infers {{is not null}} constraints which are pushed down into the plan. The {{EliminateOuterJoin}} rule then eliminates the Right Outer join because join condition can not be null anymore. Disabling the one of the rules gives the correct answer.

This is not trivial to fix. We should either disable constraint propagation for outer side(s) of outer joins, or we have to introduce a special constraint so we can make a difference between propagated constraints and an actual filter clause.;;;","23/Aug/16 23:23;hvanhovell;A small update. Disregard my previous diagnoses. This is caused by a bug in the {{EliminateOuterJoin}} rule. This converts the Right Outer join into an Inner join, see the following optimizer log:
{noformat}
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.EliminateOuterJoin ===
 Project [sum(coalesce(int_col_5, int_col_2))#34L, (coalesce(int_col_5, int_col_2) * 2)#32]                                                                                                                                                                                                                                                                                                                                                                                                                                        Project [sum(coalesce(int_col_5, int_col_2))#34L, (coalesce(int_col_5, int_col_2) * 2)#32]
 +- Filter (isnotnull(sum(cast(coalesce(int_col_5#4, int_col_2#13) as bigint))#37L) && (sum(cast(coalesce(int_col_5#4, int_col_2#13) as bigint))#37L > cast((coalesce(int_col_5#4, int_col_2#13)#38 * 2) as bigint)))                                                                                                                                                                                                                                                                                                              +- Filter (isnotnull(sum(cast(coalesce(int_col_5#4, int_col_2#13) as bigint))#37L) && (sum(cast(coalesce(int_col_5#4, int_col_2#13) as bigint))#37L > cast((coalesce(int_col_5#4, int_col_2#13)#38 * 2) as bigint)))
    +- Aggregate [greatest(coalesce(int_col_5#14, 109), coalesce(int_col_5#4, -449)), coalesce(int_col_5#4, int_col_2#13)], [sum(cast(coalesce(int_col_5#4, int_col_2#13) as bigint)) AS sum(coalesce(int_col_5, int_col_2))#34L, (coalesce(int_col_5#4, int_col_2#13) * 2) AS (coalesce(int_col_5, int_col_2) * 2)#32, sum(cast(coalesce(int_col_5#4, int_col_2#13) as bigint)) AS sum(cast(coalesce(int_col_5#4, int_col_2#13) as bigint))#37L, coalesce(int_col_5#4, int_col_2#13) AS coalesce(int_col_5#4, int_col_2#13)#38]      +- Aggregate [greatest(coalesce(int_col_5#14, 109), coalesce(int_col_5#4, -449)), coalesce(int_col_5#4, int_col_2#13)], [sum(cast(coalesce(int_col_5#4, int_col_2#13) as bigint)) AS sum(coalesce(int_col_5, int_col_2))#34L, (coalesce(int_col_5#4, int_col_2#13) * 2) AS (coalesce(int_col_5, int_col_2) * 2)#32, sum(cast(coalesce(int_col_5#4, int_col_2#13) as bigint)) AS sum(cast(coalesce(int_col_5#4, int_col_2#13) as bigint))#37L, coalesce(int_col_5#4, int_col_2#13) AS coalesce(int_col_5#4, int_col_2#13)#38]
       +- Filter isnotnull(coalesce(int_col_5#4, int_col_2#13))                                                                                                                                                                                                                                                                                                                                                                                                                                                                          +- Filter isnotnull(coalesce(int_col_5#4, int_col_2#13))
!         +- Join RightOuter, (int_col_2#13 = int_col_5#4)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  +- Join Inner, (int_col_2#13 = int_col_5#4)
             :- Project [value#2 AS int_col_5#4]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               :- Project [value#2 AS int_col_5#4]
             :  +- SerializeFromObject [input[0, int, true] AS value#2]                                                                                                                                                                                                                                                                                                                                                                                                                                                                        :  +- SerializeFromObject [input[0, int, true] AS value#2]
             :     +- ExternalRDD [obj#1]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      :     +- ExternalRDD [obj#1]
             +- Project [_1#10 AS int_col_2#13, _2#11 AS int_col_5#14]                                                                                                                                                                                                                                                                                                                                                                                                                                                                         +- Project [_1#10 AS int_col_2#13, _2#11 AS int_col_5#14]
                +- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true], top level non-flat input object)._1 AS _1#10, assertnotnull(input[0, scala.Tuple2, true], top level non-flat input object)._2 AS _2#11]                                                                                                                                                                                                                                                                                                                       +- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true], top level non-flat input object)._1 AS _1#10, assertnotnull(input[0, scala.Tuple2, true], top level non-flat input object)._2 AS _2#11]
                   +- ExternalRDD [obj#9]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            +- ExternalRDD [obj#9]
                
{noformat}

PR https://github.com/apache/spark/pull/14661 fixes this.;;;","24/Aug/16 21:19;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14661;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""SELECT COUNT(NULL) OVER ()"" throws UnsupportedOperationException during analysis",SPARK-17098,12997755,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,joshrosen,joshrosen,17/Aug/16 00:07,21/Aug/16 20:09,14/Jul/23 06:29,21/Aug/16 20:09,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,1,,,,,,"Running

{code}
SELECT COUNT(NULL) OVER ()
{code}

throws an UnsupportedOperationException during analysis:

{code}
java.lang.UnsupportedOperationException: Cannot evaluate expression: cast(0 as bigint) windowspecdefinition(ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)
	at org.apache.spark.sql.catalyst.expressions.Unevaluable$class.eval(Expression.scala:221)
	at org.apache.spark.sql.catalyst.expressions.WindowExpression.eval(windowExpressions.scala:288)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$18$$anonfun$applyOrElse$3.applyOrElse(Optimizer.scala:759)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$18$$anonfun$applyOrElse$3.applyOrElse(Optimizer.scala:752)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:279)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:279)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionDown$1(QueryPlan.scala:156)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:166)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:170)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:170)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:175)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:175)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$18.applyOrElse(Optimizer.scala:752)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$18.applyOrElse(Optimizer.scala:751)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:279)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:279)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:268)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.apply(Optimizer.scala:751)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.apply(Optimizer.scala:750)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
	at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)
	at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:83)
	at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2558)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:1924)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2139)
{code}

Given that 

{code}
SELECT COUNT(0) OVER ()
{code}

works fine my hunch is that this is uncovering a bug in the ordering of our optimizer rules or a bug in the constant-folding rule itself. This particular example is probably unimportant by itself but may be an indicator of other problems.",,apachespark,dongjoon,joshrosen,tsuresh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 17 17:39:05 UTC 2016,,,,,,,,,,"0|i32ecf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/16 00:11;joshrosen;Actually, given the error here I think that the problem could be that sometimes {{WindowExpression.foldable == true}} even though {{WindowExpression}} is {{Unevaluable}}:

{code}
case class WindowExpression(
    windowFunction: Expression,
    windowSpec: WindowSpecDefinition) extends Expression with Unevaluable {

  override def children: Seq[Expression] = windowFunction :: windowSpec :: Nil

  override def dataType: DataType = windowFunction.dataType
  override def foldable: Boolean = windowFunction.foldable
  override def nullable: Boolean = windowFunction.nullable

  override def toString: String = s""$windowFunction $windowSpec""
  override def sql: String = windowFunction.sql + "" OVER "" + windowSpec.sql
}
{code}

/cc [~hvanhovell], FYI. ;;;","17/Aug/16 17:00;dongjoon;Hi, [~joshrosen].
According to your error message `cast(0 as bigint)`, it seems a bug in `NullPropagation` optimizer.
I'll make a PR for this so that you can review that.;;;","17/Aug/16 17:21;dongjoon;Actually, `SELECT COUNT(1 + NULL) OVER ()` also should return 0, too.;;;","17/Aug/16 17:39;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/14689;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Roundtrip encoding of array<struct<>> fields is wrong when whole-stage codegen is disabled,SPARK-17093,12997715,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,proflin,joshrosen,joshrosen,16/Aug/16 21:25,31/Aug/16 21:16,14/Jul/23 06:29,25/Aug/16 12:20,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,correctness,,,,,"The following failing test demonstrates a bug where Spark mis-encodes array-of-struct fields if whole-stage codegen is disabled:

{code}
withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> ""false"") {
  val data = Array(Array((1, 2), (3, 4)))
  val ds = spark.sparkContext.parallelize(data).toDS()
  assert(ds.collect() === data)
}
{code}

When wholestage codegen is enabled (the default), this works fine. When it's disabled, as in the test above, Spark returns {{Array(Array((3,4), (3,4)))}}. Because the last element of the array appears to be repeated my best guess is that the interpreted evaluation codepath forgot to {{copy()}} somewhere.",,apachespark,joshrosen,kiszk,proflin,rxin,tsuresh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17061,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 25 10:19:09 UTC 2016,,,,,,,,,,"0|i32e3j:",9223372036854775807,,,,,,,,,,,,,2.0.1,,,,,,,,,,,"17/Aug/16 02:50;proflin;Oh the interpreted evaluation codepath indeed forgot to {{copy}} somewhere. I'll submit a patch shortly, thanks.;;;","18/Aug/16 07:32;apachespark;User 'lw-lin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14698;;;","25/Aug/16 10:19;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/14806;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IsolatedClientLoader fails to load Hive client when sharesHadoopClasses is false,SPARK-17088,12997646,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,16/Aug/16 17:18,22/Dec/18 19:36,14/Jul/23 06:29,23/Jan/18 20:52,,,,,,,,,2.4.0,,,,SQL,,,,,,,,0,,,,,,"There's a bug in a very rare code path in {{IsolatedClientLoader}}:

{code}
          case e: RuntimeException if e.getMessage.contains(""hadoop"") =>
            // If the error message contains hadoop, it is probably because the hadoop
            // version cannot be resolved (e.g. it is a vendor specific version like
            // 2.0.0-cdh4.1.1). If it is the case, we will try just
            // ""org.apache.hadoop:hadoop-client:2.4.0"". ""org.apache.hadoop:hadoop-client:2.4.0""
            // is used just because we used to hard code it as the hadoop artifact to download.
            logWarning(s""Failed to resolve Hadoop artifacts for the version ${hadoopVersion}. "" +
              s""We will change the hadoop version from ${hadoopVersion} to 2.4.0 and try again. "" +
              ""Hadoop classes will not be shared between Spark and Hive metastore client. "" +
              ""It is recommended to set jars used by Hive metastore client through "" +
              ""spark.sql.hive.metastore.jars in the production environment."")
            sharesHadoopClasses = false
{code}

That's the rare part. But when {{sharesHadoopClasses}} is set to false, the instantiation of {{HiveClientImpl}} fails:

{code}
      classLoader
        .loadClass(classOf[HiveClientImpl].getName)
        .getConstructors.head
        .newInstance(version, sparkConf, hadoopConf, config, classLoader, this)
        .asInstanceOf[HiveClient]
{code}

{{hadoopConf}} here is an instance of {{Configuration}} loaded by the main Spark class loader, but in this case {{HiveClientImpl}} expects an instance of {{Configuration}} loaded by the isolated class loader (yay class loaders are fun). So you get an error like this:

{noformat}
2016-08-10 13:51:20.742 - stderr> Exception in thread ""main"" java.lang.IllegalArgumentException: argument type mismatch
2016-08-10 13:51:20.743 - stderr> 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2016-08-10 13:51:20.743 - stderr> 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
2016-08-10 13:51:20.743 - stderr> 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2016-08-10 13:51:20.743 - stderr> 	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
2016-08-10 13:51:20.744 - stderr> 	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
2016-08-10 13:51:20.744 - stderr> 	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:354)
2016-08-10 13:51:20.744 - stderr> 	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:258)
2016-08-10 13:51:20.744 - stderr> 	at org.apache.spark.sql.hive.HiveSharedState.metadataHive$lzycompute(HiveSharedState.scala:39)
2016-08-10 13:51:20.745 - stderr> 	at org.apache.spark.sql.hive.HiveSharedState.metadataHive(HiveSharedState.scala:38)
{noformat}",,apachespark,vanzin,wm624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 24 05:08:03 UTC 2018,,,,,,,,,,"0|i32do7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/18 22:33;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20169;;;","23/Jan/18 20:52;vanzin;Issue resolved by pull request 20169
[https://github.com/apache/spark/pull/20169];;;","24/Jan/18 05:08;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20377;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QuantileDiscretizer throws InvalidArgumentException (parameter splits given invalid value) on valid data,SPARK-17086,12997635,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,VinceXie,barrybecker4,barrybecker4,16/Aug/16 16:25,06/Feb/19 17:41,14/Jul/23 06:29,24/Aug/16 09:17,2.0.0,,,,,,,,2.0.1,2.1.0,,,ML,,,,,,,,0,,,,,,"I discovered this bug when working with a build from the master branch (which I believe is 2.1.0). This used to work fine when running spark 1.6.2.

I have a dataframe with an ""intData"" column that has values like 
{code}
1 3 2 1 1 2 3 2 2 2 1 3
{code}
I have a stage in my pipeline that uses the QuantileDiscretizer to produce equal weight splits like this
{code}
new QuantileDiscretizer()
        .setInputCol(""intData"")
        .setOutputCol(""intData_bin"")
        .setNumBuckets(10)
        .fit(df)
{code}
But when that gets run it (incorrectly) throws this error:
{code}
parameter splits given invalid value [-Infinity, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, Infinity]
{code}
I don't think that there should be duplicate splits generated should there be?",,apachespark,barrybecker4,Matias Rotenberg,qhuang,VinceXie,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/16 17:57;barrybecker4;titanic.csv;https://issues.apache.org/jira/secure/attachment/12825303/titanic.csv",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 06 17:21:53 UTC 2019,,,,,,,,,,"0|i32dlr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/16 08:16;VinceXie;confirmed issue doesnt exist on Spark-1.6.2.
I will work on this issue.;;;","18/Aug/16 06:02;yanboliang;If the number of distinct input data is less than {{numBuckets}}, it should not split the data into buckets. We should figure out a proper way to identify this condition and throw corresponding exception.;;;","18/Aug/16 09:52;srowen;The issue is that approxQuantile may return quantiles where neighboring values are the same. This much is correct, really. It's perfectly possible for two different quantiles to have identical values. However this is used as input to the discretizer's split parameter, which wants strictly increasing quantiles. I think this requirement should just be relaxed. It's not invalid for two successive quantiles to be equal.

The bucket defined by [1.0, 1.0) will only receive the value 1.0, but that's fine. In a case like this, the bucket [1.0,2.0) will actually never get any values. But that's fine too. It may be an unuseful result but it's correct given the request to create 10 buckets.;;;","18/Aug/16 10:42;VinceXie;[~yanboliang] yes, actually that case was handled on spark-1.6.2
[~srowen] currently it will throw an illegal exception in this case:
java.lang.IllegalArgumentException: quantileDiscretizer_07696c9dca6c parameter splits given invalid value

so, what i'm doing now is to have a check before calling approxQuantile, that, if the distinct input data count is less than numBuckets, we will simply return an array with distinct elements as splits, for those cases where number of distinct input data is greater than numBuckets, we will just go to approxQuantile as the way it is now to generate a splits set. 

For example, with an input data shown in this case, we will output splits : [-Infinity, 1.0, 2.0, 3.0, Infinity]

what do you think? [~yanboliang] [~srowen];;;","18/Aug/16 10:47;srowen;Yes, I know it generates an error. I think it should accept quantiles like the one in the example above.
No I don't think it's valid to return distinct values as quantiles. In particular, you'd return fewer than the requested number of quantiles.
Imagine input 1 1 1 1 1 1 1 1 1 2 3. You're suggesting returning [-Infinity, 1.0, 2.0, 3.0, Infinity] in this case as well, but that would imply at least that the median is 2, when it's 1.;;;","18/Aug/16 10:48;yanboliang;[~sowen] 
The bucket defined by [1.0, 1.0) will only receive the value 1.0, I think this scenario is OK. But if we provide the splits as {{[-Infinity, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, Infinity]}}, it will output {{[-Infinity, 1.0), 1.0,  1.0, [1.0, 2.0), 2.0, 2.0, [2.0, 3.0), 3.0, [3.0, Infinity]}}. 
From the document, {{QuantileDiscretizer}} takes a column with continuous features and outputs a column with binned categorical features. So I think it does not make sense if we put the same continuous value into different categorical features. Thanks.;;;","18/Aug/16 10:54;srowen;You are right, some buckets will get no values. If the input has so few distinct (integer) values, 10 buckets is too many and there's no way 3 distinct values can ever fall into more than 3 distinct buckets. That much is actually fine IMHO.

But yes it's ambiguous because 2 can logically go into [2,2) or [2,3). If it were consistently mapped into one of them, like the first matching bucket, I think it would still be valid output. If there's no easy way to make this mapping consistently then I agree it should probably be an error to end up with splits like this.

(I'm also not sure why 8 splits are output in the example but 10 buckets were requested. It just defines 7 buckets.);;;","18/Aug/16 10:57;VinceXie;[~srowen] in the example you just took, yes, it will return [-Infinity, 1.0, 2.0, 3.0, Infinity]. that's also the result we saw on spark-1.6.2;;;","18/Aug/16 11:06;srowen;I suppose it depends on the desired semantics of QuantileDiscretizer. It sounds like already would return fewer buckets than requested. (That could or should be documented.) 

It makes it sound like it tries to make the buckets match quantiles of the input, even if it doesn't guarantee it. The bins you describe here would result in pretty lopsided binning, but, any consistent scheme would be the same.

OK I think I would agree with matching the 1.6.2 behavior then and documenting that the number of buckets may be smaller than requested, rather than return buckets some of which will always be empty. Let's just document / add a test for it.

I don't think the test should involve the number of distinct input elements (which could be expensive to compute); you just want to collapse adjacent splits that are equal right? That will cover more cases too.;;;","18/Aug/16 11:10;VinceXie;Agree! [~srowen];;;","18/Aug/16 14:32;barrybecker4;I think I agree with the discussion. Here is a summary of the conclusions just to check my understanding:
 - It's fine for appxQuantile to return duplicate splits. It should always return the requested number of quantiles corresponding to the length of the probabilities array pased to it.
 - QuantileBucketizer, on the other hand, may return fewer than the number of buckets requested. It should not give an error when the number of buckets requested is fewer than the number of distinct values. If the call to appxQuartile returns duplicate splits, just discard the duplicates when passing the splits to QBucketizer. This saves you from having to compute unique values first in order to check to see if that number is less that the requested number of bins. I think its fine that QBucketizer work this way. You want it to be robust and not give errors for edge cases like this. The objective is to return buckets that have as close to equal weight bins as possible with simple split values.

If the data was \[1,1,1,1,1,1,1,1,4,5,10\] and I asked for 10 bins, then I would expect the splits to be \[-Inf, 1, 4, 5, 10, Inf\] even though the mean is 1 and appxQuartile returned 1 repeated several time. If I asked for 2 bins, then I think the splits might be \[-Inf, 1, 4, Inf\]. If three bins are requested, would you get \[-Inf, 1, 4, 5, Inf] or [-Inf, 1, 4, 10, Inf\]? Maybe, in cases like this you should get \[-Inf, 1, 4, 5, 10, Inf\] even though only 3 bins were requested. In other words, if there are only a small number of unique integer values in the data, and the number of bins is slightly less than that number, maybe it should be increased to match it since that is likely to be more meaningful. For now, just removing duplicates is probably enough.;;;","20/Aug/16 12:09;srowen;Yeah sounds good -- feel free to make a PR.;;;","22/Aug/16 01:44;qhuang;I also agree on that it does not make sense if we put the same continuous value into different categorical features. [~yanboliang]
But the original exception is confusing and difficult to understand to users:
""quantileDiscretizer_d03027d0b77c parameter splits given invalid value""
Thus, we could add an exception for quantileDiscretizer like in R:

> x<-c(1,1,1,1,1,1,1,1,4,5,10)
> quantile(x)
  0%  25%  50%  75% 100% 
 1.0  1.0  1.0  2.5 10.0 
> a<-quantile(x)
> cut(x,a)
Error in cut.default(x, a) : 'breaks' are not unique


;;;","22/Aug/16 03:58;yanboliang;We should not throw exception in this case. If the number of distinct input data is less than {{numBuckets}}, we will simply return an array with distinct elements as splits. But we should not actually compute  the number of distinct input elements which is very expensive, we can collapse adjacent splits produced by {{approxQuantile}} that are equal.;;;","22/Aug/16 08:04;apachespark;User 'VinceShieh' has created a pull request for this issue:
https://github.com/apache/spark/pull/14747;;;","24/Aug/16 09:17;srowen;Issue resolved by pull request 14747
[https://github.com/apache/spark/pull/14747];;;","24/Aug/16 12:17;barrybecker4;Is it possible to get this fix into 2.0.1? Maybe it would be possible to work around it by manually stripping the duplicate splits in client code?;;;","24/Aug/16 12:47;srowen;OK, seems reasonable to me. Backported to 2.0.x;;;","24/Aug/16 16:38;barrybecker4;Thanks. 
BTW, I hope there are some test cases where the column to bin has NaN values (for nulls).
I seem to recall there being duplicate NaN split points being added for every occurrence of NaN in the data (or something like that).
I will open a separate issue on this if I can nail down the specifics and make a simple test case.
;;;","06/Feb/19 17:21;Matias Rotenberg;I seem to be running into this exact issue in 2.2.0 when using QuantileDiscretizer:

IllegalArgumentException: u'Bucketizer_467c983fd201c1561cf3 parameter splits given invalid value [-Infinity,-1.0,-1.0,-1.0,-0.999999999996,-0.999999994038,-0.999998539337,-0.993442759767,0.997389694724,0.999999999505,Infinity].

 

Could it be that the change was not ported to newer versions?

 

edit:

When I getSplits there are actually no duplicate values:\

splits =  
[-inf, -1.0, -0.999999999999999, -0.9999999999999638, -0.9999999999962722, -0.9999999940383268, -0.9999985393371443, -0.9934427597674274, 0.9973896947239832, 0.9999999995050777, inf]
 
Could it be a matter of precision?
 ;;;",,,,,,,,,,,,,,,,,,,,,,,
Improve the error message when encountering an incompatible DataSourceRegister,SPARK-17065,12997395,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,15/Aug/16 21:03,15/Aug/16 22:56,14/Jul/23 06:29,15/Aug/16 22:56,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"When encountering an incompatible DataSourceRegister, it's better to add instructions to remove or upgrade it.",,apachespark,yhuai,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 15 22:56:08 UTC 2016,,,,,,,,,,"0|i32c4f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/16 21:07;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/14651;;;","15/Aug/16 22:56;yhuai;Issue resolved by pull request 14651
[https://github.com/apache/spark/pull/14651];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect results returned following a join of two datasets and a map step where total number of columns >100,SPARK-17061,12997311,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,proflin,jamiehutton,jamiehutton,15/Aug/16 15:20,31/Aug/16 21:42,14/Jul/23 06:29,25/Aug/16 12:21,2.0.0,2.0.1,,,,,,,2.0.1,2.1.0,,,Spark Core,,,,,,,,0,correctness,,,,,"We have hit a consistent bug where we have a dataset with more than 100 columns. I am raising as a blocker because spark is returning the WRONG results rather than erroring, leading to data integrity issues

I have put together the following test case which will show the issue (it will run in spark-shell). In this example i am joining a dataset with lots of fields onto another dataset. 

The join works fine and if you show the dataset you will get the expected result. However if you run a map step over the dataset you end up with a strange error where the sequence that is in the right dataset now only contains the last value.

Whilst this test may seem a rather contrived example, what we are doing here is a very standard analtical pattern. My original code was designed to:
 - take a dataset of child records
 - groupByKey up to the parent: giving a Dataset of (ParentID, Seq[Children])
 - join the children onto the parent by parentID: giving ((Parent),(ParentID,Seq[Children])
 - map over the result to give a tuple of (Parent,Seq[Children])

Notes:
- The issue is resolved by having less fields - as soon as we go <= 100 the integrity issue goes away. Try removing one of the fields from BigCaseClass below
- The issue will arise based on the total number of fields in the resulting dataset. Below i have a small case class and a big case class, but two case classes of 50 variable would give the same issue
- the issue occurs where the case class being joined on (on the right) has a case class type. It doesnt occur if you have a Seq[String]
- If i go back to an RDD for the map step after the join i can workaround the issue, but i lose all the benefits of datasets


Scala code test case:

  case class Name(name: String)
  case class SmallCaseClass (joinkey: Integer, names: Seq[Name])    
  case class BigCaseClass  (field1: Integer,field2: Integer,field3: Integer,field4: Integer,field5: Integer,field6: Integer,field7: Integer,field8: Integer,field9: Integer,field10: Integer,field11: Integer,field12: Integer,field13: Integer,field14: Integer,field15: Integer,field16: Integer,field17: Integer,field18: Integer,field19: Integer,field20: Integer,field21: Integer,field22: Integer,field23: Integer,field24: Integer,field25: Integer,field26: Integer,field27: Integer,field28: Integer,field29: Integer,field30: Integer,field31: Integer,field32: Integer,field33: Integer,field34: Integer,field35: Integer,field36: Integer,field37: Integer,field38: Integer,field39: Integer,field40: Integer,field41: Integer,field42: Integer,field43: Integer,field44: Integer,field45: Integer,field46: Integer,field47: Integer,field48: Integer,field49: Integer,field50: Integer,field51: Integer,field52: Integer,field53: Integer,field54: Integer,field55: Integer,field56: Integer,field57: Integer,field58: Integer,field59: Integer,field60: Integer,field61: Integer,field62: Integer,field63: Integer,field64: Integer,field65: Integer,field66: Integer,field67: Integer,field68: Integer,field69: Integer,field70: Integer,field71: Integer,field72: Integer,field73: Integer,field74: Integer,field75: Integer,field76: Integer,field77: Integer,field78: Integer,field79: Integer,field80: Integer,field81: Integer,field82: Integer,field83: Integer,field84: Integer,field85: Integer,field86: Integer,field87: Integer,field88: Integer,field89: Integer,field90: Integer,field91: Integer,field92: Integer,field93: Integer,field94: Integer,field95: Integer,field96: Integer,field97: Integer,field98: Integer,field99: Integer)
      
    val bigCC=Seq(BigCaseClass(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99))
    
    val smallCC=Seq(SmallCaseClass(1,Seq(
        Name(""Jamie""), 
        Name(""Ian""),
        Name(""Dave""),
        Name(""Will"")
        )))
    
    
    val bigCCDS = spark.createDataset(spark.sparkContext.parallelize(bigCC))
    val smallCCDS = spark.createDataset(spark.sparkContext.parallelize(smallCC))
    
    val joined_test=bigCCDS.as(""A"").joinWith(smallCCDS.as(""B""),  $""A.field1""===$""B.joinkey"", ""LEFT"")
    
    /*This next step is fine - it shows all 4 names:
     * [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99]
     * [1,WrappedArray([Jamie], [Ian], [Dave], [Will])]
     * */
    joined_test.show(false)
    
    /*This one ends up repeating will - I did the most simple map step possible here
     * [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99]
     * [1,WrappedArray([Will], [Will], [Will], [Will])]
     * */
    joined_test.map(identity).show(false)
    
    /*This one works because we have less than 100 fields:
     * [Jamie], [Ian], [Dave], [Will]*/
    joined_test.map(_._2).show(false)
    
",,apachespark,jamiehutton,proflin,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16664,,,,SPARK-17093,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 29 02:52:17 UTC 2016,,,,,,,,,,"0|i32blr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/16 15:37;srowen;Search JIRA please, and don't set blocker;;;","15/Aug/16 15:51;jamiehutton;Apologies for setting blocker. I wont use that again.

Is the above definitely the same issue as the one you marked as a duplicate? That does seem to be slightly different as it relates to persist and 200 columns. Did you manage to run the above test case on a 2.0.1 codebase and did it work?;;;","15/Aug/16 15:56;srowen;It is likely to be -- see also SPARK-17043. At least, I'd try a version with this fix before reopening this one.;;;","15/Aug/16 17:06;jamiehutton;I have just downloaded 2.0.1 nightly build from here:
http://people.apache.org/~pwendell/spark-nightly/spark-branch-2.0-bin/spark-2.0.1-SNAPSHOT-2016_08_15_00_23-e02d0d0-bin/

Unfortunately the issue is still present. I am going to down-grade the issue to critical as you suggested and re-open if thats ok. Please can you guys take a look at it?

Thanks;;;","15/Aug/16 17:07;jamiehutton;Tested in 2.0.1 nightly snapshot and still not resolved so this appears not to be a dupe;;;","16/Aug/16 02:14;proflin;This can be reproduced against the master branch; let me look into this. Thanks.;;;","18/Aug/16 07:32;apachespark;User 'lw-lin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14698;;;","25/Aug/16 10:19;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/14806;;;","26/Aug/16 13:19;jamiehutton;Just wanted to say we have pulled the latest nightly with this fix and it solves our original issue. Thank you so much for getting this one fixed :-);;;","29/Aug/16 02:52;proflin;Oh cool! Thank you for the well-formed reproducer!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix a wrong assert in MemoryStore,SPARK-17056,12997228,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,viirya,viirya,15/Aug/16 05:17,27/Apr/21 00:08,14/Jul/23 06:29,27/Sep/16 23:05,,,,,,,,,2.0.1,2.1.0,,,Block Manager,Spark Core,,,,,,,0,,,,,,There is an assert in MemoryStore's putIteratorAsValues method which is used to check if unroll memory is not released too much. This assert looks wrong.,,apachespark,joshrosen,kiszk,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 27 23:05:08 UTC 2016,,,,,,,,,,"0|i32b3b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/16 05:19;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/14642;;;","27/Sep/16 23:05;joshrosen;Issue resolved by pull request 14642
[https://github.com/apache/spark/pull/14642];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
we should use hadoopConf in InsertIntoHiveTable,SPARK-17051,12997190,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,14/Aug/16 14:35,20/Sep/16 16:54,14/Jul/23 06:29,20/Sep/16 16:54,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,joshrosen,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 20 16:54:00 UTC 2016,,,,,,,,,,"0|i32auv:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"14/Aug/16 14:38;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/14634;;;","19/Sep/16 20:49;joshrosen;[~cloud_fan], what's the status of this issue? Should this still be targeted for 2.0.1?;;;","20/Sep/16 16:54;yhuai;Issue resolved by pull request 14634
[https://github.com/apache/spark/pull/14634];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 2 cannot create table when CLUSTERED.,SPARK-17047,12997134,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,mich,mich,13/Aug/16 13:31,05/Oct/17 17:50,14/Jul/23 06:29,05/Oct/17 17:50,2.0.0,2.1.1,2.2.0,,,,,,2.3.0,,,,SQL,,,,,,,,2,,,,,,"This does not work with CLUSTERED BY clause in Spark 2 now!




CREATE TABLE test.dummy2
 (
     ID INT
   , CLUSTERED INT
   , SCATTERED INT
   , RANDOMISED INT
   , RANDOM_STRING VARCHAR(50)
   , SMALL_VC VARCHAR(10)
   , PADDING  VARCHAR(10)
)
CLUSTERED BY (ID) INTO 256 BUCKETS
STORED AS ORC
TBLPROPERTIES ( ""orc.compress""=""SNAPPY"",
""orc.create.index""=""true"",
""orc.bloom.filter.columns""=""ID"",
""orc.bloom.filter.fpp""=""0.05"",
""orc.stripe.size""=""268435456"",
""orc.row.index.stride""=""10000"" )

scala> HiveContext.sql(sqltext)
org.apache.spark.sql.catalyst.parser.ParseException:
Operation not allowed: CREATE TABLE ... CLUSTERED BY(line 2, pos 0)",,damdr,dongjoon,mich,th76,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17729,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Thu Oct 05 17:50:00 UTC 2017,,,,,,,,,,"0|i32aif:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/17 17:50;dongjoon;This is resolved by SPARK-17729.

{code}
scala> sql(""""""
     | CREATE TABLE t2(
     | ID INT
     | , CLUSTERED INT
     | , SCATTERED INT
     | , RANDOMISED INT
     | , RANDOM_STRING VARCHAR(50)
     | , SMALL_VC VARCHAR(10)
     | , PADDING VARCHAR(10)
     | )
     | CLUSTERED BY (ID) INTO 256 BUCKETS
     | STORED AS ORC
     | TBLPROPERTIES ( ""orc.compress""=""SNAPPY"",
     | ""orc.create.index""=""true"",
     | ""orc.bloom.filter.columns""=""ID"",
     | ""orc.bloom.filter.fpp""=""0.05"",
     | ""orc.stripe.size""=""268435456"",
     | ""orc.row.index.stride""=""10000"" )
     | """""")

scala> spark.version
res3: String = 2.3.0-SNAPSHOT
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingSource reports metrics for lastCompletedBatch instead of lastReceivedBatch,SPARK-17038,12996972,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,iamshrek,ozzieba,ozzieba,12/Aug/16 15:26,17/Aug/16 23:32,14/Jul/23 06:29,17/Aug/16 23:32,1.6.2,2.0.0,,,,,,,1.6.3,2.0.1,2.1.0,,DStreams,,,,,,,,0,metrics,,,,,"StreamingSource's lastReceivedBatch_submissionTime, lastReceivedBatch_processingTimeStart, and lastReceivedBatch_processingTimeEnd all use data from lastCompletedBatch instead of lastReceivedBatch. In particular, this makes it impossible to match lastReceivedBatch_records with a batchID/submission time.
This is apparent when looking at StreamingSource.scala, lines 89-94.
I would guess that just replacing Completed->Received in those lines would fix the issue, but I haven't tested it.",,apachespark,iamshrek,ozzieba,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 17 07:17:06 UTC 2016,,,,,,,,,,"0|i329if:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Aug/16 22:00;zsxwing;Good catch. Could you submit a PR to fix it, please?;;;","15/Aug/16 22:23;iamshrek;hi [~ozzieba] if you don't have time, I can just submit a quick path on this :);;;","17/Aug/16 07:15;iamshrek;Hi [~ozzieba] I guess you too busy to respond or submit a PR, and I'm now just submitting a PR, really sorry not waiting for a longer time;;;","17/Aug/16 07:17;apachespark;User 'keypointt' has created a pull request for this issue:
https://github.com/apache/spark/pull/14681;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Conversion of datetime.max to microseconds produces incorrect value,SPARK-17035,12996920,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,ptkool,ptkool,12/Aug/16 11:15,08/Oct/16 09:17,14/Jul/23 06:29,16/Aug/16 17:01,2.0.0,,,,,,,,2.1.0,,,,PySpark,,,,,,,,1,,,,,,"Conversion of datetime.max to microseconds produces incorrect value. For example,

{noformat}
from datetime import datetime
from pyspark.sql import Row
from pyspark.sql.types import StructType, StructField, TimestampType

schema = StructType([StructField(""dt"", TimestampType(), False)])
data = [{""dt"": datetime.max}]

# convert python objects to sql data
sql_data = [schema.toInternal(row) for row in data]

# Value is wrong.
sql_data
[(2.534023188e+17,)]
{noformat}

This value should be [(253402318799999999,)].",,apachespark,davies,dongjoon,ptkool,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 16 17:01:58 UTC 2016,,,,,,,,,,"0|i3296v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Aug/16 21:31;dongjoon;Hi, [~ptkool].
You're right. It seems the microsecond part of `Timestamp` type is lost.
I'll make a PR for this issue soon.;;;","13/Aug/16 21:40;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/14631;;;","15/Aug/16 00:12;ptkool;I have a fix for this issue if you would like to assign the problem to me.

On Sat, Aug 13, 2016 at 5:31 PM, Dongjoon Hyun (JIRA) <jira@apache.org>




-- 
Michael Styles
Senior Data Platform Engineer Lead
Shopify
;;;","16/Aug/16 17:01;davies;Issue resolved by pull request 14631
[https://github.com/apache/spark/pull/14631];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ordinal in ORDER BY or GROUP BY should be treated as an unresolved expression,SPARK-17034,12996906,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,clockfly,clockfly,clockfly,12/Aug/16 10:12,08/Dec/16 22:50,14/Jul/23 06:29,16/Aug/16 07:52,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Ordinals in GROUP BY or ORDER BY like ""1"" in ""order by 1"" or ""group by 1"" should be considered as unresolved before analysis. But in current code, it uses ""Literal"" expression to store the ordinal. This is inappropriate as ""Literal"" itself is a resolved expression, it gives the user a wrong message that the ordinals has already been resolved.",,apachespark,clockfly,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16955,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 16 21:01:10 UTC 2016,,,,,,,,,,"0|i3293r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Aug/16 10:13;apachespark;User 'clockfly' has created a pull request for this issue:
https://github.com/apache/spark/pull/14616;;;","16/Aug/16 07:52;cloud_fan;Issue resolved by pull request 14616
[https://github.com/apache/spark/pull/14616];;;","16/Aug/16 21:01;apachespark;User 'petermaxlee' has created a pull request for this issue:
https://github.com/apache/spark/pull/14672;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dataset toJSON goes through RDD form instead of transforming dataset itself,SPARK-17029,12996798,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,robert3005,robert3005,robert3005,11/Aug/16 23:41,11/May/17 07:28,14/Jul/23 06:29,11/May/17 07:27,,,,,,,,,2.3.0,,,,,,,,,,,,0,,,,,,No longer necessary and can be optimized with datasets,,aash,apachespark,cloud_fan,robert3005,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 11 07:27:34 UTC 2017,,,,,,,,,,"0|i328fr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/16 23:50;apachespark;User 'robert3005' has created a pull request for this issue:
https://github.com/apache/spark/pull/14615;;;","11/Aug/16 23:53;aash;Note RDD form usage from https://issues.apache.org/jira/browse/SPARK-10705;;;","11/May/17 07:27;cloud_fan;Issue resolved by pull request 14615
[https://github.com/apache/spark/pull/14615];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PolynomialExpansion.choose is prone to integer overflow ,SPARK-17027,12996778,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zero323,zero323,zero323,11/Aug/16 22:26,14/Aug/16 11:18,14/Jul/23 06:29,14/Aug/16 11:01,1.6.0,2.0.0,,,,,,,2.0.1,2.1.0,,,ML,,,,,,,,0,,,,,,"Current implementation computes power of k directly and because of that it is susceptible to integer overflow on relatively small input (4 features, degree equal 10).  It would be better to use recursive formula instead.",,apachespark,Elie A.,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,https://stackoverflow.com/questions/38897510/spark-polynomial-expansion-vecor-size-exceeded,,,,,,,,,,9223372036854775807,,,Sun Aug 14 11:01:37 UTC 2016,,,,,,,,,,"0|i328bb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/16 22:35;srowen;Is the problem in the naive calculation of n choose k?

{code}
  private def choose(n: Int, k: Int): Int = {
    Range(n, n - k, -1).product / Range(k, 1, -1).product
  }
{code}

Let's just call http://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/util/CombinatoricsUtils.html#binomialCoefficient(int,%20int);;;","11/Aug/16 22:38;zero323;Yes, this is exactly the problem. 


{code}
choose(14, 10)
// res0: Int = -182
{code};;;","11/Aug/16 23:26;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/14614;;;","14/Aug/16 11:01;srowen;Issue resolved by pull request 14614
[https://github.com/apache/spark/pull/14614];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential deadlock in driver handling message,SPARK-17022,12996657,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,WangTao,WangTao,WangTao,11/Aug/16 15:53,17/May/20 18:13,14/Jul/23 06:29,11/Aug/16 22:10,1.5.0,1.5.1,1.5.2,1.6.0,1.6.1,2.0.0,,,2.0.1,2.1.0,,,Spark Core,YARN,,,,,,,0,,,,,,"Suggest t1 < t2 < t3 
At t1, someone called YarnSchedulerBackend.doRequestTotalExecutors from one of three functions: CoarseGrainedSchedulerBackend.killExecutors, CoarseGrainedSchedulerBackend.requestTotalExecutors or CoarseGrainedSchedulerBackend.requestExecutors, in all of which will hold the lock `CoarseGrainedSchedulerBackend`.
Then YarnSchedulerBackend.doRequestTotalExecutors will send a RequestExecutors message to `yarnSchedulerEndpoint` and wait for reply.

At t2, someone send a RemoveExecutor to `yarnSchedulerEndpoint` and the message is received by the endpoint.

At t3, the RequestExexutor message sent at t1 is received by the endpoint.

Then the endpoint would first handle RemoveExecutor then the RequestExecutor message.

When handling RemoveExecutor, it would send the same message to `driverEndpoint` and wait for reply.

In `driverEndpoint` it will request lock `CoarseGrainedSchedulerBackend` to handle that message, while the lock has been occupied in t1.

So it would cause a deadlock.

We have found the issue in our deployment, it would block the driver to make it handle no messages until the two message all went timeout.",,apachespark,jasonmoore2k,jinxing6042@126.com,umesh9794@gmail.com,WangTao,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 07 13:59:40 UTC 2017,,,,,,,,,,"0|i327kn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/16 16:57;apachespark;User 'WangTaoTheTonic' has created a pull request for this issue:
https://github.com/apache/spark/pull/14605;;;","12/Aug/16 07:39;jasonmoore2k;This one is maybe related to SPARK-16533 and/or SPARK-16702, right?  My team works in an environment where preemption (and killing of executors) is a common occurrence, so have been burnt a bit by this one.   We had been putting together a patch, but I'll see how this one holds up.;;;","12/Aug/16 07:54;WangTao;looks like they are related, especially with SPARK-16702.;;;","29/Jan/17 14:55;jinxing6042@126.com;[~WangTao]
Thanks a lot for the PR you made, which make lots of sense. But could I ask one question?

??Then the endpoint would first handle RemoveExecutor then the RequestExecutor message.??
 
Yes, but it doesn't mean that *RequestExecutor* cannot be started to handle until the completion of handling *RemoveExecutor*. I guess they can be processed concurrently. If that's correct, *RequestExecutor* can still be handled, after which lock can be released.

Any thoughts about this ?;;;","07/Feb/17 13:59;WangTao;

Well, `YarnSchedulerEndpoint` is a **ThreadSafeRpcEndpoint**, which can only handle one message at a time.


[quote]
Thread-safety means processing of one message happens before processing of the next message by
 the same [[ThreadSafeRpcEndpoint]].
[/quote]







;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
group-by/order-by ordinal should throw AnalysisException instead of UnresolvedException,SPARK-17016,12996536,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petermaxlee,petermaxlee,petermaxlee,11/Aug/16 06:13,11/Aug/16 17:51,14/Jul/23 06:29,11/Aug/16 08:52,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,,,apachespark,petermaxlee,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17015,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 11 08:52:32 UTC 2016,,,,,,,,,,"0|i326tr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/16 06:17;apachespark;User 'petermaxlee' has created a pull request for this issue:
https://github.com/apache/spark/pull/14594;;;","11/Aug/16 08:52;rxin;This was subsumed by https://issues.apache.org/jira/browse/SPARK-17015;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
negative numeric literal parsing,SPARK-17013,12996525,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petermaxlee,petermaxlee,petermaxlee,11/Aug/16 05:08,12/Aug/16 06:56,14/Jul/23 06:29,12/Aug/16 06:56,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"As found in https://github.com/apache/spark/pull/14592/files#r74367410, Spark 2.0 parses negative numeric literals as the unary minus of positive literals. This introduces problems for the edge cases such as -9223372036854775809 being parsed as decimal instead of bigint.
",,apachespark,petermaxlee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 11 21:06:08 UTC 2016,,,,,,,,,,"0|i326rb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/16 08:29;petermaxlee;I have a fix for this, but it would be easier to review after https://github.com/apache/spark/pull/14598 goes in.
;;;","11/Aug/16 08:39;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/14599;;;","11/Aug/16 21:06;apachespark;User 'petermaxlee' has created a pull request for this issue:
https://github.com/apache/spark/pull/14608;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[MINOR]Wrong description in memory management document,SPARK-17010,12996505,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,WangTao,WangTao,WangTao,11/Aug/16 02:43,11/Aug/16 08:52,14/Jul/23 06:29,11/Aug/16 05:30,2.0.0,,,,,,,,2.0.1,2.1.0,,,Documentation,,,,,,,,0,,,,,,"The value of spark.memory.fraction is changed to 0.6 from 0.75, but the remain percent in document is unchanged.

spark.memory.fraction expresses the size of M as a fraction of the (JVM heap space - 300MB) (default 0.6). The rest of the space `(25%)` is reserved for user data structures, internal metadata in Spark, and safeguarding against OOM errors in the case of sparse and unusually large records.",,apachespark,WangTao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 11 02:46:05 UTC 2016,,,,,,,,,,"0|i326mv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/16 02:46;apachespark;User 'WangTaoTheTonic' has created a pull request for this issue:
https://github.com/apache/spark/pull/14591;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
release-build.sh is missing hive-thriftserver for scala 2.11,SPARK-17003,12996445,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,10/Aug/16 22:56,12/Aug/16 17:29,14/Jul/23 06:29,12/Aug/16 17:29,1.6.2,,,,,,,,1.6.3,,,,Build,,,,,,,,0,,,,,,"The same issue as SPARK-16453. But for branch 1.6, we are missing the profile for scala 2.11 build.",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16453,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 12 17:29:36 UTC 2016,,,,,,,,,,"0|i3269j:",9223372036854775807,,,,,,,,,,,,,1.6.3,,,,,,,,,,,"10/Aug/16 22:57;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/14586;;;","12/Aug/16 17:29;yhuai;Issue resolved by pull request 14586
[https://github.com/apache/spark/pull/14586];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"select($""column1"", explode($""column2"")) is extremely slow",SPARK-16998,12996323,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,TPolzer,TPolzer,10/Aug/16 16:25,19/Nov/17 05:47,14/Jul/23 06:29,20/Nov/16 16:07,,,,,,,,,2.2.0,,,,SQL,,,,,,,,0,,,,,,"Using a Dataset containing 10.000 rows, each containing null and an array of 5.000 Ints, I observe the following performance (in local mode):
{noformat}
scala> time(ds.select(explode($""value"")).sample(false, 0.0000001, 1).collect)
1.219052 seconds                                                                
res9: Array[org.apache.spark.sql.Row] = Array([3761], [3766], [3196])

scala> time(ds.select($""dummy"", explode($""value"")).sample(false, 0.0000001, 1).collect)
20.219447 seconds                                                               
res5: Array[org.apache.spark.sql.Row] = Array([null,3761], [null,3766], [null,3196])
{noformat}",,ewanleith,hvanhovell,kiszk,maropu,Tagar,TPolzer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-15214,,,,SPARK-21657,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 19 05:47:49 UTC 2017,,,,,,,,,,"0|i325if:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/16 15:59;maropu;I checked performance;
{code}
$./bin/spark-shell --master=local[1]

import org.apache.spark.sql.types._
import org.apache.spark.sql.Row

def timer[R](block: => R): R = {
  val t0 = System.nanoTime()
  val result = block
  val t1 = System.nanoTime()
  println(""Elapsed time: "" + ((t1 - t0 + 0.0) / 1000000000.0)+ ""s"")
  result
}

val numArray = X
val sqlCtx = new org.apache.spark.sql.SQLContext(sc)
val schema = StructType(StructField(""c0"", IntegerType):: StructField(""c1"", ArrayType(IntegerType)) :: Nil)
val rdd = sc.parallelize(0 :: Nil, 1).flatMap { _ => (0 until 1000).map(j => Row(j, (0 until numArray).toArray)) }
val df = sqlCtx.createDataFrame(rdd, schema).cache
df.queryExecution.executedPlan(0).execute().foreach(x => Unit)
timer {
 df.select($""c0"", explode($""c1"")).queryExecution.executedPlan(2).execute().foreach(x => Unit)
}
{code}

Performance results are as follows;
{code}
numArray: Int = 1024, Elapsed time: 0.485094303s
numArray: Int = 2048, Elapsed time: 1.78344344s
numArray: Int = 4096, Elapsed time: 7.037558308s
numArray: Int = 8192, Elapsed time: 26.498065697s
numArray: Int = 16384, Elapsed time: 117.13229056s
{code}

The elapsed time exponentially grew with the increase of `numArray`.
It seems the root cause of this bottleneck is many object(JoinedRow)-copys occurred in `GenerateExec` with join=true.
However, I cannot find a simpler way to fix this.;;;","25/Aug/16 20:26;hvanhovell;I still have a code generation PR lying around: https://github.com/apache/spark/pull/13065 That should fix a lot of the performance issues.

I could bring it up to date, if there are any takers.;;;","26/Aug/16 03:56;maropu;If no problem, I'll pick up the pr.;;;","26/Aug/16 15:37;hvanhovell;[~maropu] Do you mind if I do it myself? I already started hacking.;;;","26/Aug/16 16:13;maropu;yea, no problem. thanks!;;;","26/Aug/16 16:15;maropu;can we link this ticket to SPARK-15214?;;;","20/Nov/16 16:04;maropu;[~hvanhovell] Since SPARK-15214 improves this query by ~11x, I think we can also close this ticket;
https://github.com/apache/spark/pull/13065/files#diff-b7bf86a20a79d572f81093300568db6eR152;;;","19/Nov/17 05:47;Tagar;Can somebody please help review PR 19683 in SPARK-21657? 
As there is still a lot of room for improvement possible in explode code generation for array of structs.. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TreeNodeException when flat mapping RelationalGroupedDataset created from DataFrame containing a column created with lit/expr,SPARK-16995,12996258,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,cperriard,cperriard,10/Aug/16 12:36,18/Aug/16 05:26,14/Jul/23 06:29,18/Aug/16 05:26,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"A TreeNodeException is thrown when executing the following minimal example in Spark 2.0. Crucial is that the column q is generated with lit/expr. 

{code}
import spark.implicits._
case class test (x: Int, q: Int)

val d = Seq(1).toDF(""x"")
d.withColumn(""q"", lit(0)).as[test].groupByKey(_.x).flatMapGroups{case (x, iter) => List()}.show
d.withColumn(""q"", expr(""0"")).as[test].groupByKey(_.x).flatMapGroups{case (x, iter) => List()}.show

// this works fine
d.withColumn(""q"", lit(0)).as[test].groupByKey(_.x).count()
{code}

The exception is: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: x#5

A possible workaround is to write the dataframe to disk before grouping and mapping.",,apachespark,cloud_fan,cperriard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 18 05:26:33 UTC 2016,,,,,,,,,,"0|i3253z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/16 13:00;srowen;You haven't shown the exception -- this is typically useful.;;;","10/Aug/16 13:29;cperriard;I added it to the description. Should I include the stack trace as well? ;;;","15/Aug/16 15:37;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/14648;;;","18/Aug/16 05:26;cloud_fan;Issue resolved by pull request 14648
[https://github.com/apache/spark/pull/14648];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filter and limit are illegally permuted.,SPARK-16994,12996224,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,TPolzer,TPolzer,10/Aug/16 09:51,31/Aug/16 21:36,14/Jul/23 06:29,19/Aug/16 13:12,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,1,correctness,,,,,"{noformat}
scala> spark.createDataset(1 to 100).limit(10).filter($""value"" % 10 === 0).explain
== Physical Plan ==
CollectLimit 10
+- *Filter ((value#875 % 10) = 0)
   +- LocalTableScan [value#875]

scala> spark.createDataset(1 to 100).limit(10).filter($""value"" % 10 === 0).collect
res23: Array[Int] = Array(10, 20, 30, 40, 50, 60, 70, 80, 90, 100)
{noformat}",,apachespark,cloud_fan,dongjoon,TPolzer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 19 13:12:46 UTC 2016,,,,,,,,,,"0|i324wf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/16 16:53;dongjoon;Hi, [~TPolzer].
Indeed, it is. `PushDownPredicate` seems to ignore `Limit`. I'll make a PR for this issue soon.;;;","10/Aug/16 17:17;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/14583;;;","19/Aug/16 07:02;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14713;;;","19/Aug/16 13:12;cloud_fan;Issue resolved by pull request 14713
[https://github.com/apache/spark/pull/14713];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Full outer join followed by inner join produces wrong results,SPARK-16991,12996182,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,smilegator,jjarutis,jjarutis,10/Aug/16 07:28,31/Aug/16 21:14,14/Jul/23 06:29,25/Aug/16 12:22,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,1,correctness,,,,,"I found strange behaviour using fullouter join in combination with inner join. It seems that inner join can't match values correctly after full outer join. Here is a reproducible example in spark 2.0.

{code}
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.0.0
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_45)
Type in expressions to have them evaluated.
Type :help for more information.

scala> val a = Seq((1,2),(2,3)).toDF(""a"",""b"")
a: org.apache.spark.sql.DataFrame = [a: int, b: int]

scala> val b = Seq((2,5),(3,4)).toDF(""a"",""c"")
b: org.apache.spark.sql.DataFrame = [a: int, c: int]

scala> val c = Seq((3,1)).toDF(""a"",""d"")
c: org.apache.spark.sql.DataFrame = [a: int, d: int]

scala> val ab = a.join(b, Seq(""a""), ""fullouter"")
ab: org.apache.spark.sql.DataFrame = [a: int, b: int ... 1 more field]

scala> ab.show
+---+----+----+
|  a|   b|   c|
+---+----+----+
|  1|   2|null|
|  3|null|   4|
|  2|   3|   5|
+---+----+----+

scala> ab.join(c, ""a"").show
+---+---+---+---+
|  a|  b|  c|  d|
+---+---+---+---+
+---+---+---+---+
{code}

Meanwhile, without the full outer, inner join works fine.

{code}
scala> b.join(c, ""a"").show
+---+---+---+
|  a|  c|  d|
+---+---+---+
|  3|  4|  1|
+---+---+---+
{code}",,apachespark,dongjoon,jjarutis,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17060,SPARK-17120,SPARK-17099,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 16 07:38:04 UTC 2016,,,,,,,,,,"0|i324nb:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"10/Aug/16 08:42;dongjoon;Hi, [~jjarutis].
Thank you for reproducible issue reporting. The root cause seems that `EliminateOuterJoin` optimizer remove the case wrongly.
I'll make a PR soon.;;;","10/Aug/16 09:45;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/14580;;;","16/Aug/16 07:38;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14661;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark history server log needs to be fixed to show https url when ssl is enabled,SPARK-16988,12996170,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,hayashidac,yeshavora,yeshavora,10/Aug/16 06:33,07/Dec/16 06:51,14/Jul/23 06:29,25/Oct/16 22:16,2.0.0,,,,,,,,2.0.2,2.1.0,,,Web UI,,,,,,,,0,,,,,,"When spark ssl is enabled, spark history server ui ( http://host:port) is redirected to https://host:port+400. 
So, spark history server log should be updated to print https url instead http url 

{code:title=spark HS log}
16/08/09 15:21:11 INFO ServerConnector: Started ServerConnector@3970a5ee{SSL-HTTP/1.1}{0.0.0.0:18481}
16/08/09 15:21:11 INFO Server: Started @4023ms
16/08/09 15:21:11 INFO Utils: Successfully started service on port 18081.
16/08/09 15:21:11 INFO HistoryServer: Bound HistoryServer to 0.0.0.0, and started at http://xxx:18081
16/08/09 15:22:52 INFO FsHistoryProvider: Replaying log path: hdfs://xxx:8020/yy/application_1470756121646_0001.inprogress{code}",,apachespark,chie8842,hayashidac,jerryshao,yeshavora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18762,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 24 16:40:05 UTC 2016,,,,,,,,,,"0|i324kn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/16 16:27;hayashidac;Can I work on this issue?;;;","24/Oct/16 16:40;apachespark;User 'hayashidac' has created a pull request for this issue:
https://github.com/apache/spark/pull/15611;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Started"" time, ""Completed"" time and ""Last Updated"" time in history server UI are not user local time",SPARK-16986,12996165,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yumwang,WeiqingYang,WeiqingYang,10/Aug/16 05:19,12/Dec/17 18:08,14/Jul/23 06:29,12/Dec/17 18:08,,,,,,,,,2.3.0,,,,Web UI,,,,,,,,0,,,,,,"Currently, ""Started"" time, ""Completed"" time and ""Last Updated"" time in history server UI are GMT. They should be the user local time.",,ajbozarth,apachespark,asukhenko,bolshakov.denis@gmail.com,kiszk,WeiqingYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18298,SPARK-19099,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 02 08:46:07 UTC 2017,,,,,,,,,,"0|i324jj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/16 05:57;apachespark;User 'Sherry302' has created a pull request for this issue:
https://github.com/apache/spark/pull/14577;;;","29/Nov/16 07:30;bolshakov.denis@gmail.com;[~srowen], could you please specify why the ticket was closed?
I understand why the PR was rejected, but in fact, the issue still exists (spark 2.0.2). 

Kind regards,
Denis;;;","29/Nov/16 09:38;srowen;Really you could call this a duplicate. There are several other issues floating around about, more generally, what time zone things should be displayed in where.;;;","29/Nov/16 09:54;bolshakov.denis@gmail.com;Could you please attach to this one? Just to see full picture.;;;","29/Nov/16 09:57;srowen;You can search JIRA too. Here's an example: https://issues.apache.org/jira/browse/SPARK-18298;;;","29/Nov/16 10:15;bolshakov.denis@gmail.com;Thx, I've set a `duplicated by` link.;;;","02/Nov/17 08:46;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/19640;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Output maybe overrided,SPARK-16985,12996150,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shenhong,shenhong,shenhong,10/Aug/16 03:15,12/Aug/16 08:58,14/Jul/23 06:29,12/Aug/16 08:58,2.0.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"In our cluster, sometimes the sql output maybe overrided. When I submit some sql, all insert into the same table, and the sql will cost less one minute, here is the detail,

1 sql1, 11:03 insert into table.
2 sql2, 11:04:11 insert into table.
3 sql3, 11:04:48 insert into table.
4 sql4, 11:05 insert into table.
5 sql5, 11:06 insert into table.
The sql3's output file will override the sql2's output file. here is the log:
{code}
16/05/04 11:04:11 INFO hive.SparkHiveHadoopWriter: XXfinalPath=hdfs://tl-sng-gdt-nn-tdw.tencent-distribute.com:54310/tmp/assorz/tdw-tdwadmin/20160504/04559505496526517_-1_1204544348/10000/_tmp.p_20160428/attempt_201605041104_0001_m_000000_1

16/05/04 11:04:48 INFO hive.SparkHiveHadoopWriter: XXfinalPath=hdfs://tl-sng-gdt-nn-tdw.tencent-distribute.com:54310/tmp/assorz/tdw-tdwadmin/20160504/04559505496526517_-1_212180468/10000/_tmp.p_20160428/attempt_201605041104_0001_m_000000_1
{code}",,apachespark,shenhong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 12 08:58:40 UTC 2016,,,,,,,,,,"0|i324g7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/16 03:29;shenhong;The reason is the output file use SimpleDateFormat(""yyyyMMddHHmm""), if two sql insert into the same table in the same minute, the output will be overrite. I think we should change dateFormat to ""yyyyMMddHHmmss"", in our cluster, we can't finished a sql in one second.;;;","10/Aug/16 04:01;apachespark;User 'shenh062326' has created a pull request for this issue:
https://github.com/apache/spark/pull/14574;;;","12/Aug/16 08:58;srowen;Resolved by https://github.com/apache/spark/pull/14574;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
executeTake tries all partitions if first parition is empty,SPARK-16984,12996138,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,robert3005,robert3005,robert3005,10/Aug/16 01:27,02/Sep/16 15:15,14/Jul/23 06:29,02/Sep/16 15:15,2.0.0,,,,,,,,2.1.0,,,,,,,,,,,,0,,,,,,in executeTake if the number of rows returned by first partition is 0 we try all partitions next time. This can lead to pathological cases where your first partition is empty and rest have data. This unfortunately can happen with skewed data. Empirically observed it's better to make few roundtrips instead of potentially killing driver with big collect,,apachespark,robert3005,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 10 01:32:05 UTC 2016,,,,,,,,,,"0|i324dj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/16 01:32;apachespark;User 'robert3005' has created a pull request for this issue:
https://github.com/apache/spark/pull/14573;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add `prettyName` to row_number, dense_rank, percent_rank, cume_dist",SPARK-16983,12996108,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,09/Aug/16 22:53,24/Aug/16 19:15,14/Jul/23 06:29,24/Aug/16 19:15,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Currently, two-word window functions like `row_number`, `dense_rank`, `percent_rank`, and `cume_dist` are expressed without `_` in error messages. We had better show the correct names.

**Before**
{code}
scala> sql(""select row_number()"").show
java.lang.UnsupportedOperationException: Cannot evaluate expression: rownumber()
{code}

**After**
{code}
scala> sql(""select row_number()"").show
java.lang.UnsupportedOperationException: Cannot evaluate expression: row_number()
{code}
",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 09 22:57:05 UTC 2016,,,,,,,,,,"0|i3246v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Aug/16 22:57;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/14571;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark-2.0.0 unable to infer schema for parquet data written by Spark-1.6.2,SPARK-16975,12995920,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,immerrr,immerrr,09/Aug/16 11:04,13/Aug/16 01:19,14/Jul/23 06:29,12/Aug/16 07:09,2.0.0,,,,,,,,2.0.1,2.1.0,,,Input/Output,,,,,,,,1,parquet,,,,,"Spark-2.0.0 seems to have some problems reading a parquet dataset generated by 1.6.2. 

{code}
In [80]: spark.read.parquet('/path/to/data')
...
AnalysisException: u'Unable to infer schema for ParquetFormat at /path/to/data. It must be specified manually;'
{code}

The dataset is ~150G and partitioned by _locality_code column. None of the partitions are empty. I have narrowed the failing dataset to the first 32 partitions of the data:

{code}
In [82]: spark.read.parquet(*subdirs[:32])
...
AnalysisException: u'Unable to infer schema for ParquetFormat at /path/to/data/_locality_code=AQ,/path/to/data/_locality_code=AI. It must be specified manually;'
{code}

Interestingly, it works OK if you remove any of the partitions from the list:
{code}
In [83]: for i in range(32): spark.read.parquet(*(subdirs[:i] + subdirs[i+1:32]))
{code}

Another strange thing is that the schemas for the first and the last 31 partitions of the subset are identical:
{code}
In [84]: spark.read.parquet(*subdirs[:31]).schema.fields == spark.read.parquet(*subdirs[1:32]).schema.fields
Out[84]: True
{code}

Which got me interested and I tried this:
{code}
In [87]: spark.read.parquet(*([subdirs[0]] * 32))
...
AnalysisException: u'Unable to infer schema for ParquetFormat at /path/to/data/_locality_code=AQ,/path/to/data/_locality_code=AQ. It must be specified manually;'

In [88]: spark.read.parquet(*([subdirs[15]] * 32))
...
AnalysisException: u'Unable to infer schema for ParquetFormat at /path/to/data/_locality_code=AX,/path/to/data/_locality_code=AX. It must be specified manually;'

In [89]: spark.read.parquet(*([subdirs[31]] * 32))
...
AnalysisException: u'Unable to infer schema for ParquetFormat at /path/to/data/_locality_code=BE,/path/to/data/_locality_code=BE. It must be specified manually;'
{code}

If I read the first partition, save it in 2.0 and try to read in the same manner, everything is fine:
{code}
In [100]: spark.read.parquet(subdirs[0]).write.parquet('spark-2.0-test')
16/08/09 11:03:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl

In [101]: df = spark.read.parquet(*(['spark-2.0-test'] * 32))
{code}

I have originally posted it to user mailing list, but with the last discoveries this clearly seems like a bug.",Ubuntu Linux 14.04,apachespark,dongjoon,immerrr,lian cheng,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 13 01:19:03 UTC 2016,,,,,,,,,,"0|i32313:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/16 19:18;rxin;cc [~dongjoon] do you have time to look into this?
;;;","10/Aug/16 19:20;dongjoon;Oh, sure. It's my pleasure. I'll take a look.;;;","10/Aug/16 19:21;dongjoon;Thank you for pinging me.;;;","10/Aug/16 19:56;dongjoon;Hi, [~immerrr].
I can not reproduce your situation, but could you change `_locality_code` into `locality_code`?
Spark 2.0 ignores the path names starting with underscore or dot; `_` or `.`;;;","10/Aug/16 20:11;dongjoon;I made a sample case having similar behaviors. I think this is related closed. [~rxin], how do you think about this?

{code}
spark-1.6.2-bin-hadoop2.6$ ls /tmp/parquet16/
_SUCCESS         _locality_code=1 _locality_code=3 _locality_code=5 _locality_code=7 _locality_code=9
_locality_code=0 _locality_code=2 _locality_code=4 _locality_code=6 _locality_code=8
{code}

{code}
scala> spark.read.parquet(""/tmp/parquet16"").show
org.apache.spark.sql.AnalysisException: Unable to infer schema for ParquetFormat at /tmp/parquet16. It must be specified manually;
scala> spark.read.parquet(""/tmp/parquet16/_locality_code=0"").show
+---+
| id|
+---+
|  0|
+---+
{code};;;","10/Aug/16 20:17;dongjoon;Ah, [~rxin]. 
For this issue, we should add a migration document for 1.6 .

Spark 2.0 itself has the save problem. We should block the illegal column names. May I make a PR for this?

{code}
scala> spark.range(10).withColumn(""_locality_code"", $""id"").write.partitionBy(""_locality_code"").save(""/tmp/parquet20"")

scala> spark.read.parquet(""/tmp/parquet20"")
org.apache.spark.sql.AnalysisException: Unable to infer schema for ParquetFormat at /tmp/parquet20. It must be specified manually;

scala> spark.range(10).withColumn(""locality_code"", $""id"").write.partitionBy(""locality_code"").save(""/tmp/parquet201"")

scala> spark.read.parquet(""/tmp/parquet201"")
res6: org.apache.spark.sql.DataFrame = [id: bigint, locality_code: int]
{code};;;","10/Aug/16 20:36;dongjoon;Let me dig more. I can find more general solution for this for Spark 1.6 / 2.0.;;;","10/Aug/16 20:38;immerrr;oh, that's unfortunate. coming from python world, underscore seems a natural prefix for ""internal things"".

what bugs me, though, is that spark2.0 had no problems reading up to 31 directories starting with underscores and bugged out only when there were 32 of them.

and i'll try the rename, give me a sec..;;;","10/Aug/16 20:41;dongjoon;In the python, could you give the command string to write parquet?

BTW, I also started to work in order to support `_col=xxx` format.;;;","10/Aug/16 20:57;immerrr;You mean the one I use to write the data? I don't have the exact string at hand, but it was a straightforward conversion from JSON inferring schema on the way, smth like
{code}
sqlContext.read.json('/path/to/json-data').write.partitionBy('_locality_code').parquet('/path/to/parquet-data', mode='overwrite')
{code}


Oh, another thing was that when I tried reading subdir-by-subdir.  When you read a single subdirectory, the _locality_code column is not present (just like in your example), but for some reason it worked ok when reading just one such subdirectory but failed reading that same directory multiple times.  There were other columns starting with underscore, though. I didn't use them to partition the data, but maybe they still somehow affected schema inference.;;;","10/Aug/16 20:58;dongjoon;Okay. Wait a second, please. I'll make PR for you.;;;","10/Aug/16 20:59;immerrr;I mean, this line from the original report
{code}
In [87]: spark.read.parquet(*([subdirs[0]] * 32))
{code}

means pass subdirs[0] 32 times as parameters to spark.read.parquet, i.e. spark.read.parquet(subdirs[0], subdirs[0], ...);;;","10/Aug/16 21:01;dongjoon;Yep. And, it raised exceptions, right?;;;","10/Aug/16 21:03;immerrr;Yes, a seemingly similar one.;;;","10/Aug/16 21:11;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/14585;;;","10/Aug/16 21:15;dongjoon;I made a PR, [~immerrr]. I tested only `sql` module tests. After passing Jenkins, could you test the PR in your environment if you have some time?
I think PySpark also will get benefit of this PR.;;;","10/Aug/16 21:18;immerrr;Great, thank you! That was so fast!

I'll try to look at it tomorrow, but cannot promise anything as my experience in Scala and building it is next to none.;;;","10/Aug/16 21:20;dongjoon;Thank you. See you tomorrow! :);;;","11/Aug/16 12:19;immerrr;I have built the code from the PR and it indeed succeeds reading the data.

I have tried doing {{df.count()}} and now I'm swarmed with warnings like this (they are just keep getting printed endlessly in the terminal): 
{code}
16/08/11 12:18:51 WARN CorruptStatistics: Ignoring statistics because created_by could not be parsed (see PARQUET-251): parquet-mr version 1.6.0
org.apache.parquet.VersionParser$VersionParseException: Could not parse created_by: parquet-mr version 1.6.0 using format: (.+) version ((.*) )?\(build ?(.*)\)
	at org.apache.parquet.VersionParser.parse(VersionParser.java:112)
	at org.apache.parquet.CorruptStatistics.shouldIgnoreStatistics(CorruptStatistics.java:60)
	at org.apache.parquet.format.converter.ParquetMetadataConverter.fromParquetStatistics(ParquetMetadataConverter.java:263)
	at org.apache.parquet.format.converter.ParquetMetadataConverter.fromParquetMetadata(ParquetMetadataConverter.java:567)
	at org.apache.parquet.format.converter.ParquetMetadataConverter.readParquetMetadata(ParquetMetadataConverter.java:544)
	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:431)
	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:386)
	at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:107)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:109)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1.apply(ParquetFileFormat.scala:369)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1.apply(ParquetFileFormat.scala:343)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:122)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:97)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}  ;;;","11/Aug/16 12:57;immerrr;But it works. I have suppressed WARN logs and {{df.count()}} returned the correct value, despite taking 2x time to finish compared to 1.6.2.;;;","11/Aug/16 13:03;immerrr;The figures were:
1.6.2: ~6s
2.0.0: ~12s

Interestingly enough, after restarting the driver, 2.0 run took far less than that:
1.6.2: ~5.7s
2.0.0: ~1.4s

Maybe, some sort of caching that survives restarts is used internally.;;;","11/Aug/16 13:33;dongjoon;Great! Thank you for confirming.;;;","12/Aug/16 04:51;dongjoon;Hi, [~rxin].
Could you review this PR?;;;","12/Aug/16 07:09;lian cheng;Issue resolved by pull request 14585
[https://github.com/apache/spark/pull/14585];;;","13/Aug/16 01:19;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/14627;;;",,,,,,,,,,,,,,,,,,
"App Name is a randomUUID even when ""spark.app.name"" exists",SPARK-16966,12995859,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,WeiqingYang,WeiqingYang,09/Aug/16 06:37,17/Aug/16 11:07,14/Jul/23 06:29,13/Aug/16 22:40,,,,,,,,,2.0.1,2.1.0,,,Spark Core,,,,,,,,0,,,,,,"When submitting an application with ""--name"":

./bin/spark-submit --name myApplicationTest --verbose --executor-cores 3 --num-executors 1 --master yarn --deploy-mode client --class org.apache.spark.examples.SparkKMeans examples/target/original-spark-examples_2.11-2.1.0-SNAPSHOT.jar hdfs://localhost:9000/lr_big.txt 2 5

In the history server UI:
App ID: application_1470694797714_0016
App Name: 70c06dc5-1b99-4b4a-a826-ea27497e977b

The App Name should not be a randomUUID ""70c06dc5-1b99-4b4a-a826-ea27497e977b""  since the ""spark.app.name"" was myApplicationTest.

The application ""org.apache.spark.examples.SparkKMeans"" above did not invoke "".appName()"". 
",,apachespark,jerryshao,WeiqingYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17105,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 13 23:20:50 UTC 2016,,,,,,,,,,"0|i322nb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Aug/16 06:53;apachespark;User 'Sherry302' has created a pull request for this issue:
https://github.com/apache/spark/pull/14556;;;","09/Aug/16 08:11;srowen;Hm, but SparkKMeans and other examples do call appName(). Even without setting {{--name}} I don't see why this ends up with a UUID for a name. (You aren't by chance using an old or modified example -- wasn't sure what ""original-spark-examples"" is?)

The problem here is that the builder options override SparkConf settings. And, if you don't specify a name in the builder options, it will still set a random one in the options, which then override anything set elsewhere like in Spark submit I think those three lines of code should be removed because SparkSubmit will have already ensured spark.app.name is set to something. If that isn't used, or the app doesn't set one directly, it's an error.

CC [~rxin] if possible for a comment because of https://github.com/apache/spark/commit/f2ee0ed4b7ecb2855cc4928a9613a07d45446f4e#diff-d91c284798f1c98bf03a31855e26d71cR768;;;","09/Aug/16 08:35;jerryshao;Yes, agreed. A better way is to handle this app name thing in {{SparkSubmitArguments}}, And there's already a bunch of codes there to figure out a proper app name. It is really not so intuitive to use a random UUID as app name. We can always guarantee app name is figured out in {{SparkSubmitArguments}} so that 3 lines can be removed.;;;","09/Aug/16 16:34;WeiqingYang;In the tests, I modified org.apache.spark.examples.SparkKMeans (code is from spark master branch), commented its appName() call. 

   val spark = SparkSession
      .builder
      // .appName(""SparkKMeans"")
      .getOrCreate();;;","09/Aug/16 16:48;srowen;OK, that explains that part then, thanks. The rest makes sense now.;;;","09/Aug/16 17:03;WeiqingYang;Thanks for the quick feedback.

If ""--name"" is not configured and appName() is not called,  ""spark.app.name"" will be ""mainClass""
-------------------------------------------
 // Set name from main class if not given
    name = Option(name).orElse(Option(mainClass)).orNull
------------------------------------------

If ""mainClass"" is always there, I think removing those three lines of code will be safe, but for pyspark and sparkr, the mainclass might be none, is it safe to remove those three lines of code? What do you think? [~srowen][~jerryshao];;;","09/Aug/16 17:15;srowen;It will be the name of the class executing, right? All that sounds correct. If the user isn't specifying a name, any reasonable name (and no error) seems like fine behavior. Right now it seems like {{--name}} is always clobbered by the value in SparkSession, which doesn't seem right.;;;","09/Aug/16 18:00;WeiqingYang;Yes, it will be the name of the class executing.;;;","10/Aug/16 00:06;jerryshao;Here is the code in {{SparkSubmitArguments}} to handle the name thing:

{code}
    // Set name from main class if not given
    name = Option(name).orElse(Option(mainClass)).orNull
    if (name == null && primaryResource != null) {
      name = Utils.stripDirectory(primaryResource)
    }
{code}

It could handle SparkPy and SparkR scenario by using the file name.

So from my understanding, if we can make sure here {{name}} is not null, it is safe to remove the codes in {{SparkSession}}. 
;;;","10/Aug/16 06:06;WeiqingYang;Thanks for the feedback. I will update my PR to remove those three lines of code.;;;","11/Aug/16 10:43;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14602;;;","13/Aug/16 15:35;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14630;;;","13/Aug/16 23:20;WeiqingYang;[~srowen] Thanks for the new PR and review.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix bound checking for SparseVector,SPARK-16965,12995844,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zjffdu,zjffdu,zjffdu,09/Aug/16 05:29,19/Aug/16 11:38,14/Jul/23 06:29,19/Aug/16 11:38,2.0.0,,,,,,,,2.1.0,,,,MLlib,PySpark,,,,,,,0,,,,,,"There's several issues in the bound checking of SparseVector

1. In scala, miss negative index checking and different bound checking is scattered in several places. Should put them in one place
2. In python, miss low/upper bound checking of indices. ",,apachespark,kiszk,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17130,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 19 11:38:29 UTC 2016,,,,,,,,,,"0|i322k7:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"09/Aug/16 05:33;apachespark;User 'zjffdu' has created a pull request for this issue:
https://github.com/apache/spark/pull/14555;;;","19/Aug/16 11:38;srowen;Issue resolved by pull request 14555
[https://github.com/apache/spark/pull/14555];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Utils.randomizeInPlace does not shuffle arrays uniformly,SPARK-16961,12995797,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nick.lavers@videoamp.com,nick.lavers@videoamp.com,nick.lavers@videoamp.com,09/Aug/16 00:10,20/Aug/16 08:05,14/Jul/23 06:29,19/Aug/16 09:13,2.0.0,,,,,,,,2.0.1,2.1.0,,,Spark Core,,,,,,,,0,,,,,,"The Utils.randomizeInPlace method, which is meant to uniformly shuffle the elements on an input array, never shuffles elements to their starting position. That is, every permutation of the input array is equally likely to be returned, except for any permutation in which any element is in the same position where it began. These permutations are never output.
This is because line 827 of Utils.scala should be
{{val j = rand.nextInt(i + 1)}}
instead of
{{val j = rand.nextInt( i )}}",,apachespark,michaelmalak,nick.lavers@videoamp.com,,,,,,,,,,,,,,,,,,,,0,0,,0%,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 20 08:05:06 UTC 2016,,,,,,,,,,"0|i3229r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Aug/16 01:14;apachespark;User 'nicklavers' has created a pull request for this issue:
https://github.com/apache/spark/pull/14551;;;","19/Aug/16 09:13;srowen;Issue resolved by pull request 14551
[https://github.com/apache/spark/pull/14551];;;","20/Aug/16 08:05;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/14730;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Comment in the CatalogTable returned from HiveMetastore is Always Empty,SPARK-16959,12995793,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,08/Aug/16 23:42,03/Sep/16 16:11,14/Jul/23 06:29,10/Aug/16 08:28,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"The `comment` in `CatalogTable` returned from Hive is always empty. We store it in the table property when creating a table. However, when we try to retrieve the table metadata from Hive metastore, we do not rebuild it. The `comment` is always empty.",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 03 06:41:11 UTC 2016,,,,,,,,,,"0|i3228v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/16 23:46;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14550;;;","10/Aug/16 08:28;cloud_fan;Issue resolved by pull request 14550
[https://github.com/apache/spark/pull/14550];;;","03/Sep/16 06:41;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14946;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using ordinals in ORDER BY causes an analysis error when the query has a GROUP BY clause using ordinals,SPARK-16955,12995731,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petermaxlee,yhuai,yhuai,08/Aug/16 18:50,08/Dec/16 22:49,14/Jul/23 06:29,12/Aug/16 13:24,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,1,,,,,,"The following queries work
{code}
select a from (select 1 as a) tmp order by 1
select a, count(*) from (select 1 as a) tmp group by 1
select a, count(*) from (select 1 as a) tmp group by 1 order by a
{code}

However, the following query does not
{code}
select a, count(*) from (select 1 as a) tmp group by 1 order by 1
{code}

{code}
org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to Group by position: '1' exceeds the size of the select list '0'. on unresolved object, tree:
Aggregate [1]
+- SubqueryAlias tmp
   +- Project [1 AS a#82]
      +- OneRowRelation$

	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveOrdinalInOrderByAndGroupBy$$anonfun$apply$11$$anonfun$34.apply(Analyzer.scala:749)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveOrdinalInOrderByAndGroupBy$$anonfun$apply$11$$anonfun$34.apply(Analyzer.scala:739)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveOrdinalInOrderByAndGroupBy$$anonfun$apply$11.applyOrElse(Analyzer.scala:739)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveOrdinalInOrderByAndGroupBy$$anonfun$apply$11.applyOrElse(Analyzer.scala:715)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:60)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveOrdinalInOrderByAndGroupBy$.apply(Analyzer.scala:715)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveOrdinalInOrderByAndGroupBy$.apply(Analyzer.scala:714)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:1237)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:1182)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:60)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:1182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:1181)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:682)
{code}",,apachespark,cloud_fan,dongjoon,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17034,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 12 13:23:35 UTC 2016,,,,,,,,,,"0|i321v3:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"08/Aug/16 18:53;yhuai;[~dongjoon] Will have time to take a look?;;;","08/Aug/16 18:54;dongjoon;Sure! Thank you, [~yhuai]. I'll take a look this.;;;","08/Aug/16 21:07;dongjoon;`ResolveAggregateFunctions` seems to have a bug to drop the ordinals. I'll make a PR after some testing.;;;","08/Aug/16 21:58;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/14546;;;","09/Aug/16 04:10;dongjoon;Hi, [~yhuai].
Could you review the PR?
The root cause was `ResolveAggregateFunctions` removed the ordinal sort orders too early.
After improving the `if` condition to check the resolution is completed, the case works well.;;;","12/Aug/16 03:06;apachespark;User 'clockfly' has created a pull request for this issue:
https://github.com/apache/spark/pull/14616;;;","12/Aug/16 13:23;cloud_fan;this bug is already fixed by https://github.com/apache/spark/pull/14595 by accident.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make requestTotalExecutors public to be consistent with requestExecutors/killExecutors,SPARK-16953,12995710,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,08/Aug/16 17:24,08/Aug/16 19:52,14/Jul/23 06:29,08/Aug/16 19:52,,,,,,,,,2.0.1,2.1.0,,,Spark Core,,,,,,,,0,,,,,,,,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 08 19:52:41 UTC 2016,,,,,,,,,,"0|i321qf:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"08/Aug/16 19:08;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/14541;;;","08/Aug/16 19:52;tdas;Issue resolved by pull request 14541
[https://github.com/apache/spark/pull/14541];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[MESOS] MesosCoarseGrainedSchedulerBackend requires spark.mesos.executor.home even if spark.executor.uri is set,SPARK-16952,12995699,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mgummelt,drcrallen,drcrallen,08/Aug/16 16:37,17/May/20 17:46,14/Jul/23 06:29,11/Aug/16 10:36,1.5.2,1.6.0,1.6.1,2.0.0,,,,,2.1.0,,,,Mesos,Scheduler,Spark Core,,,,,,0,,,,,,"In the Mesos coarse grained scheduler, setting `spark.executor.uri` bypasses the code path which requires `spark.mesos.executor.home` since the uri effectively provides the executor home.

But `org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend#createCommand` requires `spark.mesos.executor.home` to be set regardless.

Our workaround is to set `spark.mesos.executor.home=/dev/null` when using an executor uri.",,apachespark,drcrallen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 11 10:36:44 UTC 2016,,,,,,,,,,"0|i321nz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Aug/16 02:04;apachespark;User 'mgummelt' has created a pull request for this issue:
https://github.com/apache/spark/pull/14552;;;","11/Aug/16 10:36;srowen;Issue resolved by pull request 14552
[https://github.com/apache/spark/pull/14552];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fromOffsets parameter in Kafka's Direct Streams does not work in python3,SPARK-16950,12995645,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,szczeles,szczeles,szczeles,08/Aug/16 12:44,30/Jan/17 12:34,14/Jul/23 06:29,09/Aug/16 16:45,2.0.0,2.0.1,2.1.0,,,,,,2.0.1,2.1.0,,,PySpark,,,,,,,,0,,,,,,"KafkaUtils.createDirectStream does not work in python3 when you set parameter fromOffsets (which is starting offsets of the stream on Kafka). This is because the {{long}} type is removed from python3 and py4j maps numeric variables to {{java.lang.Integer}} or {{java.lang.Long}} depending on number size, which causes ClassCastException for small offsets variables.

This behaviour was noticed before and tests for this functionality are disabled in python3: https://github.com/apache/spark/blob/89e67d6667d5f8be9c6fb6c120fbcd350ae2950d/python/pyspark/streaming/tests.py#L1061",,apachespark,davies,russell.jurney,szczeles,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17411,SPARK-15559,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 06 05:38:38 UTC 2016,,,,,,,,,,"0|i321bz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/16 12:54;apachespark;User 'szczeles' has created a pull request for this issue:
https://github.com/apache/spark/pull/14540;;;","09/Aug/16 16:45;davies;Issue resolved by pull request 14540
[https://github.com/apache/spark/pull/14540];;;","06/Oct/16 01:41;russell.jurney;Probably doing something wrong, but I'm getting an exception when trying to create with offset 0 and I'm wondering if this patch worked? Probably me, but I don't know where else to complain.

{code:title=test.py}
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils, OffsetRange, TopicAndPartition

sc = SparkContext(""local[2]"", ""activity_summary"")
ssc = StreamingContext(sc, 1)

BROKERS = 'localhost:9092'
TOPIC = 'push'
PARTITION = 0

topicAndPartition = TopicAndPartition(TOPIC, PARTITION)
fromOffsets = {topicAndPartition: PARTITION}
directKafkaStream = KafkaUtils.createDirectStream(ssc, [TOPIC], {""metadata.broker.list"": BROKERS}, fromOffsets=fromOffsets)
{code}

{code:title=error.java}
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-6-3ed224d6a1eb> in <module>()
----> 1 directKafkaStream = KafkaUtils.createDirectStream(ssc, [TOPIC], {""metadata.broker.list"": BROKERS}, fromOffsets=fromOffsets)

/Users/rjurney/Software/Agile_Data_Code_2/spark/python/pyspark/streaming/kafka.py in createDirectStream(ssc, topics, kafkaParams, fromOffsets, keyDecoder, valueDecoder, messageHandler)
    128             func = funcWithoutMessageHandler
    129             jstream = helper.createDirectStreamWithoutMessageHandler(
--> 130                 ssc._jssc, kafkaParams, set(topics), jfromOffsets)
    131         else:
    132             ser = AutoBatchedSerializer(PickleSerializer())

/Users/rjurney/Software/Agile_Data_Code_2/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1131         answer = self.gateway_client.send_command(command)
   1132         return_value = get_return_value(
-> 1133             answer, self.gateway_client, self.target_id, self.name)
   1134
   1135         for temp_arg in temp_args:

/Users/rjurney/Software/Agile_Data_Code_2/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    317                 raise Py4JJavaError(
    318                     ""An error occurred while calling {0}{1}{2}.\n"".
--> 319                     format(target_id, ""."", name), value)
    320             else:
    321                 raise Py4JError(

Py4JJavaError: An error occurred while calling o65.createDirectStreamWithoutMessageHandler.
: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long
	at org.apache.spark.streaming.kafka.KafkaUtilsPythonHelper$$anonfun$17.apply(KafkaUtils.scala:717)
	at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
	at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.copyToBuffer(TraversableOnce.scala:275)
	at scala.collection.AbstractTraversable.copyToBuffer(Traversable.scala:104)
	at scala.collection.MapLike$class.toBuffer(MapLike.scala:326)
	at scala.collection.AbstractMap.toBuffer(Map.scala:59)
	at scala.collection.MapLike$class.toSeq(MapLike.scala:323)
	at scala.collection.AbstractMap.toSeq(Map.scala:59)
	at org.apache.spark.streaming.kafka.KafkaUtilsPythonHelper.createDirectStream(KafkaUtils.scala:717)
	at org.apache.spark.streaming.kafka.KafkaUtilsPythonHelper.createDirectStreamWithoutMessageHandler(KafkaUtils.scala:688)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
{code};;;","06/Oct/16 05:38;szczeles;Hello Russell,
your code works in my environment. Please ensure you have loaded the 2.0.1 version of {{spark-streaming-kafka-0-8 package}}.
Probably you need to use
{noformat}
./bin/pyspark --jars external/kafka-0-8-assembly/target/spark-streaming-kafka-0-8-assembly_2.11-2.0.1.jar
{noformat}
instead of {{\-\-packages}}, because the newest deployed kafka package is still 2.0.0: https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka-0-8_2.11

Hope that helps!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use metastore schema instead of inferring schema for ORC in HiveMetastoreCatalog,SPARK-16948,12995570,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ekhliang,rajesh.balamohan,rajesh.balamohan,08/Aug/16 08:21,07/Dec/16 04:52,14/Jul/23 06:29,07/Dec/16 04:52,,,,,,,,,2.1.0,,,,SQL,,,,,,,,1,,,,,,"Querying empty partitioned ORC tables from spark-sql throws exception with ""spark.sql.hive.convertMetastoreOrc=true"".

{noformat}
java.util.NoSuchElementException: None.get
        at scala.None$.get(Option.scala:347)
        at scala.None$.get(Option.scala:345)
        at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$12.apply(HiveMetastoreCatalog.scala:297)
        at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$12.apply(HiveMetastoreCatalog.scala:284)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.sql.hive.HiveMetastoreCatalog.org$apache$spark$sql$hive$HiveMetastoreCatalog$$convertToLogicalRelation(HiveMetastoreCatalog.scala:284)
        at org.apache.spark.sql.hive.HiveMetastoreCatalog$OrcConversions$.org$apache$spark$sql$hive$HiveMetastoreCatalog$OrcConversions$$convertToOrcRelation(HiveMetastoreCatalo)
        at org.apache.spark.sql.hive.HiveMetastoreCatalog$OrcConversions$$anonfun$apply$2.applyOrElse(HiveMetastoreCatalog.scala:423)
        at org.apache.spark.sql.hive.HiveMetastoreCatalog$OrcConversions$$anonfun$apply$2.applyOrElse(HiveMetastoreCatalog.scala:414)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:301)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:301)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:298)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:298)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:322)
{noformat}

",,apachespark,dongjoon,rajesh.balamohan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 08 08:31:06 UTC 2016,,,,,,,,,,"0|i320vb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/16 08:31;apachespark;User 'rajeshbalamohan' has created a pull request for this issue:
https://github.com/apache/spark/pull/14537;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CREATE TABLE LIKE generates a non-empty table when source is a data source table,SPARK-16943,12995536,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,08/Aug/16 04:24,06/Sep/16 02:47,14/Jul/23 06:29,01/Sep/16 08:40,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"When the source table is a data source table, the table generated by CREATE TABLE LIKE is non-empty. The expected table should be empty.",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 03 06:41:06 UTC 2016,,,,,,,,,,"0|i320nr:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"08/Aug/16 04:30;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14531;;;","03/Sep/16 06:41;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14946;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CREATE TABLE LIKE generates External table when source table is an External Hive Serde table,SPARK-16942,12995535,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,08/Aug/16 04:21,06/Sep/16 02:47,14/Jul/23 06:29,01/Sep/16 08:40,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"When the table type of source table is an EXTERNAL Hive serde table, {{CREATE TABLE LIKE}} will generate an EXTERNAL table. The expected table type should be MANAGED

The table type of the generated table is `EXTERNAL` when the source table is an external Hive Serde table. Currently, we explicitly set it to `MANAGED`, but Hive is checking the table property `EXTERNAL` to decide whether the table is `EXTERNAL` or not. (See https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java#L1407-L1408) Thus, the created table is still `EXTERNAL`. 
",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 03 06:41:09 UTC 2016,,,,,,,,,,"0|i320nj:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"08/Aug/16 04:33;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14531;;;","03/Sep/16 06:41;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14946;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSQLOperationManager should use synchronized Map to store SessionHandle,SPARK-16941,12995528,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,carlmartin,carlmartin,carlmartin,08/Aug/16 03:30,11/Aug/16 10:30,14/Jul/23 06:29,11/Aug/16 10:28,1.5.1,2.0.0,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"When I had run a high concurrency sql query with thriefserver, I found this error without any previous error:
*Error: java.util.NoSuchElementException: key not found: SessionHandle [a2ea264f-d29d-43c4-842f-4e0f2a3cf877] (state=,code=0)*

So I check the code in *SparkSQLOperationManager*, and found the *Map(sessionToContexts and sessionToActivePool)* is not thread safe in it.
I found this error in version 1.5.1 but I think the last master branch will still have this error.

",,apachespark,carlmartin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 11 10:28:38 UTC 2016,,,,,,,,,,"0|i320lz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/16 06:16;apachespark;User 'SaintBacchus' has created a pull request for this issue:
https://github.com/apache/spark/pull/14534;;;","11/Aug/16 10:28;srowen;Issue resolved by pull request 14534
[https://github.com/apache/spark/pull/14534];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`checkAnswer` should raise `TestFailedException` for wrong results,SPARK-16940,12995520,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,08/Aug/16 00:39,09/Aug/16 08:46,14/Jul/23 06:29,09/Aug/16 08:45,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"This issue fixes the following to make `checkAnswer` raise `TestFailedException` again instead of `java.util.NoSuchElementException: key not found: TZ` in the environments without `TZ` variable. Also, this issue adds `QueryTestSuite` class for testing `QueryTest` itself.

{code}
- |Timezone Env: ${sys.env(""TZ"")}
+ |Timezone Env: ${sys.env.getOrElse(""TZ"", """")}
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 09 08:45:57 UTC 2016,,,,,,,,,,"0|i320k7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/16 00:48;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/14528;;;","09/Aug/16 08:45;srowen;Issue resolved by pull request 14528
[https://github.com/apache/spark/pull/14528];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix build error by using `Tuple1` explicitly in StringFunctionSuite,SPARK-16939,12995488,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,07/Aug/16 15:50,07/Aug/16 19:54,14/Jul/23 06:29,07/Aug/16 19:54,,,,,,,,,1.6.3,,,,SQL,,,,,,,,0,,,,,,"This issue aims to fix a build error on branch 1.6, but we had better have this in master branch, too. There are other ongoing PR, too.

{code}
[error] /home/jenkins/workspace/spark-branch-1.6-compile-maven-with-yarn-2.3/sql/core/src/test/scala/org/apache/spark/sql/StringFunctionsSuite.scala:82: value toDF is not a member of Seq[String]
[error]     val df = Seq(""aaaac"").toDF(""s"")
[error]                           ^
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 07 19:54:55 UTC 2016,,,,,,,,,,"0|i320d3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/16 15:59;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/14526;;;","07/Aug/16 19:54;srowen;Resolved by https://github.com/apache/spark/pull/14526;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Case Sensitivity Support for Refresh Temp Table,SPARK-16936,12995461,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,07/Aug/16 05:45,08/Aug/16 14:36,14/Jul/23 06:29,08/Aug/16 14:36,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"Currently, the `refreshTable` API is always case sensitive.

When users use the view name without the exact case match, the API silently ignores the call. Users might expect the command has been successfully completed. However, when users run the subsequent SQL commands, they might still get the exception, like 
{noformat}
Job aborted due to stage failure: 
Task 1 in stage 4.0 failed 1 times, most recent failure: Lost task 1.0 in stage 4.0 (TID 7, localhost): 
java.io.FileNotFoundException: 
File file:/private/var/folders/4b/sgmfldk15js406vk7lw5llzw0000gn/T/spark-bd4b9ea6-9aec-49c5-8f05-01cff426211e/part-r-00000-0c84b915-c032-4f2e-abf5-1d48fdbddf38.snappy.parquet does not exist
{noformat}
",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 08 14:36:49 UTC 2016,,,,,,,,,,"0|i32073:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/16 05:46;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14523;;;","08/Aug/16 14:36;cloud_fan;Issue resolved by pull request 14523
[https://github.com/apache/spark/pull/14523];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ApplicationMaster's code that waits for SparkContext is race-prone,SPARK-16930,12995388,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,05/Aug/16 23:46,17/May/20 18:13,14/Jul/23 06:29,17/Aug/16 18:12,,,,,,,,,2.1.0,,,,Spark Core,YARN,,,,,,,0,,,,,,"While taking a look at SPARK-15937 and checking if there's something wrong with the code, I noticed two races that explain the behavior.

Because they're really narrow races, I'm a little wary of declaring them the cause of that bug. Also because the logs posted there don't really explain what went wrong (and don't really look like a SparkContext was run at all).

The races I found are:

- it's possible, but very unlikely, for an application to instantiate a SparkContext and stop it before the AM enters the loop where it checks for the instance.

- it's possible, but very unlikely, for an application to stop the SparkContext after the AM is already waiting for one, has been notified of its creation, but hasn't yet stored the SparkContext reference in a local variable.

I'll fix those and clean up the code a bit in the process.",,apachespark,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 08 17:35:07 UTC 2016,,,,,,,,,,"0|i31zqv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/16 17:35;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14542;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partition columns are present in columns metadata for partition but not table,SPARK-16926,12995344,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chobrian,chobrian,chobrian,05/Aug/16 21:04,08/Oct/16 09:16,14/Jul/23 06:29,01/Sep/16 21:13,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"A change introduced in SPARK-14388 removes partition columns from the column metadata of tables, but not for partitions. This causes TableReader to believe that the schema is different and create an unnecessary conversion object inspector, taking the else codepath in TableReader below:

{code}
    val soi = if (rawDeser.getObjectInspector.equals(tableDeser.getObjectInspector)) {
      rawDeser.getObjectInspector.asInstanceOf[StructObjectInspector]
    } else {
      ObjectInspectorConverters.getConvertedOI(
        rawDeser.getObjectInspector,
        tableDeser.getObjectInspector).asInstanceOf[StructObjectInspector]
    }
{code}

Printing the properties as debug output confirms the difference for the Hive table.

Table properties (tableDesc.getProperties):
{code}
16/08/04 20:36:58 DEBUG HadoopTableReader: columns.types, string:bigint:string:bigint:bigint:array<string>
{code}

Partition properties (partProps):
{code}
16/08/04 20:36:58 DEBUG HadoopTableReader: columns.types, string:bigint:string:bigint:bigint:array<string>:string:string:string
{code}

Where the final three string columns are partition columns",,apachespark,chobrian,davies,tejasp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 02 00:43:05 UTC 2016,,,,,,,,,,"0|i31zh3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/16 22:32;apachespark;User 'dafrista' has created a pull request for this issue:
https://github.com/apache/spark/pull/14515;;;","01/Sep/16 21:13;davies;Issue resolved by pull request 14515
[https://github.com/apache/spark/pull/14515];;;","02/Sep/16 00:43;apachespark;User 'dafrista' has created a pull request for this issue:
https://github.com/apache/spark/pull/14930;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark tasks which cause JVM to exit with a zero exit code may cause app to hang in Standalone mode,SPARK-16925,12995343,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,joshrosen,joshrosen,05/Aug/16 21:04,07/Aug/16 02:41,14/Jul/23 06:29,07/Aug/16 02:41,1.6.0,2.0.0,,,,,,,1.6.3,2.0.1,2.1.0,,Deploy,,,,,,,,0,,,,,,"If you have a Spark standalone cluster which runs a single application and you have a Spark task which repeatedly fails by causing the executor JVM to exit with a _zero_ exit code then this may temporarily freeze / hang the Spark application.

For example, running

{code}
        sc.parallelize(1 to 1, 1).foreachPartition { _ => System.exit(0) }
{code}

on a cluster will cause all executors to die but those executors won't be replaced unless another Spark application or worker joins or leaves the cluster. This is caused by a bug in the standalone Master where {{schedule()}} is only called on executor exit when the exit code is non-zero, whereas I think that we should always call {{schedule()}} even on a ""clean"" executor shutdown since {{schedule()}} should always be safe to call.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 07 02:41:32 UTC 2016,,,,,,,,,,"0|i31zgv:",9223372036854775807,,,,,,,,,,,,,1.6.3,2.0.1,,,,,,,,,,"05/Aug/16 21:16;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14510;;;","07/Aug/16 02:41;joshrosen;Fixed by my patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Query with Broadcast Hash join fails due to executor OOM in Spark 2.0,SPARK-16922,12995292,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,sitalkedia@gmail.com,sitalkedia@gmail.com,05/Aug/16 17:59,17/May/20 18:31,14/Jul/23 06:29,06/Sep/16 17:47,2.0.0,,,,,,,,2.0.1,2.1.0,,,Shuffle,Spark Core,,,,,,,0,,,,,,"A query which used to work in Spark 1.6 fails with executor OOM in 2.0.

Stack trace - 
{code}
	at org.apache.spark.unsafe.types.UTF8String.getBytes(UTF8String.java:229)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator$agg_VectorizedHashMap.hash$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator$agg_VectorizedHashMap.findOrInsert(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:161)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

{code}

Query plan in Spark 1.6

{code}
== Physical Plan ==
TungstenAggregate(key=[field1#101], functions=[(sum((field2#74 / 100.0)),mode=Final,isDistinct=false)], output=[field1#101,field3#3])
+- TungstenExchange hashpartitioning(field1#101,200), None
   +- TungstenAggregate(key=[field1#101], functions=[(sum((field2#74 / 100.0)),mode=Partial,isDistinct=false)], output=[field1#101,sum#111])
      +- Project [field1#101,field2#74]
         +- BroadcastHashJoin [field5#63L], [cast(cast(field4#97 as decimal(20,0)) as bigint)], BuildRight
            :- ConvertToUnsafe
            :  +- HiveTableScan [field2#74,field5#63L], MetastoreRelation foo, table1, Some(a), [(ds#57 >= 2013-10-01),(ds#57 <= 2013-12-31)]
            +- ConvertToUnsafe
               +- HiveTableScan [field1#101,field4#97], MetastoreRelation foo, table2, Some(b)
{code}


Query plan in 2.0
{code}
== Physical Plan ==
*HashAggregate(keys=[field1#160], functions=[sum((field2#133 / 100.0))])
+- Exchange hashpartitioning(field1#160, 200)
   +- *HashAggregate(keys=[field1#160], functions=[partial_sum((field2#133 / 100.0))])
      +- *Project [field2#133, field1#160]
         +- *BroadcastHashJoin [field5#122L], [cast(cast(field4#156 as decimal(20,0)) as bigint)], Inner, BuildRight
            :- *Filter isnotnull(field5#122L)
            :  +- HiveTableScan [field5#122L, field2#133], MetastoreRelation foo, table1, a, [isnotnull(ds#116), (ds#116 >= 2013-10-01), (ds#116 <= 2013-12-31)]
            +- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[0, string, false] as decimal(20,0)) as bigint)))
               +- *Filter isnotnull(field4#156)
                  +- HiveTableScan [field4#156, field1#160], MetastoreRelation foo, table2, b
{code}",,apachespark,davies,michaelmalak,rxin,sameerag,sitalkedia@gmail.com,tejasp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 05 17:53:46 UTC 2016,,,,,,,,,,"0|i31z5j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/16 18:00;sitalkedia@gmail.com;cc - [~rxin] ;;;","05/Aug/16 18:33;sitalkedia@gmail.com;PS - Rerunning the query with spark.sql.codegen.aggregate.map.columns.max=0 to disabled vectorized aggregation for columnar map also OOMs, but with a different stack trace. 

{code}
[Stage 1:>                                                        (0 + 4) / 184]16/08/05 11:26:31 WARN TaskSetManager: Lost task 2.0 in stage 1.0 (TID 4, hadoop4774.prn2.facebook.com): java.lang.OutOfMemoryError: Java heap space
	at org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:73)
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:214)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:161)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

{code};;;","06/Aug/16 19:31;sitalkedia@gmail.com;Update - The query works fine when Broadcast hash join in turned off, so the issue might be in broadcast hash join. I put some debug print in UnsafeRowWriter class (https://github.com/apache/spark/blob/branch-2.0/sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/codegen/UnsafeRowWriter.java#L214) and I found that it is receiving a row of size around 800MB and OOMing while trying to grow the buffer holder. It might suggest that there is some data corruption going on probably in the Broadcast hash join. 

cc- [~davies] - Any pointer on how to debug this issue further? ;;;","12/Aug/16 20:05;sitalkedia@gmail.com;I found that the regression was introduced in https://github.com/apache/spark/pull/12278, which introduced a new data structure (LongHashedRelation) for long types. I made a hack to use UnsafeHashedRelation instead of LongHashedRelation in https://github.com/apache/spark/blob/branch-2.0/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala#L105 and things started working fine.  This might be due to some data corruption happening in LongHashedRelation. 

cc - [~davies];;;","12/Aug/16 20:36;davies;[~sitalkedia@gmail.com] There are two integer overflow bugs fixed recently in LongHashedRelation, could you test with latest master? How is the range of your joining key?;;;","12/Aug/16 20:38;davies;I think it's fixed by https://github.com/apache/spark/pull/14464/files;;;","12/Aug/16 20:47;sitalkedia@gmail.com;I am using the fix in https://github.com/apache/spark/pull/14464/files, still the issue remains. The joining key lies in the range [43304915L to 10150946266075397L];;;","15/Aug/16 18:58;davies;Have you also have this one? https://github.com/apache/spark/pull/14373;;;","15/Aug/16 19:31;sitalkedia@gmail.com;Yes, I have the above mentioned PR as well. ;;;","18/Aug/16 21:58;davies;Is this failure determistic or not? Happened on every task or some or them? Could you also try to disable the dense mode?;;;","18/Aug/16 22:01;sitalkedia@gmail.com;The failure is deterministic, we are reproducing the issue for every run of the job (Its not only one job, there are multiple jobs that are failing because of this). For now, we have made a change to not use the  LongHashedRelation to workaround this issue. ;;;","18/Aug/16 22:02;sitalkedia@gmail.com;>> Could you also try to disable the dense mode?

I tried disabling the dense mode, that did not help either. ;;;","18/Aug/16 22:05;davies;Which serializer are you using? java serializer or Kyro?;;;","19/Aug/16 01:01;sitalkedia@gmail.com;Kryo ;;;","01/Sep/16 21:00;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/14927;;;","01/Sep/16 21:02;davies;[~sitalkedia@gmail.com] I think I found the cause and fix it, could you help to test it (also check the performance improvement comparing to BytesToBytesMap)?;;;","01/Sep/16 21:57;sitalkedia@gmail.com;Thanks for the fix [~davies]. I will test this change with our job to see if that fixes the issue. 

PS - I am away this week. Will get to it some time next week. ;;;","06/Sep/16 17:47;davies;Issue resolved by pull request 14927
[https://github.com/apache/spark/pull/14927];;;","06/Sep/16 19:44;sitalkedia@gmail.com;[~davies] -  Thanks for looking into this. I tested the failing job with the fix and it works fine now. ;;;","06/Sep/16 19:54;davies;Is there any performance difference comparing to BytesToBytesMap?;;;","07/Sep/16 00:36;sitalkedia@gmail.com;There is no noticable performance gain I observed comparing to BytesToBytesMap. Part of the reason might be due to the fact that BroadcastHashJoin was consuming very less percentage of the total job time. ;;;","05/Oct/16 17:53;davies;Thanks for the feedback, that's reasonable.;;;",,,,,,,,,,,,,,,,,,,,,
Parquet table reading performance regression when vectorized record reader is not used,SPARK-16907,12995072,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,clockfly,clockfly,clockfly,05/Aug/16 01:16,05/Aug/16 03:34,14/Jul/23 06:29,05/Aug/16 03:34,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"For this parquet reading benchmark, Spark 2.0 is 20%-30% slower than Spark 1.6.

{code}
// Test Env: Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz, Intel SSD SC2KW24
// Generates parquet table with nested columns
spark.range(100000000).select(struct($""id"").as(""nc"")).write.parquet(""/tmp/data4"")

def time[R](block: => R): Long = {
    val t0 = System.nanoTime()
    val result = block    // call-by-name
    val t1 = System.nanoTime()
    println(""Elapsed time: "" + (t1 - t0)/1000000 + ""ms"")
    (t1 - t0)/1000000
}

val x = ((0 until 20).toList.map(x => time(spark.read.parquet(""/tmp/data4"").filter($""nc.id"" < 100).collect()))).sum/20
{code}
",,apachespark,clockfly,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16320,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 05 01:17:04 UTC 2016,,,,,,,,,,"0|i31xsn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/16 01:17;apachespark;User 'clockfly' has created a pull request for this issue:
https://github.com/apache/spark/pull/14445;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adds more input type information for TypedAggregateExpression,SPARK-16906,12995055,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,clockfly,clockfly,clockfly,04/Aug/16 22:59,08/Dec/16 23:09,14/Jul/23 06:29,08/Aug/16 14:24,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"For TypedAggregateExpression 
{code}
case class TypedAggregateExpression(
    aggregator: Aggregator[Any, Any, Any],
    inputDeserializer: Option[Expression],
    bufferSerializer: Seq[NamedExpression],
    bufferDeserializer: Expression,
    outputSerializer: Seq[Expression],
    outputExternalType: DataType,
    dataType: DataType,
    nullable: Boolean) extends DeclarativeAggregate with NonSQLExpression
{code}

Aggregator of TypedAggregateExpression usually contains a closure like: 
{code}
class TypedSumDouble[IN](f: IN => Double) extends Aggregator[IN, Double, Double]
{code}

It will be great if we can add more info in TypedAggregateExpression to describe the closure input type {{IN}}, like class, and schema, so that we can use this info to make customized optimization like SPARK-14083


",,apachespark,clockfly,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 08 14:24:04 UTC 2016,,,,,,,,,,"0|i31xov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/16 23:06;apachespark;User 'clockfly' has created a pull request for this issue:
https://github.com/apache/spark/pull/14501;;;","08/Aug/16 14:24;cloud_fan;Issue resolved by pull request 14501
[https://github.com/apache/spark/pull/14501];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support SQL DDL: MSCK REPAIR TABLE,SPARK-16905,12995050,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,davies,davies,04/Aug/16 22:21,11/Aug/16 21:00,14/Jul/23 06:29,09/Aug/16 17:04,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"MSCK REPAIR TABLE could be used to recover the partitions in metastore based on partitions in file system.

Another syntax is:

ALTER TABLE table RECOVER PARTITIONS",,apachespark,davies,hvivani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 11 21:00:06 UTC 2016,,,,,,,,,,"0|i31xnr:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"05/Aug/16 18:09;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/14500;;;","09/Aug/16 17:04;davies;Issue resolved by pull request 14500
[https://github.com/apache/spark/pull/14500];;;","11/Aug/16 21:00;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/14607;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive settings in hive-site.xml may be overridden by Hive's default values,SPARK-16901,12995005,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,04/Aug/16 18:58,10/Aug/16 18:43,14/Jul/23 06:29,05/Aug/16 22:52,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"When we create the HiveConf for metastore client, we use a Hadoop Conf as the base, which may contain Hive settings in hive-site.xml (https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala#L49). However, HiveConf's initialize function basically ignores the base Hadoop Conf and always its default values (i.e. settings with non-null default values) as the base (https://github.com/apache/hive/blob/release-1.2.1/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java#L2687). So, even a user put {{javax.jdo.option.ConnectionURL}} in hive-site.xml, it is not used and Hive will use its default, which is {{jdbc:derby:;databaseName=metastore_db;create=true}}.",,apachespark,mwc,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 05 22:52:32 UTC 2016,,,,,,,,,,"0|i31xdr:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"04/Aug/16 20:23;yhuai;This issue only appear if the value of {{spark.sql.hive.metastore.jars}} is not builtin because if {{spark.sql.hive.metastore.jars}} is builtin, Hive can find the hive-site.xml and make it a overlay of the HiveConf.;;;","04/Aug/16 20:35;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/14497;;;","05/Aug/16 22:52;yhuai;Issue resolved by pull request 14497
[https://github.com/apache/spark/pull/14497];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Adds argument type information for typed logical plan like MapElements, TypedFilter, and AppendColumn",SPARK-16898,12994952,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,clockfly,clockfly,clockfly,04/Aug/16 16:18,08/Dec/16 23:09,14/Jul/23 06:29,09/Aug/16 00:37,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Typed logical plan like MapElements, TypedFilter, and AppendColumn contains a closure field: {{func: (T) => Boolean}}. For example class TypedFilter's signature is:

{code}
case class TypedFilter(
    func: AnyRef,
    deserializer: Expression,
    child: LogicalPlan) extends UnaryNode
{code} 

From the above class signature, we cannot easily find:
1. What is the input argument's type of the closure {{func}}? How do we know which apply method to pick if there are multiple overloaded apply methods?
2. What is the input argument's schema? 

With this info, it is easier for us to define some custom optimizer rule to translate these typed logical plan to more efficient implementation, like the closure optimization idea in SPARK-14083.",,apachespark,clockfly,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 09 00:37:42 UTC 2016,,,,,,,,,,"0|i31x1z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/16 16:24;apachespark;User 'clockfly' has created a pull request for this issue:
https://github.com/apache/spark/pull/14494;;;","09/Aug/16 00:37;cloud_fan;Issue resolved by pull request 14494
[https://github.com/apache/spark/pull/14494];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Loading csv with duplicate column names,SPARK-16896,12994868,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,anshbansal,anshbansal,04/Aug/16 11:42,12/Dec/22 18:11,14/Jul/23 06:29,11/Oct/16 02:22,2.0.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,1,,,,,,"It would be great if the library allows us to load csv with duplicate column names. I understand that having duplicate columns in the data is odd but sometimes we get data that has duplicate columns. Getting upstream data like that can happen. We may choose to ignore them but currently there is no way to drop those as we are not able to load them at all. Currently as a pre-processing I loaded the data into R, changed the column names and then make a fixed version with which Spark Java API can work.

But if talk about other options, e.g. R has read.csv which automatically takes care of such situation by appending a number to the column name.

Also case sensitivity in column names can also cause problems. I mean if we have columns like

ColumnName, columnName

I may want to have them as separate. But the option to do this is not documented.",,anshbansal,apachespark,falaki,maropu,nlauchande,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 22 04:50:07 UTC 2016,,,,,,,,,,"0|i31wjb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/16 11:43;anshbansal;[~hyukjin.kwon] cc;;;","05/Aug/16 00:57;gurwls223;I agree with appending a number to the deplicated column names. Could you please take a look of this JIRA? [~rxin] and [~falaki]
;;;","05/Aug/16 01:08;falaki;I suggest we generally follow the restrictions of SparkSQL, which does not accept duplicate column names. So I agree with numbering column names if there are duplicates.;;;","09/Aug/16 06:17;nlauchande;I wonder if anyone is tackling this issue. I want to get started contributing to Apache Spark and this sound like quite an interesting intro task. ;;;","09/Aug/16 06:42;gurwls223;I don't mind if you go ahead (I was looking at this problem though).

One thing I want to say is, we might better match the behaviour to [read.csv|https://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html] in R if possible in this case.

In addition, we are handling {{nullValue}} in handling the header with making numbers already. I guess we should clarify and write the behaviour in the PR description including the cases in R.

Also, do not forget to follow https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark for making a contribution.

;;;","09/Aug/16 08:43;nlauchande;Quick confirmation needed  : this change needs to happen in the https://github.com/databricks/spark-csv package if i understand correctly . 

Will start working on it later today, will come back with some more newbie questions.;;;","09/Aug/16 08:49;srowen;No, the code is in Spark now. ;;;","09/Aug/16 09:26;gurwls223;[~srowen] Oh, I am sorry, I coundn't see the comment above. Please ignore the comment I just left.  I thought you left the comment for mine.;;;","09/Aug/16 09:50;nlauchande;Cool. So i am assuming that the entry point for the loading csv functionality that needs to be fixed is the one described in : https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala .;;;","09/Aug/16 09:56;gurwls223;[~nlauchande] Just FYI, actual codes that need to be corrected will be around [here|https://github.com/apache/spark/blob/cb1b9d34f37a5574de43f61e7036c4b8b81defbf/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala#L61-L67].

Maybe we should check the duplication and then give some numbers. I haven't checked the behaviour in R though.

Also, please make sure for a test. I believe we usually need a test for a patch.
;;;","18/Aug/16 10:57;gurwls223;Hi [~nlauchande], are you currently working on this?
;;;","18/Aug/16 11:01;nlauchande;Hi Hyukjin Kwon  i did start. But couldn't make a lot of progress within the last couple of days . Feel free to grab it . I can try find another easier and less critical beginner task .;;;","22/Aug/16 01:44;gurwls223;Yup, then I will work on this and submit a PR within few days. Thank you!;;;","22/Aug/16 04:50;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/14745;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StructuredNetworkWordCount code comment incorrectly refers to DataFrame instead of Dataset,SPARK-16886,12994777,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,ganeshchand,ganeshchand,04/Aug/16 01:39,12/Dec/22 17:35,14/Jul/23 06:29,11/Aug/16 10:32,2.0.0,,,,,,,,2.1.0,,,,Examples,,,,,,,,0,,,,,,"Both StructuredNetworkWordCount.scala and StructuredNetworkWordCount.java has the following code comment:
{code}
// Create DataFrame representing the stream of input lines from connection to host:port
val lines = spark.readStream
      .format(""socket"")
       .option(""host"", host)
       .option(""port"", port)
       .load().as[String]
{code}

The above comment should be
{code}
// Create Dataset representing the stream of input lines from connection to host:port
val lines = spark.readStream
      .format(""socket"")
       .option(""host"", host)
       .option(""port"", port)
       .load().as[String]
{code}

because .as[String] returns a Dataset.

Affects:
* examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala 
* examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java 
* examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala
* examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java




",,apachespark,ganeshchand,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 11 10:32:02 UTC 2016,,,,,,,,,,"0|i31vz3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/16 02:38;ganeshchand;Submitted a pull request - https://github.com/apache/spark/pull/14491;;;","04/Aug/16 02:39;apachespark;User 'ganeshchand' has created a pull request for this issue:
https://github.com/apache/spark/pull/14491;;;","09/Aug/16 10:51;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/14564;;;","11/Aug/16 10:32;srowen;Issue resolved by pull request 14564
[https://github.com/apache/spark/pull/14564];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL decimal type is not properly cast to number when collecting SparkDataFrame,SPARK-16883,12994735,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wm624,falaki,falaki,03/Aug/16 22:53,02/Sep/16 16:28,14/Jul/23 06:29,02/Sep/16 08:51,2.0.0,,,,,,,,2.0.1,2.1.0,,,SparkR,,,,,,,,0,,,,,,"To reproduce run following code. As you can see ""y"" is a list of values.
{code}
registerTempTable(createDataFrame(iris), ""iris"")
str(collect(sql(""select cast('1' as double) as x, cast('2' as decimal) as y  from iris limit 5"")))

'data.frame':	5 obs. of  2 variables:
 $ x: num  1 1 1 1 1
 $ y:List of 5
  ..$ : num 2
  ..$ : num 2
  ..$ : num 2
  ..$ : num 2
  ..$ : num 2
{code}",,apachespark,falaki,felixcheung,shivaram,sunrui,wm624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 11 23:26:04 UTC 2016,,,,,,,,,,"0|i31vpr:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"04/Aug/16 18:47;wm624;I will take a look.;;;","04/Aug/16 22:56;wm624;> test <- sql(""select cast('1' as double) as x, cast('2' as decimal) as y  from iris limit 5"")
> test
SparkDataFrame[x:double, y:decimal(10,0)]
> take(test, 5)
  x y
1 1 2
2 1 2
3 1 2
4 1 2
5 1 2
> test2 <- collect(sql(""select cast('1' as double) as x, cast('2' as decimal) as y  from iris limit 5""))
> test2
  x y
1 1 2
2 1 2
3 1 2
4 1 2
5 1 2
> str(test2)
'data.frame':	5 obs. of  2 variables:
 $ x: num  1 1 1 1 1
 $ y:List of 5
  ..$ : num 2
  ..$ : num 2
  ..$ : num 2
  ..$ : num 2
  ..$ : num 2
> class(test2)
[1] ""data.frame""
> test2
  x y
1 1 2
2 1 2
3 1 2
4 1 2
5 1 2
> take(test2, 5)
Error in (function (classes, fdef, mtable)  : 
  unable to find an inherited method for function ‘take’ for signature ‘""data.frame"", ""numeric""’
> test2
  x y
1 1 2
2 1 2
3 1 2
4 1 2
5 1 2
> class(test2)
[1] ""data.frame""

It looks like str() function has some issue for decimal. ;;;","04/Aug/16 23:05;falaki;I think that is because we are not converting {{decimal}} to a {{numeric}} properly. In {{pkg/R/types.R}} decimal is supposed to translate to numeric.;;;","05/Aug/16 05:41;wm624;Right. I am trying to modify it. I will send a PR once I fix it.;;;","05/Aug/16 06:45;wm624;I found the root cause. I am trying to propose a clean solution to handle this case.

PRIMITIVE_TYPES <- as.environment(list(
...
  ""decimal"" = ""numeric"",
...

But when backend returns decimal(10, 0), it can't handle this type in collect() and coltypes().

collect() won't throw Error, but print the list as shown in the above example.;;;","05/Aug/16 16:42;shivaram;The thing to check then is how the serialization / deserialization is happening. That is in SerDe.scala we should be writing the Decimal out as numeric for this to work correctly;;;","05/Aug/16 21:32;falaki;Thanks [~shivaram]! This may require changing the serialization. Who thought this might happen! ;) ;;;","08/Aug/16 16:43;wm624;I can give a try.;;;","09/Aug/16 23:09;wm624;I add some debug information in SerDe.scala, as shown below:
16/08/09 16:05:18 INFO SerDe: writeObject start
16/08/09 16:05:18 INFO SerDe: [x: double, y: decimal(10,0)]
16/08/09 16:05:18 INFO SerDe: writeObject end
16/08/09 16:05:18 INFO SerDe: writeType start
16/08/09 16:05:18 INFO SerDe: jobj
16/08/09 16:05:18 INFO SerDe: writeType end
16/08/09 16:05:18 INFO SerDe: writeObject start
16/08/09 16:05:18 INFO SerDe: StructType(StructField(x,DoubleType,true), StructField(y,DecimalType(10,0),true))
16/08/09 16:05:18 INFO SerDe: writeObject end
16/08/09 16:05:18 INFO SerDe: writeType start
16/08/09 16:05:18 INFO SerDe: jobj
16/08/09 16:05:18 INFO SerDe: writeType end

It serializes the value as a Java object, which is correct. The problem is in the frontend (sparkR).

To verify my guess, I simply add ""decimal(10,0)"" = ""numeric"" in the PRIMITIVE_TYPES mapping table. It works as below:
'data.frame':	5 obs. of  2 variables:
 $ x: num  1 1 1 1 1
 $ y: num  2 2 2 2 2

I think the right fix should be in the frontend by adding a handling function with regex to match special cases like decimal (10,0). In general, we should map decimal and decimal(x,y) to numeric. ;;;","09/Aug/16 23:13;wm624;[~shivaram] What do you think about my proposed solution?;;;","09/Aug/16 23:14;shivaram;Hmm why do we want to serialize it as a java object here ? Can't we serialize it as a double similar to how BigDecimal is handled in https://github.com/apache/spark/blob/b89b3a5c8e391fcaebe7ef3c77ef16bb9431d6ab/core/src/main/scala/org/apache/spark/api/r/SerDe.scala#L294 ?;;;","09/Aug/16 23:22;wm624;I think it is because 
[x: double, y: decimal(10,0)]
StructType(StructField(x,DoubleType,true), StructField(y,DecimalType(10,0),true))

does not match any type inside writeObject() and hit the default case.

It is not serialized field by feild. 
;;;","09/Aug/16 23:36;shivaram;Yeah I see that - The change I am proposing is to add another case inside writeObject to convert DecimalTypes to doubles using something like the function in https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala#L344

I guess the question is whether this change is more intrusive than the other one and how will this impact handling more types in the future. It'll be great if you can try out one or both of them and open a PR for more discussion.;;;","10/Aug/16 00:02;wm624;OK.I will try both. For the frontend solution, I can create a special handling function which will be used for handling all special cases like decimal(10, 0) with return value of Boolean type. The callers will only change a little bit by calling this function when it is not the PRIMITIVE types. In the future, we can add more in this special function to handle other cases. These cases are those which are not Complex types but not covered by PRIMITIVE types mapping.

For the second solution, I will make changes as you suggested. I will learn how serialize StructType(StructField(x,DoubleType,true), StructField(y,DecimalType(10,0),true)) by adding the value type. 

The second solution could be simpler, I guess.
;;;","11/Aug/16 23:26;apachespark;User 'wangmiao1981' has created a pull request for this issue:
https://github.com/apache/spark/pull/14613;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Improve ANN training, add training data persist if needed",SPARK-16880,12994627,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,weichenxu123,weichenxu123,weichenxu123,03/Aug/16 16:18,04/Aug/16 20:42,14/Jul/23 06:29,04/Aug/16 20:41,,,,,,,,,2.0.1,2.1.0,,,ML,MLlib,,,,,,,0,,,,,,"The ANN layer training does not persist input data RDD,
so that it may cause overhead cost if the RDD need to compute from lineage.",,apachespark,michaelmalak,weichenxu123,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 04 20:41:54 UTC 2016,,,,,,,,,,"0|i31v1r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/16 16:28;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/14483;;;","04/Aug/16 20:41;srowen;Issue resolved by pull request 14483
[https://github.com/apache/spark/pull/14483];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
force spill NPE,SPARK-16873,12994531,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sharkd,sharkd,sharkd,03/Aug/16 09:35,06/Nov/16 14:19,14/Jul/23 06:29,04/Aug/16 02:22,2.0.0,,,,,,,,1.6.3,2.0.1,2.1.0,,Spark Core,,,,,,,,0,,,,,,"16/07/31 20:52:38 INFO executor.Executor: Running task 1090.1 in stage 18.0 (TID 23793)
16/07/31 20:52:38 INFO storage.ShuffleBlockFetcherIterator: Getting 5000 non-empty blocks out of 5000 blocks
16/07/31 20:52:38 INFO collection.ExternalSorter: Thread 151 spilling in-memory map of 410.3 MB to disk (1 time so far)
16/07/31 20:52:38 INFO storage.ShuffleBlockFetcherIterator: Started 30 remote fetches in 214 ms
16/07/31 20:52:44 INFO collection.ExternalSorter: spill memory to 

file:/data5/yarnenv/local/usercache/tesla/appcache/application_1465785263942_56138/blockmgr-e9cc29b9-ca1a-460a-ad76-

32f8ee437e51/0d/temp_shuffle_dc8f4489-2289-4b99-a605-81c873dc9e17, fileSize:46.0 MB
16/07/31 20:52:44 WARN memory.ExecutionMemoryPool: Internal error: release called on 430168384 bytes but task only has 

424958272 bytes of memory from the on-heap execution pool
16/07/31 20:52:45 INFO collection.ExternalSorter: Thread 152 spilling in-memory map of 398.7 MB to disk (1 time so far)
16/07/31 20:52:52 INFO collection.ExternalSorter: Thread 151 spilling in-memory map of 389.6 MB to disk (2 times so far)
16/07/31 20:52:54 INFO collection.ExternalSorter: spill memory to 

file:/data11/yarnenv/local/usercache/tesla/appcache/application_1465785263942_56138/blockmgr-56e1ec4d-9560-4019-83f7-

ec6fd3fe78f9/24/temp_shuffle_10a210a4-38df-40e6-821d-00e15da12eaa, fileSize:45.5 MB
16/07/31 20:52:54 WARN memory.ExecutionMemoryPool: Internal error: release called on 413772021 bytes but task only has 

408561909 bytes of memory from the on-heap execution pool
16/07/31 20:52:55 INFO collection.ExternalSorter: spill memory to 

file:/data2/yarnenv/local/usercache/tesla/appcache/application_1465785263942_56138/blockmgr-f4fe32a2-c930-4b49-8feb-

722536c290d7/10/temp_shuffle_4ae9dd49-6039-4a44-901a-ee4650351ebc, fileSize:44.7 MB
16/07/31 20:52:56 INFO collection.ExternalSorter: Thread 152 spilling in-memory map of 389.6 MB to disk (2 times so far)
16/07/31 20:53:30 INFO collection.ExternalSorter: spill memory to 

file:/data6/yarnenv/local/usercache/tesla/appcache/application_1465785263942_56138/blockmgr-95b49f89-4155-435c-826b-

7a616662d47a/1b/temp_shuffle_ea43939f-04d4-42a9-805f-44e01afc9a13, fileSize:44.7 MB
16/07/31 20:53:30 INFO collection.ExternalSorter: Thread 151 spilling in-memory map of 389.6 MB to disk (3 times so far)
16/07/31 20:53:49 INFO collection.ExternalSorter: spill memory to 

file:/data10/yarnenv/local/usercache/tesla/appcache/application_1465785263942_56138/blockmgr-4f736364-e192-4f31-bfe9-

8eaa29ff2114/3f/temp_shuffle_680e07ec-404e-4710-9d14-1665b300e05c, fileSize:44.7 MB
16/07/31 20:54:04 INFO collection.ExternalSorter: Task 23585 force spilling in-memory map to disk and  it will release 164.3 

MB memory
16/07/31 20:54:04 INFO collection.ExternalSorter: spill memory to 

file:/data4/yarnenv/local/usercache/tesla/appcache/application_1465785263942_56138/blockmgr-db5f46c3-d7a4-4f93-8b77-

565e469696fb/09/temp_shuffle_ec3ece08-4569-4197-893a-4a5dfcbbf9fa, fileSize:0.0 B
16/07/31 20:54:04 WARN memory.TaskMemoryManager: leak 164.3 MB memory from 

org.apache.spark.util.collection.ExternalSorter@3db4b52d
16/07/31 20:54:04 ERROR executor.Executor: Managed memory leak detected; size = 190458101 bytes, TID = 23585
16/07/31 20:54:04 ERROR executor.Executor: Exception in task 1013.0 in stage 18.0 (TID 23585)
java.lang.NullPointerException
	at org.apache.spark.util.collection.ExternalSorter$SpillReader.cleanup(ExternalSorter.scala:625)
	at org.apache.spark.util.collection.ExternalSorter$SpillReader.nextBatchStream(ExternalSorter.scala:540)
	at org.apache.spark.util.collection.ExternalSorter$SpillReader.<init>(ExternalSorter.scala:508)
	at org.apache.spark.util.collection.ExternalSorter$SpillableIterator.spill(ExternalSorter.scala:814)
	at org.apache.spark.util.collection.ExternalSorter.forceSpill(ExternalSorter.scala:254)
	at org.apache.spark.util.collection.Spillable.spill(Spillable.scala:109)
	at org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:154)
	at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:249)
	at org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:112)
	at org.apache.spark.shuffle.sort.ShuffleExternalSorter.acquireNewPageIfNecessary(ShuffleExternalSorter.java:346)
	at org.apache.spark.shuffle.sort.ShuffleExternalSorter.insertRecord(ShuffleExternalSorter.java:367)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.insertRecordIntoSorter(UnsafeShuffleWriter.java:237)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:164)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
16/07/31 20:54:30 INFO executor.Executor: Executor is trying to kill task 1090.1 in stage 18.0 (TID 23793)
16/07/31 20:54:30 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
16/07/31 20:54:31 INFO storage.MemoryStore: MemoryStore cleared
16/07/31 20:54:31 INFO storage.BlockManager: BlockManager stopped
16/07/31 20:54:31 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
16/07/31 20:54:31 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing 

remote transports.
16/07/31 20:54:31 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
16/07/31 20:54:31 INFO util.ShutdownHookManager: Shutdown hook called",,apachespark,KaiXu,sharkd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18289,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 03 12:04:04 UTC 2016,,,,,,,,,,"0|i31ugf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/16 10:53;srowen;Seems easy to avoid dereferencing the null stream reference in the cleanup method, go ahead.;;;","03/Aug/16 12:04;apachespark;User 'sharkdtu' has created a pull request for this issue:
https://github.com/apache/spark/pull/14479;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executor will be both dead and alive when this executor reregister itself to driver.,SPARK-16868,12994512,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,carlmartin,carlmartin,carlmartin,03/Aug/16 07:48,11/Aug/16 21:56,14/Jul/23 06:29,11/Aug/16 21:56,,,,,,,,,2.1.0,,,,Web UI,,,,,,,,0,,,,,,"In a rare condition, Executor will register its block manager twice.
!https://issues.apache.org/jira/secure/attachment/12821794/2016-8-3%2015-41-47.jpg!
When unregister it from BlockManagerMaster, driver mark it as ""DEAD"" in executors WebUI.
But when the heartbeat reregister the block manager again, this executor will also have another status ""Active"".
!https://issues.apache.org/jira/secure/attachment/12821795/2016-8-3%2015-51-13.jpg!

",,ajbozarth,apachespark,carlmartin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/16 07:49;carlmartin;2016-8-3 15-41-47.jpg;https://issues.apache.org/jira/secure/attachment/12821794/2016-8-3+15-41-47.jpg","03/Aug/16 07:51;carlmartin;2016-8-3 15-51-13.jpg;https://issues.apache.org/jira/secure/attachment/12821795/2016-8-3+15-51-13.jpg",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 08 02:03:05 UTC 2016,,,,,,,,,,"0|i31uc7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/16 10:33;srowen;This is just cosmetic right?
It may be on purpose even, showing the existence of both the previous dead executor and current live one. ;;;","03/Aug/16 11:28;carlmartin;This is the same executor not different jvm process. So I think it can't have two status.;;;","08/Aug/16 02:03;apachespark;User 'SaintBacchus' has created a pull request for this issue:
https://github.com/apache/spark/pull/14530;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Analysis error for DataSet typed selection,SPARK-16853,12994239,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,clockfly,clockfly,clockfly,02/Aug/16 10:17,08/Dec/16 23:06,14/Jul/23 06:29,04/Aug/16 11:47,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"For DataSet typed selection
{code}
def select[U1: Encoder](c1: TypedColumn[T, U1]): Dataset[U1]
{code}

If U1 contains sub-fields, then it reports AnalysisException 

Reproducer:
{code}
scala> case class A(a: Int, b: Int)
scala> Seq((0, A(1,2))).toDS.select($""_2"".as[A])
org.apache.spark.sql.AnalysisException: cannot resolve '`a`' given input columns: [_2];
  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  at 
...
{code}",,apachespark,clockfly,cloud_fan,yinxusen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 04 11:47:30 UTC 2016,,,,,,,,,,"0|i31snr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/16 03:00;apachespark;User 'clockfly' has created a pull request for this issue:
https://github.com/apache/spark/pull/14474;;;","04/Aug/16 11:47;cloud_fan;Issue resolved by pull request 14474
[https://github.com/apache/spark/pull/14474];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve error message for greatest/least,SPARK-16850,12994192,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petermaxlee,petermaxlee,petermaxlee,02/Aug/16 06:15,02/Aug/16 17:22,14/Jul/23 06:29,02/Aug/16 11:34,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"Greatest/least function does not have the most friendly error message for data types:

Error in SQL statement: AnalysisException: cannot resolve 'greatest(CAST(1.0 AS DECIMAL(2,1)), ""1.0"")' due to data type mismatch: The expressions should all have the same type, got GREATEST (ArrayBuffer(DecimalType(2,1), StringType)).; line 1 pos 7

We should report the human readable data type instead, rather than having ""ArrayBuffer"" and ""StringType"".
",,apachespark,cloud_fan,petermaxlee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 02 11:34:44 UTC 2016,,,,,,,,,,"0|i31sdb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/16 06:24;apachespark;User 'petermaxlee' has created a pull request for this issue:
https://github.com/apache/spark/pull/14453;;;","02/Aug/16 11:34;cloud_fan;Issue resolved by pull request 14453
[https://github.com/apache/spark/pull/14453];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CleanupAliases may leave redundant aliases at end of analysis state,SPARK-16839,12994089,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,eyalfa,eyalfa,eyalfa,01/Aug/16 21:46,01/Nov/16 16:48,14/Jul/23 06:29,01/Nov/16 16:13,1.6.1,2.0.0,,,,,,,2.1.0,,,,SQL,,,,,,,,0,alias,analysis,analyzers,sql,struct,"[SPARK-9634] [SPARK-9323] [SQL]  introduced CleanupReferences which removes unnecessary Aliases while keeping required ones such as top level Projection and struct attributes. this mechanism is implemented by maintaining a boolean flag during a top-down expression transformation, I found a case where this mechanism leaves redundant aliases in the tree (within a right sibling of a create_struct node).",,apachespark,eyalfa,,,,,,,,,,,,,,,,,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 01 16:48:27 UTC 2016,,,,,,,,,,"0|i31rqf:",9223372036854775807,,,,,cloud_fan,,,,,,,,,,,,,,,,,,,"01/Aug/16 22:14;apachespark;User 'eyalfa' has created a pull request for this issue:
https://github.com/apache/spark/pull/14444;;;","01/Nov/16 16:48;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/15718;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TimeWindow incorrectly drops slideDuration in constructors,SPARK-16837,12994033,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tmagrino,tmagrino,tmagrino,01/Aug/16 17:27,31/Aug/16 21:37,14/Jul/23 06:29,02/Aug/16 16:16,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,correctness,,,,,"Right now, the constructors for the TimeWindow expression in Catalyst incorrectly uses the windowDuration in place of the slideDuration.  This will cause incorrect windowing semantics after time window expressions are analyzed by Catalyst.

Relevant code is here: https://github.com/apache/spark/blob/branch-2.0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/TimeWindow.scala#L29-L54",,apachespark,tmagrino,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 02 16:16:58 UTC 2016,,,,,,,,,,"0|i31rdz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/16 17:38;srowen;Good catch, definitely submit a fix;;;","01/Aug/16 18:36;apachespark;User 'tmagrino' has created a pull request for this issue:
https://github.com/apache/spark/pull/14441;;;","02/Aug/16 16:16;srowen;Issue resolved by pull request 14441
[https://github.com/apache/spark/pull/14441];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive date/time function error,SPARK-16836,12994016,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,hvanhovell,quartox,quartox,01/Aug/16 16:27,02/Aug/16 17:09,14/Jul/23 06:29,02/Aug/16 17:09,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"Previously available hive functions for date/time are not available in Spark 2.0 (e.g. current_date, current_timestamp). These functions work in Spark 1.6.2 with HiveContext.

Example (from spark-shell):
{noformat}
scala> spark.sql(""select current_date"")
org.apache.spark.sql.AnalysisException: cannot resolve '`current_date`' given input columns: []; line 1 pos 7
  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:77)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:74)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:301)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:301)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:190)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:200)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2$1.apply(QueryPlan.scala:204)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.immutable.List.map(List.scala:285)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:204)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$5.apply(QueryPlan.scala:209)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:209)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:74)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:67)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:67)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:58)
  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)
  ... 48 elided

{noformat}
",,apachespark,bomeng,hvanhovell,quartox,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 01 19:04:06 UTC 2016,,,,,,,,,,"0|i31ra7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/16 18:09;hvanhovell;Add braces as a work around: {{select current_timestamp()}}

My only problem with fixing this is, is that we need to hardcode this in the parser (add a rule to {{primaryExpression}}); which is IMO a bit hacky.
;;;","01/Aug/16 19:04;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/14442;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark CrossValidator reports incorrect avgMetrics,SPARK-16831,12993886,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mmoroz,mmoroz,mmoroz,01/Aug/16 08:40,18/Nov/16 22:19,14/Jul/23 06:29,03/Aug/16 11:19,2.0.0,,,,,,,,2.0.1,2.1.0,,,ML,PySpark,,,,,,,0,,,,,,"The avgMetrics are summed up across all folds instead of being averaged. This is an easy fix in CrossValidator._fit() function: {code}metrics[j]+=metric{code} should be {code}metrics[j]+=metric/nFolds{code}.

{code}
dataset = spark.createDataFrame(
  [(Vectors.dense([0.0]), 0.0),
   (Vectors.dense([0.4]), 1.0),
   (Vectors.dense([0.5]), 0.0),
   (Vectors.dense([0.6]), 1.0),
   (Vectors.dense([1.0]), 1.0)] * 1000,
  [""features"", ""label""]).cache()

paramGrid = pyspark.ml.tuning.ParamGridBuilder().build()
tvs = pyspark.ml.tuning.TrainValidationSplit(estimator=pyspark.ml.regression.LinearRegression(), 
                           estimatorParamMaps=paramGrid,
                           evaluator=pyspark.ml.evaluation.RegressionEvaluator(),
                           trainRatio=0.8)
model = tvs.fit(train)
print(model.validationMetrics)

for folds in (3, 5, 10):
  cv = pyspark.ml.tuning.CrossValidator(estimator=pyspark.ml.regression.LinearRegression(), 
                                      estimatorParamMaps=paramGrid, 
                                      evaluator=pyspark.ml.evaluation.RegressionEvaluator(),
                                      numFolds=folds
                                     )
  cvModel = cv.fit(dataset)
  print(folds, cvModel.avgMetrics)
{code}",,apachespark,mmoroz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 03 11:19:50 UTC 2016,,,,,,,,,,"0|i31qhb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/16 13:33;srowen;The Scala version does scale this correctly. Looks like a bug for the Pyspark version, go ahead and fix it.;;;","02/Aug/16 08:06;apachespark;User 'pkch' has created a pull request for this issue:
https://github.com/apache/spark/pull/14456;;;","03/Aug/16 11:19;srowen;Issue resolved by pull request 14456
[https://github.com/apache/spark/pull/14456];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sparkR sc.setLogLevel doesn't work,SPARK-16829,12993863,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wm624,wm624,wm624,01/Aug/16 06:10,08/Dec/16 22:46,14/Jul/23 06:29,03/Sep/16 20:57,,,,,,,,,2.1.0,,,,SparkR,,,,,,,,0,,,,,,"./bin/sparkR
Launching java with spark-submit command /Users/mwang/spark_ws_0904/bin/spark-submit   ""sparkr-shell"" /var/folders/s_/83b0sgvj2kl2kwq4stvft_pm0000gn/T//RtmpQxJGiZ/backend_porte9474603ed1e 
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel).

> sc.setLogLevel(""INFO"")
Error: could not find function ""sc.setLogLevel""

sc.setLogLevel doesn't exist.",,apachespark,shivaram,wm624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 03 20:57:23 UTC 2016,,,,,,,,,,"0|i31qc7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/16 06:14;apachespark;User 'wangmiao1981' has created a pull request for this issue:
https://github.com/apache/spark/pull/14433;;;","03/Sep/16 20:57;shivaram;Issue resolved by pull request 14433
[https://github.com/apache/spark/pull/14433];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stop reporting spill metrics as shuffle metrics,SPARK-16827,12993836,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chobrian,sitalkedia@gmail.com,sitalkedia@gmail.com,01/Aug/16 00:24,12/Nov/16 17:27,14/Jul/23 06:29,12/Oct/16 23:01,2.0.0,,,,,,,,,,,,Shuffle,Spark Core,,,,,,,0,performance,,,,,"One of our hive job which looks like this -

{code}
 SELECT  userid
     FROM  table1 a
     JOIN table2 b
      ON    a.ds = '2016-07-15'
      AND  b.ds = '2016-07-15'
      AND  a.source_id = b.id
{code}

After upgrade to Spark 2.0 the job is significantly slow.  Digging a little into it, we found out that one of the stages produces excessive amount of shuffle data.  Please note that this is a regression from Spark 1.6. Stage 2 of the job which used to produce 32KB shuffle data with 1.6, now produces more than 400GB with Spark 2.0. We also tried turning off whole stage code generation but that did not help. 

PS - Even if the intermediate shuffle data size is huge, the job still produces accurate output.",,apachespark,chobrian,dreamworks007,lianhuiwang,michaelmalak,rxin,sitalkedia@gmail.com,thomastechs,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 12 17:27:49 UTC 2016,,,,,,,,,,"0|i31q67:",9223372036854775807,,,,,,,,,,,,,2.0.3,2.1.0,,,,,,,,,,"01/Aug/16 00:33;sitalkedia@gmail.com;[~rxin] - Any idea how to debug this issue?;;;","01/Aug/16 00:35;rxin;Did the plan in Spark 1.6 use a broadcast join, and in 2.0 use a shuffle join?
;;;","01/Aug/16 00:51;sitalkedia@gmail.com;That is not the case. There is no broadcast join involved, its using shuffle join in both 1.6 and 2.0.

Plan in 1.6 -
{code}
Project [target_id#136L AS userid#115L]
+- SortMergeJoin [source_id#135L], [id#140L]
   :- Sort [source_id#135L ASC], false, 0
   :  +- TungstenExchange hashpartitioning(source_id#135L,800), None
   :     +- ConvertToUnsafe
   :        +- HiveTableScan [target_id#136L,source_id#135L], MetastoreRelation foo, table1, Some(a), [(ds#134 = 2016-07-15)]
   +- Sort [id#140L ASC], false, 0
      +- TungstenExchange hashpartitioning(id#140L,800), None
         +- ConvertToUnsafe
            +- HiveTableScan [id#140L], MetastoreRelation foo, table2, Some(b), [(ds#139 = 2016-07-15)]

{code}

Plan in 2.0  - 

{code}
*Project [target_id#151L AS userid#111L]
+- *SortMergeJoin [source_id#150L], [id#155L], Inner
   :- *Sort [source_id#150L ASC], false, 0
   :  +- Exchange hashpartitioning(source_id#150L, 800)
   :     +- *Filter isnotnull(source_id#150L)
   :        +- HiveTableScan [source_id#150L, target_id#151L], MetastoreRelation foo, table1, a, [isnotnull(ds#149), (ds#149 = 2016-07-15)]
   +- *Sort [id#155L ASC], false, 0
      +- Exchange hashpartitioning(id#155L, 800)
         +- *Filter isnotnull(id#155L)
            +- HiveTableScan [id#155L], MetastoreRelation foo, table2, b, [isnotnull(ds#154), (ds#154 = 2016-07-15)]
{code}
;;;","01/Aug/16 04:52;sitalkedia@gmail.com;Actually it seems like this is a bug in shuffle write metrics calculation, actual amount of shuffle data written might be the same,  because the final output is exactly the same. ;;;","01/Aug/16 17:24;rxin;Does the performance diff?;;;","01/Aug/16 17:29;sitalkedia@gmail.com;Performance is worse for this job. But I suspect this is not because of shuffle data write. Will update once I figure out the reason.;;;","03/Aug/16 18:09;thomastechs;I understand what you have mentioned is w.r.t to the 1.6 vs 2.0;  Having said that when I faced performance issue, because of skewed partitions, I had to go for a re-partition before the join, which helped improving the performance way better.

One thing, I am trying to understand is the real execution stages from webUI and how they can be mapped to the explain plans.Please share some light in this , if it is possible, may be a little out of context here.;;;","04/Oct/16 17:58;chobrian;We found that the ""shuffle write"" metric was also including writes of spill files, inflating the amount of shuffle writes. This even showed up for final stages when no shuffle writes should take place. I'll upload screenshots on the PR.;;;","04/Oct/16 18:02;apachespark;User 'dafrista' has created a pull request for this issue:
https://github.com/apache/spark/pull/15347;;;","04/Oct/16 19:05;rxin;We should probably separate the on-disk spill from the shuffle size. Would you have time to work on that?
;;;","04/Oct/16 19:17;chobrian;Sure, having the actual spill metrics is something we're interested in as well. I'd like to work on it, but I might not get to it immediately.;;;","12/Oct/16 23:22;apachespark;User 'dafrista' has created a pull request for this issue:
https://github.com/apache/spark/pull/15455;;;","13/Oct/16 01:12;dreamworks007;[~rxin], for this one, I think spill byte (both memory and disk), and shuffle bytes are already logged and reported, right ?
Also, if I want to add spill time metrics, do you suggest I create a parent class DiskWriteMetrics, and ShuffleWriteMetrics and my new class (eg SpillWriteMetrics) inherit from it, and then pass parent class(DiskWriteMetrics) to UnsafeSorterSpillWriter https://github.com/facebook/FB-Spark/blob/fb-2.0/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java#L209 ?  

Or do you suggest rename the ShuffleWriteMetrics class to something like WriteMetrics ?;;;","13/Oct/16 03:43;rxin;See https://github.com/apache/spark/pull/15455/files 

The spill there is not tracked.

Also I don't think it is that useful to track spill time, because you can't actually measure that accurately due to pipelining.
;;;","13/Oct/16 06:23;dreamworks007;I am quite new to spark, but I think I am getting some confusion on the requirement now - 
[~sitalkedia@gmail.com] , [~chobrian] , could you please give some comment here as for the requirement based on what [~rxin] said ?;;;","24/Oct/16 21:18;apachespark;User 'dreamworks007' has created a pull request for this issue:
https://github.com/apache/spark/pull/15616;;;","12/Nov/16 17:27;dreamworks007;ping ping..;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
java.util.Hashtable limits the throughput of PARSE_URL(),SPARK-16826,12993828,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sylvinus,sylvinus,sylvinus,31/Jul/16 21:31,12/Jan/18 08:50,14/Jul/23 06:29,05/Aug/16 19:56,2.0.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,1,,,,,,"Hello!

I'm using {{c4.8xlarge}} instances on EC2 with 36 cores and doing lots of {{parse_url(url, ""host"")}} in Spark SQL.

Unfortunately it seems that there is an internal thread-safe cache in there, and the instances end up being 90% idle.

When I view the thread dump for my executors, most of the executor threads are ""BLOCKED"", in that state:
{code}
java.util.Hashtable.get(Hashtable.java:362)
java.net.URL.getURLStreamHandler(URL.java:1135)
java.net.URL.<init>(URL.java:599)
java.net.URL.<init>(URL.java:490)
java.net.URL.<init>(URL.java:439)
org.apache.spark.sql.catalyst.expressions.ParseUrl.getUrl(stringExpressions.scala:731)
org.apache.spark.sql.catalyst.expressions.ParseUrl.parseUrlWithoutKey(stringExpressions.scala:772)
org.apache.spark.sql.catalyst.expressions.ParseUrl.eval(stringExpressions.scala:785)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)
org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$$anonfun$create$2.apply(GeneratePredicate.scala:69)
org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$$anonfun$create$2.apply(GeneratePredicate.scala:69)
org.apache.spark.sql.execution.FilterExec$$anonfun$17$$anonfun$apply$2.apply(basicPhysicalOperators.scala:203)
org.apache.spark.sql.execution.FilterExec$$anonfun$17$$anonfun$apply$2.apply(basicPhysicalOperators.scala:202)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:147)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
org.apache.spark.scheduler.Task.run(Task.scala:85)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
java.lang.Thread.run(Thread.java:745)
{code}

However, when I switch from 1 executor with 36 cores to 9 executors with 4 cores, throughput is almost 10x higher and the CPUs are back at ~100% use.

Thanks!",,apachespark,dongjoon,sylvinus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23056,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 05 19:56:09 UTC 2016,,,,,,,,,,"0|i31q4f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/16 14:24;srowen;The contention is in the JDK class java.net.URL itself, and it does look like an unfortunate bottleneck from ages ago. There's a static, globally synchronized Hashtable here, which explains why multiple JVM (executors) alleviates the problem. We can't fix that directly. See also http://www.supermind.org/blog/580/java-net-url-synchronization-bottleneck

We also probably have to rely on the behavior of this class to parse a URL. I don't see a good alternative here? You can avoid the lookup if you pass in the URLStreamHandler manually which would mean reimplementing some of the same logic in URL. That then means the overhead of looking up a SecurityManager though.

What about just avoiding a load of calls to parseURL at once, is that at all reasonable? like rearranging the pipeline to filter or remove duplicates earlier upstream?;;;","02/Aug/16 01:14;sylvinus;[~srowen] thanks for the pointers! 

I'm parsing every hyperlink found in Common Crawl, so there are billions of unique ones, no way around it.

Wouldn't it be possible to switch to another implementation with an API similar to java.net.URL? As I understand it we never need the URLStreamHandler in the first place anyway?

I'm not a Java expert but what about {{java.net.URI}} or {{org.apache.catalina.util.URL}} for instance?
;;;","02/Aug/16 02:04;srowen;URI.toURL just follows the same code path. Does URI itself parse all the same fields? Didn't think so because URIs are a superset of URLs.

Definitely open to suggestions. Anything that can parse the same fields respectably is OK.;;;","02/Aug/16 02:21;sylvinus;Sorry I can't be more helpful on the Java side... But I think there must be some high-quality URL parsing code somewhere in the Apache foundation already :-);;;","02/Aug/16 02:41;sylvinus;[~srowen] what about this? 
https://github.com/sylvinus/spark/commit/98119a08368b1cd1faf3f25a32910ad6717c5c02

The tests seem to pass and I don't think it uses the problematic code paths in java.net.URL (except for getFile but that may be could be fixed easily);;;","04/Aug/16 00:09;apachespark;User 'sylvinus' has created a pull request for this issue:
https://github.com/apache/spark/pull/14488;;;","05/Aug/16 19:56;srowen;Issue resolved by pull request 14488
[https://github.com/apache/spark/pull/14488];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exchange reuse incorrectly reuses scans over different sets of partitions,SPARK-16818,12993764,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,ekhliang,ekhliang,ekhliang,30/Jul/16 22:01,08/Dec/16 23:05,14/Jul/23 06:29,31/Jul/16 05:47,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,correctness,,,,,"This happens because the file scan operator does not take into account partition pruning in its implementation of `sameResult()`. As a result, executions may be incorrect on self-joins over the same base file relation. Here's a minimal test case to reproduce:

{code}
    spark.conf.set(""spark.sql.exchange.reuse"", true)  // defaults to true in 2.0
    withTempPath { path =>
      val tempDir = path.getCanonicalPath
      spark.range(10)
        .selectExpr(""id % 2 as a"", ""id % 3 as b"", ""id as c"")
        .write
        .partitionBy(""a"")
        .parquet(tempDir)
      val df = spark.read.parquet(tempDir)
      val df1 = df.where(""a = 0"").groupBy(""b"").agg(""c"" -> ""sum"")
      val df2 = df.where(""a = 1"").groupBy(""b"").agg(""c"" -> ""sum"")
      checkAnswer(df1.join(df2, ""b""), Row(0, 6, 12) :: Row(1, 4, 8) :: Row(2, 10, 5) :: Nil)
{code}

When exchange reuse is on, the result is
{code}
+---+------+------+
|  b|sum(c)|sum(c)|
+---+------+------+
|  0|     6|     6|
|  1|     4|     4|
|  2|    10|    10|
+---+------+------+
{code}

The correct result is
{code}
+---+------+------+
|  b|sum(c)|sum(c)|
+---+------+------+
|  0|     6|    12|
|  1|     4|     8|
|  2|    10|     5|
+---+------+------+
{code}",,apachespark,ekhliang,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 31 05:57:03 UTC 2016,,,,,,,,,,"0|i31pq7:",9223372036854775807,,,,,,,,,,,,,2.0.1,,,,,,,,,,,"30/Jul/16 22:05;apachespark;User 'ericl' has created a pull request for this issue:
https://github.com/apache/spark/pull/14425;;;","31/Jul/16 05:49;rxin;I've merged this in master, but this still needs to be merged into branch-2.0.
;;;","31/Jul/16 05:57;apachespark;User 'ericl' has created a pull request for this issue:
https://github.com/apache/spark/pull/14427;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dataset[List[T]] leads to ArrayStoreException,SPARK-16815,12993728,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,michalsenkyr,TPolzer,TPolzer,30/Jul/16 08:54,06/Jan/17 08:10,14/Jul/23 06:29,06/Jan/17 08:10,,,,,,,,,2.2.0,,,,SQL,,,,,,,,0,,,,,,"{noformat}
scala> spark.sqlContext.createDataset(sc.parallelize(List(1) :: Nil)).collect
java.lang.ArrayStoreException: scala.collection.mutable.WrappedArray$ofRef      
  at scala.collection.mutable.ArrayBuilder$ofRef.$plus$eq(ArrayBuilder.scala:87)
  at scala.collection.mutable.ArrayBuilder$ofRef.$plus$eq(ArrayBuilder.scala:56)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2218)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
  at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2568)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2217)
  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2222)
  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2222)
  at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2581)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2222)
  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2198)
  ... 48 elided

{noformat}",,cloud_fan,jerryshao,kiszk,michalsenkyr,TPolzer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16792,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 06 08:06:47 UTC 2017,,,,,,,,,,"0|i31pi7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/16 06:01;jerryshao;From my understanding you can use 

{code}
spark.sqlContext.createDataset(sc.parallelize(List(1) :: Nil)).as[Seq[Int]].collect
{code}

to fix this issue.;;;","10/Dec/16 00:25;michalsenkyr;I made a PR that should fix this issue [#16240|https://github.com/apache/spark/pull/16240];;;","06/Jan/17 07:09;cloud_fan;#16240 has been merged, can you try it again to see if it's fixed?;;;","06/Jan/17 08:06;michalsenkyr;Yes, it is:
{code}
scala> spark.sqlContext.createDataset(sc.parallelize(List(1) :: Nil)).collect
res0: Array[List[Int]] = Array(List(1))
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
History Server main page does not honor APPLICATION_WEB_PROXY_BASE,SPARK-16808,12993687,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vijoshi,mgummelt,mgummelt,29/Jul/16 23:09,15/Nov/16 21:10,14/Jul/23 06:29,09/Nov/16 18:42,2.0.0,,,,,,,,2.0.3,2.1.0,,,,,,,,,,,2,,,,,,"The root of the history server is rendered dynamically with javascript, and this doesn't honor APPLICATION_WEB_PROXY_BASE: https://github.com/apache/spark/blob/master/core/src/main/resources/org/apache/spark/ui/static/historypage-template.html#L67

Other links in the history server do honor it: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/ui/UIUtils.scala#L146

This means the links on the history server root page are broken when deployed behind a proxy.",,apachespark,jantes,mgummelt,rxin,vijoshi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 11 22:36:04 UTC 2016,,,,,,,,,,"0|i31p93:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/16 04:44;vijoshi;The same issue also affects the case where ""spark.ui.proxyBase"" is set and it does not get honored in HistoryServer 2.0. 

This is a regression in 2.0 from 1.6. 

In 2.0, the refactoring of HistoryPage.scala changed the History application listing to occur from an ajax call. The page listing is rendered using ui/static/historypage-template.html which does not take into account the ""spark.ui.proxyBase"" / APPLICATION_WEB_PROXYBASE when generating the links to the applications.

The existing History Server test suite -spark/deploy/history/HistoryServerSuite.scala has an explicit test case to validate this scenario: ""relative links are prefixed with uiRoot (spark.ui.proxyBase)""  - but this test case is broken too because the html it's receiving and validating no longer contains any application links (as noted earlier, they are now generated through a js ajax call etc which does not happen in this test) - so even though the test case passes, it's failing it's purpose. ;;;","02/Nov/16 12:45;jantes;One possible solution is in the /core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala. 
{code:java}
  private[history] def getAttemptURI(appId: String, attemptId: Option[String]): String = {
    val attemptSuffix = attemptId.map { id => s""/$id"" }.getOrElse("""")
    UIUtils.prependBaseUri(s""${HistoryServer.UI_PATH_PREFIX}/${appId}${attemptSuffix}"") 
  }
{code}
In the 1.6.x line of code the UIUtils.prependBaseUri was positioned where this method was called.  The rewrite removed it.  Fixing it here appears to be better semantically.;;;","02/Nov/16 19:33;apachespark;User 'jantes' has created a pull request for this issue:
https://github.com/apache/spark/pull/15739;;;","02/Nov/16 23:48;apachespark;User 'vijoshi' has created a pull request for this issue:
https://github.com/apache/spark/pull/15742;;;","02/Nov/16 23:49;vijoshi;Added a pull request with my take at this issue as well.;;;","11/Nov/16 22:36;apachespark;User 'vijoshi' has created a pull request for this issue:
https://github.com/apache/spark/pull/15855;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correlated subqueries containing non-deterministic operators return incorrect results,SPARK-16804,12993655,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nsyca,nsyca,nsyca,29/Jul/16 21:37,15/Nov/16 22:22,14/Jul/23 06:29,08/Aug/16 10:16,2.0.0,,,,,,,,2.0.2,2.1.0,,,SQL,,,,,,,,0,,,,,,"Correlated subqueries with LIMIT could return incorrect results. The rule ResolveSubquery in the Analysis phase moves correlated predicates to a join predicates and neglect the semantic of the LIMIT.

Example:

{noformat}
Seq(1, 2).toDF(""c1"").createOrReplaceTempView(""t1"")
Seq(1, 2).toDF(""c2"").createOrReplaceTempView(""t2"")

sql(""select c1 from t1 where exists (select 1 from t2 where t1.c1=t2.c2 LIMIT 1)"").show
+---+                                                                           
| c1|
+---+
|  1|
+---+
{noformat}

The correct result contains both rows from T1.",,apachespark,nsyca,,,,,,,,,,,,,,,,,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18455,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 04 22:29:09 UTC 2016,,,,,,,,,,"0|i31p1z:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"29/Jul/16 21:57;apachespark;User 'nsyca' has created a pull request for this issue:
https://github.com/apache/spark/pull/14411;;;","29/Jul/16 21:59;nsyca;I am working on a fix. Could you please assign this JIRA to nsyca?;;;","29/Jul/16 22:06;nsyca;{noformat}
scala> sql(""select c1 from t1 where exists (select 1 from t2 where t1.c1=t2.c2 LIMIT 1)"").explain(true)
== Parsed Logical Plan ==
'Project ['c1]
+- 'Filter exists#21
   :  +- 'SubqueryAlias exists#21
   :     +- 'GlobalLimit 1
   :        +- 'LocalLimit 1
   :           +- 'Project [unresolvedalias(1, None)]
   :              +- 'Filter ('t1.c1 = 't2.c2)
   :                 +- 'UnresolvedRelation `t2`
   +- 'UnresolvedRelation `t1`

== Analyzed Logical Plan ==
c1: int
Project [c1#17]
+- Filter predicate-subquery#21 [(c1#17 = c2#10)]
   :  +- SubqueryAlias predicate-subquery#21 [(c1#17 = c2#10)]   <== This correlated predicate is incorrectly moved above the LIMIT
   :     +- GlobalLimit 1
   :        +- LocalLimit 1
   :           +- Project [1 AS 1#26, c2#10]
   :              +- SubqueryAlias t2
   :                 +- Project [value#8 AS c2#10]
   :                    +- LocalRelation [value#8]
   +- SubqueryAlias t1
      +- Project [value#15 AS c1#17]
         +- LocalRelation [value#15]
{noformat}
By rewriting the correlated predicate in the subquery in Analysis phase from below the LIMIT 1 operation to above it causing the scan of the subquery table to return only 1 row. The correct semantic is the LIMIT 1 must be applied on the subquery for each input value from the parent table.;;;","29/Jul/16 22:14;nsyca;This fix blocks any correlated subquery when there is a LIMIT operation on the path from the parent table to the correlated predicate. We may consider relaxing this restriction once we have a better support on processing correlated subquery in run-time. SPARK-13417 is an umbrella task to track this effort.

Note that if the LIMIT is not in the correlated path, Spark returns correct result.

Example:

sql(""select c1 from t1 where exists (select 1 from t2 where t1.c1=t2.c2) and exists (select 1 from t2 LIMIT 1)"").show

will return both rows from T1, which is correctly handled with and without this proposed fix.

This fix will change the behaviour of the query

sql(""select c1 from t1 where exists (select 1 from t2 where t1.c1=t2.c2 LIMIT 1)"").show

to return an error from the Analysis phase as shown below:

org.apache.spark.sql.AnalysisException: Accessing outer query column is not allowed in a LIMIT: LocalLimit 1
...;;;","29/Jul/16 22:39;nsyca;To demonstrate that this fix does not unnecessarily block the ""good"" cases (where LIMIT is present but NOT on the correlated path), here is an example, which produce the same result set in both with and without this proposed fix.

{noformat}
scala> sql(""select c1 from t1 where exists (select 1 from (select 1 from t2 limit 1) where
t1.c1=t2.c2)"").show
+---+
| c1|
+---+
|  1|
+---+
{noformat};;;","06/Aug/16 16:23;nsyca;The PR also extends the fix to block the {{TABLESAMPLE}} operation in any correlated subquery. ;;;","09/Aug/16 02:53;nsyca;Thank you, @hvanhovell, for merging my PR.;;;","04/Nov/16 22:29;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/15772;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SaveAsTable does not work when source DataFrame is built on a Hive Table,SPARK-16803,12993652,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,29/Jul/16 21:24,20/Jan/18 19:16,14/Jul/23 06:29,22/Nov/16 23:11,2.0.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"{noformat}
scala> sql(""create table sample.sample stored as SEQUENCEFILE as select 1 as key, 'abc' as value"")
res2: org.apache.spark.sql.DataFrame = []

scala> val df = sql(""select key, value as value from sample.sample"")
df: org.apache.spark.sql.DataFrame = [key: int, value: string]

scala> df.write.mode(""append"").saveAsTable(""sample.sample"")

scala> sql(""select * from sample.sample"").show()
+---+-----+
|key|value|
+---+-----+
|  1|  abc|
|  1|  abc|
+---+-----+
{noformat}
In Spark 1.6, it works, but Spark 2.0 does not work. The error message from Spark 2.0 is
{noformat}
scala> df.write.mode(""append"").saveAsTable(""sample.sample"")
org.apache.spark.sql.AnalysisException: Saving data in MetastoreRelation sample, sample
 is not supported.;
{noformat}

So far, we do not plan to support it in Spark 2.0. Spark 1.6 works because it internally uses {{insertInto}}. But, if we change it back it will break the semantic of {{saveAsTable}} (this method uses by-name resolution instead of using by-position resolution used by {{insertInto}}).

Instead, users should use {{insertInto}} API. We should correct the error messages. Users can understand how to bypass it before we support it. ",,apachespark,nelsonc,smilegator,Tagar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16789,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 20 19:16:40 UTC 2018,,,,,,,,,,"0|i31p1b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Jul/16 21:31;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14410;;;","11/Aug/16 23:14;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14612;;;","18/Nov/16 02:31;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/15926;;;","22/Nov/16 23:11;smilegator;Issue resolved by pull request 15926
[https://github.com/apache/spark/pull/15926];;;","05/Jul/17 18:18;Tagar;Any chance `saveAsTable` can be reverted to use `insertInto()`? Like it was in Spark 1.6.
So this will fix the problem.
The PR only changed error message, not underlying root cause problem.
Thanks.;;;","20/Jan/18 19:16;smilegator;[~Tagar] It has been resolved by https://issues.apache.org/jira/browse/SPARK-19152;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
joins.LongToUnsafeRowMap crashes with ArrayIndexOutOfBoundsException,SPARK-16802,12993633,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,sylvinus,sylvinus,29/Jul/16 20:30,19/Sep/16 18:14,14/Jul/23 06:29,04/Aug/16 18:21,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"Hello!

This is a little similar to [SPARK-16740|https://issues.apache.org/jira/browse/SPARK-16740] (should I have reopened it?).

I would recommend to give another full review to {{HashedRelation.scala}}, particularly the new {{LongToUnsafeRowMap}} code. I've had a few other errors that I haven't managed to reproduce so far, as well as what I suspect could be memory leaks (I have a query in a loop OOMing after a few iterations despite not caching its results).

Here is the script to reproduce the ArrayIndexOutOfBoundsException on the current 2.0 branch:

{code}
import os
import random

from pyspark import SparkContext
from pyspark.sql import types as SparkTypes
from pyspark.sql import SQLContext

sc = SparkContext()
sqlc = SQLContext(sc)

schema1 = SparkTypes.StructType([
    SparkTypes.StructField(""id1"", SparkTypes.LongType(), nullable=True)
])
schema2 = SparkTypes.StructType([
    SparkTypes.StructField(""id2"", SparkTypes.LongType(), nullable=True)
])


def randlong():
    return random.randint(-9223372036854775808, 9223372036854775807)

while True:
    l1, l2 = randlong(), randlong()

    # Sample values that crash:
    # l1, l2 = 4661454128115150227, -5543241376386463808

    print ""Testing with %s, %s"" % (l1, l2)
    data1 = [(l1, ), (l2, )]
    data2 = [(l1, )]

    df1 = sqlc.createDataFrame(sc.parallelize(data1), schema1)
    df2 = sqlc.createDataFrame(sc.parallelize(data2), schema2)

    crash = True
    if crash:
        os.system(""rm -rf /tmp/sparkbug"")
        df1.write.parquet(""/tmp/sparkbug/vertex"")
        df2.write.parquet(""/tmp/sparkbug/edge"")

        df1 = sqlc.read.load(""/tmp/sparkbug/vertex"")
        df2 = sqlc.read.load(""/tmp/sparkbug/edge"")

    sqlc.registerDataFrameAsTable(df1, ""df1"")
    sqlc.registerDataFrameAsTable(df2, ""df2"")

    result_df = sqlc.sql(""""""
        SELECT
            df1.id1
        FROM df1
        LEFT OUTER JOIN df2 ON df1.id1 = df2.id2
    """""")

    print result_df.collect()
{code}

{code}
java.lang.ArrayIndexOutOfBoundsException: 1728150825
	at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap.getValue(HashedRelation.scala:463)
	at org.apache.spark.sql.execution.joins.LongHashedRelation.getValue(HashedRelation.scala:762)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.to(SerDeUtil.scala:112)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toBuffer(SerDeUtil.scala:112)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toArray(SerDeUtil.scala:112)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:899)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:899)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1898)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/07/29 20:19:00 WARN TaskSetManager: Lost task 0.0 in stage 17.0 (TID 50, localhost): java.lang.ArrayIndexOutOfBoundsException: 1728150825
	at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap.getValue(HashedRelation.scala:463)
	at org.apache.spark.sql.execution.joins.LongHashedRelation.getValue(HashedRelation.scala:762)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.to(SerDeUtil.scala:112)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toBuffer(SerDeUtil.scala:112)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toArray(SerDeUtil.scala:112)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:899)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:899)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1898)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,davies,kiszk,naegelejd,rxin,sylvinus,wm624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 10 18:11:59 UTC 2016,,,,,,,,,,"0|i31ox3:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"29/Jul/16 21:43;wm624;Very easy to reproduce. I am learning SQL code now.;;;","29/Jul/16 23:51;sylvinus;Maybe useful for others: an ugly workaround to avoid this code path is to cast the join as string and do something like this instead:

{code}
        SELECT
            df1.id1
        FROM df1
        LEFT OUTER JOIN df2 ON cast(df1.id1 as string) = cast(df2.id2 as string)
{code};;;","02/Aug/16 00:41;wm624;With latest code, it should have been fixed. I re-run the test code for 10+ mintues. ;;;","02/Aug/16 13:59;srowen;Do you have a particular fix that addressed it [~wm624]? which version did you reproduce it on, and where is it working?;;;","02/Aug/16 19:06;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/14464;;;","04/Aug/16 18:21;davies;Issue resolved by pull request 14464
[https://github.com/apache/spark/pull/14464];;;","10/Aug/16 18:11;wm624;Sorry for missing your comments. My email box doesn't show up the message. I will check the settings on JIRA.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dataset containing a Case Class with a List type causes a CompileException (converting sequence to list),SPARK-16792,12993482,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,michalsenkyr,jamiehutton,jamiehutton,29/Jul/16 08:40,20/Oct/17 10:00,14/Jul/23 06:29,06/Jan/17 07:07,2.0.0,,,,,,,,2.2.0,,,,SQL,,,,,,,,0,,,,,,"The issue occurs when we run a .map over a dataset containing Case Class with a List in it. A self contained test case is below:

case class TestCC(key: Int, letters: List[String]) //List causes the issue - a Seq/Array works fine

/*simple test data*/
val ds1 = sc.makeRDD(Seq(
(List(""D"")),
(List(""S"",""H"")),
(List(""F"",""H"")),
(List(""D"",""L"",""L""))
)).map(x=>(x.length,x)).toDF(""key"",""letters"").as[TestCC]

//This will fail
val test1=ds1.map{_.key}
test1.show

Error: 

Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 72, Column 70: No applicable constructor/method found for actual parameters ""int, scala.collection.Seq""; candidates are: ""TestCC(int, scala.collection.immutable.List)""

It seems to be internally converting the List to a sequence, then it cant convert it back...

If you change the List[String] to Seq[String] or Array[String] the issue doesnt appear",,apachespark,djalova,jamiehutton,kiszk,michalsenkyr,tsuresh,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22296,,,,,,,,,,,,,,SPARK-16815,,,,SPARK-19088,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 10 00:16:06 UTC 2016,,,,,,,,,,"0|i31nzj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/16 14:40;michalsenkyr;Found a simpler example:
{code:none}
case class X(l: List[String])  // works for Seq
spark.createDataset(Seq(List(""A""))).map(X).show
{code}

Exception thrown:
{code:none}
Caused by: java.lang.ClassCastException: scala.collection.mutable.WrappedArray$ofRef cannot be cast to scala.collection.immutable.List
  at X$.apply(<console>:11)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
{code};;;","10/Dec/16 00:16;apachespark;User 'michalsenkyr' has created a pull request for this issue:
https://github.com/apache/spark/pull/16240;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
casting structs fails on Timestamp fields (interpreted mode only),SPARK-16791,12993471,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,eyalfa,eyalfa,eyalfa,29/Jul/16 07:51,01/Aug/16 14:48,14/Jul/23 06:29,01/Aug/16 14:45,1.6.1,2.0.0,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"When casting a struct with a Timestamp field, a MatchError is thrown (pasted below).
the root cause for this is in org.apache.spark.sql.catalyst.expressions.Cast#cast (https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala#L419).

case dt if dt == child.dataType => identity[Any]

should be modified to:

case dt if dt == from => identity[Any]

it seems to explode for timestamp because org.apache.spark.sql.catalyst.expressions.Cast#castToTimestamp does'nt have an identity check or fallback case in its pattern matching.

I'll shortly open a pull request with a failing test case and a fix.

Caused by: scala.MatchError: TimestampType (of class org.apache.spark.sql.types.TimestampType$)
	at org.apache.spark.sql.catalyst.expressions.Cast.castToTimestamp(Cast.scala:185)
	at org.apache.spark.sql.catalyst.expressions.Cast.org$apache$spark$sql$catalyst$expressions$Cast$$cast(Cast.scala:424)
	at org.apache.spark.sql.catalyst.expressions.Cast$$anonfun$1.apply(Cast.scala:403)
	at org.apache.spark.sql.catalyst.expressions.Cast$$anonfun$1.apply(Cast.scala:402)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.sql.catalyst.expressions.Cast.castStruct(Cast.scala:402)
	at org.apache.spark.sql.catalyst.expressions.Cast.org$apache$spark$sql$catalyst$expressions$Cast$$cast(Cast.scala:435)
	at org.apache.spark.sql.catalyst.expressions.Cast.cast$lzycompute(Cast.scala:443)
	at org.apache.spark.sql.catalyst.expressions.Cast.cast(Cast.scala:443)
	at org.apache.spark.sql.catalyst.expressions.Cast.nullSafeEval(Cast.scala:445)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:324)
	at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper$class.evaluate(ExpressionEvalHelper.scala:80)
	at org.apache.spark.sql.catalyst.expressions.CastSuite.evaluate(CastSuite.scala:33)
	... 58 more",,apachespark,cloud_fan,eyalfa,rxin,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 01 14:45:55 UTC 2016,,,,,,,,,,"0|i31nx3:",9223372036854775807,,,,,ueshin,,,,,,,,,,,,,,,,,,,"29/Jul/16 08:06;eyalfa;created pull request https://github.com/apache/spark/pull/14400;;;","29/Jul/16 08:06;apachespark;User 'eyalfa' has created a pull request for this issue:
https://github.com/apache/spark/pull/14400;;;","01/Aug/16 14:45;cloud_fan;Issue resolved by pull request 14400
[https://github.com/apache/spark/pull/14400];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkContext.addFile() should not fail if called twice with the same file,SPARK-16787,12993418,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,29/Jul/16 01:11,17/Jun/18 09:04,14/Jul/23 06:29,02/Aug/16 19:03,1.6.2,2.0.0,,,,,,,2.0.1,2.1.0,,,Spark Core,,,,,,,,0,,,,,,"The behavior of SparkContext.addFile() changed slightly with the introduction of the Netty-RPC-based file server, which was introduced in Spark 1.6 (where it was disabled by default) and became the default / only file server in Spark 2.0.0.

Prior to 2.0, calling SparkContext.addFile() twice with the same path would succeed and would cause future tasks to receive an updated copy of the file. This behavior was never explicitly documented but Spark has behaved this way since very early 1.x versions (some of the relevant lines in Executor.updateDependencies() have existed since 2012).

In 2.0 (or 1.6 with the Netty file server enabled), the second addFile() call will fail with a requirement error because NettyStreamManager tries to guard against duplicate file registration.

I believe that this change of behavior was unintentional and propose to remove the {{require}} check so that Spark 2.0 matches 1.x's default behavior.

This problem also affects addJar() in a more subtle way: the fileServer.addJar() call will also fail with an exception but that exception is logged and ignored due to some code which was added in 2014 in order to ignore errors caused by missing Spark examples JARs when running on YARN cluster mode (AFAIK).",,apachespark,igor.berman,joshrosen,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 17 09:04:01 UTC 2018,,,,,,,,,,"0|i31nlb:",9223372036854775807,,,,,,,,,,,,,2.0.1,,,,,,,,,,,"29/Jul/16 04:56;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14396;;;","02/Aug/16 19:03;joshrosen;Issue resolved by pull request 14396
[https://github.com/apache/spark/pull/14396];;;","17/Jun/18 09:04;igor.berman;[~joshrosen] I'm late for the party, but the new behaviour of adding file only once is a bit confusing with regard to spark.files.overwrite property

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dapply doesn't return array or raw columns,SPARK-16785,12993377,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,clarkfitzg,clarkfitzg,clarkfitzg,28/Jul/16 22:10,07/Sep/16 06:44,14/Jul/23 06:29,07/Sep/16 06:43,2.0.0,,,,,,,,2.0.1,2.1.0,,,SparkR,,,,,,,,0,,,,,,"Calling SparkR::dapplyCollect with R functions that return dataframes produces an error. This comes up when returning columns of binary data- ie. serialized fitted models. Also happens when functions return columns containing vectors.

The error message:

R computation failed with
 Error in (function (..., deparse.level = 1, make.row.names = TRUE, stringsAsFactors = default.stringsAsFactors())  :
  invalid list argument: all variables should have the same length

Reproducible example: https://github.com/clarkfitzg/phd_research/blob/master/ddR/spark/sparkR_dapplyCollect7.R

Relates to SPARK-16611",Mac OS X,apachespark,clarkfitzg,felixcheung,shivaram,sunrui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 07 06:43:03 UTC 2016,,,,,,,,,,"0|i31nc7:",9223372036854775807,,,,,shivaram,,,,,,,,,,,,,,,,,,,"28/Jul/16 22:18;clarkfitzg;[~shivaram] and I have had some email correspondence on this: 

bq. parallelizing data containing binary values: To create a SparkDataFrame from a local R data frame we first convert it to a list of rows and then partition the rows. This happens on the driver 1. The rows are then reconstructed back into a data.frame on the workers 2. The rbind.data.frame fails for the the binary example. I've created a small reproducible example at https://gist.github.com/shivaram/2f07208a8ebc0832098328fa0a3fac9d

1 https://github.com/apache/spark/blob/0869b3a5f028b64c2da511e70b02ab42f65fc949/R/pkg/R/SQLContext.R#L209
2 https://github.com/apache/spark/blob/0869b3a5f028b64c2da511e70b02ab42f65fc949/R/pkg/inst/worker/worker.R#L39;;;","28/Jul/16 22:51;clarkfitzg;To fix this I propose to treat the rows as a list of dataframes instead of as a list of lists:

{{rowlist = split(df, 1:nrow(df))}}
{{df2 = do.call(rbind, rowlist)}};;;","29/Jul/16 16:35;shivaram;[~clarkfitzg] Could you create a PR with this patch and a corresponding unit test ? I think this sounds fine to me as an approach but I am not sure if there are other parts of the code which rely on it being list of lists.  Also I just found that the PR we added mapply in (https://github.com/apache/spark/pull/9099) had some benchmark results  -- Would be great to see how the benchmarks look with this proposal;;;","29/Jul/16 22:03;clarkfitzg;Yes I can do a PR for this. It may take me a little while though.;;;","18/Aug/16 00:51;clarkfitzg;Starting on this now.;;;","19/Aug/16 07:55;clarkfitzg;Making some slow progress digging into this. Here's the failing test:
{code}
  df_listcols <- data.frame(key = 1:3)
  df_listcols$bytes <- lapply(df_listcols$key, serialize, connection = NULL)
  df_listcols_spark <- createDataFrame(df_listcols)
  result1 <- collect(df_listcols_spark)
  expect_identical(df_listcols, result1)
  result2 <- dapplyCollect(df_listcols_spark, function(x) x)   # FAILS HERE
  expect_equal(df_listcols, result2)
{code}

And the error message:

{code}
# R computation failed with
# Error in (function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE,  :
#  arguments imply differing number of rows: 3, 26
#        at org.apache.spark.api.r.RRunner.compute(RRunner.scala:108)
{code}

A separate but related issue with array columns that [~shivaram] has mentioned is that the dataframe can't be collected if this column is added:

{code}
  df_listcols$arr <- lapply(df_listcols$key,
                            function(x) seq(0, 1, length.out=15))
{code}

Will continue looking at this on Monday.;;;","19/Aug/16 07:57;clarkfitzg;Also my proposal above:

bq. to treat the rows as a list of dataframes instead of as a list of lists

did not work. Although I do think that approach has advantages- ie. do.call(rbind, list_of_rows) would always work correctly.;;;","24/Aug/16 06:22;apachespark;User 'clarkfitzg' has created a pull request for this issue:
https://github.com/apache/spark/pull/14783;;;","07/Sep/16 06:43;shivaram;Issue resolved by pull request 14783
[https://github.com/apache/spark/pull/14783];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java launched by PySpark as gateway may not be the same java used in the spark environment,SPARK-16781,12993332,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,mberman,mberman,28/Jul/16 18:34,24/Aug/16 19:05,14/Jul/23 06:29,24/Aug/16 19:04,1.6.2,,,,,,,,2.0.1,2.1.0,,,PySpark,,,,,,,,0,,,,,,"When launching spark on a system with multiple javas installed, there are a few options for choosing which JRE to use, setting `JAVA_HOME` being the most straightforward.

However, when pyspark's internal py4j launches its JavaGateway, it always invokes `java` directly, without qualification. This means you get whatever java's first on your path, which is not necessarily the same one in spark's JAVA_HOME.

This could be seen as a py4j issue, but from their point of view, the fix is easy: make sure the java you want is first on your path. I can't figure out a way to make that reliably happen through the pyspark executor launch path, and it seems like something that would ideally happen automatically. If I set JAVA_HOME when launching spark, I would expect that to be the only java used throughout the stack.",,apachespark,mberman,WeiqingYang,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17220,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 24 19:05:38 UTC 2016,,,,,,,,,,"0|i31n27:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/16 01:12;zjffdu;JAVA_HOME will be set by yarn, not sure about other cluster managers. ;;;","15/Aug/16 09:06;srowen;Yeah, I think this is something that's up to the execution environment, and thus an issue with py4j, or YARN, or your OS or whatever. I don't see what Spark can do differently?;;;","15/Aug/16 13:57;mberman;In 0.10.3, py4j introduced an option to use the java from JAVA_HOME instead of just launching a bare {{java}} command. So one thing PySpark could do to help with this situation would be to update to that version, and then pass {{java_path=None}} when launching the gateway.

(change: https://github.com/bartdag/py4j/commit/189684c);;;","22/Aug/16 08:54;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14748;;;","24/Aug/16 19:04;srowen;Issue resolved by pull request 14748
[https://github.com/apache/spark/pull/14748];;;","24/Aug/16 19:05;srowen;Updated to 0.10.3 to enable this to pick up JAVA_HOME;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Infinite recursion loop in org.apache.spark.sql.catalyst.trees.TreeNode when table name collides.,SPARK-16771,12993246,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,fpin,fpin,28/Jul/16 13:59,10/Nov/16 09:00,14/Jul/23 06:29,12/Aug/16 17:08,1.6.2,2.0.0,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"How to reproduce: 

In spark-sql on Hive

{code}
DROP TABLE IF EXISTS t1 ;
CREATE TABLE test.t1(col1 string) ;

WITH t1 AS (
  SELECT  col1
  FROM t1 
)
SELECT col1
FROM t1
LIMIT 2
;
{code}

This make a nice StackOverflowError:

{code}
java.lang.StackOverflowError
	at scala.collection.TraversableLike$class.builder$1(TraversableLike.scala:230)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:348)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionDown$1(QueryPlan.scala:156)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:166)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:170)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:170)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:175)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:175)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:144)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$CTESubstitution$$anonfun$substituteCTE$1.applyOrElse(Analyzer.scala:147)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$CTESubstitution$$anonfun$substituteCTE$1.applyOrElse(Analyzer.scala:133)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:279)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:279)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:284)
...
{code}

This does not happen if I change the name of the CTE.

I guess Catalyst get caught in an infinite recursion loop because the CTE and the source table have the same name.



",,apachespark,dongjoon,fpin,WeiqingYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 29 15:10:01 UTC 2016,,,,,,,,,,"0|i31mj3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Jul/16 04:46;dongjoon;Hi, [~fpin]. You're right. I can regenerate like the following.

{code}
spark.range(10).createOrReplaceTempView(""t"")
sql(""WITH t AS (SELECT 1 FROM t) SELECT * FROM t"")
{code}

{code}
spark.range(10).createOrReplaceTempView(""t1"")
spark.range(10).createOrReplaceTempView(""t2"")
sql(""WITH t1 AS (SELECT 1 FROM t2), t2 AS (SELECT 1 FROM t1) SELECT * FROM t1, t2"")
{code}

I'll make a PR soon.;;;","29/Jul/16 05:01;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/14397;;;","29/Jul/16 12:34;fpin;wow, that was quick, thanks a lot [~dongjoon] !;;;","29/Jul/16 15:10;dongjoon;It's my pleasure! BTW, the PR should be passed the review process. :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark shell not usable with german keyboard due to JLine version,SPARK-16770,12993243,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,stsc-pentasys,stsc-pentasys,stsc-pentasys,28/Jul/16 13:32,04/Aug/16 07:01,14/Jul/23 06:29,04/Aug/16 00:07,2.0.0,,,,,,,,2.0.1,2.1.0,,,Spark Shell,,,,,,,,0,,,,,,"It is impossible to enter a right square bracket with a single keystroke using a german keyboard layout. The problem is known from former Scala version, responsible is jline-2.12.jar (see https://issues.scala-lang.org/browse/SI-8759).
Workaround: Replace jline-2.12.jar by jline.2.12.1.jar in the jars folder.",,apachespark,stsc-pentasys,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 04 07:01:35 UTC 2016,,,,,,,,,,"0|i31mif:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/16 19:23;srowen;Sure, if you would please open a pull request to update the dependency. Check 'mvn dependency:tree' to make sure it causes all instances of the dependency to be updated.;;;","31/Jul/16 13:27;apachespark;User 'stsc-pentasys' has created a pull request for this issue:
https://github.com/apache/spark/pull/14429;;;","04/Aug/16 00:07;srowen;Issue resolved by pull request 14429
[https://github.com/apache/spark/pull/14429];;;","04/Aug/16 07:01;stsc-pentasys;JLine dependency updated, works properly with german keyboard.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade derby to 10.12.1.1 from 10.11.1.1,SPARK-16751,12992918,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aroberts,aroberts,aroberts,27/Jul/16 12:52,29/Jul/16 13:18,14/Jul/23 06:29,29/Jul/16 11:46,1.3.1,1.4.1,1.5.2,1.6.2,2.0.0,,,,1.6.3,2.0.1,2.1.0,,Build,,,,,,,,0,,,,,,"This JIRA is to upgrade the derby version from 10.11.1.1 to 10.12.1.1

Sean and I figured that we only use derby for tests and so the initial pull request was to not include it in the jars folder for Spark. I now believe it is required based on comments for the pull request and so this is only a dependency upgrade.

The upgrade is due to an already disclosed vulnerability (CVE-2015-1832) in derby 10.11.1.1. We used https://www.versioneye.com/search and will be checking for any other problems in a variety of libraries too: investigating if we can set up a Jenkins job to check our pom on a regular basis so we can stay ahead of the game for matters like this.

This was raised on the mailing list at http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-2-0-0-RC5-tp18367p18465.html by Stephen Hellberg and replied to by Sean Owen.

I've checked the impact to previous Spark releases and this particular version of derby is the only relatively recent and without vulnerabilities version (I checked up to the 1.3 branch) so ideally we'd backport this for all impacted Spark releases.

I've marked this as critical and ticked the important checkbox as it's going to impact every user, there isn't a security component (should we add one?) and hence the build tag.",All platforms and major Spark releases,apachespark,aroberts,asukhenko,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Fri Jul 29 13:18:08 UTC 2016,,,,,,,,,,"0|i31ki7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/16 12:54;apachespark;User 'a-roberts' has created a pull request for this issue:
https://github.com/apache/spark/pull/14379;;;","27/Jul/16 23:23;srowen;We can do this, sure, but why is it Critical? tests have zero surface area for users, because users don't even consume tests.;;;","28/Jul/16 07:36;aroberts;But they do download the jar which contains a CVE, so by either developing Spark or downloading it they have access to the attacks available with the CVE

-> Adam works for company X, Adam's company distributes Spark to its customers
-> Its customers now have this vulnerable version of derby on their system
-> A disgruntled employee at such a company decides to use their Spark install (derby 10.11.x being on the classpath) to cause damage outlined in the CVE info
-> Alternatively a user has access to a Spark service hosted on an external system which again will have the vulnerable derby on the classpath and can therefore cause damage outlined in the CVE;;;","28/Jul/16 17:31;srowen;Yeah, I see from the PR that it's actually packaged. I don't know that it actually surfaces the attack vector in Spark in practice, but, let's just apply this of course.;;;","29/Jul/16 11:46;srowen;Issue resolved by pull request 14379
[https://github.com/apache/spark/pull/14379];;;","29/Jul/16 13:18;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14403;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ML GaussianMixture training failed due to feature column type mistake,SPARK-16750,12992868,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yanboliang,yanboliang,yanboliang,27/Jul/16 09:43,13/Sep/16 08:17,14/Jul/23 06:29,29/Jul/16 11:40,,,,,,,,,2.0.1,2.1.0,,,ML,,,,,,,,0,,,,,,"ML GaussianMixture training failed due to feature column type mistake. The feature column type should be {{ml.linalg.VectorUDT}} but got {{mllib.linalg.VectorUDT}} by mistake.
This bug is easy to reproduce by the following code:
{code}
val df = spark.createDataFrame(
  Seq(
    (1, Vectors.dense(0.0, 1.0, 4.0)),
    (2, Vectors.dense(1.0, 0.0, 4.0)),
    (3, Vectors.dense(1.0, 0.0, 5.0)),
    (4, Vectors.dense(0.0, 0.0, 5.0)))
).toDF(""id"", ""features"")

val scaler = new MinMaxScaler()
  .setInputCol(""features"")
  .setOutputCol(""features_scaled"")
  .setMin(0.0)
  .setMax(5.0)

val gmm = new GaussianMixture()
  .setFeaturesCol(""features_scaled"")
  .setK(2)

val pipeline = new Pipeline().setStages(Array(scaler, gmm))
pipeline.fit(df)

requirement failed: Column features_scaled must be of type org.apache.spark.mllib.linalg.VectorUDT@f71b0bce but was actually org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7.
java.lang.IllegalArgumentException: requirement failed: Column features_scaled must be of type org.apache.spark.mllib.linalg.VectorUDT@f71b0bce but was actually org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7.
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)
	at org.apache.spark.ml.clustering.GaussianMixtureParams$class.validateAndTransformSchema(GaussianMixture.scala:64)
	at org.apache.spark.ml.clustering.GaussianMixture.validateAndTransformSchema(GaussianMixture.scala:275)
	at org.apache.spark.ml.clustering.GaussianMixture.transformSchema(GaussianMixture.scala:342)
	at org.apache.spark.ml.Pipeline$$anonfun$transformSchema$4.apply(Pipeline.scala:180)
	at org.apache.spark.ml.Pipeline$$anonfun$transformSchema$4.apply(Pipeline.scala:180)
	at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)
	at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)
	at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:186)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:180)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:70)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:132)
{code}
Why the unit tests did not complain this errors? Because some estimators/transformers missed calling {{transformSchema(dataset.schema)}} firstly during {{fit}} or {{transform}}. I will also add this function to all estimators/transformers who missed.",,apachespark,silversurfer,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 13 08:17:15 UTC 2016,,,,,,,,,,"0|i31k73:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/16 09:53;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/14378;;;","29/Jul/16 11:40;srowen;Issue resolved by pull request 14378
[https://github.com/apache/spark/pull/14378];;;","05/Aug/16 16:09;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/14455;;;","12/Sep/16 22:48;silversurfer;Did this fix make it to the release version 2.0.0. If not, is there a work around for the same ?;;;","13/Sep/16 00:07;silversurfer;It seems the release branch was cut on 19th July and this change made it post that. Any work around guys ?;;;","13/Sep/16 08:17;srowen;You can see above that it is resolved for 2.0.1, not 2.0.0. You can also look at what branches / tags contain the commit. Not sure what you mean by workaround -- no, you need a version of Spark with this fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Errors thrown by UDFs cause TreeNodeException when the query has an ORDER BY clause,SPARK-16748,12992802,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,yhuai,yhuai,27/Jul/16 05:19,30/Jul/16 03:00,14/Jul/23 06:29,30/Jul/16 03:00,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"{code}
import org.apache.spark.sql.functions._
val myUDF = udf((c: String) => s""""""${c.take(5)}"""""")
spark.sql(""SELECT cast(null as string) as a"").select(myUDF($""a"").as(""b"")).orderBy($""b"").collect
{code}

{code}
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
Exchange rangepartitioning(b#345 ASC, 200)
+- *Project [UDF(null) AS b#345]
   +- Scan OneRowRelation[]

	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:50)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:113)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:233)
	at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:113)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:361)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:225)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:272)
	at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2183)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2182)
	at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2187)
	at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2187)
	at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2187)
	at org.apache.spark.sql.Dataset.collect(Dataset.scala:2163)
{code}",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 30 03:00:32 UTC 2016,,,,,,,,,,"0|i31jsf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/16 05:22;yhuai;Seems we should should not wrap the NPE in a TreeNodeException.;;;","29/Jul/16 02:10;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/14395;;;","30/Jul/16 03:00;yhuai;Issue resolved by pull request 14395
[https://github.com/apache/spark/pull/14395];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
joins.LongToUnsafeRowMap crashes with NegativeArraySizeException,SPARK-16740,12992698,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sylvinus,sylvinus,sylvinus,26/Jul/16 19:32,31/Oct/16 12:38,14/Jul/23 06:29,28/Jul/16 16:52,2.0.0,,,,,,,,2.0.1,2.1.0,,,PySpark,Spark Core,SQL,,,,,,0,,,,,,"Hello,

Here is a crash in Spark SQL joins, with a minimal reproducible test case. Interestingly, it only seems to happen when reading Parquet data (I added a {{crash = True}} variable to show it)

This is an {{left_outer}} example, but it also crashes with a regular {{inner}} join.

{code}
import os

from pyspark import SparkContext
from pyspark.sql import types as SparkTypes
from pyspark.sql import SQLContext

sc = SparkContext()
sqlc = SQLContext(sc)

schema1 = SparkTypes.StructType([
    SparkTypes.StructField(""id1"", SparkTypes.LongType(), nullable=True)
])
schema2 = SparkTypes.StructType([
    SparkTypes.StructField(""id2"", SparkTypes.LongType(), nullable=True)
])

# Valid Long values (-9223372036854775808 < -5543241376386463808 , 4661454128115150227 < 9223372036854775807)
data1 = [(4661454128115150227,), (-5543241376386463808,)]
data2 = [(650460285, )]

df1 = sqlc.createDataFrame(sc.parallelize(data1), schema1)
df2 = sqlc.createDataFrame(sc.parallelize(data2), schema2)

crash = True
if crash:
    os.system(""rm -rf /tmp/sparkbug"")
    df1.write.parquet(""/tmp/sparkbug/vertex"")
    df2.write.parquet(""/tmp/sparkbug/edge"")

    df1 = sqlc.read.load(""/tmp/sparkbug/vertex"")
    df2 = sqlc.read.load(""/tmp/sparkbug/edge"")

result_df = df2.join(df1, on=(df1.id1 == df2.id2), how=""left_outer"")

# Should print [Row(id2=650460285, id1=None)]
print result_df.collect()
{code}

When ran with {{spark-submit}}, the final {{collect()}} call crashes with this:

{code}
py4j.protocol.Py4JJavaError: An error occurred while calling o61.collectToPython.
: org.apache.spark.SparkException: Exception thrown in awaitResult:
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:194)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:120)
	at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:229)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:125)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:125)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:124)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:98)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenOuter(BroadcastHashJoinExec.scala:242)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.BatchedDataSourceScanExec.consume(ExistingRDD.scala:225)
	at org.apache.spark.sql.execution.BatchedDataSourceScanExec.doProduce(ExistingRDD.scala:328)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.BatchedDataSourceScanExec.produce(ExistingRDD.scala:225)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:309)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:347)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)
	at org.apache.spark.sql.Dataset.javaToPython(Dataset.scala:2507)
	at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:2513)
	at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2513)
	at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2513)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:2512)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:211)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NegativeArraySizeException
	at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap.optimize(HashedRelation.scala:619)
	at org.apache.spark.sql.execution.joins.LongHashedRelation$.apply(HashedRelation.scala:806)
	at org.apache.spark.sql.execution.joins.HashedRelation$.apply(HashedRelation.scala:105)
	at org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:816)
	at org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:812)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:90)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:72)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:94)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more
{code}",,apachespark,dongjoon,harishk15,kiszk,lianhuiwang,naegelejd,sylvinus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 31 00:58:56 UTC 2016,,,,,,,,,,"0|i31j5b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/16 19:54;sylvinus;I'm not a Scala expert but from a quick review of the code, it appears that it's easy to overflow the {{range}} variable in {{optimize()}}:
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala#L608

In my example, that would yield
{code}
scala> val range = 4661454128115150227L - (-5543241376386463808L)
range: Long = -8242048569207937581
{code}

Maybe we should add {{range >= 0}} as a condition of doing that optimization?;;;","26/Jul/16 21:18;dongjoon;Hi, [~sylvinus]. 
It looks like that. Could you make a PR for this issue?;;;","26/Jul/16 21:19;dongjoon;Maybe, you will modify the following line?
{code}
val range = maxKey - minKey
{code};;;","26/Jul/16 22:00;apachespark;User 'sylvinus' has created a pull request for this issue:
https://github.com/apache/spark/pull/14373;;;","26/Jul/16 22:00;sylvinus;Thanks! I just did. Let me know if that's okay.;;;","26/Jul/16 22:03;dongjoon;You had better look up the one who made that code with `git blame` command and ask him after passing Jenkins.
That is the fastest way to get reviewed. :);;;","26/Jul/16 22:09;sylvinus;OK! Looks like that would be [~davies] :-);;;","30/Oct/16 22:49;harishk15;is this fix is available in 2.0.2 snapshot?. Please confirm;;;","31/Oct/16 00:05;dongjoon;Hi, [~harishk15]

Yep. The patch is still there in branch-2.0. I guess you can test that with Spark 2.0.2-rc1, too.

If you think you meet some related issue in 2.0.2-rc1, please file a Jira issue.;;;","31/Oct/16 00:58;harishk15;Thank you. I downloaded the 2.0.2 snapshot with 2.7 Hadoop (i think its on 10/13). I can still reproduce this issue. If the ""2.0.2-rc1"" was updated after 10/13 then i will take the updates and try. Can you please help me to find the latest download path.?
I am going to try 2.0.3 snap shot from below location -- any suggestions?
http://people.apache.org/~pwendell/spark-nightly/spark-branch-2.0-bin/latest/spark-2.0.3-SNAPSHOT-bin-hadoop2.7.tgz;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unused codes in subexpressionEliminationForWholeStageCodegen,SPARK-16732,12992515,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yucai,yucai,yucai,26/Jul/16 08:35,01/Sep/16 21:12,14/Jul/23 06:29,30/Jul/16 04:16,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,Some codes in subexpressionEliminationForWholeStageCodegen are never used actually. Remove them using this jira.,,apachespark,yucai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 26 08:43:04 UTC 2016,,,,,,,,,,"0|i31i0n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/16 08:43;apachespark;User 'yucai' has created a pull request for this issue:
https://github.com/apache/spark/pull/14366;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 2.0 breaks various Hive cast functions,SPARK-16730,12992474,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petermaxlee,petermaxlee,petermaxlee,26/Jul/16 03:52,28/Jul/16 06:11,14/Jul/23 06:29,28/Jul/16 06:11,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"In Spark 1.x, it is possible to use ""int"", ""string"", and other functions to perform type cast. This functionality is broken in Spark 2.0, because Spark no longer falls back to Hive for these functions.",,apachespark,cloud_fan,michaelmalak,petermaxlee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 28 06:11:36 UTC 2016,,,,,,,,,,"0|i31hrj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/16 04:54;apachespark;User 'petermaxlee' has created a pull request for this issue:
https://github.com/apache/spark/pull/14362;;;","26/Jul/16 07:40;apachespark;User 'petermaxlee' has created a pull request for this issue:
https://github.com/apache/spark/pull/14364;;;","28/Jul/16 06:11;cloud_fan;Issue resolved by pull request 14364
[https://github.com/apache/spark/pull/14364];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark should throw analysis exception for invalid casts to date type,SPARK-16729,12992456,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petermaxlee,petermaxlee,petermaxlee,26/Jul/16 02:06,27/Jul/16 15:11,14/Jul/23 06:29,27/Jul/16 08:05,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"Spark currently throws exceptions for invalid casts for all other data types except date type. Somehow date type returns null. It should be consistent and throws analysis exception as well.
",,apachespark,cloud_fan,petermaxlee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 27 08:05:50 UTC 2016,,,,,,,,,,"0|i31hnj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/16 02:16;apachespark;User 'petermaxlee' has created a pull request for this issue:
https://github.com/apache/spark/pull/14358;;;","27/Jul/16 08:05;cloud_fan;Issue resolved by pull request 14358
[https://github.com/apache/spark/pull/14358];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose DefinedByConstructorParams,SPARK-16724,12992441,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,25/Jul/16 23:58,26/Jul/16 03:41,14/Jul/23 06:29,26/Jul/16 03:41,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,Generally we don't mark things in catalyst/execution as private.  Instead they are not included in scala doc as they are not considered stable APIs,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 26 01:04:05 UTC 2016,,,,,,,,,,"0|i31hk7:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"26/Jul/16 01:04;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/14356;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lead/lag needs to respect nulls ,SPARK-16721,12992427,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,25/Jul/16 23:14,31/Aug/16 21:38,14/Jul/23 06:29,26/Jul/16 03:59,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,correctness,,,,,"Seems 2.0.0 changes the behavior of lead and lag to ignore nulls. This PR is changing the behavior back to 1.6's behavior, which is respecting nulls.

For example 
{code}
SELECT
b,
lag(a, 1, 321) OVER (ORDER BY b) as lag,
lead(a, 1, 321) OVER (ORDER BY b) as lead
FROM (SELECT cast(null as int) as a, 1 as b
UNION ALL
select cast(null as int) as id, 2 as b) tmp
{code}
This query should return 
{code}
+---+----+----+
|  b| lag|lead|
+---+----+----+
|  1| 321|null|
|  2|null| 321|
+---+----+----+
{code}
instead of 
{code}
+---+---+----+
|  b|lag|lead|
+---+---+----+
|  1|321| 321|
|  2|321| 321|
+---+---+----+
{code}",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 26 03:59:30 UTC 2016,,,,,,,,,,"0|i31hh3:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"25/Jul/16 23:15;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/14284;;;","26/Jul/16 03:59;yhuai;Issue resolved by pull request 14284
[https://github.com/apache/spark/pull/14284];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix a potential ExprId conflict for SubexpressionEliminationSuite.""Semantic equals and hash""",SPARK-16715,12992390,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,25/Jul/16 20:33,27/Jul/16 00:53,14/Jul/23 06:29,25/Jul/16 23:09,,,,,,,,,2.0.1,2.1.0,,,Tests,,,,,,,,0,,,,,,"SubexpressionEliminationSuite.""Semantic equals and hash"" assumes the default AttributeReference's exprId wont' be ""ExprId(1)"". However, that depends on when this test runs. It may happen to use ""ExprId(1)"".",,apachespark,biglobster,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 27 00:53:43 UTC 2016,,,,,,,,,,"0|i31h8v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/16 20:37;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/14350;;;","26/Jul/16 23:46;apachespark;User 'biglobster' has created a pull request for this issue:
https://github.com/apache/spark/pull/14374;;;","27/Jul/16 00:53;biglobster;Sorry, mistake the jira id.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to create a decimal arrays with literals having different inferred precessions and scales,SPARK-16714,12992387,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,yhuai,yhuai,25/Jul/16 20:23,18/Aug/16 04:45,14/Jul/23 06:29,03/Aug/16 18:15,2.0.0,,,,,,,,2.0.1,2.1.0,,,,,,,,,,,1,,,,,,"In Spark 2.0, we will parse float literals as decimals. However, it introduces a side-effect, which is described below. 
 
{code}
select array(0.001, 0.02)
{code}
causes
{code}
org.apache.spark.sql.AnalysisException: cannot resolve 'array(CAST(0.001 AS DECIMAL(3,3)), CAST(0.02 AS DECIMAL(2,2)))' due to data type mismatch: input to function array should all be the same type, but it's [decimal(3,3), decimal(2,2)]; line 1 pos 7
{code}",,apachespark,dongjoon,kiszk,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 18 04:45:03 UTC 2016,,,,,,,,,,"0|i31h87:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"25/Jul/16 22:30;dongjoon;Hi, [~yhuai].
May I create a PR for this?;;;","25/Jul/16 22:35;yhuai;Sure. Thank you!;;;","25/Jul/16 22:39;dongjoon;Thank you! :);;;","25/Jul/16 22:51;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/14353;;;","28/Jul/16 07:38;apachespark;User 'petermaxlee' has created a pull request for this issue:
https://github.com/apache/spark/pull/14389;;;","01/Aug/16 14:34;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/14439;;;","03/Aug/16 18:15;yhuai;Issue resolved by pull request 14439
[https://github.com/apache/spark/pull/14439];;;","03/Aug/16 18:20;yhuai;[~petermaxlee] Can you create a new jira for your pr? I just merged [~cloud_fan]'s quick fix.;;;","18/Aug/16 04:45;apachespark;User 'petermaxlee' has created a pull request for this issue:
https://github.com/apache/spark/pull/14696;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YarnShuffleService doesn't re-init properly on YARN rolling upgrade,SPARK-16711,12992314,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,25/Jul/16 15:42,17/May/20 18:14,14/Jul/23 06:29,02/Sep/16 17:43,1.5.2,,,,,,,,2.0.1,2.1.0,,,Shuffle,Spark Core,YARN,,,,,,0,,,,,,"When a yarn rolling upgrade happens the Spark YarnShuffleService isn't re-initializing the tokens soon enough which causes running applications to fail with NullPointerExceptions rather then IOExceptions which causes clients to not retry which in turn causes the application to totally fail when it should have just retried and succeeded.

2016-07-22 23:22:05,460 [shuffle-server-1] ERROR server.TransportRequestHandler: Error while invoking RpcHandler#receive() on RPC id 6235606084052282795
java.lang.NullPointerException: Password cannot be null if SASL is enabled
        at org.spark-project.guava.base.Preconditions.checkNotNull(Preconditions.java:208)
        at org.apache.spark.network.sasl.SparkSaslServer.encodePassword(SparkSaslServer.java:196)
        at org.apache.spark.network.sasl.SparkSaslServer$DigestCallbackHandler.handle(SparkSaslServer.java:166)
        at com.sun.security.sasl.digest.DigestMD5Server.validateClientResponse(DigestMD5Server.java:589)
        at com.sun.security.sasl.digest.DigestMD5Server.evaluateResponse(DigestMD5Server.java:244)
        at org.apache.spark.network.sasl.SparkSaslServer.response(SparkSaslServer.java:119)
        at org.apache.spark.network.sasl.SaslRpcHandler.receive(SaslRpcHandler.java:101)
        at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:149)
        at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:102)
        at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:104)
        at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
        at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:254)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
        at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:86)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:787)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:130)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
     at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
        at java.lang.Thread.run(Thread.java:745)
",,apachespark,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 07 16:22:09 UTC 2016,,,,,,,,,,"0|i31grz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/16 15:45;tgraves;Note that what happens here is the yarnshuffleservice  inits and immediately opens the port so clients can connect immediately.  Really we should have read everything from our back up database and repopulated things before opening the port.  I think we are currently relying on the Nodemanager calling initializeApplication again, but this is to late, by the time this happens the client could get the NullPointerException and fail.;;;","25/Jul/16 15:52;tgraves;Note this happens when security is on.  The ExecutorShuffleInfo is what we store in the DB, which doesn't looks like it has the secrets at all so we have to rely on the NM initializeApplication calls, but we don't know if/when those will happen so we may need to to storing those.;;;","19/Aug/16 14:24;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/14718;;;","07/Sep/16 16:22;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/14997;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extra space in WindowSpecDefinition SQL representation,SPARK-16703,12992138,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lian cheng,lian cheng,lian cheng,25/Jul/16 05:33,25/Jul/16 16:43,14/Jul/23 06:29,25/Jul/16 16:43,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"For a {{WindowSpecDefinition}} whose {{partitionSpec}} is empty, there's an extra space in its SQL representation:

{code:sql}
( ORDER BY `a` ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)
{code}",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 25 05:35:05 UTC 2016,,,,,,,,,,"0|i31fov:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"25/Jul/16 05:35;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/14334;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StructType doesn't accept Python dicts anymore,SPARK-16700,12992127,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,sylvinus,sylvinus,25/Jul/16 01:44,25/Aug/16 21:59,14/Jul/23 06:29,15/Aug/16 19:41,2.0.0,,,,,,,,2.0.1,2.1.0,,,PySpark,,,,,,,,1,releasenotes,,,,,"Hello,

I found this issue while testing my codebase with 2.0.0-rc5

StructType in Spark 1.6.2 accepts the Python <dict> type, which is very handy. 2.0.0-rc5 does not and throws an error.

I don't know if this was intended but I'd advocate for this behaviour to remain the same. MapType is probably wasteful when your key names never change and switching to Python tuples would be cumbersome.

Here is a minimal script to reproduce the issue: 

{code}
from pyspark import SparkContext
from pyspark.sql import types as SparkTypes
from pyspark.sql import SQLContext


sc = SparkContext()
sqlc = SQLContext(sc)

struct_schema = SparkTypes.StructType([
    SparkTypes.StructField(""id"", SparkTypes.LongType())
])

rdd = sc.parallelize([{""id"": 0}, {""id"": 1}])

df = sqlc.createDataFrame(rdd, struct_schema)

print df.collect()

# 1.6.2 prints [Row(id=0), Row(id=1)]

# 2.0.0-rc5 raises TypeError: StructType can not accept object {'id': 0} in type <type 'dict'>

{code}

Thanks!",,apachespark,davies,jaycode,joshrosen,rxin,sylvinus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 15 19:41:49 UTC 2016,,,,,,,,,,"0|i31fmf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/16 16:16;sylvinus;I dug into this a bit more: 

{{_verify_type({}, struct_schema)}} was already raising a similar exception in Spark 1.6.2, however schema validation wasn't being enforced at all by {{createDataFrame}} : https://github.com/apache/spark/blob/branch-1.6/python/pyspark/sql/context.py#L418

In 2.0.0, it seems that it is done over each row of the data:
https://github.com/apache/spark/blob/master/python/pyspark/sql/session.py#L504

I think there are 2 issues that should be fixed here:
 - {{_verify_type({}, struct_schema)}} shouldn't raise, because as far as I can tell dicts behave as expected and have their items correctly mapped as struct fields.
 - There should be a way to go back to 1.6.x-like behaviour and disable schema verification in {{createDataFrame}}. The {{prepare()}} function is being map()'d over all the data coming from Python, which I think will definitely hurt performance for large datasets and complex schemas. Leaving it on by default but adding a flag to disable it would be a good solution. Without this users will probably have to implement their own {{createDataFrame}} function like I did.
;;;","30/Jul/16 12:32;jaycode;When using `Row` object, but with multiple struct types, also returns similar error:

{code}
_struct = [
  SparkTypes.StructField('string_field', SparkTypes.StringType(), True),
  SparkTypes.StructField('long_field', SparkTypes.LongType(), True),
  SparkTypes.StructField('double_field', SparkTypes.DoubleType(), True)
]
_rdd = sc.parallelize([Row(string_field='1', long_field=1, double_field=1.1)])

## Both methods do not work:
# _schema = SparkTypes.StructType()
# for _s in _struct:
#   _schema.add(_s)
_schema = SparkTypes.StructType(_struct)

_df = sqlContext.createDataFrame(_rdd, schema=_schema)
_df.take(1)
{code}

Returned error:

{code}
DoubleType can not accept object '1' in type <type 'str'>
{code};;;","01/Aug/16 20:18;davies;There are two separate problems here:

1) Spark 2.0 enforce data type checking when creating a DataFrame, it's safer but slower. It makes sense to have a flag for that (on by default)

2) Row object is similar to named tuple (not dict), the columns are ordered. When it's created in a way like dict, we have no way to know the order of columns, so they are sorted by name, then it does not match with the schema provided. We should check the schema (order of columns) when create a DataFrame from RDD of Row (we assume they matched);;;","02/Aug/16 23:51;davies;Sent PR https://github.com/apache/spark/pull/14469 to address these, could you help to test and review them?;;;","02/Aug/16 23:51;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/14469;;;","03/Aug/16 22:43;sylvinus;The verifySchema flag works great, and the {{dict}} issue seems to be fixed for me. Thanks a lot!!;;;","15/Aug/16 19:41;joshrosen;Issue resolved by pull request 14469
[https://github.com/apache/spark/pull/14469];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix performance bug in hash aggregate on long string keys,SPARK-16699,12992116,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,qifan,qifan,qifan,24/Jul/16 22:28,25/Jul/16 04:58,14/Jul/23 06:29,25/Jul/16 04:53,2.0.0,,,,,,,,2.0.1,2.1.0,,,Spark Core,,,,,,,,0,,,,,,"In the following code in `VectorizedHashMapGenerator.scala`:

{code}
    def hashBytes(b: String): String = {
      val hash = ctx.freshName(""hash"")
      s""""""
         |int $result = 0;
         |for (int i = 0; i < $b.length; i++) {
         |  ${genComputeHash(ctx, s""$b[i]"", ByteType, hash)}
         |  $result = ($result ^ (0x9e3779b9)) + $hash + ($result << 6) + ($result >>> 2);
         |}
       """""".stripMargin
    }
{code}

when b=input.getBytes(), the current 2.0 code results in getBytes() being called n times, n being length of input. getBytes() involves memory copy is thus expensive and causes a performance degradation.
Fix is to evaluate getBytes() before the for loop.",,apachespark,dongjoon,kiszk,qifan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 25 01:37:13 UTC 2016,,,,,,,,,,"0|i31fjz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/16 22:45;apachespark;User 'ooq' has created a pull request for this issue:
https://github.com/apache/spark/pull/14337;;;","25/Jul/16 01:37;dongjoon;Hi, [~qifan].
Nice catch! By the way, usually, only committers set `FIX VERSION` field.
You had better leave it blank next time. :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"json parsing regression - ""."" in keys",SPARK-16698,12992109,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,TPolzer,TPolzer,24/Jul/16 19:01,12/Dec/22 18:10,14/Jul/23 06:29,25/Jul/16 14:53,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"The commit 83775bc78e183791f75a99cdfbcd68a67ca0d472 ""\[SPARK-14158]\[SQL] implement buildReader for json data source"" breaks parsing of json files with ""."" in keys.

E.g. the test input for spark-solr https://github.com/lucidworks/spark-solr/blob/master/src/test/resources/test-data/events.json

{noformat}
scala> sqlContext.read.json(""src/test/resources/test-data/events.json"").collectAsList
org.apache.spark.sql.AnalysisException: Unable to resolve params.title_s given [_version_, count_l, doc_id_s, flag_s, id, params.title_s, params.url_s, session_id_s, timestamp_tdt, type_s, tz_timestamp_txt, user_id_s];
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolve$1$$anonfun$apply$5.apply(LogicalPlan.scala:131)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolve$1$$anonfun$apply$5.apply(LogicalPlan.scala:131)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolve$1.apply(LogicalPlan.scala:130)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolve$1.apply(LogicalPlan.scala:126)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
  at scala.collection.Iterator$class.foreach(Iterator.scala:742)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1194)
  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
  at org.apache.spark.sql.types.StructType.foreach(StructType.scala:94)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
  at org.apache.spark.sql.types.StructType.map(StructType.scala:94)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:126)
  at org.apache.spark.sql.execution.datasources.FileSourceStrategy$.apply(FileSourceStrategy.scala:80)
  at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)
  at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)
  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:396)
  at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:59)
  at org.apache.spark.sql.catalyst.planning.QueryPlanner.planLater(QueryPlanner.scala:54)
  at org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$.apply(SparkStrategies.scala:53)
  at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)
  at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)
  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:396)
  at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:59)
  at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:52)
  at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:50)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:57)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:57)
  at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2321)
  at org.apache.spark.sql.Dataset.collectAsList(Dataset.scala:2040)
  ... 49 elided
{noformat}",,apachespark,krisden,lian cheng,mathieude,TPolzer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17232,SPARK-17341,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 25 14:53:23 UTC 2016,,,,,,,,,,"0|i31fif:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/16 00:45;gurwls223;FYI, this does not happen when it is read from json RDD. Let me please leave a code to reproduce that self-contains the issue.

This does not work (in `JsonSuite.scala`)

{code}
test(""SPARK-16698 - json parsing regression - ""."" in keys"") {
  withTempPath { path =>
    val json ="""""" {""a.b"":""data""}""""""
    spark.sparkContext
      .parallelize(json :: Nil)
      .saveAsTextFile(path.getAbsolutePath)
    spark.read.json(path.getAbsolutePath).collect()
  }
}
{code}

This works

{code}
test(""SPARK-16698 - json parsing regression - ""."" in keys"") {
  withTempPath { path =>
    val json ="""""" {""a.b"":""data""}""""""
    val rdd = spark.sparkContext
      .parallelize(json :: Nil)
    spark.read.json(rdd).collect()
  }
}
{code}
;;;","25/Jul/16 01:26;gurwls223;It seems it does not work for all `FileFormat` data sources. The code below also does not work.

{code}
test(""SPARK-16698 - csv parsing regression - ""."" in keys"") {
  withTempPath { path =>
    val csv =""""""
        |a.b
        |123
      """""".stripMargin
    spark.sparkContext
      .parallelize(csv :: Nil)
      .saveAsTextFile(path.getAbsolutePath)
    spark.read
      .option(""inferSchema"", ""true"")
      .option(""header"", ""true"")
      .csv(path.getAbsolutePath).select(""`a.b`"").show()
  }
}
{code}

This is related with https://github.com/apache/spark/blob/37f3be5d29192db0a54f6c4699237b149bd0ecae/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala#L88-L89

;;;","25/Jul/16 02:35;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/14339;;;","25/Jul/16 14:53;lian cheng;Issue resolved by pull request 14339
[https://github.com/apache/spark/pull/14339];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove R deprecated methods,SPARK-16693,12992059,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,23/Jul/16 22:19,27/Oct/18 22:12,14/Jul/23 06:29,27/Oct/18 22:12,2.0.0,,,,,,,,3.0.0,,,,SparkR,,,,,,,,0,,,,,,"For methods deprecated in Spark 2.0.0, we should remove them in 2.1.0 -> 3.0.0",,apachespark,dongjoon,felixcheung,markhamstra,rxin,shivaram,sunrui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 26 04:07:43 UTC 2018,,,,,,,,,,"0|i31f7b:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,,"23/Jul/16 22:23;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/14330;;;","23/Jul/16 23:05;markhamstra;The 2.1.0 release is the very earliest that we can allow removal of things first deprecated in 2.0.0.  That is different from saying that we *should* remove those things in 2.1.0.  The decision as to exactly when we should and will remove things first deprecated in 2.0.0 needs more discussion. ;;;","25/Jul/16 18:57;shivaram;Yeah lets hold off on this for now. For one, these functions aren't blocking other features (like say the CRAN checks). Also we should wait to hear from users if they prefer to use the deprecated function for some reason.  We can do a discussion on dev/user@spark say a month or so after 2.0 gets pushed out and then do this PR just before 2.1. 

;;;","25/Jul/16 19:05;markhamstra;As much as makes sense and is possible, we should also strive to coordinate removal of stuff deprecated in 2.0.0 of SparkR with removal of stuff deprecated elsewhere in Spark 2.0.0.  It's just easier for all parties concerned to be able to say that all the things were removed as of release x.y.z than to have Spark developers and users need to keep track that SparkR removed stuff in release x.y.a, Spark SQL removed stuff in x.y.b, etc.;;;","25/Jul/16 19:23;felixcheung;Deprecated methods because of Spark JVM API changes are only a very small subset (actually, there's only 1: registerTempTable), agreed we should coordinate with JVM side if we could.
There are bigger changes to SparkR specific APIs (sqlContext parameter removal, sparkR.init) I think would be very good to clean up soon. They present changes to behavior they would become harder to maintain over time.
SparkR API has been ""experimental"" so I didn't think there would be a big problem, but we could certainly wait to get some feedback.;;;","22/Jan/17 00:46;felixcheung;I want to bring this up again and potentially to discuss in dev@, now that we are on Spark 2.2.

Not only having these are harder to work with, making it harder to add new parameter (eg. numPartitions) and so on, but more importantly, the wrapper/stub methods (e.g createDataFrame.default) shows up in auto-complete, tooltips, help doc and so on and create confusion.

Moreover, on a slightly orthogonal note, we should also consider removing all the internal RDD methods, or at least making them non-export S3 methods. Right now every time we are adding a new method having the same name of the existing, internal-only RDD method in R we would need to update the generic, and rename the existing method (by appending ""RDD"" to its name), and all its call sites, otherwise we would get a check-cran warning of lacking documentation. And by adding such method to the NAMESPACE file the existing RDD-only will get exposed as well (again showing up in auto-complete etc.), unless it is renamed. Since we are renaming the existing method to fix this, we are also breaking backward compatibility anyway (although the method was not public so strictly speaking there has not been any guarantee).

But I could scope this JIRA to only sqlContext methods and leave this 2nd issue to a different JIRA.
;;;","22/Jan/17 01:23;rxin;We should absolutely remove the RDD methods.
;;;","02/Jan/18 06:39;felixcheung;[~shivaram]we didn't do this, not sure if we should to get this in 2.3.0.
what do you think? ;;;","02/Jan/18 14:51;srowen;Would this be a breaking change though?;;;","02/Jan/18 17:01;felixcheung;These are all non public methods, so officially not public APIs, but people have been known to call them.


;;;","02/Jan/18 21:46;shivaram;Did we have the discussion on dev@ ? I think its a good idea to remove this but I just want to make sure we gave enough of a heads up on dev@ and user@ ;;;","04/Jan/18 07:00;felixcheung;I thought we did but I couldn't find any record.
I suppose we keep this till 2.4.0;;;","24/Oct/18 16:34;srowen;Seems good to target for Spark 3;;;","26/Oct/18 04:07;felixcheung;rebuilt this on spark 3.0.0;;;","26/Oct/18 04:07;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/22843;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dataset.sample with seed: result seems to depend on downstream usage,SPARK-16686,12991912,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,josephkb,josephkb,22/Jul/16 17:59,19/Aug/16 18:19,14/Jul/23 06:29,26/Jul/16 04:06,1.6.2,2.0.0,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"Summary to reproduce bug:
* Create a DataFrame DF, and sample it with a fixed seed.
* Collect that DataFrame -> result1
* Call a particular UDF on that DataFrame -> result2

You would expect results 1 and 2 to use the same rows from DF, but they appear not to.
Note: result1 and result2 are both deterministic.

See the attached notebook for details.  Cells in the notebook were executed in order.","Spark 1.6.2 and Spark 2.0 - RC4
Standalone
Single-worker cluster",apachespark,josephkb,mengxr,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-15382,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/16 17:59;josephkb;DataFrame.sample bug - 2.0.html;https://issues.apache.org/jira/secure/attachment/12819685/DataFrame.sample+bug+-+2.0.html",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 23 10:21:05 UTC 2016,,,,,,,,,,"0|i31ean:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/16 10:21;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/14327;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disallow Creating/Altering a View when the same-name Table Exists,SPARK-16678,12991702,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,22/Jul/16 04:11,26/Jul/16 01:33,14/Jul/23 06:29,26/Jul/16 01:33,2.0.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"When we create OR alter a view, we check whether the view already exists. In the current implementation, if a table with the same name exists, we treat it as a view. However, this is not the right behavior. We should follow what Hive does. For example,
{noformat}
hive> CREATE TABLE tab1 (id int);
OK
Time taken: 0.196 seconds
hive> CREATE OR REPLACE VIEW tab1 AS SELECT * FROM t1;
FAILED: SemanticException [Error 10218]: Existing table is not a view
 The following is an existing table, not a view: default.tab1
hive> ALTER VIEW tab1 AS SELECT * FROM t1;
FAILED: SemanticException [Error 10218]: Existing table is not a view
 The following is an existing table, not a view: default.tab1
hive> CREATE VIEW IF NOT EXISTS tab1 AS SELECT * FROM t1;
OK
Time taken: 0.678 seconds
{noformat}",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 26 01:33:17 UTC 2016,,,,,,,,,,"0|i31czz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/16 04:21;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14314;;;","26/Jul/16 01:33;cloud_fan;Issue resolved by pull request 14314
[https://github.com/apache/spark/pull/14314];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Strange Error when Issuing Load Table Against A View,SPARK-16677,12991701,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,22/Jul/16 04:02,26/Jul/16 01:56,14/Jul/23 06:29,26/Jul/16 01:56,2.0.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Users should not be allowed to issue LOAD DATA against a view. Currently, when users doing it, we got a very strange runtime error:

For example,
{noformat}
LOAD DATA LOCAL INPATH ""$testData"" INTO TABLE $viewName
{noformat}

{noformat}
java.lang.reflect.InvocationTargetException was thrown.
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.sql.hive.client.Shim_v0_14.loadTable(HiveShim.scala:680)
{noformat}",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 26 01:56:34 UTC 2016,,,,,,,,,,"0|i31czr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/16 04:21;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14314;;;","22/Jul/16 07:47;srowen;Is there more to this error? InvocationTargetException is just a wrapper.;;;","26/Jul/16 01:56;cloud_fan;Issue resolved by pull request 14314
[https://github.com/apache/spark/pull/14314];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New Executor Page displays columns that used to be conditionally hidden,SPARK-16673,12991615,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ajbozarth,ajbozarth,ajbozarth,21/Jul/16 21:12,19/Aug/16 15:04,14/Jul/23 06:29,19/Aug/16 15:04,,,,,,,,,2.1.0,,,,Web UI,,,,,,,,0,,,,,,"SPARK-15951 switched the Executors page to use JQuery DataTables, but it also removed the functionality of conditionally hiding the Logs and Thread Dump columns. In the case of the Logs column this is not a big issue, but in the case of Thread Dump, previously it was never shown on the History Server since it isn't available.

We should reintroduce the functionality to hide these columns according to the same conditions as before.",,ajbozarth,apachespark,nblintao,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 27 18:21:06 UTC 2016,,,,,,,,,,"0|i31cgn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/16 21:12;ajbozarth;SPARK-16520 currently has a pr submitted that may also need this functionality;;;","21/Jul/16 22:27;ajbozarth;Once SPARK-16520 is settled I'm willing to tackle this;;;","21/Jul/16 23:17;nblintao;Great! If you are going tackle this AFTER SPARK-16520 is settled, could you please help conditionally hide Workers columns too? Thanks.;;;","27/Jul/16 18:21;apachespark;User 'ajbozarth' has created a pull request for this issue:
https://github.com/apache/spark/pull/14382;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQLBuilder should not raise exceptions on EXISTS queries,SPARK-16672,12991600,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,21/Jul/16 20:11,10/Nov/16 09:01,14/Jul/23 06:29,26/Jul/16 02:52,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"Currently, `SQLBuilder` raises `empty.reduceLeft` exceptions on **unoptimized** `EXISTS` queries. We had better prevent this.
{code}
scala> sql(""CREATE TABLE t1(a int)"")
scala> val df = sql(""select * from t1 b where exists (select * from t1 a)"")
scala> new org.apache.spark.sql.catalyst.SQLBuilder(df).toSQL
java.lang.UnsupportedOperationException: empty.reduceLeft
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 21 20:16:05 UTC 2016,,,,,,,,,,"0|i31cdb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/16 20:16;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/14307;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 1.6.2 - Persist call on Data frames with more than 200 columns is wiping out the data.,SPARK-16664,12991445,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,darkcaller,skolli,skolli,21/Jul/16 13:22,28/Aug/17 21:56,14/Jul/23 06:29,29/Jul/16 11:28,1.6.2,,,,,,,,1.6.3,2.0.1,2.1.0,,Spark Core,,,,,,,,1,,,,,,"Calling persist on a data frame with more than 200 columns is removing the data from the data frame. This is an issue in Spark 1.6.2. Works with out any issues in Spark 1.6.1

Following test case demonstrates problem. Please let me know if you need any additional information. Thanks.

{code}
import org.apache.spark._
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.types._
import org.apache.spark.sql.{Row, SQLContext}
import org.scalatest.FunSuite

class TestSpark162_1 extends FunSuite {

  test(""test data frame with 200 columns"") {
    val sparkConfig = new SparkConf()
    val parallelism = 5
    sparkConfig.set(""spark.default.parallelism"", s""$parallelism"")
    sparkConfig.set(""spark.sql.shuffle.partitions"", s""$parallelism"")

    val sc = new SparkContext(s""local[3]"", ""TestNestedJson"", sparkConfig)
    val sqlContext = new SQLContext(sc)

    // create dataframe with 200 columns and fake 200 values
    val size = 200
    val rdd: RDD[Seq[Long]] = sc.parallelize(Seq(Seq.range(0, size)))
    val rowRdd: RDD[Row] = rdd.map(d => Row.fromSeq(d))

    val schemas = List.range(0, size).map(a => StructField(""name""+ a, LongType, true))
    val testSchema = StructType(schemas)
    val testDf = sqlContext.createDataFrame(rowRdd, testSchema)

    // test value
    assert(testDf.persist.take(1).apply(0).toSeq(100).asInstanceOf[Long] == 100)
    sc.stop()
  }

  test(""test data frame with 201 columns"") {
    val sparkConfig = new SparkConf()
    val parallelism = 5
    sparkConfig.set(""spark.default.parallelism"", s""$parallelism"")
    sparkConfig.set(""spark.sql.shuffle.partitions"", s""$parallelism"")

    val sc = new SparkContext(s""local[3]"", ""TestNestedJson"", sparkConfig)
    val sqlContext = new SQLContext(sc)

    // create dataframe with 201 columns and fake 201 values
    val size = 201
    val rdd: RDD[Seq[Long]] = sc.parallelize(Seq(Seq.range(0, size)))
    val rowRdd: RDD[Row] = rdd.map(d => Row.fromSeq(d))

    val schemas = List.range(0, size).map(a => StructField(""name""+ a, LongType, true))
    val testSchema = StructType(schemas)
    val testDf = sqlContext.createDataFrame(rowRdd, testSchema)


    // test value
    assert(testDf.persist.take(1).apply(0).toSeq(100).asInstanceOf[Long] == 100)
    sc.stop()
  }
}
{code}",,apachespark,barrybecker4,bdolbeare,darkcaller,dongjoon,jchamp,kiszk,mhornbech,michaelmalak,nezihyigitbasi,nihals,proflin,skolli,tgraves,yzhou30,,,,,,,,,,,,,,,,,,,,,,,SPARK-17043,SPARK-16716,SPARK-17061,SPARK-17218,SPARK-17294,SPARK-21851,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Fri Jul 29 17:52:06 UTC 2016,,,,,,,,,,"0|i31bev:",9223372036854775807,,,,,,,,,,,,,2.0.1,,,,,,,,,,,"21/Jul/16 13:49;srowen;What does ""wipe out data"" mean here? your query is for 100 elements, not 200.;;;","21/Jul/16 14:10;yzhou30;Hi there,

The first test case creates a dataframe with a 1 row dataframe, fill 200 columns from 0 to 199, then persist, and then get the 1 row back, and verify that 100th column value is 100.
The second test case doing the same thing but creates a dataframe with 201 columns.

In 2nd case, the entire dataframe became 'empty' after persist() call.

Thanks.

-ying
;;;","21/Jul/16 15:06;skolli;Here is a demonstration from the spark shell: 

{code}
$SPARK_HOME/bin/spark-shell --master local[4]
{code}

{code}
scala>

scala> import org.apache.spark._
import org.apache.spark._

scala> import org.apache.spark.rdd.RDD
import org.apache.spark.rdd.RDD

scala> import org.apache.spark.sql.types._
import org.apache.spark.sql.types._

scala> import org.apache.spark.sql.{Row, SQLContext}
import org.apache.spark.sql.{Row, SQLContext}

scala>

scala> val size = 201
size: Int = 201

scala> val rdd: RDD[Seq[Long]] = sc.parallelize(Seq(Seq.range(0, size)))
rdd: org.apache.spark.rdd.RDD[Seq[Long]] = ParallelCollectionRDD[36] at parallelize at <console>:53

scala> val rowRdd: RDD[Row] = rdd.map(d => Row.fromSeq(d))
rowRdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[37] at map at <console>:55

scala>

scala> val schemas = List.range(0, size).map(a => StructField(""name""+ a, LongType, true))
schemas: List[org.apache.spark.sql.types.StructField] = List(StructField(name0,LongType,true), StructField(name1,LongType,true), StructField(name2,LongType,true), StructField(name3,LongType,true), StructField(na                                                                       me4,LongType,true), StructField(name5,LongType,true), StructField(name6,LongType,true), StructField(name7,LongType,true), StructField(name8,LongType,true), StructField(name9,LongType,true), StructField(name10,Lo                                                                       ngType,true), StructField(name11,LongType,true), StructField(name12,LongType,true), StructField(name13,LongType,true), StructField(name14,LongType,true), StructField(name15,LongType,true), StructField(name16,Lon                                                                       gType,true), StructField(name17,LongType,true), StructField(name18,LongType,true), StructField(name19,LongType,true), StructField(name20,LongType,true), StructField...
scala> val testSchema = StructType(schemas)
testSchema: org.apache.spark.sql.types.StructType = StructType(StructField(name0,LongType,true), StructField(name1,LongType,true), StructField(name2,LongType,true), StructField(name3,LongType,true), StructField(                                                                       name4,LongType,true), StructField(name5,LongType,true), StructField(name6,LongType,true), StructField(name7,LongType,true), StructField(name8,LongType,true), StructField(name9,LongType,true), StructField(name10,                                                                       LongType,true), StructField(name11,LongType,true), StructField(name12,LongType,true), StructField(name13,LongType,true), StructField(name14,LongType,true), StructField(name15,LongType,true), StructField(name16,L                                                                       ongType,true), StructField(name17,LongType,true), StructField(name18,LongType,true), StructField(name19,LongType,true), StructField(name20,LongType,true), StructFie...
scala> val testDf = sqlContext.createDataFrame(rowRdd, testSchema)
testDf: org.apache.spark.sql.DataFrame = [name0: bigint, name1: bigint, name2: bigint, name3: bigint, name4: bigint, name5: bigint, name6: bigint, name7: bigint, name8: bigint, name9: bigint, name10: bigint, nam                                                                       e11: bigint, name12: bigint, name13: bigint, name14: bigint, name15: bigint, name16: bigint, name17: bigint, name18: bigint, name19: bigint, name20: bigint, name21: bigint, name22: bigint, name23: bigint, name24                                                                       : bigint, name25: bigint, name26: bigint, name27: bigint, name28: bigint, name29: bigint, name30: bigint, name31: bigint, name32: bigint, name33: bigint, name34: bigint, name35: bigint, name36: bigint, name37: b                                                                       igint, name38: bigint, name39: bigint, name40: bigint, name41: bigint, name42: bigint, name43: bigint, name44: bigint, name45: bigint, name46: bigint, name47: bigin...
scala>
{code}


{color:green}*Take the first row from the data frame before calling persist:*{color}
{code}
scala> testDf.take(1)
res9: Array[org.apache.spark.sql.Row] = Array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,                                                                       58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,1                                                                       21,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,                                                                       174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200])
{code}

{color:red}*Take the first row from the data frame after calling persist:*{color}
{code}
scala> testDf.persist.take(1)
res10: Array[org.apache.spark.sql.Row] = Array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0                                                                      ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])
{code};;;","21/Jul/16 18:44;dongjoon;It looks like that. FYI, here is the result of current master. As you said, it's really 1.6.2 specific.
{code}
scala> import org.apache.spark.sql.types._
scala> val size = 200
scala> val rdd: org.apache.spark.rdd.RDD[Seq[Long]] = spark.sparkContext.parallelize(Seq(Seq.range(0, size)))
scala> val rowRdd = rdd.map(d => org.apache.spark.sql.Row.fromSeq(d))
scala> val schemas = List.range(0, size).map(a => StructField(""name"" + a, LongType, true))
scala> val testDf = spark.createDataFrame(rowRdd, StructType(schemas))
scala> testDf.persist().take(1)
res0: Array[org.apache.spark.sql.Row] = Array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199])
{code};;;","21/Jul/16 18:50;dongjoon;Also, FYI, RC5 is the same with the current master.;;;","22/Jul/16 11:53;darkcaller;According to the original post, the size should be 201 to prove master works;;;","22/Jul/16 12:23;skolli;[~dongjoon] Problem exists in master also. I tried a nightly build from the following and it failed.

{noformat}
http://people.apache.org/~pwendell/spark-nightly/spark-master-bin/latest/

spark-2.1.0-SNAPSHOT-bin-hadoop2.4.tgz	2016-07-22 08:15	175M
{noformat};;;","23/Jul/16 04:19;proflin;I think I've found the root cause and I'm going to submit a patch shortly; thanks!;;;","23/Jul/16 04:47;apachespark;User 'breakdawn' has created a pull request for this issue:
https://github.com/apache/spark/pull/14324;;;","23/Jul/16 05:57;dongjoon;Oh, I missed the 201 cases. Sorry.;;;","28/Jul/16 08:37;jchamp;- We are experiencing the same problem here with Spark 1.6.2 on Ubuntu / CentOS / MacOS when using a DataFrame with more than 200 columns :
-> Data is Wiped ( This was working in Spark 1.6.1 )

- And with more than 8117 cols, it's not working, but there is an exception :
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:555)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:575)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:572)
	at org.spark-project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark-project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	... 28 more
Caused by: org.codehaus.janino.JaninoRuntimeException: Code of method ""()Z"" of class ""org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator"" grows beyond 64 KB



;;;","29/Jul/16 11:28;srowen;Issue resolved by pull request 14324
[https://github.com/apache/spark/pull/14324];;;","29/Jul/16 11:30;tgraves;I am out of the office until 8/8. Please contact my manager or file support request if you need immediate help.
;;;","29/Jul/16 17:52;apachespark;User 'breakdawn' has created a pull request for this issue:
https://github.com/apache/spark/pull/14404;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CreateTableAsSelectSuite is flaky,SPARK-16656,12991270,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,20/Jul/16 22:36,16/Aug/16 20:43,14/Jul/23 06:29,21/Jul/16 19:12,,,,,,,,,1.6.3,2.0.1,2.1.0,,SQL,,,,,,,,0,,,,,,https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/62593/testReport/junit/org.apache.spark.sql.sources/CreateTableAsSelectSuite/create_a_table__drop_it_and_create_another_one_with_the_same_name/,,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 16 16:58:29 UTC 2016,,,,,,,,,,"0|i31abz:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"20/Jul/16 22:45;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/14289;;;","21/Jul/16 19:12;yhuai;https://github.com/apache/spark/pull/14289 has been merged to branch 2.0 and master.;;;","16/Aug/16 16:58;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/14668;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LAST_VALUE(FALSE) OVER () throws IndexOutOfBoundsException,SPARK-16648,12991092,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,liancheng,20/Jul/16 12:07,22/Nov/16 16:10,14/Jul/23 06:29,25/Jul/16 09:23,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"The following simple SQL query reproduces this issue:

{code:sql}
SELECT LAST_VALUE(FALSE) OVER ();
{code}

Exception thrown:

{noformat}
java.lang.IndexOutOfBoundsException: 0
  at scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:43)
  at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:48)
  at scala.collection.mutable.ArrayBuffer.remove(ArrayBuffer.scala:169)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:244)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
  at org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren(TreeNode.scala:214)
  at org.apache.spark.sql.catalyst.analysis.TypeCoercion$ImplicitTypeCasts$$anonfun$apply$12.applyOrElse(TypeCoercion.scala:637)
  at org.apache.spark.sql.catalyst.analysis.TypeCoercion$ImplicitTypeCasts$$anonfun$apply$12.applyOrElse(TypeCoercion.scala:615)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:279)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:279)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:278)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:284)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:284)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:284)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:284)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionDown$1(QueryPlan.scala:156)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:166)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:170)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.immutable.List.map(List.scala:285)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:170)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:175)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:175)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:144)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveExpressions$1.applyOrElse(LogicalPlan.scala:79)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveExpressions$1.applyOrElse(LogicalPlan.scala:78)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:60)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressions(LogicalPlan.scala:78)
  at org.apache.spark.sql.catalyst.analysis.TypeCoercion$ImplicitTypeCasts$.apply(TypeCoercion.scala:615)
  at org.apache.spark.sql.catalyst.analysis.TypeCoercion$ImplicitTypeCasts$.apply(TypeCoercion.scala:614)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
  at scala.collection.immutable.List.foldLeft(List.scala:84)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:65)
  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:63)
  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)
  ... 48 elided
{noformat}

This bug is a regression. Spark 1.6 doesn't have this issue.",,apachespark,cloud_fan,emlyn,liancheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18358,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 27 23:31:35 UTC 2016,,,,,,,,,,"0|i3198n:",9223372036854775807,,,,,,,,,,,,,2.0.1,,,,,,,,,,,"20/Jul/16 13:55;liancheng;The problematic code is the newly introduced {{TreeNode.withNewChildren}}. {{Last}} is a unary expression with two {{Expression}} arguments:

{code}
case class Last(child: Expression, ignoreNullsExpr: Expression) extends DeclarativeAggregate {
  ...
  override def children: Seq[Expression] = child :: Nil
  ...
}
{code}

Argument {{ignoreNullsExpr}} defaults to {{Literal.FalseLiteral}}. Thus {{LAST_VALUE(FALSE)}} is equivalent to {{Last(Literal.FalseLiteral, Literal.FalseLiteral)}}. This breaks the following case branch in {{TreeNode.withNewChildren}}:

{code}
case arg: TreeNode[_] if containsChild(arg) =>    // Both `child` and `ignoreNullsExpr` hit this branch,
  val newChild = remainingNewChildren.remove(0)   // but only `child` is the real child node of `Last`.
  val oldChild = remainingOldChildren.remove(0)
  if (newChild fastEquals oldChild) {
    oldChild
  } else {
    changed = true
    newChild
  }
{code}
;;;","21/Jul/16 04:56;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/14295;;;","25/Jul/16 09:23;cloud_fan;Issue resolved by pull request 14295
[https://github.com/apache/spark/pull/14295];;;","27/Oct/16 23:31;emlyn;Edit: I've opened a new issue for this at SPARK-18172.

Since Spark 2.0.1, the following pyspark snippet fails (I believe it worked under 2.0.0, so this issue seems like the most likely cause of change in behaviour):
{code}
from pyspark.sql import functions as F
ds = spark.createDataFrame(sc.parallelize([[1, 1, 2], [1, 2, 3], [1, 3, 4]]))
ds.groupBy(ds._1).agg(F.first(ds._2), F.countDistinct(ds._2), F.countDistinct(ds._2, ds._3)).show()
{code}
It works if any of the three arguments to {{.agg}} is removed.

The stack trace is:
{code}
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-3-73596fd1f689> in <module>()
----> 1 ds.groupBy(ds._1).agg(F.first(ds._2),F.countDistinct(ds._2),F.countDistinct(ds._2, ds._3)).show()

/usr/local/Cellar/apache-spark/2.0.1/libexec/python/pyspark/sql/dataframe.py in show(self, n, truncate)
    285         +---+-----+
    286         """"""
--> 287         print(self._jdf.showString(n, truncate))
    288
    289     def __repr__(self):

/usr/local/Cellar/apache-spark/2.0.1/libexec/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1131         answer = self.gateway_client.send_command(command)
   1132         return_value = get_return_value(
-> 1133             answer, self.gateway_client, self.target_id, self.name)
   1134
   1135         for temp_arg in temp_args:

/usr/local/Cellar/apache-spark/2.0.1/libexec/python/pyspark/sql/utils.py in deco(*a, **kw)
     61     def deco(*a, **kw):
     62         try:
---> 63             return f(*a, **kw)
     64         except py4j.protocol.Py4JJavaError as e:
     65             s = e.java_exception.toString()

/usr/local/Cellar/apache-spark/2.0.1/libexec/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    317                 raise Py4JJavaError(
    318                     ""An error occurred while calling {0}{1}{2}.\n"".
--> 319                     format(target_id, ""."", name), value)
    320             else:
    321                 raise Py4JError(

Py4JJavaError: An error occurred while calling o76.showString.
: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: makeCopy, tree: first(_2#1L)()
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.makeCopy(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren(TreeNode.scala:256)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$.org$apache$spark$sql$catalyst$optimizer$RewriteDistinctAggregates$$patchAggregateFunctionChildren$1(RewriteDistinctAggregates.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$$anonfun$16.apply(RewriteDistinctAggregates.scala:182)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$$anonfun$16.apply(RewriteDistinctAggregates.scala:180)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$.rewrite(RewriteDistinctAggregates.scala:180)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$$anonfun$apply$1.applyOrElse(RewriteDistinctAggregates.scala:105)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$$anonfun$apply$1.applyOrElse(RewriteDistinctAggregates.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:301)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:301)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:298)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$.apply(RewriteDistinctAggregates.scala:104)
	at org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregates$.apply(RewriteDistinctAggregates.scala:102)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
	at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)
	at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:83)
	at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2572)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:1934)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2149)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1$$anonfun$apply$13.apply(TreeNode.scala:413)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1$$anonfun$apply$13.apply(TreeNode.scala:413)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1.apply(TreeNode.scala:412)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1.apply(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	... 62 more
Caused by: org.apache.spark.sql.AnalysisException: The second argument of First should be a boolean literal.;
	at org.apache.spark.sql.catalyst.expressions.aggregate.First.<init>(First.scala:43)
	... 72 more
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LEAST doesn't accept numeric arguments with different data types,SPARK-16646,12991050,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,lian cheng,liancheng,20/Jul/16 08:54,12/Dec/22 18:11,14/Jul/23 06:29,03/Aug/16 19:30,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"{code:sql}
SELECT LEAST(1, 1.5);
{code}

{noformat}
Error: org.apache.spark.sql.AnalysisException: cannot resolve 'least(1, CAST(2.1 AS DECIMAL(2,1)))' due to data type mismatch: The expressions should all have the same type, got LEAST (ArrayBuffer(IntegerType, DecimalType(2,1))).; line 1 pos 7 (state=,code=0)
{noformat}

This query works for 1.6.",,apachespark,cloud_fan,liancheng,lian cheng,maver1ck,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 03 19:30:57 UTC 2016,,,,,,,,,,"0|i318zb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/16 12:14;liancheng;{{GREATEST}} has similar issue.;;;","20/Jul/16 12:22;gurwls223;Just FYI, I just tried to reproduce this.

In 1.6.x, in HiveContext the codes below:

{code}
SELECT LEAST(1, 1.5)
{code}

casts 1.5 as double. So, here, https://github.com/apache/spark/blob/162d04a30e38bb83d35865679145f8ea80b84c26/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TypeCoercion.scala#L506-L508

this finds a tightest common type from both {{IntegerType}} and {{DoubleType}} as {{DoubleType}}, then it works okay.

But in master branach, 

It casts 1.5 as decimal(2, 1). So,  it fails to find a tightest common type from both {{IntegerType}} and {{DecimalType(2, 1)}}.

;;;","21/Jul/16 02:24;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/14294;;;","21/Jul/16 03:16;gurwls223;Maybe I had to ask if you are working on this. I will close mine if you are!;;;","21/Jul/16 05:05;lian cheng;Thanks for the help! I'm not working on this.;;;","22/Jul/16 04:31;cloud_fan;some other thoughts about decimal type in Spark SQL:

1. In MySQL, the max scale is half of the max precision, so that 2 decimal type always have a legal wider type. In Postgres, it has a special decimal type, a decimal type without precision. It has nearly unlimited precision, so that 2 decimal type always have a legal wider type too. However, in Spark SQL, the max precision and scale are both 38, which means 2 decimal type can not always have a legal wider type, we have to truncate.

2. the decimal type truncate logic in Spark SQL is kind of weird. e.g. decimal(76, 38) will be truncated to decimal(38, 38), which means we drop all the integral part.;;;","22/Jul/16 05:00;gurwls223;I see! please let me leave my thought as well just in case it is helpful.

Actually, I thought it should not lose values (fractions or integral parts) but just throw an exception if it goes over 38 while trying to find a tight common type (and this is what my PR does for now).

For JSON data source, it falls back to double type in schema inference in this case.

If it should allow the change in original values with such operations, how about falling back to double type?;;;","22/Jul/16 08:04;lian cheng;Could you please help check Hive's behavior here? Especially cases when we have to lose precision. Thanks!;;;","22/Jul/16 08:13;gurwls223;My pleasure! Let me look into this and will bring some details tomorrow. Thank you!;;;","22/Jul/16 10:19;gurwls223;It seems basically comparison between numbers and decimal, and decimals with different precision and scale were not allowed but from 2.0, this restriction was relexted.

here, https://issues.apache.org/jira/browse/HIVE-12070 and https://issues.apache.org/jira/browse/HIVE-12082

So, now Hive in current master produces as below:

{code}
hive> SELECT LEAST(2, 1.5BD);
OK
1.5
{code}

However, when the precision is too high, it became as below:

{code}
hive> SELECT LEAST(100000000000000000, 1.000000000000000000005BD);
OK
1
{code}

This seems goes double,

{code}
hive> SELECT LEAST(12, 1.00000000000000000000000000005BD);
OK
1.0000000000000000000000000001
{code}

In more details, [GenericUDFBaseNwayCompare.initialize|https://github.com/apache/hive/blob/26b5c7b56a4f28ce3eabc0207566cce46b29b558/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBaseNwayCompare.java#L64-L69] is called first to find a common type. 

And then,  [FunctionRegistry.getCommonClassForComparison|https://github.com/apache/hive/blob/a55f4a3a4b017d7f3b9279ef7d843e4b5f7fcfa0/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java#L764-L767] is called. Because [PrimitiveObjectInspectorUtils.NUMERIC_GRUOP|https://github.com/apache/hive/blob/1b5ee3d88799a5e80948b7b0f5ca96bba8580efe/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java#L1235-L1243] seems including {{DECIAML}}, it becomes {{TypeInfoFactory.doubleTypeInfo}}. So, it seems always a double in this case.

If it fails to find a common type, this becomes a double anyway, (see [here|https://github.com/apache/hive/blob/26b5c7b56a4f28ce3eabc0207566cce46b29b558/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBaseNwayCompare.java#L71-L73]).;;;","22/Jul/16 10:20;gurwls223;[~lian cheng] Should we also follow this? I will follow your decision.;;;","29/Jul/16 02:04;cloud_fan;We are discussing this internally, can you hold it for a while? We may decide to increase the max precision to 76 and keep max scale as 38, then we don't have this problem.;;;","29/Jul/16 03:21;gurwls223;Sure, I will close the PR for meanwhile. Then please update me after that.;;;","01/Aug/16 14:34;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/14439;;;","03/Aug/16 19:30;yhuai;Issue resolved by pull request 14439
[https://github.com/apache/spark/pull/14439];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
constraints propagation may fail the query,SPARK-16644,12991030,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,20/Jul/16 07:34,21/Jul/16 01:49,14/Jul/23 06:29,21/Jul/16 01:48,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"{code}
create table(a int, b int);
select
  a,
  max(b) as c1,
  b as c2
from tbl
where a = b
group by a, b
having c1 = 1
{code}

this query fails in 2.0, but works in 1.6",,apachespark,cloud_fan,rxin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 21 01:48:19 UTC 2016,,,,,,,,,,"0|i318uv:",9223372036854775807,,,,,,,,,,,,,2.0.1,,,,,,,,,,,"20/Jul/16 08:23;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/14281;;;","20/Jul/16 22:55;yhuai;oh, I think this problem only happens if an aggregate expression uses a grouping expression and that exact grouping expression is part of the output of the aggregate operator.;;;","21/Jul/16 01:48;yhuai;Issue resolved by pull request 14281
[https://github.com/apache/spark/pull/14281];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResolveWindowFrame should not be triggered on UnresolvedFunctions.,SPARK-16642,12991020,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,20/Jul/16 07:02,26/Jul/16 03:59,14/Jul/23 06:29,26/Jul/16 03:59,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"The case at https://github.com/apache/spark/blob/75146be6ba5e9f559f5f15430310bb476ee0812c/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L1790-L1792 is shown below
{code}
case we @ WindowExpression(e, s @ WindowSpecDefinition(_, o, UnspecifiedFrame)) =>
          val frame = SpecifiedWindowFrame.defaultWindowFrame(o.nonEmpty, acceptWindowFrame = true)
          we.copy(windowSpec = s.copy(frameSpecification = frame))
{code}
This case will be triggered even when the function is an unresolved. So, when the functions like lead are used, we may see errors like {{Window Frame RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW must match the required frame ROWS BETWEEN 1 FOLLOWING AND 1 FOLLOWING.}} because we wrongly set the the frame specification.",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 26 03:59:26 UTC 2016,,,,,,,,,,"0|i318sn:",9223372036854775807,,,,,,,,,,,,,2.0.1,,,,,,,,,,,"20/Jul/16 08:58;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/14284;;;","26/Jul/16 03:59;yhuai;Issue resolved by pull request 14284
[https://github.com/apache/spark/pull/14284];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
query fails if having condition contains grouping column,SPARK-16639,12990993,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,cloud_fan,cloud_fan,20/Jul/16 04:12,28/Jul/16 15:00,14/Jul/23 06:29,28/Jul/16 14:35,,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"{code}
create table tbl(a int, b string);
select count(b) from tbl group by a + 1 having a + 1 = 2;
{code}

this will fail analysis",,apachespark,cloud_fan,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 28 14:35:16 UTC 2016,,,,,,,,,,"0|i318mn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/16 04:41;cloud_fan;1.6 also fails;;;","21/Jul/16 05:09;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/14296;;;","28/Jul/16 14:35;cloud_fan;Issue resolved by pull request 14296
[https://github.com/apache/spark/pull/14296];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GenericArrayData can't be loaded in certain JVMs,SPARK-16634,12990953,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,19/Jul/16 23:21,20/Jul/16 17:39,14/Jul/23 06:29,20/Jul/16 17:39,2.1.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"There's an annoying bug in some JVMs that causes certain scala-generated bytecode to not load. The current code in GenericArrayData.scala triggers that bug (at least with 1.7.0_67, maybe others).

Since it's easy to work around the bug, I'd rather do that instead of asking people who might be running that version to have to upgrade.

Error:

{noformat}
16/07/19 16:02:35 INFO scheduler.TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on executor vanzin-st1-3.vpc.cloudera.com: java.lang.VerifyError (Bad <init> method call from inside of a branch
Exception Details:
  Location:
    org/apache/spark/sql/catalyst/util/GenericArrayData.<init>(Ljava/lang/Object;)V @52: invokespecial
  Reason:
    Error exists in the bytecode
  Bytecode:
    0000000: 2a2b 4d2c c100 dc99 000e 2cc0 00dc 4e2d
    0000010: 3a04 a700 20b2 0129 2c04 b601 2d99 001b
    0000020: 2c3a 05b2 007a 1905 b600 7eb9 00fe 0100
    0000030: 3a04 1904 b700 f3b1 bb01 2f59 2cb7 0131
    0000040: bf                                     
  Stackmap Table:
    full_frame(@21,{UninitializedThis,Object[#177],Object[#177]},{UninitializedThis})
    full_frame(@50,{UninitializedThis,Object[#177],Object[#177],Top,Object[#220]},{UninitializedThis})
    full_frame(@56,{UninitializedThis,Object[#177],Object[#177]},{UninitializedThis})
) [duplicate 2]
{noformat}

I didn't run into this with 2.0, not sure whether the issue exists there.",,apachespark,michaelmalak,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 19 23:24:04 UTC 2016,,,,,,,,,,"0|i318dr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/16 23:24;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14271;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
lag/lead using constant input values does not return the default value when the offset row does not exist,SPARK-16633,12990947,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,yhuai,yhuai,yhuai,19/Jul/16 22:49,31/Aug/16 21:16,14/Jul/23 06:29,26/Jul/16 03:59,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,correctness,,,,,Please see the attached notebook. Seems lag/lead somehow fail to recognize that a offset row does not exist and generate wrong results.,,apachespark,lian cheng,rxin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/16 00:13;lian cheng;window_function_bug.html;https://issues.apache.org/jira/secure/attachment/12818975/window_function_bug.html",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 26 03:59:21 UTC 2016,,,,,,,,,,"0|i318cf:",9223372036854775807,,,,,,,,,,,,,2.0.1,,,,,,,,,,,"20/Jul/16 00:13;lian cheng;JIRA went down right before [~yhuai] tried to upload the notebook mentioned in the description. Here it is.;;;","20/Jul/16 05:59;yhuai;Seems this bug only affects cases that use a constant as the column put in lag/lead.;;;","20/Jul/16 06:52;yhuai;Seems OffsetWindowFunctionFrame cannot distinguish the case that the offset row does not exist and the case that the offset row's value is null.

For example, 
{code}
SELECT
  row_number() OVER (ORDER BY id) as row_number,
  lead(id, 1, 321) OVER (ORDER BY id) as lag
FROM (SELECT cast(null as int) as id UNION ALL select cast(null as int) as id) tmp
{code}
The current master returns
{code}
+----------+---+
|row_number|lag|
+----------+---+
|         1|321|
|         2|321|
+----------+---+
{code}

However, the correct result is
{code}
 row_number | lag 
------------+-----
          1 |    null
          2 | 321

{code};;;","20/Jul/16 08:59;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/14284;;;","25/Jul/16 23:14;yhuai;https://issues.apache.org/jira/browse/SPARK-16721;;;","26/Jul/16 03:59;yhuai;Issue resolved by pull request 14284
[https://github.com/apache/spark/pull/14284];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vectorized parquet reader fails to read certain fields from Hive tables,SPARK-16632,12990936,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,19/Jul/16 22:11,14/Oct/16 05:28,14/Jul/23 06:29,20/Jul/16 05:06,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"The vectorized parquet reader fails to read certain tables created by Hive. When the tables have type ""tinyint"" or ""smallint"", Catalyst converts those to ""ByteType"" and ""ShortType"" respectively. But when Hive writes those tables in parquet format, the parquet schema in the files contains ""int32"" fields.

To reproduce, run these commands in the hive shell (or beeline):

{code}
create table abyte (value tinyint) stored as parquet;
create table ashort (value smallint) stored as parquet;
insert into abyte values (1);
insert into ashort values (1);
{code}

Then query them with Spark 2.0:

{code}
spark.sql(""select * from abyte"").show();
spark.sql(""select * from ashort"").show();
{code}

You'll see this exception (for the byte case):

{noformat}
16/07/13 12:24:23 ERROR datasources.InsertIntoHadoopFsRelationCommand: Aborting job.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, scm-centos71-iqalat-2.gce.cloudera.com): org.apache.spark.SparkException: Task failed while writing rows
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getByte(OnHeapColumnVector.java:159)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1325)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258)
	... 8 more
{noformat}

This works when you point Spark directly at the files (instead of using the metastore data), or when you disable the vectorized parquet reader.

The root cause seems to be that Hive creates these tables with a not-so-complete schema:

{noformat}
$ parquet-tools schema /tmp/byte.parquet 
message hive_schema {
  optional int32 value;
}
{noformat}

There's no indication that the field is a 32-bit field used to store 8-bit values. When the ParquetReadSupport code tries to consolidate both schemas, it just chooses whatever is in the parquet file for primitive types (see ParquetReadSupport.clipParquetType); the vectorized reader uses the catalyst schema, which comes from the Hive metastore, and says it's a byte field, so when it tries to read the data, the byte data stored in ""OnHeapColumnVector"" is null.

I have tested a small change to {{ParquetReadSupport.clipParquetType}} that fixes this particular issue, but I haven't run any other tests, so I'll do that while I wait for others to chime in and maybe tell me that's not the right place to fix this.
",Hive 1.1 (CDH),apachespark,dongjoon,lian cheng,lianhuiwang,rxin,vanzin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-14294,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 14 05:28:11 UTC 2016,,,,,,,,,,"0|i3189z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/16 22:33;yhuai;[~vanzin] So, you mean that OnHeapColumnVector does not reserve byteData in reserveInternal?;;;","19/Jul/16 22:36;vanzin;I mean that because it only considers the type specified in the parquet file, it initializes the {{intData}} field. But when processing the query, Catalyst uses the Hive type ({{tinyint}} => {{ByteType}}) and tries to read from {{byteData}}, which is null.;;;","19/Jul/16 23:37;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14272;;;","20/Jul/16 05:06;lian cheng;Issue resolved by pull request 14272
[https://github.com/apache/spark/pull/14272];;;","20/Jul/16 05:50;lian cheng;[~vanzin] Did you post the wrong stack trace? This issue is about the read path, but the stack trace is about write path.;;;","20/Jul/16 06:42;lian cheng;Discussed with [~yhuai] after merging [PR #14272|https://github.com/apache/spark/pull/14272] and found that there's a much simpler fix for this issue.

Firstly, the real root cause of this issue is [this line|https://github.com/apache/spark/blob/v2.0.0-rc5/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java#L139] in {{SpecificParquetRecordReaderBase}}. In this line, we converted Parquet type {{requestedSchema}} into {{sparkSchema}}, which is a Spark {{StructType}}, and use it as the requested schema passed down from the query planner.

However, {{requestedSchema}} is tailored from the Parquet schema read from the physical file to be scanned, and doesn't contain proper Parquet type annotation since it's written by Hive. Thus {{sparkSchema}} has the wrong type information.

On the other hand, we always set the Spark requested schema in the Hadoop configuration. This is done in {{ParquetFileFormat.initializeLocalJobFunc()}} (see [here|https://github.com/apache/spark/blob/v2.0.0-rc5/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala#L292-L294]). Thus, instead of converting from {{requestedSchema}}, we can simply read out the original Spark requested schema and set it to {{sparkSchema}} to fix this issue.
;;;","20/Jul/16 06:53;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/14278;;;","20/Jul/16 18:43;vanzin;Yes, that's the right stack trace. It's a CTAS query which is probably why the write path shows up there.;;;","21/Jul/16 09:30;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/14300;;;","21/Jul/16 14:39;lian cheng;Oh, I see, thanks for the explanation.;;;","14/Oct/16 05:27;dongjoon;This was backported at the following commit.

https://github.com/apache/spark/commit/f9367d6;;;","14/Oct/16 05:28;dongjoon;{code}
spark-2.0:branch-2.0$ git log --oneline | grep SPARK-16632
933d76a [SPARK-16632][SQL] Revert PR #14272: Respect Hive schema when merging parquet schema
f9367d6 [SPARK-16632][SQL] Use Spark requested schema to guide vectorized Parquet reader initialization
c2b5b3c [SPARK-16632][SQL] Respect Hive schema when merging parquet schema.
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OrcConversions should not convert an ORC table represented by MetastoreRelation to HadoopFsRelation if metastore schema does not match schema stored in ORC files,SPARK-16628,12990855,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,yhuai,yhuai,19/Jul/16 18:28,13/Oct/17 15:25,14/Jul/23 06:29,13/Oct/17 15:25,,,,,,,,,2.2.1,2.3.0,,,SQL,,,,,,,,2,,,,,,"When {{spark.sql.hive.convertMetastoreOrc}} is enabled, we will convert a ORC table represented by a MetastoreRelation to HadoopFsRelation that uses Spark's OrcFileFormat internally. This conversion aims to make table scanning have a better performance since at runtime, the code path to scan HadoopFsRelation's performance is better. However, OrcFileFormat's implementation is based on the assumption that ORC files store their schema with correct column names. However, before Hive 2.0, an ORC table created by Hive does not store column name correctly in the ORC files (HIVE-4243). So, for this kind of ORC datasets, we cannot really convert the code path. 

Right now, if ORC tables are created by Hive 1.x or 0.x, enabling {{spark.sql.hive.convertMetastoreOrc}} will introduce a runtime exception for non-partitioned ORC tables and drop the metastore schema for partitioned ORC tables.",,apachespark,cloud_fan,dongjoon,dougb,lianhuiwang,mgaido,nseggert,tejasp,tgraves,viirya,WeiqingYang,xwu0226,yhuai,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,SPARK-15757,,,,,,,,,,SPARK-15705,SPARK-14387,SPARK-18355,SPARK-21686,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 13 15:25:12 UTC 2017,,,,,,,,,,"0|i317rz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/16 18:30;yhuai;I think there are two possible fixes. 

1. Make the conversion rule not convert the code path when the inferred schema does not match the schema stored in metastore.
2. Improve ORCFileFormat to make it handle cases that schema stored in the ORC file does not match the schema stored in metastore.;;;","19/Jul/16 18:38;yhuai;cc [~tejasp];;;","20/Jul/16 02:02;tejasp;Thanks for notifying [~yhuai]. Is this specific to ORC only ? I remember for the change I made, the codepath was similar to what Parquet used (and there was spark.sql.hive.convertMetastoreParquet as well).;;;","20/Jul/16 08:42;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/14282;;;","20/Jul/16 09:19;viirya;I've tried to address this issue by the PR with the first option.;;;","22/Jul/16 08:45;viirya;I think it depends whether Hive also writes wrong column names into Parquet files? ;;;","25/Jul/16 18:11;yhuai;For Hive 0.x and 1.x, ORC files do not have column names correctly set in the file. So, if we enable this conversion, even if the table is stored in the metastore, the schema will be wrong after the conversion.;;;","25/Jul/16 22:17;nseggert;Yeah, I attempted to fix this myself by having it just take the schema from the MetaStore instead of the file, but that doesn't work, because you're trying to read the file using the wrong schema. I think you'd probably need to make some sort of translation map. That's about the point where I realized I was in over my head.;;;","26/Jul/16 08:41;viirya;I submitted another PR to implement the option 2 solution that handles schema translation for file-based datasource tables.;;;","26/Jul/16 08:43;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/14365;;;","10/Nov/16 06:51;dongjoon;Hi, is there any progress on this issue?;;;","13/Oct/17 04:35;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19470;;;","13/Oct/17 15:25;cloud_fan;Issue resolved by pull request 19470
[https://github.com/apache/spark/pull/19470];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Oracle JDBC table creation fails with ORA-00902: invalid datatype,SPARK-16625,12990762,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,darabos,darabos,19/Jul/16 13:20,22/Sep/17 14:47,14/Jul/23 06:29,05/Aug/16 15:12,1.6.2,,,,,,,,2.1.0,2.1.2,,,SQL,,,,,,,,0,,,,,,"Unfortunately I know very little about databases, but I figure this is a bug.

I have a DataFrame with the following schema: 
{noformat}
StructType(StructField(dst,StringType,true), StructField(id,LongType,true), StructField(src,StringType,true))
{noformat}

I am trying to write it to an Oracle database like this:

{code:java}
String url = ""jdbc:oracle:thin:root/rootroot@<ip address>:1521:db"";
java.util.Properties p = new java.util.Properties();
p.setProperty(""driver"", ""oracle.jdbc.OracleDriver"");
df.write().mode(""overwrite"").jdbc(url, ""my_table"", p);
{code}

And I get:

{noformat}
Exception in thread ""main"" java.sql.SQLSyntaxErrorException: ORA-00902: invalid datatype

	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:461)
	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:402)
	at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1108)
	at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:541)
	at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:264)
	at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:598)
	at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:213)
	at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:26)
	at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:1241)
	at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1558)
	at oracle.jdbc.driver.OracleStatement.executeUpdateInternal(OracleStatement.java:2498)
	at oracle.jdbc.driver.OracleStatement.executeUpdate(OracleStatement.java:2431)
	at oracle.jdbc.driver.OracleStatementWrapper.executeUpdate(OracleStatementWrapper.java:975)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:302)
{noformat}

The Oracle server I am running against is the one I get on Amazon RDS for engine type {{oracle-se}}. The same code (with the right driver) against the RDS instance with engine type {{MySQL}} works.

The error message is the same as in https://issues.apache.org/jira/browse/SPARK-12941. Could it be that {{Long}} is also translated into the wrong data type? Thanks.",,apachespark,darabos,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 17 08:29:12 UTC 2017,,,,,,,,,,"0|i3177b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/16 09:32;yumwang;{code:java}
    val jdbcUrl = ""jdbc:oracle:thin:@localhost:1521/orcl""
    val props = new Properties()
    props.put(""driver"", ""oracle.jdbc.driver.OracleDriver"")
    props.put(""oracle.jdbc.mapDateToTimestamp"", ""false"")

    val schema = StructType(Seq(
      StructField(""boolean_type"", BooleanType, true),
      StructField(""integer_type"", IntegerType, true),
      StructField(""long_type"", LongType, true),
      StructField(""float_Type"", FloatType, true),
      StructField(""double_type"", DoubleType, true),
      StructField(""byte_type"", ByteType, true),
      StructField(""short_type"", ShortType, true),
      StructField(""string_type"", StringType, true),
      StructField(""binary_type"", BinaryType, true),
      StructField(""date_type"", DateType, true),
      StructField(""timestamp_type"", TimestampType, true)
    ))

    val tableName = ""test_oracle_general_types""
    val booleanVal = true
    val integerVal = 1
    val longVal = 2L
    val floatVal = 3.0f
    val doubleVal = 4.0
    val byteVal = 2.toByte
    val shortVal = 5.toShort
    val stringVal = ""string""
    val binaryVal = Array[Byte](6, 7, 8)
    val dateVal = Date.valueOf(""2016-07-26"")
    val timestampVal = Timestamp.valueOf(""2016-07-26 11:49:45"")

    val data = spark.sparkContext.parallelize(Seq(
      Row(
        booleanVal, integerVal, longVal, floatVal, doubleVal, byteVal, shortVal, stringVal,
        binaryVal, dateVal, timestampVal
      )))

    val dfWrite = spark.createDataFrame(data, schema)
    dfWrite.write.jdbc(jdbcUrl, tableName, props)
{code}

For DataFrame dfWrite,  the generated create table statement:

{code:sql}
CREATE TABLE test_oracle_general_types (
""boolean_type"" BIT(1), 
""integer_type"" INTEGER, 
""long_type"" BIGINT, 
""float_Type"" REAL, 
""double_type"" DOUBLE PRECISION, 
""byte_type"" BYTE, 
""short_type"" INTEGER, 
""string_type"" VARCHAR2(255), 
""binary_type"" BLOB, 
""date_type"" DATE, 
""timestamp_type"" TIMESTAMP )
{code}
Obviously, the statement is incorrect for Oracle,  I will submit a PR and then the generated create table statement:
{code:sql}
CREATE TABLE test_oracle_general_types (
""boolean_type"" NUMBER(1), 
""integer_type"" NUMBER(10), 
""long_type"" NUMBER(19), 
""float_Type"" NUMBER(19, 4), 
""double_type"" NUMBER(19, 4), 
""byte_type"" NUMBER(3), 
""short_type"" NUMBER(5), 
""string_type"" VARCHAR2(255), 
""binary_type"" BLOB, 
""date_type"" DATE, 
""timestamp_type"" TIMESTAMP )
{code};;;","27/Jul/16 09:36;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/14377;;;","05/Aug/16 15:12;srowen;Issue resolved by pull request 14377
[https://github.com/apache/spark/pull/14377];;;","22/Feb/17 16:17;srowen;Does anyone have a view on whether this is OK to back-port to 2.0.x or 1.6.x? ;;;","17/Sep/17 08:29;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/19259;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix NullPointerException when the returned value of the called method in Invoke is null,SPARK-16622,12990712,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,19/Jul/16 10:12,23/Jul/16 02:27,14/Jul/23 06:29,23/Jul/16 02:27,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"Currently we don't check the value returned by called method in Invoke. When the returned value is null, NullPointerException will be thrown.",,apachespark,cloud_fan,kiszk,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 23 02:27:50 UTC 2016,,,,,,,,,,"0|i316w7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/16 10:16;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/14259;;;","23/Jul/16 02:27;cloud_fan;Issue resolved by pull request 14259
[https://github.com/apache/spark/pull/14259];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RDD.pipe(command: String) in Spark 2.0 does not work when command is specified with some options,SPARK-16620,12990657,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,proflin,proflin,proflin,19/Jul/16 06:06,19/Jul/16 17:25,14/Jul/23 06:29,19/Jul/16 17:25,2.0.0,,,,,,,,2.0.0,,,,Spark Core,,,,,,,,0,,,,,,"Currently {{RDD.pipe(command: String)}}:

- works only when the command is specified without any options, such as {{RDD.pipe(""wc"")}}
- does NOT work when the command is specified with some options, such as {{RDD.pipe(""wc -l"")}}

This is a regression from Spark 1.6.",,apachespark,michaelmalak,proflin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16613,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 19 08:06:30 UTC 2016,,,,,,,,,,"0|i316jz:",9223372036854775807,,,,,,,,,,,,,2.0.0,,,,,,,,,,,"19/Jul/16 06:13;apachespark;User 'lw-lin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14256;;;","19/Jul/16 08:06;srowen;I'm calling this a Blocker just because a big part of the pipe() functionality was broken by an earlier commit for 2.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RDD.pipe returns values for empty partitions,SPARK-16613,12990565,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,finkel,finkel,18/Jul/16 21:37,20/Jul/16 16:49,14/Jul/23 06:29,20/Jul/16 16:49,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,,1,,,,,,"Suppose we have such Spark code

{code}
object PipeExample {
  def main(args: Array[String]) {

    val fstRdd = sc.parallelize(List(""hi"", ""hello"", ""how"", ""are"", ""you""))
    val pipeRdd = fstRdd.pipe(""/Users/finkel/spark-pipe-example/src/main/resources/len.sh"")

    pipeRdd.collect.foreach(println)
  }
}
{code}

It uses a bash script to convert a string to its length.

{code}
#!/bin/sh
read input
len=${#input}
echo $len
{code}

So far so good, but when I run the code, it prints incorrect output. For example:

{code}
0
2
0
5
3
0
3
3
{code}

I expect to see

{code}
2
5
3
3
3
{code}

which is correct output for the app. I think it's a bug. It's expected to see only positive integers and avoid zeros.

Environment:

1. Spark version is 1.6.2
2. Scala version is 2.11.6
",,apachespark,dongjoon,finkel,proflin,rxin,tejasp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16620,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 20 08:43:36 UTC 2016,,,,,,,,,,"0|i315zj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/16 06:08;dongjoon;Thank you for amazingly clean reporting. I could easily regenerate that. In 1.6 branch, the problem still exists. In the current master, I experienced `StackOverflowError`.
{code}
scala> val fstRdd = sc.parallelize(List(""hi"", ""hello"", ""how"", ""are"", ""you""))
fstRdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:24

scala> val pipeRdd = fstRdd.pipe(""/Users/dongjoon/spark/len.sh"")
java.lang.StackOverflowError
  at com.fasterxml.jackson.databind.deser.impl.PropertyBasedCreator.startBuilding(PropertyBasedCreator.java:130)
{code}
It's not final investigation, but it's worth to look inside.;;;","19/Jul/16 07:04;proflin;Hi [~finkel], [~dongjoon], I think this relates to how many partitions we'll get after we {{sc.parallelize(...)}}.
For instance, {{sc.parallelize(..., 5)}} produces:
{code}
2
5
3
3
3
{code}
{{sc.parallelize(..., 8)}} produces:
{code}
0
2
0
5
3
0
3
3
{code}
And be careful, {{sc.parallelize(..., 1)}} produces only:
{code}
2
{code};;;","19/Jul/16 07:07;dongjoon;Oh. Thank you for analysis!;;;","19/Jul/16 07:08;dongjoon;Then, it seems not a bug in 1.6.2.;;;","19/Jul/16 07:17;proflin;[~dongjoon], at about the same time we reproduced the {{StackOverflowError}} from {{RDD.pipe()}}, :)
And besides that stack overflow, {{RDD.pipe(String)}} also doesn't work with commands with options like ""wc -l"" in 2.0, so I filed SPARK-16620 aiming to fix both issues.;;;","19/Jul/16 07:26;dongjoon;Yep. I thought so. You're very fast! Great.;;;","19/Jul/16 09:11;srowen;Interesting, I think an issue here is that the semantics of RDD.pipe() are unclear. Input and output are RDD[String], and it seems like each element of the output must be stdout of running the command on a single input element.

In reality, the process is run once per partition, and each input element is sent to the stdin, separated by newlines. The lines of the output are parsed as the output of the partition -- which means that a process which outputs many lines of output results in many elements of the RDD.

Right now, the process is still invoked for an empty partition, and for this script, correctly results in ""0"".

The docs do say ""pipes elements to an external process"" which sort of implies the current behavior.

I think we should, in any event, clarify docs and probably also modify the behavior to output nothing for an empty partition -- not even the result of the process when presented with no input.

I'm reluctant to change the semantics of the method to run one process per input, even if that strikes me as more logical. This means we're kind of stuck with this problem that it's not necessarily possible to match outputs 1:1 with inputs, but that's just a constraint on the type of command you can use with this I guess.

CC [~andrewor14] if available, but moreso [~tejasp];;;","19/Jul/16 10:49;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14260;;;","19/Jul/16 18:14;rxin;After thinking about this more I don't think we should break the old semantics. We can document it though;;;","20/Jul/16 03:05;tejasp;[~srowen] , [~rxin] : I feel that invoking the pipe command even for empty partitions is bad. Users should be exposed to Spark partitions. Leaving the existing behavior as it is might mean that if number of partitions change OR partitioning scheme changes, the results generated for the same input data can differ. This might be annoying. One would expect Spark's behavior to be same as running the pipe command in standalone way over terminal in which case there won't be any empty partitions as partition is a Spark internal concept.

Hive's ScriptOperator invokes the user binary only when it sees a row : https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java#L339 

Spark's ScriptTransformation as well follows the same convention:
https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/ScriptTransformation.scala#L244;;;","20/Jul/16 05:13;rxin;But the problem of result changing depending on partitioning always exist -- it is there because how the underlying commands are handling the inputs; it's not there because Spark calls the command based on the input. 

We can add a flag to control it if we really want that behavior.
;;;","20/Jul/16 08:43;srowen;Yeah it's a tough call. The current behavior is at least consistent: entirely partition-oriented, one process per partition exactly, always. I agree it's not quite what I'd expect, but maybe the first thing we can do now is at least update the docs without changing the behavior.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When writing ORC files, orc.compress should not be overridden if users do not set ""compression"" in the options",SPARK-16610,12990498,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,yhuai,yhuai,18/Jul/16 18:22,12/Dec/22 18:11,14/Jul/23 06:29,09/Aug/16 02:25,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"For ORC source, Spark SQL has a writer option {{compression}}, which is used to set the codec and its value will be also set to orc.compress (the orc conf used for codec). However, if a user only set {{orc.compress}} in the writer option, we should not use the default value of ""compression"" (snappy) as the codec. Instead, we should respect the value of {{orc.compress}}.",,apachespark,cloud_fan,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 09 02:25:48 UTC 2016,,,,,,,,,,"0|i315kn:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"18/Jul/16 18:23;yhuai;[~hyukjin.kwon] Will you have time to take a look at this bug? Thank you!;;;","19/Jul/16 01:59;gurwls223;Thank you for cc me! Yea, actually I pointed this out in the PR. Please check out this https://github.com/apache/spark/pull/13048#discussion_r63132718;;;","03/Aug/16 00:15;gurwls223;One thought is, we might have to document that we don't respect Hadoop configuration anymore officially if it is.;;;","03/Aug/16 18:10;yhuai;It is a ORC setting. It should be accepted by the dataframe option (when you use df.write.option). I do not think it makes sense to drop this conf key provided by ORC and just use Spark's key.;;;","04/Aug/16 02:56;gurwls223;Actually, my initial proposal of the above was including that but I followed the suggestion at the end.

I hope I didn't misunderstood both you and [~rxin].

Do you mind if I ask feedback please? 

 



;;;","05/Aug/16 17:55;yhuai;Sure. For ORC, {{orc.compression}} is the actual conf that lets ORC writer know what codec to use. I think it is fine to have {{compression}}. But, if a user uses {{orc.compression}} and do not use {{compression}}, we just cannot ignore the value of {{orc.compression}} and use the default value of {{compression}} to override {{orc.compression}}. ;;;","06/Aug/16 02:15;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/14518;;;","09/Aug/16 02:25;cloud_fan;Issue resolved by pull request 14518
[https://github.com/apache/spark/pull/14518];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spark2.0 cannot ""select"" data from a table stored as an orc file which has been created by hive while hive or spark1.6 supports",SPARK-16605,12990369,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,marymwu,marymwu,18/Jul/16 12:15,09/Nov/17 20:14,14/Jul/23 06:29,10/Nov/16 08:52,2.0.0,2.1.2,2.2.0,,,,,,2.2.1,2.3.0,,,SQL,,,,,,,,0,,,,,,"Spark2.0 cannot ""select"" data from a table stored as an orc file which has been created by hive while hive or spark1.6 supports

Steps:
1. Use hive to create a table ""tbtxt"" stored as txt and load data into it.
2. Use hive to create a table ""tborc"" stored as orc and insert the data from table ""tbtxt"" . Example, ""create table tborc stored as orc as select * from tbtxt""
3. Use spark2.0 to ""select * from tborc;"".-->error occurs,java.lang.IllegalArgumentException: Field ""nid"" does not exist.",,cloud_fan,dongjoon,marymwu,xwu0226,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-14387,,,,,,,,,,,,,,,,,,"18/Jul/16 12:16;marymwu;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12818537/screenshot-1.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 09 20:13:22 UTC 2017,,,,,,,,,,"0|i314s7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/16 21:16;xwu0226;The current issue for dealing with ORC data inserted by Hive is that the schema stored in orc file inserted by hive is using dummy column name such as ""_col1, _col2, ..."". Hive knows how to read the data. However, in Spark SQL, for performance gain, it tries to convert ORC table to its native ORC relation for scanning, in that it infers schema from orc file directly but getting the table schema from hive metastore. There are then mismatch here. 

Try the workaround that turns off this conversion for performance: 
{code}set spark.sql.hive.convertMetastoreOrc=false{code}

Then, see if it works. ;;;","01/Aug/16 06:28;marymwu;I see,thanks!;;;","10/Nov/16 08:52;cloud_fan;this conf is false by default now, I'm closing this ticket;;;","09/Nov/17 20:13;dongjoon;This duplicates SPARK-14387.
SPARK-14387 is resolved in Spark 2.2.1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spark2.0-error occurs when execute the sql statement which includes ""nvl"" function while spark1.6 supports",SPARK-16602,12990350,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,marymwu,marymwu,18/Jul/16 11:39,19/Jul/16 17:27,14/Jul/23 06:29,19/Jul/16 17:27,2.0.0,,,,,,,,2.0.0,,,,SQL,,,,,,,,1,,,,,,"Spark2.0-error occurs when execute the sql statement which includes ""nvl"" function while spark1.6 supports

Error: org.apache.spark.sql.AnalysisException: cannot resolve 'nvl(b.`new_user`, 0)' due to data type mismatch: input to function coalesce should all be the same type, but it's [string, int]; line 2 pos 73 (state=,code=0)",,apachespark,dongjoon,marymwu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 18 22:13:04 UTC 2016,,,,,,,,,,"0|i314nz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/16 21:45;dongjoon;Right. I get the same result with you for the following query. 1.6 works. 2.0 raises exception.
{code}
sql(""select nvl('0', '1'), nvl('0', 1)"")
{code}
I'll investigate this.;;;","18/Jul/16 22:13;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/14251;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix latex formula syntax error in mllib,SPARK-16600,12990320,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,weichenxu123,weichenxu123,weichenxu123,18/Jul/16 09:29,20/Jul/16 02:31,14/Jul/23 06:29,19/Jul/16 11:08,,,,,,,,,2.0.0,,,,Documentation,MLlib,,,,,,,0,,,,,,"I found several place the latex formula use
\partial\xx
it should be 
\partial xx
",,apachespark,weichenxu123,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 19 11:08:10 UTC 2016,,,,,,,,,,"0|i314hb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/16 09:33;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/14246;;;","19/Jul/16 11:08;srowen;Issue resolved by pull request 14246
[https://github.com/apache/spark/pull/14246];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Chained cartesian produces incorrect number of records,SPARK-16589,12990133,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,a1ray,zero323,zero323,16/Jul/16 21:48,15/Dec/16 05:04,14/Jul/23 06:29,08/Dec/16 19:11,1.4.0,1.5.0,1.6.0,2.0.0,,,,,2.0.3,2.1.0,,,PySpark,,,,,,,,1,correctness,,,,,"Chaining cartesian calls in PySpark results in the number of records lower than expected. It can be reproduced as follows:

{code}
rdd = sc.parallelize(range(10), 1)
rdd.cartesian(rdd).cartesian(rdd).count()
## 355

rdd.cartesian(rdd).cartesian(rdd).distinct().count()
## 251
{code}

It looks like it is related to serialization. If we reserialize after initial cartesian:

{code}
rdd.cartesian(rdd)._reserialize(BatchedSerializer(PickleSerializer(), 1)).cartesian(rdd).count()
## 1000
{code}

or insert identity map:

{code}
rdd.cartesian(rdd).map(lambda x: x).cartesian(rdd).count()
## 1000
{code}

it yields correct results.
 ",,a1ray,apachespark,dongjoon,holden,michaelmalak,nchammas,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17756,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 02 15:56:04 UTC 2016,,,,,,,,,,"0|i313br:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/16 22:35;dongjoon;Oh, Indeed, there is a bug of PySpark.
Could you make a PR for this?;;;","18/Jul/16 17:08;zero323;[~dongjoon] I'll work on that but I am not exactly sure what is the best approach here. If feel like my current fix is slightly suboptimal.;;;","18/Jul/16 17:10;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/14248;;;","18/Jul/16 17:29;dongjoon;For me, it looks good to me. In the PR, you can get advices faster if you ask review to `JoshRosen`.;;;","19/Jul/16 14:35;zero323;Thanks [~dongjoon].

[~joshrosen]  Could you take a look at this?;;;","22/Jul/16 22:24;holden;Yah I think we should explore whats going on a bit more detail here - serializing it seems like we might just be hiding something which is generally broken. I could be wrong though but I think this needs a bit more investigation before going to the PR stage.;;;","24/Jul/16 17:07;zero323;[~holdenk] Makes sense. I was thinking more about design than other possible issues but it is probably better safe than sorry. It still should be fixed as fast as possible. It is really ugly bug and is easy to miss.

I doubt there are many legitimate cases when one can do something like this though (I guess this is why it hasn't been reported before).;;;","03/Oct/16 20:35;holden;Is this something you are still investigating/working on actively?;;;","04/Oct/16 12:34;zero323;Not actively so if you want to give it a shot go ahead. 

I investigated a little bit deeper and tried to fix it closer to the sources but it ended in a hell full of special cases which makes me think that we should never expose data requiring `CartesianDeserializer` directly (there is also a SPARK-16589).;;;","02/Dec/16 15:56;apachespark;User 'aray' has created a pull request for this issue:
https://github.com/apache/spark/pull/16121;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"spark-class crash with ""[: too many arguments"" instead of displaying the correct error message",SPARK-16586,12990046,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,zasdfgbnm,zasdfgbnm,16/Jul/16 08:57,08/Aug/16 17:35,14/Jul/23 06:29,08/Aug/16 17:35,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,,0,,,,,,"When trying to run spark on a machine that cannot provide enough memory for java to use, instead of printing the correct error message, spark-class will crash with {{spark-class: line 83: [: too many arguments}}

Simple shell commands to trigger this problem are:
{code}
ulimit -v 100000
./sbin/start-master.sh
{code}",,apachespark,zasdfgbnm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 05 20:04:05 UTC 2016,,,,,,,,,,"0|i312sf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/16 09:06;srowen;Can you give an example? ;;;","16/Jul/16 09:09;apachespark;User 'zasdfgbnm' has created a pull request for this issue:
https://github.com/apache/spark/pull/14231;;;","05/Aug/16 20:04;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14508;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
partition calculation mismatch with sc.binaryFiles,SPARK-16575,12989934,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,fidato13,suhashm,suhashm,15/Jul/16 20:32,08/Nov/16 02:41,14/Jul/23 06:29,08/Nov/16 02:41,1.6.1,1.6.2,,,,,,,2.1.0,,,,Input/Output,Java API,Shuffle,Spark Core,Spark Shell,,,,0,,,,,,"sc.binaryFiles is always creating an RDD with number of partitions as 2.

Steps to reproduce: (Tested this bug on databricks community edition)
1. Try to create an RDD using sc.binaryFiles. In this example, airlines folder has 1922 files.
     Ex: {noformat}val binaryRDD = sc.binaryFiles(""/databricks-datasets/airlines/*""){noformat}

2. check the number of partitions of the above RDD
    - binaryRDD.partitions.size = 2. (expected value is more than 2)

3. If the RDD is created using sc.textFile, then the number of partitions are 1921.

4. Using the same sc.binaryFiles will create 1921 partitions in Spark 1.5.1 version.

For explanation with screenshot, please look at the link below,
http://apache-spark-developers-list.1001551.n3.nabble.com/Partition-calculation-issue-with-sc-binaryFiles-on-Spark-1-6-2-tt18314.html
",,apachespark,fidato13,rxin,suhashm,Tagar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 13 19:40:30 UTC 2016,,,,,,,,,,"0|i3123j:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"15/Jul/16 20:35;suhashm;There is a workaround for this, by using sc.binaryFiles(path, minPartitions) and passing a huge number say 20000 to the minPartitions, will set the partition accordingly. But still this is a bug and ideally when the RDD is created, it should set the number of partitions to the number of files and not 2 always.;;;","02/Oct/16 22:35;apachespark;User 'fidato13' has created a pull request for this issue:
https://github.com/apache/spark/pull/15327;;;","02/Oct/16 22:44;rxin;As responded on the pull request: ""I don't actually think this is a bug, because it is intended to do some coalescing. If there is an issue, the issue would be that we don't take the cost of individual files into account in this code path. The Spark SQL automatic coalescing code path does take that into account."";;;","02/Oct/16 22:49;suhashm;[~rxin] But the behavior was different in Spark 1.5.1 while loading the same files. Does it do  the coalescing even when the initial load of 1000+ files?;;;","02/Oct/16 22:52;rxin;The old behavior doesn't make sense either, because it risks creating tons of partitions in practice when there are a lot of small files. Again, I think the appropriate fix is to take into account there is a cost to processing each file (i.e. a file whose size is zero should not be treated as ""free""), similar to what Spark SQL does with the setting ""spark.sql.files.openCostInBytes"".
;;;","13/Oct/16 19:40;fidato13;[~rxin] I have now added the support of openCostInBytes, similar to SQL (thanks for pointing out). It does now create an optimized number of partitions. Request you to review and suggest. Once again, Thanks for your suggestion, It worked like a charm!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repeat calling Spark SQL thrift server fetchResults return empty for ExecuteStatement operation,SPARK-16563,12989681,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,alicegugu,alicegugu,alicegugu,15/Jul/16 05:58,31/Dec/16 11:52,14/Jul/23 06:29,09/Aug/16 01:01,1.6.2,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"Repeat calling FetchResults(... orientation=FetchOrientation.FETCH_FIRST ..) of spark sql thrift service will return empty set after calling ExecuteStatement of TCLIService. 

The bug exist in *function public RowSet getNextRowSet(FetchOrientation orientation, long maxRows)*
https://github.com/apache/spark/blob/02c8072eea72425e89256347e1f373a3e76e6eba/sql/hive-thriftserver/src/main/java/org/apache/hive/service/cli/operation/SQLOperation.java#L332

The iterator for geting result can be used for only once, so repeat calling FetchResults with FETCH_FIRST parameter will return empty result. 


FetchOrientation.FETCH_FIRST",,alicegugu,apachespark,vishalagrwal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18857,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 28 04:16:28 UTC 2016,,,,,,,,,,"0|i310jb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/16 08:30;apachespark;User 'alicegugu' has created a pull request for this issue:
https://github.com/apache/spark/pull/14218;;;","28/Dec/16 04:16;vishalagrwal;Fix for this issue is causing Thrift server to hang while fetching large amount of data from Thrift server. Raised a bug for the same SPARK-18857. kindly check.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential numerical problem in MultivariateOnlineSummarizer min/max,SPARK-16561,12989660,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weichenxu123,weichenxu123,weichenxu123,15/Jul/16 04:27,18/Nov/16 22:23,14/Jul/23 06:29,23/Jul/16 11:32,2.0.0,,,,,,,,2.1.0,,,,MLlib,,,,,,,,0,,,,,,"In `MultivariateOnlineSummarizer` min/max method, 
use judgement ""nnz(i) < weightSum"", it will cause some numerial problem
and make result unstable.

for example,
add two vector:
[10, -10] with weight 1e10
[0, 0] with weight 1e-10

using MultivariateOnlineSummarizer.min/max we will get
minVector = [10, -10]
maxVector = [10, -10]

but the right result should be
minVector = [0, -10]
maxVector = [10, 0]

The bug reason is that
(1e10 + 1e-10) == 1e10 (Double type)
because of the floating rounding.
and different accumulating or merging order may cause different result,
such as:

[10, -10] with weight 1e10
[0, 0] with weight 1e-7
....
(100 lines data [0, 0] with weight 1e-7)

using the input data order listed above, we will get the result:
minVector = [10, -10]
maxVector = [10, -10]

but if the input data order is as following:

[0, 0] with weight 1e-7
....
(100 lines data [0, 0] with weight 1e-7)
[10, -10] with weight 1e10

than it the result will be:
minVector = [0, -10]
maxVector = [10, 0]

that's because:
1e10 + 1e-7 + ... + 1e-7(add 100 times) == 1e10  (Double type)
but
1e-7 + ... + 1e-7(add 100 times) + 1e10 = 1.000000000000001E10 != 1e10  (Double type)



",,apachespark,weichenxu123,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 23 11:32:49 UTC 2016,,,,,,,,,,"0|i310en:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/16 04:32;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/14216;;;","23/Jul/16 11:32;srowen;Issue resolved by pull request 14216
[https://github.com/apache/spark/pull/14216];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Got java.lang.ArithmeticException when Num of Buckets is Set to Zero,SPARK-16559,12989636,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,smilegator,smilegator,smilegator,15/Jul/16 00:29,19/Oct/16 09:47,14/Jul/23 06:29,19/Oct/16 05:16,2.0.0,,,,,,,,,,,,SQL,,,,,,,,0,,,,,,"Got a run-time java.lang.ArithmeticException when num of buckets is set to zero.

For example,
{noformat}
CREATE TABLE t USING PARQUET
OPTIONS (PATH '${path.toString}')
CLUSTERED BY (a) SORTED BY (b) INTO 0 BUCKETS
AS SELECT 1 AS a, 2 AS b
{noformat}
The exception we got is
{noformat}
ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 1.0 (TID 2)
java.lang.ArithmeticException: / by zero
{noformat}
",,apachespark,dongjoon,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 19 09:47:49 UTC 2016,,,,,,,,,,"0|i3109b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/16 00:30;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14210;;;","19/Oct/16 05:14;dongjoon;Hi, [~smilegator].
It seems to be resolved in your PR, didn't it?;;;","19/Oct/16 05:15;smilegator;Yeah, resolved.;;;","19/Oct/16 09:47;dongjoon;Thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
examples/mllib/LDAExample should use MLVector instead of MLlib Vector,SPARK-16558,12989622,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yinxusen,yinxusen,yinxusen,14/Jul/16 23:40,02/Aug/16 14:33,14/Jul/23 06:29,02/Aug/16 14:33,,,,,,,,,2.0.1,2.1.0,,,Examples,MLlib,,,,,,,0,,,,,,"mllib.LDAExample uses ML pipeline and MLlib LDA algorithm. The former transforms original data into MLVector format, while the latter uses MLlibVector format.",,apachespark,yinxusen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 14 23:43:04 UTC 2016,,,,,,,,,,"0|i31067:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/16 23:43;apachespark;User 'yinxusen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14212;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Silent Ignore Bucket Specification When Creating Table Using Schema Inference ,SPARK-16556,12989603,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,14/Jul/16 22:40,22/Jul/16 05:29,14/Jul/23 06:29,22/Jul/16 05:29,2.0.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"When creating a data source table without explicit specification of schema or SELECT clause, we silently ignore the bucket specification (CLUSTERED BY... SORTED BY...). For example, 
{noformat}
CREATE TABLE jsonTable
USING org.apache.spark.sql.json
OPTIONS (
  path '${tempDir.getCanonicalPath}'
)
CLUSTERED BY (inexistentColumnA) SORTED BY (inexistentColumnB) INTO 2 BUCKETS
{noformat}

Instead, we should issue an error message. 
",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 14 22:55:04 UTC 2016,,,,,,,,,,"0|i3101z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/16 22:55;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14210;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Work around Jekyll error-handling bug which led to silent doc build failures,SPARK-16555,12989595,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,14/Jul/16 22:10,20/Jul/16 02:31,14/Jul/23 06:29,14/Jul/16 22:55,,,,,,,,,2.0.0,,,,Documentation,Project Infra,,,,,,,0,,,,,,"If a custom Jekyll template tag throws Ruby's equivalent of a ""file not found"" exception, then Jekyll will stop the doc building process but will exit with a successful status, causing our doc publishing jobs to silently fail.

This is caused by https://github.com/jekyll/jekyll/issues/5104, a case of bad error-handling logic in Jekyll. We should work around this in Spark to avoid more silent documentation build breaks.

(See SPARK-16553 for an example of a build failure which was ignored due to this Jekyll bug).",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 14 22:14:04 UTC 2016,,,,,,,,,,"0|i31007:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/16 22:14;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14209;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in Spark SQL Programming guide that links to examples,SPARK-16553,12989573,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shivaram,shivaram,shivaram,14/Jul/16 21:00,20/Jul/16 02:31,14/Jul/23 06:29,14/Jul/16 21:19,,,,,,,,,2.0.0,,,,Documentation,,,,,,,,0,,,,,,The reference to the scala file has a typo at https://github.com/apache/spark/blob/91575cac32e470d7079a55fb86d66332aba599d0/docs/sql-programming-guide.md#running-sql-queries-programmatically,,apachespark,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 14 21:02:05 UTC 2016,,,,,,,,,,"0|i30zvb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/16 21:02;apachespark;User 'shivaram' has created a pull request for this issue:
https://github.com/apache/spark/pull/14208;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Caching data with replication doesn't replicate data,SPARK-16550,12989543,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ekhliang,shubhamc,shubhamc,14/Jul/16 19:05,22/Aug/16 23:32,14/Jul/23 06:29,22/Aug/16 23:31,2.0.0,,,,,,,,2.0.1,2.1.0,,,Block Manager,Spark Core,,,,,,,0,,,,,,"Caching multiple replicas of blocks is currently broken. The following examples show replication doesn't happen for various use-cases:

These were run using Spark 2.0.0-preview, in local-cluster[2,1,1024] mode

{noformat}
case class TestInteger(i: Int)
val data = sc.parallelize((1 to 1000).map(TestInteger(_)), 10).persist(MEMORY_ONLY_2)
data.count
{noformat}

sc.getExecutorStorageStatus.map(s => s.rddBlocksById(data.id).size).sum shows only 10 blocks as opposed to the expected 20
Block replication fails on the executors with a java.lang.RuntimeException: java.lang.ClassNotFoundException: $line14.$read$$iw$$iw$TestInteger

{noformat}
val data1 = sc.parallelize(1 to 1000, 10).persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY_2)
data1.count

Block replication again fails with the following errors:
16/07/14 14:50:40 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() on RPC id 8567643992794608648
com.esotericsoftware.kryo.KryoException: Encountered unregistered class ID: 13994
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:137)
	at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:670)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:781)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:229)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:169)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:213)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutBytes$1.apply(BlockManager.scala:775)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutBytes$1.apply(BlockManager.scala:753)
{noformat}

sc.getExecutorStorageStatus.map(s => s.rddBlocksById(data1.id).size).sum again shows 10 blocks

Caching serialized data works for native types, but not for custom classes

{noformat}
val data3 = sc.parallelize(1 to 1000, 10).persist(MEMORY_ONLY_SER_2)
data3.count
{noformat}

works as intended.

But 
{noformat}
val data4 = sc.parallelize((1 to 1000).map(TestInteger(_)), 10).persist(MEMORY_ONLY_SER_2)
data4.count
{noformat}

Again doesn't replicate data and executors show the same ClassNotFoundException

These examples worked fine and showed expected results with Spark 1.6.2",,apachespark,joshrosen,neelesh77,rxin,shubhamc,skadambi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 25 21:11:59 UTC 2016,,,,,,,,,,"0|i30zon:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/16 19:17;srowen;You have a shell-related problem here that caused the failure. It has nothing to do with replication. Basically, you'll find that case classes in the shell don't necessarily work in all cases. These are more functions of how Scala works and not Spark issues per se. Compiled programs will work fine. I'll close this.;;;","14/Jul/16 22:03;shubhamc;Example code: 
{noformat}
case class TestInteger(i: Int)

object TestApp {
  def main(args: Array[String]) {
    val conf = (new SparkConf).setAppName(""Test app"").setMaster(""yarn-client"")
    val sc = new SparkContext(conf)

    val data = sc.parallelize(1 to 1000, 10).persist(StorageLevel.MEMORY_ONLY_2)
    println(data.count)
    println(s""Total number of blocks in data: ${sc.getExecutorStorageStatus.map(_.rddBlocksById(data.id).size).sum}"")

    val dataTestInt = sc.parallelize((1 to 1000).map(TestInteger(_)), 10).persist(StorageLevel.MEMORY_ONLY_2)
    println(dataTestInt.count)
    println(s""Total number of blocks in dataTestInt: ${sc.getExecutorStorageStatus.map(_.rddBlocksById(dataTestInt.id).size).sum}"")

    val dataInteger = sc.parallelize((1 to 1000).map(new Integer(_)), 10).persist(StorageLevel.MEMORY_ONLY_2)
    println(dataInteger.count)
    println(s""Total number of blocks in dataInteger: ${sc.getExecutorStorageStatus.map(_.rddBlocksById(dataInteger.id).size).sum}"")

    val dataSerialized = sc.parallelize(1 to 1000, 10).persist(StorageLevel.MEMORY_ONLY_SER_2)
    println(dataSerialized.count)
    println(s""Total number of blocks in dataSerialized: ${sc.getExecutorStorageStatus.map(_.rddBlocksById(dataSerialized.id).size).sum}"")

    val dataTestIntSer = sc.parallelize((1 to 1000).map(TestInteger(_)), 10).persist(StorageLevel.MEMORY_ONLY_SER_2)
    println(dataTestIntSer.count)
    println(s""Total number of blocks in dataTestIntSer: ${sc.getExecutorStorageStatus.map(_.rddBlocksById(dataTestIntSer.id).size).sum}"")

  }
}
{noformat}

Output:
{quote}
1000
Total number of blocks in data: 10
1000
Total number of blocks in dataTestInt: 10
1000
Total number of blocks in dataInteger: 20
1000
Total number of blocks in dataSerialized: 20
1000
Total number of blocks in dataTestIntSer: 10
{quote}

The issue exists when I submit a compiled program as well. The exception stack traces are similar to the ones posted above.

I think part of the problem might be related to [SPARK-13990|https://issues.apache.org/jira/browse/SPARK-13990]

This code works fine in Spark 1.6.2, both in the shell and when submitted as compiled code.;;;","14/Jul/16 22:13;srowen;[~shubhamc] don't set Blocker. Even if this is a real issue, it would not block a release.;;;","14/Jul/16 22:41;rxin;cc [~joshrosen] who wrote SPARK-13990.
;;;","14/Jul/16 23:25;joshrosen;*For the case involving a REPL-defined class:* The ClassNotFoundException is occurring because the NettyBlockRpcServer is attempting to deserialize a ClassTag field and that ClassTag references a class which is only present in the REPL classloader and not in the executor JVM's default classloader. One potential solution is to use the executor's REPL / context classloader for deserializing ClassTags, but this might be slightly non-straightforward due to some complicated initialization ordering dependencies.

*For the case involving integers:* I omitted a classTag inside a call in {{doGetLocalBytes()}}, which is called as part of the write-side of the block replication code.;;;","15/Jul/16 17:57;joshrosen;I have a partial fix for the ""unable to replicate integers"" issue, including an enhancement to the caching tests in DistributedSuite to make them capable of catching this bug: https://github.com/JoshRosen/spark/tree/SPARK-16550

All that remains is to fix the ClassTag + Classloading issue so that this also works for REPL-defined classes (or, alternatively, just erase the caching ClassTag to ClassTag\[Any] since we're just going to use the default serializer).;;;","21/Jul/16 23:57;apachespark;User 'ericl' has created a pull request for this issue:
https://github.com/apache/spark/pull/14311;;;","22/Jul/16 02:46;rxin;[~shubhamc] can you test the patch [~ekhliang] submitted?;;;","25/Jul/16 21:11;shubhamc;[~rxin] I did some basic testing using the patch, and it shows the desired replication. Thanks [~ekhliang]!

The patch, however, does fail some unit tests in DistributedSuite.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.io.CharConversionException: Invalid UTF-32 character  prevents me from querying my data,SPARK-16548,12989510,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,epahomov,epahomov,14/Jul/16 17:05,24/Jul/19 19:03,14/Jul/23 06:29,26/Apr/17 03:44,1.6.1,,,,,,,,2.2.0,2.3.0,,,SQL,,,,,,,,0,,,,,,"Basically, when I query my json data I get 
{code}
java.io.CharConversionException: Invalid UTF-32 character 0x7b2265(above 10ffff)  at char #192, byte #771)
	at com.fasterxml.jackson.core.io.UTF32Reader.reportInvalid(UTF32Reader.java:189)
	at com.fasterxml.jackson.core.io.UTF32Reader.read(UTF32Reader.java:150)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.loadMore(ReaderBasedJsonParser.java:153)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:1855)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:571)
	at org.apache.spark.sql.catalyst.expressions.GetJsonObject$$anonfun$eval$2$$anonfun$4.apply(jsonExpressions.scala:142)
{code}

I do not like it. If you can not process one json among 100500 please return null, do not fail everything. I have dirty one line fix, and I understand how I can make it more reasonable. What is our position - what behaviour we wanna get?",,apachespark,bijithkumar,cloud_fan,epahomov,marmbrus,shivakumar.ss,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20314,,,,,,,,,,SPARK-23094,,,,,,,,,,,,,,"08/Apr/19 22:43;bijithkumar;corrupted.json;https://issues.apache.org/jira/secure/attachment/12965255/corrupted.json",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 24 19:03:59 UTC 2019,,,,,,,,,,"0|i30zhb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/16 17:43;srowen;Tough call, just because returning 'null' is also arguably wrong, silently swallowing the error. In your own code you could handle this, but, I know this comes from inside Spark SQL. You can pre-process your input for invalid data. That is likely a good idea. I am not sure I would change Spark here.;;;","14/Jul/16 18:47;epahomov;[~sowen] if your concern is ""silently"" we can wright errors to logs. WDYT?;;;","14/Jul/16 18:51;srowen;Yeah sure, but is that much better? the job continues and is missing some data. Maybe that's OK, maybe not, but it's also something you can handle in preprocessing;;;","19/Apr/17 23:20;marmbrus;I'm not sure I agree.  The default behavior for parsing corrupted JSON is to return {{null}} and fill in the column {{_corrupt_record}} (same for casts that fail or other error cases).  It's weird that this one case is different.;;;","21/Apr/17 18:28;apachespark;User 'ewasserman' has created a pull request for this issue:
https://github.com/apache/spark/pull/17693;;;","05/Apr/19 19:48;bijithkumar;[~cloud_fan] I am getting the same Exception in Spark 2.3.2. Wondering why would that happen since this is fixed in 2.3.0
{code:java}
java.io.CharConversionException: Invalid UTF-32 character 0x4d89aa(above 10ffff) at char #63, byte #255) at com.fasterxml.jackson.core.io.UTF32Reader.reportInvalid(UTF32Reader.java:189) at com.fasterxml.jackson.core.io.UTF32Reader.read(UTF32Reader.java:150) at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.loadMore(ReaderBasedJsonParser.java:153) at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2017) at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:577) at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$parse$2.apply(JacksonParser.scala:350) at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$parse$2.apply(JacksonParser.scala:347) at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2589) at org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:347) at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$3.apply(JsonDataSource.scala:128) at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$3.apply(JsonDataSource.scala:128) at org.apache.spark.sql.execution.datasources.FailureSafeParser.parse(FailureSafeParser.scala:61) at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$readFile$2.apply(JsonDataSource.scala:132) at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$readFile$2.apply(JsonDataSource.scala:132) at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439) at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191) at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) at org.apache.spark.scheduler.Task.run(Task.scala:109) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)
{code};;;","08/Apr/19 14:28;cloud_fan;Do you have a small dateset to reproduce it?;;;","08/Apr/19 22:46;bijithkumar;[~cloud_fan], I couldn't find the specific character of the corrupted data that is causing the issue. However, here is the corrupted section from file to reproduce the issue. Please see attached - [^corrupted.json]. ;;;","24/Jul/19 19:03;shivakumar.ss;Hello guys, 

I am using Spark 2.3.0 and facing the same issue, i have to process huge amount of files and job is getting failed due to 1 or 2 files. 

I have following code snippet to read the json files 

{{val df = sqlContext.read.schema(Schemas.request_01)}}
{{.json(inputPath1,inputPath2)}}

{{df.show(10)}}

 

 

Following is the exception which i am receiving. 

{{}}{{19/07/25 00:17:50 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, 192.168.1.100, executor 0): java.io.CharConversionException: Invalid UTF-32 character 0x4d89aa(above 10ffff) at char #63, byte #255)}}
{{ at com.fasterxml.jackson.core.io.UTF32Reader.reportInvalid(UTF32Reader.java:189)}}
{{ at com.fasterxml.jackson.core.io.UTF32Reader.read(UTF32Reader.java:150)}}
{{ at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.loadMore(ReaderBasedJsonParser.java:153)}}
{{ at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2017)}}
{{ at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:577)}}
{{ at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$parse$2.apply(JacksonParser.scala:350)}}
{{ at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$parse$2.apply(JacksonParser.scala:347)}}
{{ at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2585)}}
{{ at org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:347)}}
{{ at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$3.apply(JsonDataSource.scala:126)}}
{{ at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$3.apply(JsonDataSource.scala:126)}}
{{ at org.apache.spark.sql.execution.datasources.FailureSafeParser.parse(FailureSafeParser.scala:61)}}
{{ at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$readFile$2.apply(JsonDataSource.scala:130)}}
{{ at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$readFile$2.apply(JsonDataSource.scala:130)}}
{{ at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)}}
{{ at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)}}
{{ at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)}}
{{ at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:106)}}
{{ at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)}}
{{ at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)}}
{{ at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)}}
{{ at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)}}
{{ at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)}}
{{ at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)}}
{{ at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)}}
{{ at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)}}
{{ at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)}}
{{ at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)}}
{{ at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)}}
{{ at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)}}
{{ at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)}}
{{ at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)}}
{{ at org.apache.spark.scheduler.Task.run(Task.scala:109)}}
{{ at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)}}
{{ at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)}}
{{ at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)}}
{{ at java.lang.Thread.run(Thread.java:748)}}

 

Can any of you help me to ignore these json files and process with other valid files.  ?

 

Thanks

Shiva Kumar SS

 

 

 

 

 

 

 

 

 

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bugs about types that result an array of null when creating dataframe using python,SPARK-16542,12989391,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zasdfgbnm,zasdfgbnm,zasdfgbnm,14/Jul/16 09:00,20/Jul/17 03:46,14/Jul/23 06:29,20/Jul/17 03:46,,,,,,,,,2.3.0,,,,PySpark,SQL,,,,,,,1,,,,,,"This is a bugs about types that result an array of null when creating DataFrame using python.

Python's array.array have richer type than python itself, e.g. we can have {{array('f',[1,2,3])}} and {{array('d',[1,2,3])}}. Codes in spark-sql didn't take this into consideration which might cause a problem that you get an array of null values when you have {{array('f')}} in your rows.

A simple code to reproduce this is:

{code}
from pyspark import SparkContext
from pyspark.sql import SQLContext,Row,DataFrame
from array import array

sc = SparkContext()
sqlContext = SQLContext(sc)

row1 = Row(floatarray=array('f',[1,2,3]), doublearray=array('d',[1,2,3]))
rows = sc.parallelize([ row1 ])
df = sqlContext.createDataFrame(rows)
df.show()
{code}

which have output
{code}
+---------------+------------------+
|    doublearray|        floatarray|
+---------------+------------------+
|[1.0, 2.0, 3.0]|[null, null, null]|
+---------------+------------------+
{code}",,apachespark,ueshin,zasdfgbnm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 20 03:46:37 UTC 2017,,,,,,,,,,"0|i30yqv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/16 09:03;apachespark;User 'zasdfgbnm' has created a pull request for this issue:
https://github.com/apache/spark/pull/14198;;;","28/Jun/17 01:37;apachespark;User 'zasdfgbnm' has created a pull request for this issue:
https://github.com/apache/spark/pull/18444;;;","20/Jul/17 03:46;ueshin;Issue resolved by pull request 18444
[https://github.com/apache/spark/pull/18444];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jars specified with --jars will added twice when running on YARN,SPARK-16540,12989368,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,jerryshao,jerryshao,14/Jul/16 07:18,17/May/20 18:14,14/Jul/23 06:29,14/Jul/16 17:41,2.0.0,,,,,,,,2.0.0,,,,Deploy,Spark Core,YARN,,,,,,0,,,,,,"Currently when running spark on yarn, jars specified with \--jars, \--packages will be added twice, one is Spark's own file server, another is yarn's distributed cache, this can be seen from log:

for example:

{code}
./bin/spark-shell --master yarn-client --jars examples/target/scala-2.11/jars/scopt_2.11-3.3.0.jar
{code}

If specified the jar to be added is scopt jar, it will added twice:

{noformat}
...
16/07/14 15:06:48 INFO Server: Started @5603ms
16/07/14 15:06:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/07/14 15:06:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
16/07/14 15:06:48 INFO SparkContext: Added JAR file:/Users/sshao/projects/apache-spark/examples/target/scala-2.11/jars/scopt_2.11-3.3.0.jar at spark://192.168.0.102:63996/jars/scopt_2.11-3.3.0.jar with timestamp 1468480008637
16/07/14 15:06:49 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
16/07/14 15:06:49 INFO Client: Requesting a new application from cluster with 1 NodeManagers
16/07/14 15:06:49 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
16/07/14 15:06:49 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
16/07/14 15:06:49 INFO Client: Setting up container launch context for our AM
16/07/14 15:06:49 INFO Client: Setting up the launch environment for our AM container
16/07/14 15:06:49 INFO Client: Preparing resources for our AM container
16/07/14 15:06:49 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
16/07/14 15:06:50 INFO Client: Uploading resource file:/private/var/folders/tb/8pw1511s2q78mj7plnq8p9g40000gn/T/spark-a446300b-84bf-43ff-bfb1-3adfb0571a42/__spark_libs__6486179704064718817.zip -> hdfs://localhost:8020/user/sshao/.sparkStaging/application_1468468348998_0009/__spark_libs__6486179704064718817.zip
16/07/14 15:06:51 INFO Client: Uploading resource file:/Users/sshao/projects/apache-spark/examples/target/scala-2.11/jars/scopt_2.11-3.3.0.jar -> hdfs://localhost:8020/user/sshao/.sparkStaging/application_1468468348998_0009/scopt_2.11-3.3.0.jar
16/07/14 15:06:51 INFO Client: Uploading resource file:/private/var/folders/tb/8pw1511s2q78mj7plnq8p9g40000gn/T/spark-a446300b-84bf-43ff-bfb1-3adfb0571a42/__spark_conf__326416236462420861.zip -> hdfs://localhost:8020/user/sshao/.sparkStaging/application_1468468348998_0009/__spark_conf__.zip
...
{noformat}

Actually it is not necessary to add into Spark's file server. This problem exists both in client and cluster modes, and it is introduced in SPARK-15782 to fix the \--packages not work in spark-shell.",,apachespark,bryanc,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 14 07:50:04 UTC 2016,,,,,,,,,,"0|i30ylr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/16 07:50;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/14196;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cannot use ""SparkR::sql""",SPARK-16538,12989307,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,felixcheung,weiluo_ren123,weiluo_ren123,14/Jul/16 01:15,14/Jul/16 19:21,14/Jul/23 06:29,14/Jul/16 16:45,2.0.0,,,,,,,,2.0.0,,,,SparkR,,,,,,,,1,,,,,,"When call ""SparkR::sql"", an error pops up. For instance

{code}
SparkR::sql("""")

Error in get(paste0(funcName, "".default"")) :
 object '::.default' not found
{code}


https://github.com/apache/spark/blob/f4767bcc7a9d1bdd301f054776aa45e7c9f344a7/R/pkg/R/SQLContext.R#L51",,apachespark,dongjoon,felixcheung,shivaram,weiluo_ren123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 14 19:21:37 UTC 2016,,,,,,,,,,"0|i30y87:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/16 01:16;weiluo_ren123;Ping [~felixcheung];;;","14/Jul/16 02:55;shivaram;This also affects other functions like `SparkR::createDataFrame` -- The problem seems to be that the `funcName` we extract in `dispatchFunc` is `.` instead of being `sql` or `createDataFrame`. The line of code is
{code}
funcName <- as.character(sys.call(sys.parent())[[1]])
{code}

Not sure what is the fix though;;;","14/Jul/16 03:17;shivaram;So it looks like what is going on is that the `funcName` we get in the case of `SparkR::createDataFrame` is a list and not a single element. Further the list contents are [::, SparkR, createDataFrame] - So a change which looks like 

{code}
  funcName <- as.character(sys.call(sys.parent())[[1]])
  if (length(funcName) > 1) {
    funcName <- funcName[length(funcName)]
  }
{code}

seems to work. [~felixcheung] is there a better way to fix this ?;;;","14/Jul/16 03:52;felixcheung;Investigating...;;;","14/Jul/16 04:51;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/14195;;;","14/Jul/16 04:52;felixcheung;Thanks for reporting this!;;;","14/Jul/16 16:45;shivaram;Issue resolved by pull request 14195
[https://github.com/apache/spark/pull/14195];;;","14/Jul/16 18:22;felixcheung;Remove Fix Version 1.6.3 since SPARK-10903 was not in Branch-1.6
https://github.com/apache/spark/blob/branch-1.6/R/pkg/R/SQLContext.R

This should not be merged to Branch-1.6;;;","14/Jul/16 19:21;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/14206;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark application not handling preemption messages,SPARK-16533,12989252,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angolon@gmail.com,LucasW,LucasW,13/Jul/16 21:13,17/May/20 17:46,14/Jul/23 06:29,01/Sep/16 17:37,1.6.0,,,,,,,,2.0.1,2.1.0,,,EC2,Input/Output,Optimizer,Scheduler,Spark Core,Spark Submit,YARN,,2,,,,,,"Here is the scenario:
I launch job 1 into Q1 and allow it to grow to 100% cluster utilization.
I wait between 15-30 mins ( for this job to complete with 100% of the cluster available takes about 1hr so job 1 is between 25-50% complete). Note that if I wait less time then the issue sometimes does not occur, it appears to be only after the job 1 is at least 25% complete.
I launch job 2 into Q2 and preemption occurs on the Q1 shrinking the job to allow 70% of cluster utilization.
At this point job 1 basically halts progress while job 2 continues to execute as normal and finishes. Job 2 either:
- Fails its attempt and restarts. By the time this attempt fails the other job is already complete meaning the second attempt has full cluster availability and finishes.
- The job remains at its current progress and simply does not finish ( I have waited ~6 hrs until finally killing the application ).
 
Looking into the error log there is this constant error message:
WARN NettyRpcEndpointRef: Error sending message [message = RemoveExecutor(454,Container container_1468422920649_0001_01_000594 on host: ip-NUMBERS.ec2.internal was preempted.)] in X attempts
 
My observations have led me to believe that the application master does not know about this container being killed and continuously asks the container to remove the executor until eventually failing the attempt or continue trying to remove the executor.
 
I have done much digging online for anyone else experiencing this issue but have come up with nothing.","Yarn version: Hadoop 2.7.1-amzn-0

AWS EMR Cluster running:
1 x r3.8xlarge (Master)
52 x r3.8xlarge (Core)

Spark version : 1.6.0
Scala version: 2.10.5
Java version: 1.8.0_51

Input size: ~10 tb
Input coming from S3

Queue Configuration:
Dynamic allocation: enabled
Preemption: enabled
Q1: 70% capacity with max of 100%
Q2: 30% capacity with max of 100%

Job Configuration:
Driver memory = 10g
Executor cores = 6
Executor memory = 10g
Deploy mode = cluster
Master = yarn
maxResultSize = 4g
Shuffle manager = hash",angolon@gmail.com,apachespark,Deng FEI,Dhruve Ashar,emaadmanzoor,gaoyajun02,jonasamrich,krisden,lenhattan86@gmail.com,liushaohui,LucasW,r_anirban,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16702,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 02 04:30:08 UTC 2016,,,,,,,,,,"0|i30xvz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/16 01:38;emaadmanzoor;I had the same issue running on EC2 with single-core (m3.medium) nodes.

I was able to resolve it using the workaround mentioned in this issue: https://issues.apache.org/jira/browse/SPARK-13906

In {{spark-defaults.conf}} set {{spark.rpc.netty.dispatcher.numThreads 2}}.;;;","19/Jul/16 20:01;LucasW;I have changed this configuration value to 2 as well as 3 and I am still experiencing the same error messages, once again only if the first job is more than ~50% complete the job fails, yet the other job continues with no problems.;;;","19/Jul/16 20:12;emaadmanzoor;It appears to be unresolved for me too.

I did not see the 50% trend but I did face this issue when my cluster was slightly overloaded (for example, 4 cores with 4 Spark applications running). I observed that my application was thrashing; the queue that was preempted reclaimed its resources immediately and then got re-preempted.

You may be facing the same issue if you also see very high container ID's (in your case, {{container_1468422920649_0001_01_000594}} which probably points to your application 1's attempt 1 being preempted over 500 times). Could you check the YARN resource manager logs to see if your application rapidly releases and reclaims resources? Does this also happen to you when your cluster is overloaded?

I have not dug further but the thrashing appears related to this YARN issue: https://issues.apache.org/jira/browse/YARN-4059

I think setting {{yarn.scheduler.fair.locality.threshold.node}} in {{yarn-site.xml}} may help avoid thrashing: this is the number of scheduling opportunities to miss since the last container assignment (as a fraction of the cluster size).



;;;","19/Jul/16 20:36;LucasW;Looking into the container ID's I did find some extremely high numbers. I will increase in the threshold.node configuration and see what the results are.;;;","19/Jul/16 23:12;LucasW;Increasing this number also did not change anything. Although I am not using the fair scheduler I am using the capacity scheduler so this attribute does not work anyway. I used the capacity scheduler alternative and still no luck
;;;","19/Jul/16 23:15;emaadmanzoor;Hopefully someone with more experience with Spark chimes in on this. I'll work on reproducing it and collecting the complete logs.

Note that I am experiencing this on Spark 2.0.0 too.;;;","19/Jul/16 23:18;LucasW;Glad I am not the only one. I would attach my error logs but unfortunately they are above the maximum file size that is allowed for upload. If anyone wants more information on this I can provide it.;;;","15/Aug/16 14:10;lenhattan86@gmail.com;This issue happens very often with large jobs.  
This one must be a critical issue.
I hope this will be solved soon to make Spark works well with Yarn dynamic allocation.;;;","19/Aug/16 04:44;apachespark;User 'angolon' has created a pull request for this issue:
https://github.com/apache/spark/pull/14710;;;","01/Sep/16 19:43;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14925;;;","02/Sep/16 04:30;apachespark;User 'angolon' has created a pull request for this issue:
https://github.com/apache/spark/pull/14933;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong Keyword in ALTER TABLE CHANGE COLUMN,SPARK-16530,12989221,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,smilegator,smilegator,smilegator,13/Jul/16 19:22,14/Jul/16 15:17,14/Jul/23 06:29,14/Jul/16 15:17,2.0.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"In .g4 file, the keyword is wrong.

Existing:
{{ALTER TABLE CHANGE COLUMNS}}
Correct:
{{ALTER TABLE CHANGE COLUMN}}
",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 13 19:28:06 UTC 2016,,,,,,,,,,"0|i30xp3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/16 19:28;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14186;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveClientImpl throws NPE when reading database from a custom metastore,SPARK-16528,12989094,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jlewandowski,jlewandowski,jlewandowski,13/Jul/16 11:37,20/Jul/16 02:31,14/Jul/23 06:29,14/Jul/16 17:18,2.0.0,,,,,,,,2.0.0,,,,SQL,,,,,,,,1,,,,,,"In _HiveClientImpl_ there is a method to create database:

{code}
  override def createDatabase(
      database: CatalogDatabase,
      ignoreIfExists: Boolean): Unit = withHiveState {
    client.createDatabase(
      new HiveDatabase(
        database.name,
        database.description,
        database.locationUri,
        database.properties.asJava),
        ignoreIfExists)
  }
{code}

The problem is that it assumes that {{database.properties}} is a not null value which is not always the truth. In fact, when the database is created, in _HiveMetaStore_ we have:

{code}
    private void createDefaultDB_core(RawStore ms) throws MetaException, InvalidObjectException {
      try {
        ms.getDatabase(DEFAULT_DATABASE_NAME);
      } catch (NoSuchObjectException e) {
        Database db = new Database(DEFAULT_DATABASE_NAME, DEFAULT_DATABASE_COMMENT,
          wh.getDefaultDatabasePath(DEFAULT_DATABASE_NAME).toString(), null);
        db.setOwnerName(PUBLIC);
        db.setOwnerType(PrincipalType.ROLE);
        ms.createDatabase(db);
      }
    }
{code}

As you can see, parameters field is set to {{null}}.
",,apachespark,bcantoni,bomeng,dongjoon,jlewandowski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 14 18:31:21 UTC 2016,,,,,,,,,,"0|i30wwv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/16 13:12;srowen;If it should just be passes through as null in this case, that's an easy fix, go ahead.;;;","14/Jul/16 05:09;jlewandowski;Ok, I'll fix it today;;;","14/Jul/16 10:20;apachespark;User 'jacek-lewandowski' has created a pull request for this issue:
https://github.com/apache/spark/pull/14200;;;","14/Jul/16 18:05;jlewandowski;so it won't be available in 2.0.0 ? it hasn't been released yet...;;;","14/Jul/16 18:31;srowen;No, but the latest RC could theoretically be the released version, in which case this would not be in for 2.0.0. If there's another RC it would become part of it. 2.0.1 is the most correct fix version at the moment.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[MESOS] Spark application throws exception on exit,SPARK-16522,12989028,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sunrui,sunrui,sunrui,13/Jul/16 07:35,31/Oct/16 18:36,14/Jul/23 06:29,10/Aug/16 09:01,2.0.0,,,,,,,,2.0.1,2.1.0,,,Mesos,,,,,,,,0,,,,,,"Spark applications running on Mesos throw exception upon exit as follows:
{noformat}
16/07/13 15:20:46 WARN NettyRpcEndpointRef: Error sending message [message = RemoveExecutor(1,Executor finished with state FINISHED)] in 3 attempts
org.apache.spark.SparkException: Exception thrown in awaitResult
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:75)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:78)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.removeExecutor(CoarseGrainedSchedulerBackend.scala:412)
	at org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend.executorTerminated(MesosCoarseGrainedSchedulerBackend.scala:555)
	at org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend.statusUpdate(MesosCoarseGrainedSchedulerBackend.scala:495)
Caused by: org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:152)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:127)
	at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:225)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:508)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	... 4 more
Exception in thread ""Thread-47"" org.apache.spark.SparkException: Error notifying standalone scheduler's driver endpoint
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.removeExecutor(CoarseGrainedSchedulerBackend.scala:415)
	at org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend.executorTerminated(MesosCoarseGrainedSchedulerBackend.scala:555)
	at org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend.statusUpdate(MesosCoarseGrainedSchedulerBackend.scala:495)
Caused by: org.apache.spark.SparkException: Error sending message [message = RemoveExecutor(1,Executor finished with state FINISHED)]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:119)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:78)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.removeExecutor(CoarseGrainedSchedulerBackend.scala:412)
	... 2 more
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:75)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
	... 4 more
Caused by: org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:152)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:127)
	at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:225)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:508)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	... 4 more
{noformat}

Applications' result is not affected by this error.

This issue can be simply reproduced by launching a spark-shell, and exit after running the following commands:
{code}
val rdd = sc.parallelize(1 to 10, 10)
rdd.map { _ + 1} collect
{code}

The root cause is that in SparkContext.stop(), MesosCoarseGrainedSchedulerBackend.stop() calls CoarseGrainedSchedulerBackend.stop(). The latter sends messages to stop executors and also stop the driver endpoint without waiting for the actual stop of executors. MesosCoarseGrainedSchedulerBackend.stop() still waits for the executors to stop in a timeout. During the wait, MesosCoarseGrainedSchedulerBackend.statusUpdate() generally will be called to update executors' status, and in turn removeExecutor() is called. But at that time, the driver endpoint is not available.",,apachespark,harishk15,jerryshao,laurentcoder,mgummelt,ogirardot,rxin,sunrui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 31 18:36:27 UTC 2016,,,,,,,,,,"0|i30wi7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/16 08:19;jerryshao;Perhaps there's race condition when exiting the Spark application, I remember there's a similar JIRA SPARK-12967 to handle it, not sure it is the exactly same scenario.;;;","13/Jul/16 09:57;sunrui;I checked SPARK-12967, this issue is different. SPARK-12967 is for the case that the RpcEnv stopped, but this issue is for the case where a specific endpoint stopped, more specifically, the driver endpoint. When this issue happens, the RpcEnv is still alive.;;;","13/Jul/16 10:31;apachespark;User 'sun-rui' has created a pull request for this issue:
https://github.com/apache/spark/pull/14175;;;","14/Jul/16 01:12;mgummelt;I've seen some stack traces recently that might have been this.  I'm trying to repro now.  Will get back to you.  Which commit/tag are you running?;;;","14/Jul/16 01:53;sunrui;[~mgummelt] The commit is e50efd53f073890d789a8448f850cc219cca7708
;;;","14/Jul/16 17:43;mgummelt;[~srowen] I'm going to look into this now and resolve it today.  Can you hold off on the next 2.0 RC until this is resolved?  It's likely a major bug.;;;","14/Jul/16 17:46;srowen;I'm not the release guy, but I also saw that RC3 looks like it has already been tagged. Something tells me it won't be the last though;;;","14/Jul/16 19:06;rxin;Does this fail the job? If it doesn't fail the job and simply prints warnings and logs exceptions that it wouldn't be a release blocker.
;;;","14/Jul/16 19:23;mgummelt;I don't think so.  Please give me a couple hours to investigate further, though.;;;","15/Jul/16 20:31;rxin;Any update?
;;;","15/Jul/16 21:17;mgummelt;This shouldn't affect functionality;;;","05/Aug/16 14:18;laurentcoder;Is this gonna be fixed soon (2.0.1) ?!
I just got a stacktrace of a developer here, who's job *failed* .. with following exception
{code}
2016-08-05 05:43:35,186 WARN  org.apache.spark.rpc.netty.NettyRpcEndpointRef: Error sending message [message = RemoveExecutor(5,Executor finished with state FINISHED)] in 1 attempts
org.apache.spark.SparkException: Exception thrown in awaitResult
        at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77)
        at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:75)
        at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
        at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
        at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
        at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
        at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
        at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
        at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:78)
        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.removeExecutor(CoarseGrainedSchedulerBackend.scala:412)
        at org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend.executorTerminated(MesosCoarseGrainedSchedulerBackend.scala:555)
        at org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend.statusUpdate(MesosCoarseGrainedSchedulerBackend.scala:495)
Caused by: org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
        at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:152)
        at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:127)
        at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:225)
        at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:508)
        at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
        ... 4 more
{code}
;;;","05/Aug/16 14:24;sunrui;sure. The PR is under review;;;","05/Aug/16 15:00;mgummelt;How did the job fail?  Return non-zero exit code?  I think this stack trace is thrown in a non-main thread;;;","09/Aug/16 08:42;srowen;Resolved by https://github.com/apache/spark/pull/14175;;;","09/Aug/16 18:44;mgummelt;Reopening so we can track this until it's merged into the 2.0 branch.

Also changed the fix version to 2.0.1
;;;","09/Aug/16 18:58;srowen;OK, I think it's more accurate to leave it Fixed for 2.1.0, and then also add 2.0.1 later if it's also fixed for that version, but it won't matter soon.;;;","10/Aug/16 05:24;apachespark;User 'sun-rui' has created a pull request for this issue:
https://github.com/apache/spark/pull/14575;;;","10/Aug/16 09:01;srowen;Issue resolved by pull request 14575
[https://github.com/apache/spark/pull/14575];;;","30/Oct/16 16:51;harishk15;I am getting same error in spark 2.0.2 snapshot. Standalone submission.
 py4j.protocol.Py4JJavaError: An error occurred while calling o37785.count.
: org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:194)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:120)
	at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:229)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:125)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:125)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:124)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:98)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:197)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:82)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:30)
	at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:62)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.consume(BroadcastHashJoinExec.scala:38)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:232)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:82)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:30)
	at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:62)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:79)
	at org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:194)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:218)
	at org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:244)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:218)
	at org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:113)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:79)
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:40)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:30)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:40)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:30)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:40)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:30)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:309)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:347)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.UnionExec$$anonfun$doExecute$1.apply(basicPhysicalOperators.scala:471)
	at org.apache.spark.sql.execution.UnionExec$$anonfun$doExecute$1.apply(basicPhysicalOperators.scala:471)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.execution.UnionExec.doExecute(basicPhysicalOperators.scala:471)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:87)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:123)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:114)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:233)
	at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:111)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:361)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:225)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:325)
	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:361)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:87)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:123)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:114)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:233)
	at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:111)
	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:138)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:361)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:87)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:123)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:114)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:233)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:138)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:138)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:361)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:87)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:123)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:114)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:233)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:138)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:361)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:240)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:287)
	at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2193)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2546)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2192)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2199)
	at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2227)
	at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2226)
	at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2559)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:2226)
	at sun.reflect.GeneratedMethodAccessor84.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [1200 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:190)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:190)
	... 224 more

16/10/30 16:32:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@2079c34a)
16/10/30 16:32:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(121,1477845179288,JobFailed(org.apache.spark.SparkException: Job 121 cancelled because SparkContext was shut down))
;;;","31/Oct/16 18:36;mgummelt;This JIRA was for a bug in Mesos.  If you're getting this error w/ Standalone, it's likely a different bug, and you should submit a separate JIRA.;;;",,,,,,,,,,,,,,,,,,,,,,
[SPARK][SQL] transformation script got failure for python script,SPARK-16515,12988954,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,adrian-wang,jameszhouyi,jameszhouyi,13/Jul/16 02:28,12/Dec/22 18:10,14/Jul/23 06:29,18/Jul/16 20:59,2.0.0,,,,,,,,2.0.0,,,,SQL,,,,,,,,0,,,,,,"Run below SQL and get transformation script error for python script like below error message.
Query SQL:
{code}
CREATE VIEW q02_spark_sql_engine_validation_power_test_0_temp AS
SELECT DISTINCT
  sessionid,
  wcs_item_sk
FROM
(
  FROM
  (
    SELECT
      wcs_user_sk,
      wcs_item_sk,
      (wcs_click_date_sk * 24 * 60 * 60 + wcs_click_time_sk) AS tstamp_inSec
    FROM web_clickstreams
    WHERE wcs_item_sk IS NOT NULL
    AND   wcs_user_sk IS NOT NULL
    DISTRIBUTE BY wcs_user_sk
    SORT BY
      wcs_user_sk,
      tstamp_inSec -- ""sessionize"" reducer script requires the cluster by uid and sort by tstamp
  ) clicksAnWebPageType
  REDUCE
    wcs_user_sk,
    tstamp_inSec,
    wcs_item_sk
  USING 'python q2-sessionize.py 3600'
  AS (
    wcs_item_sk BIGINT,
    sessionid STRING)
) q02_tmp_sessionize
CLUSTER BY sessionid
{code}

Error Message:
{code}
16/07/06 16:59:02 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 157.0 (TID 171, hw-node5): org.apache.spark.SparkException: Subprocess exited with status 1. Error: Traceback (most recent call last):
  File ""q2-sessionize.py"", line 49, in <module>
    user_sk, tstamp_str, item_sk  = line.strip().split(""\t"")
ValueError: too many values to unpack
	at org.apache.spark.sql.hive.execution.ScriptTransformation$$anon$1.checkFailureAndPropagate(ScriptTransformation.scala:144)
	at org.apache.spark.sql.hive.execution.ScriptTransformation$$anon$1.hasNext(ScriptTransformation.scala:192)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Subprocess exited with status 1. Error: Traceback (most recent call last):
  File ""q2-sessionize.py"", line 49, in <module>
    user_sk, tstamp_str, item_sk  = line.strip().split(""\t"")
ValueError: too many values to unpack
	at org.apache.spark.sql.hive.execution.ScriptTransformation$$anon$1.checkFailureAndPropagate(ScriptTransformation.scala:144)
	at org.apache.spark.sql.hive.execution.ScriptTransformation$$anon$1.hasNext(ScriptTransformation.scala:181)
	... 14 more

16/07/06 16:59:02 INFO scheduler.TaskSetManager: Lost task 7.0 in stage 157.0 (TID 173) on executor hw-node5: org.apache.spark.SparkException (Subprocess exited with status 1. Error: Traceback (most recent call last):
  File ""q2-sessionize.py"", line 49, in <module>
    user_sk, tstamp_str, item_sk  = line.strip().split(""\t"")
ValueError: too many values to unpack
) [duplicate 1]
{code}",,adrian-wang,apachespark,jameszhouyi,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 20 07:31:04 UTC 2016,,,,,,,,,,"0|i30w1r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/16 02:34;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/14169;;;","13/Jul/16 02:35;gurwls223;Hi, do you mind if I ask narrow it down or some codes with sample data so that I can reproduce?
I cannot reproduce with the information above. It would be nicer if there is some codes and sample data to test.;;;","13/Jul/16 02:36;gurwls223;Oh, I will refer the PR. Thanks.;;;","13/Jul/16 03:02;jameszhouyi;Thank you for your quick attention for this issue.;;;","13/Jul/16 03:14;adrian-wang;The problem is spark did not find the right record writer from its conf when it has to write records to standard output. So when python read data from standard input, it crashes.;;;","18/Jul/16 18:57;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/14249;;;","18/Jul/16 20:59;yhuai;Issue resolved by pull request 14249
[https://github.com/apache/spark/pull/14249];;;","20/Jul/16 07:31;apachespark;User 'lw-lin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14280;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARN shuffle service should throw errors when it fails to start,SPARK-16505,12988845,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,12/Jul/16 20:22,17/May/20 18:14,14/Jul/23 06:29,14/Jul/16 14:42,2.0.0,,,,,,,,2.1.0,,,,Spark Core,YARN,,,,,,,0,,,,,,"Right now the YARN shuffle service will swallow errors that happen during startup and just log them:

{code}
    try {
      blockHandler = new ExternalShuffleBlockHandler(transportConf, registeredExecutorFile);
    } catch (Exception e) {
      logger.error(""Failed to initialize external shuffle service"", e);
    }
{code}

This causes two undesirable things to happen:

- because {{blockHandler}} will remain {{null}} when an error happens, every request to the shuffle service will cause an NPE
- because the NM is running, containers may be assigned to that host, only to fail to register with the shuffle service.

Example of the first:

{noformat}
2016-05-25 15:01:12,198  ERROR org.apache.spark.network.TransportContext: Error while initializing Netty pipeline
java.lang.NullPointerException
	at org.apache.spark.network.server.TransportRequestHandler.<init>(TransportRequestHandler.java:77)
	at org.apache.spark.network.TransportContext.createChannelHandler(TransportContext.java:159)
	at org.apache.spark.network.TransportContext.initializePipeline(TransportContext.java:135)
	at org.apache.spark.network.server.TransportServer$1.initChannel(TransportServer.java:123)
	at org.apache.spark.network.server.TransportServer$1.initChannel(TransportServer.java:116)
	at io.netty.channel.ChannelInitializer.channelRegistered(ChannelInitializer.java:69)
{noformat}

Example of the second:

{noformat}
16/05/25 15:01:12 INFO storage.BlockManager: Registering executor with local external shuffle service.
16/05/25 15:01:12 ERROR client.TransportClient: Failed to send RPC 5736508221708472525 to qxhddn01.ascap.com/10.6.41.31:7337: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
16/05/25 15:01:12 ERROR storage.BlockManager: Failed to connect to external shuffle server, will retry 2 more times after waiting 5 seconds...
java.lang.RuntimeException: java.io.IOException: Failed to send RPC 5736508221708472525 to qxhddn01.ascap.com/10.6.41.31:7337: java.nio.channels.ClosedChannelException
	at org.spark-project.guava.base.Throwables.propagate(Throwables.java:160)
	at org.apache.spark.network.client.TransportClient.sendRpcSync(TransportClient.java:272)
{noformat}",,apachespark,neelesh77,tgraves,umesh9794@gmail.com,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 12 20:48:05 UTC 2016,,,,,,,,,,"0|i30vdj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/16 20:48;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14162;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test harness to prevent expression code generation from reusing variable names,SPARK-16489,12988501,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,12/Jul/16 01:08,13/Jul/16 06:14,14/Jul/23 06:29,12/Jul/16 17:08,,,,,,,,,1.6.3,2.0.0,,,SQL,,,,,,,,0,,,,,,"In code generation, it is incorrect for expressions to reuse variable names across different instances of itself. As an example, SPARK-16488 reports a bug in which pmod expression reuses variable name ""r"".

This patch updates ExpressionEvalHelper test harness to always project two instances of the same expression, which will help us catch variable reuse problems in expression unit tests.",,apachespark,lianhuiwang,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16488,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 12 01:11:06 UTC 2016,,,,,,,,,,"0|i30tpr:",9223372036854775807,,,,,,,,,,,,,2.0.0,,,,,,,,,,,"12/Jul/16 01:11;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14146;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some batches might not get marked as fully processed in JobGenerator,SPARK-16487,12988494,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ahmed.mahran,ahmed.mahran,ahmed.mahran,12/Jul/16 00:34,22/Jul/16 11:40,14/Jul/23 06:29,22/Jul/16 11:39,,,,,,,,,2.1.0,,,,DStreams,,,,,,,,0,,,,,,"In JobGenerator, the code reads like that some batches might not get marked as fully processed. In the following flowchart, the batch should get marked fully processed before endpoint C however it is not. Currently, this does not actually cause an issue, as the condition {code}(time - zeroTime) is multiple of checkpoint duration?{code} always evaluates to true as the checkpoint duration is always set to be equal to the batch duration.

!https://s31.postimg.org/udy9lti2j/spark_streaming_job_generator.png|width=700!
[Image URL|https://s31.postimg.org/udy9lti2j/spark_streaming_job_generator.png]",,ahmed.mahran,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 22 11:39:27 UTC 2016,,,,,,,,,,"0|i30to7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/16 00:59;apachespark;User 'ahmed-mahran' has created a pull request for this issue:
https://github.com/apache/spark/pull/14145;;;","22/Jul/16 11:39;srowen;Issue resolved by pull request 14145
[https://github.com/apache/spark/pull/14145];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If a table's schema is inferred at runtime, describe table command does not show the schema",SPARK-16482,12988407,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,smilegator,yhuai,yhuai,11/Jul/16 19:55,13/Jul/16 22:24,14/Jul/23 06:29,13/Jul/16 22:24,2.0.0,,,,,,,,2.0.0,,,,SQL,,,,,,,,0,,,,,,"If we create a table pointing to a parquet/json datasets without specifying the schema, describe table command does not show the schema at all. It only shows {{#  Schema of this table is inferred at runtime}}. In 1.6, describe table does show the schema of such a table.",,apachespark,smilegator,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 13 22:24:20 UTC 2016,,,,,,,,,,"0|i30t5z:",9223372036854775807,,,,,,,,,,,,,2.0.0,2.0.1,2.1.0,,,,,,,,,"11/Jul/16 20:34;smilegator;Do you want me to do it?;;;","11/Jul/16 20:37;yhuai;Yea. Thanks! Seems we can just use lookupRelation to get the schema.;;;","11/Jul/16 21:19;smilegator;Let me try it. : );;;","12/Jul/16 04:04;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14148;;;","13/Jul/16 22:24;yhuai;Issue resolved by pull request 14148
[https://github.com/apache/spark/pull/14148];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BisectingKMeans Algorithm failing with java.util.NoSuchElementException: key not found,SPARK-16473,12988121,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imatiach,alokob.be@gmail.com,alokob.be@gmail.com,10/Jul/16 11:09,05/Oct/17 21:04,14/Jul/23 06:29,24/Jan/17 11:30,1.6.1,2.0.0,,,,,,,2.1.1,2.2.0,,,ML,MLlib,,,,,,,0,,,,,,"Hello , 

I am using apache spark 1.6.1. 
I am executing bisecting k means algorithm on a specific dataset .
Dataset details :- 
K=100,
input vector =100K*100k
Memory assigned 16GB per node ,
number of nodes =2.

 Till K=75 it os working fine , but when I set k=100 , it fails with java.util.NoSuchElementException: key not found. 

*I suspect it is failing because of lack of some resources , but somehow exception does not convey anything as why this spark job failed.* 

Please can someone point me to root cause of this exception , why it is failing. 

This is the exception stack-trace:- 
{code}
java.util.NoSuchElementException: key not found: 166 
        at scala.collection.MapLike$class.default(MapLike.scala:228) 
        at scala.collection.AbstractMap.default(Map.scala:58) 
        at scala.collection.MapLike$class.apply(MapLike.scala:141) 
        at scala.collection.AbstractMap.apply(Map.scala:58) 
        at org.apache.spark.mllib.clustering.BisectingKMeans$$anonfun$org$apache$spark$mllib$clustering$BisectingKMeans$$updateAssignments$1$$anonfun$2.apply$mcDJ$sp(BisectingKMeans.scala:338)
        at org.apache.spark.mllib.clustering.BisectingKMeans$$anonfun$org$apache$spark$mllib$clustering$BisectingKMeans$$updateAssignments$1$$anonfun$2.apply(BisectingKMeans.scala:337)
        at org.apache.spark.mllib.clustering.BisectingKMeans$$anonfun$org$apache$spark$mllib$clustering$BisectingKMeans$$updateAssignments$1$$anonfun$2.apply(BisectingKMeans.scala:337)
        at scala.collection.TraversableOnce$$anonfun$minBy$1.apply(TraversableOnce.scala:231) 
        at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111) 
        at scala.collection.immutable.List.foldLeft(List.scala:84) 
        at scala.collection.LinearSeqOptimized$class.reduceLeft(LinearSeqOptimized.scala:125) 
        at scala.collection.immutable.List.reduceLeft(List.scala:84) 
        at scala.collection.TraversableOnce$class.minBy(TraversableOnce.scala:231) 
        at scala.collection.AbstractTraversable.minBy(Traversable.scala:105) 
        at org.apache.spark.mllib.clustering.BisectingKMeans$$anonfun$org$apache$spark$mllib$clustering$BisectingKMeans$$updateAssignments$1.apply(BisectingKMeans.scala:337) 
        at org.apache.spark.mllib.clustering.BisectingKMeans$$anonfun$org$apache$spark$mllib$clustering$BisectingKMeans$$updateAssignments$1.apply(BisectingKMeans.scala:334) 
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328) 
        at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:389) 
{code}

Issue is that , it is failing but not giving any explicit message as to why it failed.",AWS EC2 linux instance. ,alokob.be@gmail.com,apachespark,imatiach,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 05 21:04:39 UTC 2017,,,,,,,,,,"0|i30ref:",9223372036854775807,,,,,,,,,,,,,2.1.1,2.2.0,,,,,,,,,,"25/Jul/16 11:09;alokob.be@gmail.com;After reducing maxIterations for BisectingKMeans it finished successfully , does that mean maxIterations are data-set specific ?;;;","27/Oct/16 09:35;alokob.be@gmail.com;This issue continue to exist for spark 2.0 ""ml"" library. Is this feature going to get any support?;;;","08/Nov/16 10:28;alokob.be@gmail.com;[~josephkb] , I have just found that you have worked on mllib , please can you help me out getting input about this issue.;;;","19/Dec/16 22:42;imatiach;I'm interested in looking into this issue.  Would it be possible to get a dataset (either the original one or some mock dataset) which can be used to reproduce this error?;;;","20/Dec/16 05:36;alokob.be@gmail.com;[~imatiach] , thanks for showing interest in this issue. I will try to share the dataset with you , please can you suggest where should I share it ? should I share it through github? is it fine?

Also , I have tried to diagnose this issue on my own , from my analysis it looks like , it is failing if it tries to bisect a node which does not have any children. I also have added a code fix , but not sure if this is the correct solution :- 

*Suggested solution*
{code:title=BisectingkMeans.scala}
 private def updateAssignments(
      assignments: RDD[(Long, VectorWithNorm)],
      divisibleIndices: Set[Long],
      newClusterCenters: Map[Long, VectorWithNorm]): RDD[(Long, VectorWithNorm)] = {
    assignments.map { case (index, v) =>
      if (divisibleIndices.contains(index)) {
        val children = Seq(leftChildIndex(index), rightChildIndex(index))
        if ( children.length>0 ) {
        val selected = children.minBy { child =>
          KMeans.fastSquaredDistance(newClusterCenters(child), v)
        }
        (selected, v)
        }else {
          (index, v)
        }
      } else {
        (index, v)
      }
    }
  }
{code}

*Original code* 
{code:title=BisectingkMeans.scala}
  private def updateAssignments(
      assignments: RDD[(Long, VectorWithNorm)],
      divisibleIndices: Set[Long],
      newClusterCenters: Map[Long, VectorWithNorm]): RDD[(Long, VectorWithNorm)] = {
    assignments.map { case (index, v) =>
      if (divisibleIndices.contains(index)) {
        val children = Seq(leftChildIndex(index), rightChildIndex(index))
        val selected = children.minBy { child =>
          KMeans.fastSquaredDistance(newClusterCenters(child), v)
        }
        (selected, v)
      } else {
        (index, v)
      }
    }
  }
{code};;;","20/Dec/16 18:33;imatiach;I will start a pull request for the change.  I would like to add a test case that verifies the bug is fixed though.  Maybe you can send the sample dataset through github, and I can take a look?;;;","20/Dec/16 18:37;apachespark;User 'imatiach-msft' has created a pull request for this issue:
https://github.com/apache/spark/pull/16355;;;","20/Dec/16 18:38;imatiach;I've added a pull request here:
https://github.com/apache/spark/pull/16355

It would be nice to add a test case in spark itself to verify the code fix.;;;","20/Dec/16 19:02;imatiach;If you could put the sample dataset on google drive or one drive and send me the link that would be great.  Putting the dataset on github would work too.  How large is the dataset?;;;","20/Dec/16 19:26;imatiach;Do you have a smaller dataset than the one in the description which can reproduce the bug?;;;","24/Jan/17 11:30;srowen;Resolved by https://github.com/apache/spark/pull/16355;;;","17/Aug/17 03:09;podongfeng;If the {{sparseDataset}} in  {{BisectingKMeansSuite}} is cached, then test case {{SPARK-16473: Verify Bisecting K-Means does not fail in edge case whereone cluster is empty after split}} always fails.
See PR https://github.com/apache/spark/pull/16763/files#diff-beaf4409631709a875704e6a4d0a1c13R37 ;;;","05/Oct/17 21:04;imatiach;[~podongfeng] interesting - it looks like the dataset representation is somehow changing when it is cached?  My guess is that the row order may be changing or the numeric values may be changing?  The test failure itself is ok if the number of clusters is equal to k (which is actually perfectly fine for the algorithm), it just means that the dataset was not generated correctly to hit the very special edge case I was looking for, where one cluster is empty after a split in bisecting k-means.  I can't seem to see the test failure error message in your PR, could you run another build and post it here?  We may need to add some debugging/print statements everywhere to determine how the data is changing when you cache it - this doesn't mean there is any bug in the algorithm, it just means the test needs to be changed so that the test data, even after caching, is the same as the original one.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 2.0 CSV does not cast null values to certain data types properly,SPARK-16462,12988041,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,proflin,proflin,proflin,09/Jul/16 14:08,08/Oct/16 14:34,14/Jul/23 06:29,18/Sep/16 18:26,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,1,,,,,,"When casting given string datum to specified type, CSV should return {{null}} for nullable types if {{datum == options.nullValue}}.

However, for certain data types like {{Boolean}}, {{TimestampType}}, {{DateType}}, CSV in 2.0 does not return {{null}} for some ""empty"" datum. This is a regression comparing to 1.6.",,admackin,apachespark,barrybecker4,proflin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17039,SPARK-17290,SPARK-15144,SPARK-16903,,,,,,,SPARK-16460,,,,SPARK-14143,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 18 18:26:23 UTC 2016,,,,,,,,,,"0|i30qwn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/16 14:25;apachespark;User 'lw-lin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14118;;;","18/Sep/16 18:26;srowen;Issue resolved by pull request 14118
[https://github.com/apache/spark/pull/14118];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 2.0 CSV ignores NULL value in Date format,SPARK-16460,12988033,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,proflin,marcelboldt,marcelboldt,09/Jul/16 12:07,18/Sep/16 18:27,14/Jul/23 06:29,18/Sep/16 18:26,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,3,,,,,,"Trying to read a CSV file to Spark (using SparkR) containing just this data row:

{code}
    1|1998-01-01||
{code}

Using Spark 1.6.2 (Hadoop 2.6) gives me 

{code}
    > head(sdf)
      id          d dtwo
    1  1 1998-01-01   NA
{code}

Spark 2.0 preview (Hadoop 2.7, Rev. 14308) fails with error: 

{panel}
> Error in invokeJava(isStatic = TRUE, className, methodName, ...) : 
  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.text.ParseException: Unparseable date: """"
	at java.text.DateFormat.parse(DateFormat.java:357)
	at org.apache.spark.sql.execution.datasources.csv.CSVTypeCast$.castTo(CSVInferSchema.scala:289)
	at org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:98)
	at org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:74)
	at org.apache.spark.sql.execution.datasources.csv.DefaultSource$$anonfun$buildReader$1$$anonfun$apply$1.apply(DefaultSource.scala:124)
	at org.apache.spark.sql.execution.datasources.csv.DefaultSource$$anonfun$buildReader$1$$anonfun$apply$1.apply(DefaultSource.scala:124)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Itera...
{panel}

The problem seems indeed the NULL value here as with a valid date in the third CSV column it works.

R code:
{code}
    #Sys.setenv(SPARK_HOME = 'c:/spark/spark-1.6.2-bin-hadoop2.6') 
    Sys.setenv(SPARK_HOME = 'C:/spark/spark-2.0.0-preview-bin-hadoop2.7')
    .libPaths(c(file.path(Sys.getenv(""SPARK_HOME""), ""R"", ""lib""), .libPaths()))
    library(SparkR)
    
    sc <-
        sparkR.init(
            master = ""local"",
            sparkPackages = ""com.databricks:spark-csv_2.11:1.4.0""
        )
    sqlContext <- sparkRSQL.init(sc)
    
    
    st <- structType(structField(""id"", ""integer""), structField(""d"", ""date""), structField(""dtwo"", ""date""))
    
    sdf <- read.df(
        sqlContext,
        path = ""d:/date_test.csv"",
        source = ""com.databricks.spark.csv"",
        schema = st,
        inferSchema = ""false"",
        delimiter = ""|"",
        dateFormat = ""yyyy-MM-dd"",
        nullValue = """",
        mode = ""PERMISSIVE""
    )
    
    head(sdf)
    
    sparkR.stop()
{code}",SparkR,apachespark,bhavesh.gadoya@wwindia.com,marcelboldt,proflin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16981,,,,SPARK-16462,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,SPARK-13667,http://stackoverflow.com/questions/38265640/spark-2-0-pre-csv-parsing-error-if-missing-values-in-date-column,,,,,,,,,,9223372036854775807,,,Sun Sep 18 18:26:40 UTC 2016,,,,,,,,,,"0|i30quv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/16 13:23;proflin;Hi, [~marcelboldt]. Thanks for reporting this! I will submit a patch shortly.

A scala reproducer (for reviewers):
{code}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._

object SPARK_16460 extends App {

  val sdf = SparkSession.builder().master(""local"").getOrCreate().read
    .schema(StructType(List(
      StructField(""id"", IntegerType),
      StructField(""d"", DateType),
      StructField(""dtwo"", DateType))))
    .option(""inferSchema"", false.toString)
    .option(""delimiter"", ""|"")
    .option(""dateFormat"", ""yyyy-MM-dd"")
    .option(""nullValue"", """")
    .option(""mode"", ""PERMISSIVE"")
    .csv(""test.csv"")

  sdf.show(1)

}
{code};;;","09/Jul/16 14:25;apachespark;User 'lw-lin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14118;;;","10/Sep/16 12:25;marcelboldt;Hi [~proflin]:Some time has passed, but I see that your pull request hasn't been merged yet. So I just wanted to confirm that I pulled and compiled it and I am using it since then successfully. Great job, and thanks!;;;","14/Sep/16 02:57;proflin;[~marcelboldt] Oh cool! Thanks for the feedback!;;;","18/Sep/16 18:26;srowen;Issue resolved by pull request 14118
[https://github.com/apache/spark/pull/14118];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent dropping current database,SPARK-16459,12988028,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,09/Jul/16 10:44,11/Jul/16 17:19,14/Jul/23 06:29,11/Jul/16 13:16,,,,,,,,,2.0.0,,,,SQL,,,,,,,,0,,,,,,"This issue prevents dropping the current database to avoid errors like the followings.

{code}
scala> sql(""create database delete_db"")
scala> sql(""use delete_db"")
scala> sql(""drop database delete_db"")
scala> sql(""create table t as select 1"")
org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'delete_db' not found;
{code}",,apachespark,dongjoon,roczei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16452,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 09 10:49:04 UTC 2016,,,,,,,,,,"0|i30qtr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/16 10:49;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/14115;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong messages when CTAS with a Partition By clause,SPARK-16457,12988020,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,09/Jul/16 07:30,08/Aug/16 14:28,14/Jul/23 06:29,08/Aug/16 14:28,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,,,,,,,,0,,,,,,"When doing a CTAS with a Partition By clause, we got a wrong error message. For example,

{noformat}
CREATE TABLE gen__tmp
PARTITIONED BY (key string)
AS SELECT key, value FROM mytable1
{noformat}

{noformat}
Operation not allowed: Schema may not be specified in a Create Table As Select (CTAS) statement(line 2, pos 0)
{noformat}",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 08 14:28:32 UTC 2016,,,,,,,,,,"0|i30qrz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/16 07:39;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14113;;;","08/Aug/16 14:28;cloud_fan;Issue resolved by pull request 14113
[https://github.com/apache/spark/pull/14113];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
release script does not publish spark-hive-thriftserver_2.10,SPARK-16453,12987936,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,yhuai,yhuai,yhuai,08/Jul/16 21:25,10/Aug/16 22:56,14/Jul/23 06:29,08/Jul/16 22:57,2.0.0,,,,,,,,2.0.0,,,,Build,,,,,,,,0,,,,,,,,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17003,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 10 22:32:05 UTC 2016,,,,,,,,,,"0|i30q9j:",9223372036854775807,,,,,,,,,,,,,2.0.0,,,,,,,,,,,"08/Jul/16 21:30;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/14108;;;","08/Jul/16 22:57;yhuai;Issue resolved by pull request 14108
[https://github.com/apache/spark/pull/14108];;;","10/Aug/16 22:32;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/14586;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spark-shell / pyspark should finish gracefully when ""SaslException: GSS initiate failed"" is hit",SPARK-16451,12987931,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,yeshavora,yeshavora,08/Jul/16 21:06,12/Dec/22 18:10,14/Jul/23 06:29,05/Jun/18 01:30,1.6.1,,,,,,,,2.4.0,,,,,,,,,,,,0,,,,,,"Steps to reproduce: (secure cluster)
* kdestroy
* spark-shell --master yarn-client

If no valid keytab is set while running spark-shell/pyspark, the spark client never exits. It keep printing below error messages. 
spark-client should call shutdown hook immediately and exit with proper error code.
Currently, user need to explicitly shutdown process. (using cntrl+c)

{code}
16/07/08 20:53:10 WARN Client: Exception encountered while connecting to the server : 
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
	at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211)
	at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:413)
	at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:595)
	at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:397)
	at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:761)
	at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:757)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:756)
	at org.apache.hadoop.ipc.Client$Connection.access$3200(Client.java:397)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1617)
	at org.apache.hadoop.ipc.Client.call(Client.java:1448)
	at org.apache.hadoop.ipc.Client.call(Client.java:1395)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at com.sun.proxy.$Proxy25.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:816)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy26.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2151)
	at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1408)
	at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1404)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1404)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1437)
	at org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter.<init>(FileSystemTimelineWriter.java:124)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.createTimelineWriter(TimelineClientImpl.java:316)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.serviceStart(TimelineClientImpl.java:308)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceStart(YarnClientImpl.java:194)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:127)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:530)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:1017)
	at $line3.$read$$iwC$$iwC.<init>(<console>:15)
	at $line3.$read$$iwC.<init>(<console>:24)
	at $line3.$read.<init>(<console>:26)
	at $line3.$read$.<init>(<console>:30)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:125)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:124)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:324)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:124)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:64)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:974)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:159)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:64)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:108)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:64)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:991)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)
	at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)
	at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:122)
	at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)
	at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:224)
	at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)
	at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)
	at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192)
	... 87 more
{code}
",,apachespark,tgraves,yeshavora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 05 01:30:03 UTC 2018,,,,,,,,,,"0|i30q8f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/16 14:07;tgraves;I think there are a bunch of cases where if the spark-shell/pyspark get exceptions (not just the sasl one), they don't shut down, they just go to the prompt, so if we are going to change it should be something generic.;;;","18/May/18 22:53;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/21368;;;","05/Jun/18 01:30;gurwls223;Issue resolved by pull request 21368
[https://github.com/apache/spark/pull/21368];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RemoveAliasOnlyProject should not remove alias with metadata,SPARK-16448,12987846,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,08/Jul/16 16:40,08/Dec/16 23:26,14/Jul/23 06:29,14/Jul/16 09:15,2.0.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,,,apachespark,cloud_fan,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-15804,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 14 09:15:05 UTC 2016,,,,,,,,,,"0|i30ppj:",9223372036854775807,,,,,,,,,,,,,2.0.0,2.1.0,,,,,,,,,,"08/Jul/16 16:54;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/14106;;;","14/Jul/16 09:15;lian cheng;Issue resolved by pull request 14106
[https://github.com/apache/spark/pull/14106];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Undeleted broadcast variables in Word2Vec causing OoM for long runs ,SPARK-16440,12987775,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,anthony-truchet,anthony-truchet,anthony-truchet,08/Jul/16 12:45,28/Feb/17 10:18,14/Jul/23 06:29,13/Jul/16 10:40,1.6.0,1.6.1,1.6.2,2.0.0,,,,,1.6.3,2.0.1,,,MLlib,,,,,,,,0,,,,,,"Three broadcast variables created at the beginning of {{Word2Vec.fit()}} are never deleted nor unpersisted. This seems to cause excessive memory consumption on the driver for a job running hundreds of successive training.

They are 
{code}
    val expTable = sc.broadcast(createExpTable())
    val bcVocab = sc.broadcast(vocab)
    val bcVocabHash = sc.broadcast(vocabHash)
{code}

",,anthony-truchet,apachespark,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,,,,,SPARK-11898,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 28 10:18:03 UTC 2017,,,,,,,,,,"0|i30p9r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/16 12:48;anthony-truchet;Hello Spark developers,

I'm preparing a patch for this issue. This will be my first contribution to Spark. I'll strive to follow the contribution guidelines, but please do not hesitate to tell me how to do it better if required :-)

;;;","08/Jul/16 12:50;srowen;Yeah it would be fine to unpersist these at the end of the method. 
I suppose I'm surprised that the Broadcast vars don't unpersist themselves when they're out of scope and garbage collected via finalize?;;;","12/Jul/16 12:09;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14153;;;","13/Jul/16 10:40;srowen;Issue resolved by pull request 14153
[https://github.com/apache/spark/pull/14153];;;","19/Jul/16 11:19;anthony-truchet;Thanks for such a quick fix [~srowen] : I was off-line for the past week that's why I couldn't submit the patch quickly enough.

I would have {{destroyed}} the variable instead of {{unpersisting}} them though as the issues was memory consumption on the *driver* side: what am I missing which made you choose the later over the former ?;;;","19/Jul/16 11:22;srowen;Oh, may be better still indeed. Feel free to submit a follow up associated to this same JIRA. ;;;","19/Jul/16 13:07;anthony-truchet;I will, as well as putting this is a try finally to ensure proper deletion even in case of errors.;;;","19/Jul/16 13:21;srowen;Yeah it seems good to destroy even in case of errors, but in practice, an error here means lots of things are wrong. Actually using try-finally to destroy every RDD/variable would make the code a mess. I think many cases where it plausibly won't matter, we don't. If there's a decent argument that errors here are common for some reason, OK, but not sure that's true.;;;","19/Jul/16 18:04;apachespark;User 'AnthonyTruchet' has created a pull request for this issue:
https://github.com/apache/spark/pull/14268;;;","21/Jul/16 08:12;anthony-truchet;Regarding the try finally: we are computing numerous learning from within a same spark context and some with vocabulary so large that they fail (yes we do try to filter out too big ones, but too big is difficult to define).

So we are in a context where we do care about resource cleaning in case of error in order to enable thousands of successive learnings some of with expected to fail.

As for core readability we can try to refactor the function to reduce the nesting or find a ""nice"" scala solution: I'll propose a patch and I'll welcome any feedback on it.;;;","28/Feb/17 10:18;apachespark;User 'AnthonyTruchet' has created a pull request for this issue:
https://github.com/apache/spark/pull/14299;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect information in SQL Query details,SPARK-16439,12987771,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,davies,maver1ck,maver1ck,08/Jul/16 12:28,19/Sep/16 18:58,14/Jul/23 06:29,19/Sep/16 18:49,2.0.0,,,,,,,,2.0.1,2.1.0,,,SQL,Web UI,,,,,,,0,,,,,,"One picture is worth a thousand words.

Please see attachment

Incorrect values are in fields:
* data size
* number of output rows
* time to collect",,apachespark,davies,dongjoon,maver1ck,proflin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/16 11:02;maver1ck;sample.png;https://issues.apache.org/jira/secure/attachment/12817119/sample.png","08/Jul/16 12:29;maver1ck;spark.jpg;https://issues.apache.org/jira/secure/attachment/12816817/spark.jpg",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 19 18:49:23 UTC 2016,,,,,,,,,,"0|i30p8v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/16 17:47;dongjoon;For me, it seems to work.

https://app.box.com/s/lw0kl1ft7z4od9fwamtoafipzfrnex8m

Could you provide more information?;;;","11/Jul/16 09:22;maver1ck;I think that the problem exist when value is greater than 1024.
Could you repeat your test ?;;;","11/Jul/16 09:37;dongjoon;If you provide your testcase. :);;;","11/Jul/16 09:38;dongjoon;The picture is not worth at all.;;;","11/Jul/16 09:39;dongjoon;If you provide more specific information, you will get the fix faster.;;;","11/Jul/16 09:39;maver1ck;I'll try to prepare sth.
This was the screenshot from testing my production env on Spark 2.0. So I can't share the data.;;;","11/Jul/16 09:42;dongjoon;Thanks. I understand. Most issues are in the same situation. It's not easy to report.;;;","11/Jul/16 11:00;maver1ck;OK. Got this.
Using spark-shell (Spark deployed with YARN)

{code}
case class Left(a: Long)
case class Right(b: Long)
val df_left = spark.range(10000).map(num => Left(num))
val df_right = spark.range(10000).map(num => Right(num))
df_left.registerTempTable(""l"")
df_right.registerTempTable(""r"")
spark.sql(""select count(*) from l join r on l.a = r.b"").collect()
{code}

Screenshot from SQL Query details attached as sample.png

Tested with:
* Firefox 47
* Chrome 51

The problem is that where number is greater than 1000 we put special unicode character u+00A0 (no-breaking space) between thousands and hundreds.
So 10000 looks like 10u00A0000;;;","11/Jul/16 12:14;proflin;Hi [~dongjoon], would you mind leaving this open for two days or three? If it's okay by you, one/two colleague(s) of mine might take this as a first step to start contributing. Thanks a lot!;;;","11/Jul/16 12:28;maver1ck;[~proflin]
I don't know if waiting is right decision if we want this to be merged to 2.0.0;;;","11/Jul/16 22:21;maver1ck;I found that problem is locale dependent.

The \u00A0 sign is added by java NumberFormat class.
It can be avoided when using NumberFormat.setGroupingUsed(false);;;","11/Jul/16 22:59;apachespark;User 'maver1ck' has created a pull request for this issue:
https://github.com/apache/spark/pull/14142;;;","13/Jul/16 09:50;srowen;Issue resolved by pull request 14142
[https://github.com/apache/spark/pull/14142];;;","14/Sep/16 23:27;davies;The separator was added on purpose, otherwise it's very difficult to read the numbers (especially the number of rows),  we need to count the number of digits to realize how large the number is. I think we should still keep that and fix the local issue (using English should be enough), I will send a PR to add it back.;;;","14/Sep/16 23:28;davies;We could bring the seperator back for better readability. ;;;","14/Sep/16 23:38;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/15106;;;","19/Sep/16 18:49;davies;Issue resolved by pull request 15106
[https://github.com/apache/spark/pull/15106];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Behavior changes if initialExecutor is less than minExecutor for dynamic allocation,SPARK-16435,12987673,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jerryshao,jerryshao,jerryshao,08/Jul/16 02:49,14/Jul/16 16:50,14/Jul/23 06:29,13/Jul/16 18:25,2.0.0,,,,,,,,2.0.0,,,,Scheduler,Spark Core,,,,,,,0,,,,,,"After SPARK-13723, the behavior changed for {{spark.dynamicAllocation.initialExecutors}} less then {{spark.dynamicAllocation.minExecutors}} situation.

initialExecutors < minExecutors is an invalid setting,

h4. Before SPARK-13723

If initialExecutors < minExecutors, Spark will throw exception with:

{code}
java.lang.IllegalArgumentException: requirement failed: initial executor number xxx must between min executor number xxx and max executor number xxx
{code}

This will clearly let user know that current configuration is invalid.


h4. After SPARK-13723

Because we also consider {{spark.executor.instances}}, so the initial number is the max value between minExecutors, initialExecutors, numExecutors.

This will silently ignore the situation where initialExecutors < minExecutors.

So at least we should add some warning logs to let user know this is an invalid configuration.

What do you think [~tgraves], [~rdblue]？

",,apachespark,jerryshao,lianhuiwang,rdblue,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 12 08:32:05 UTC 2016,,,,,,,,,,"0|i30oo7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/16 16:01;rdblue;I think a warning is appropriate, but there's no need to throw an exception. The initial number of executors is just immediately bumped up to the min, which is what the min is there for.;;;","11/Jul/16 14:08;tgraves;Yeah I agree,I'm not as worried about this one as I am the max because if you are choosing to use less then the minimum I don't see that hurting anything.  A warning seems appropriate.;;;","12/Jul/16 00:35;jerryshao;OK, I will file a small patch to add the warning log about this invalid configuration.;;;","12/Jul/16 08:32;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/14149;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve StreamingQuery.explain when no data arrives,SPARK-16433,12987650,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,08/Jul/16 00:44,12/Jul/16 01:11,14/Jul/23 06:29,12/Jul/16 01:11,2.0.0,,,,,,,,2.0.0,,,,SQL,,,,,,,,0,,,,,,"StreamingQuery.explain shows ""N/A"" when no data arrives. It's pretty confusing.",,apachespark,tdas,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 12 01:11:51 UTC 2016,,,,,,,,,,"0|i30oj3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/16 00:48;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/14100;;;","12/Jul/16 01:11;tdas;Issue resolved by pull request 14100
[https://github.com/apache/spark/pull/14100];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Empty blocks fail to serialize due to assert in ChunkedByteBuffer,SPARK-16432,12987645,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ekhliang,ekhliang,ekhliang,08/Jul/16 00:17,09/Jul/16 03:18,14/Jul/23 06:29,09/Jul/16 03:18,2.0.0,,,,,,,,2.0.0,,,,Spark Core,,,,,,,,0,,,,,,See https://github.com/apache/spark/pull/11748#issuecomment-230760283,,apachespark,ekhliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 08 00:21:03 UTC 2016,,,,,,,,,,"0|i30ohz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/16 00:21;apachespark;User 'ericl' has created a pull request for this issue:
https://github.com/apache/spark/pull/14099;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IsotonicRegression produces NaNs with certain data,SPARK-16426,12987590,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nseggert,nseggert,nseggert,07/Jul/16 19:48,15/Jul/16 11:31,14/Jul/23 06:29,15/Jul/16 11:30,1.3.1,1.4.1,1.5.2,1.6.2,,,,,2.1.0,,,,MLlib,,,,,,,,0,,,,,,"{code}
val r = sc.parallelize(Seq[(Double, Double, Double)]((2, 1, 1), (1, 1, 1), (0, 2, 1), (1, 2, 1), (0.5, 3, 1), (0, 3, 1)), 2)
val i = new IsotonicRegression().run(r)

scala> i.predict(3.0)
res12: Double = NaN

scala> i.predictions
res13: Array[Double] = Array(0.75, 0.75, NaN, NaN)
{code}

I believe I understand the problem so I'll submit a PR shortly.

The problem happens when rows with the same feature value but different labels end up on different partitions. The merge function in poolAdjacentViolators introduces 0-weight points to be used for linear interpolation. This works fine, as long as they are always next to a non-0-weight point, but in the above case, you can end up with two 0-weight points  with the same feature value, which end up next to each other in the final PAV step. If these points are pooled, it creates a NaN.

One solution to this is to ensure that the all points with identical feature values end up on the same partition. This is the solution I intend to submit a PR for. Another option would be to try to get rid of the 0-weight points, but that seems trickier to me.",,apachespark,dougb,nseggert,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 15 11:30:36 UTC 2016,,,,,,,,,,"0|i30o5r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/16 21:24;apachespark;User 'neggert' has created a pull request for this issue:
https://github.com/apache/spark/pull/14140;;;","15/Jul/16 11:30;srowen;Issue resolved by pull request 14140
[https://github.com/apache/spark/pull/14140];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR summary() fails on column of type logical,SPARK-16425,12987572,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,neil@dewar-us.com,neil@dewar-us.com,07/Jul/16 18:33,14/Jul/16 16:50,14/Jul/23 06:29,08/Jul/16 00:48,1.6.1,,,,,,,,2.0.0,,,,SparkR,SQL,,,,,,,0,,,,,,"I created a DataFrame.  I added a logical column to the DataFrame using:
  sdfAdults <- withColumn(sdfAdults, ""IsGT50K"", sdfAdults$gt50=="" <=50K"")
The resulting column was reported using str() as being of type logical, with values TRUE and FALSE.

I subsequently ran the command:
   summary(sdfAdults)
The command failed reporting that the mean could not be calculated on a column of type logical.

Error in invokeJava(isStatic = FALSE, objId$id, methodName, ...) : 
  org.apache.spark.sql.AnalysisException: cannot resolve 'avg(IsGT50K)' due to data type mismatch: function average requires numeric types, not BooleanType;
	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:65)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:57)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:335)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:335)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:334)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:332)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:332)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:281)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)",Databricks.com,apachespark,dongjoon,neil@dewar-us.com,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 08 01:29:39 UTC 2016,,,,,,,,,,"0|i30o1r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/16 19:02;dongjoon;Unfortunately, Spark seems to support only numeric types for all languages, Scala/Python/R.
{code}
scala> spark.createDataFrame(Seq((1,2.0,""a"",true))).describe().collect().foreach(println)
[count,1,1]
[mean,1.0,2.0]
[stddev,NaN,NaN]
[min,1,2.0]
[max,1,2.0]
{code};;;","07/Jul/16 19:03;dongjoon;The possible quick fix is preventing the failure by excluding non-numeric types. But, that's not what you want here, isn't it?;;;","07/Jul/16 19:05;dongjoon;In SparkR, `summary` is an alias for `describe(all columns)`.;;;","07/Jul/16 19:17;dongjoon;Oh, sorry. I'll comment after more investigation.;;;","07/Jul/16 22:05;dongjoon;I'll make a PR to prevent errors first.;;;","07/Jul/16 22:13;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/14096;;;","08/Jul/16 00:20;neil@dewar-us.com;Thanks very much.  I wasn't aware of the limitation on data types in DataFrames.  I think it would be a great outcome if withColumn() was not able to create columns of invalid types.  I can re-write my script to initiate the column with 1 or 0 values.

If I remember rightly, I have seen withColumn do some other inappropriate things in the past - like creating two columns with the same name.  I will try to find some time to research and log another bug with anything else I find.

Please let me know if I did anything wrong when I logged this request - it was my first attempt.

;;;","08/Jul/16 00:30;dongjoon;It is a good issue. You're good at this if it was your first attempt. :);;;","08/Jul/16 00:48;shivaram;Issue resolved by pull request 14096
[https://github.com/apache/spark/pull/14096];;;","08/Jul/16 01:29;neil@dewar-us.com;Thanks so much Dongjoon.  As someone without the programming skills to contribute to the project, thank you for your contribution - Spark is incredibly valuable!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"maven 3.3.3 missing from mirror, breaks older builds",SPARK-16422,12987556,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,srowen,tgraves,tgraves,07/Jul/16 17:57,08/Jul/16 13:47,14/Jul/23 06:29,07/Jul/16 18:27,1.6.2,,,,,,,,1.6.3,,,,Build,,,,,,,,0,,,,,,"Trying to build spark 1.6.2 but it fails because the maven 3.3.3 is gone.

https://www.apache.org/dyn/closer.lua?action=download&filename=/maven/maven-3/3.3.3/binaries/apache-maven-3.3.3-bin.tar.gz

Is this something that we control.  I saw the latest 2.0 builds were updating to 3.3.9 and that exists there.

Various mirrors and atleast all the ones I've hit are missing it:
http://mirrors.koehn.com/apache//maven/maven-3/",,dongjoon,lresende,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 08 13:47:55 UTC 2016,,,,,,,,,,"0|i30ny7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/16 18:13;dongjoon;Hi, could you update your 1.6 branch?

The following PR fixes that recently and is merged into branch 1.6.

https://github.com/apache/spark/pull/14066;;;","07/Jul/16 18:16;tgraves;thanks, didn't see that go by.
;;;","07/Jul/16 18:22;tgraves;Oh I didn't see it go by because there is no jira associated with it.   [~srowen]  why isn't there a jira for that?;;;","07/Jul/16 18:27;srowen;Same reason as ever -- there's no difference between the ""what"" and ""how"" to fix it.;;;","07/Jul/16 18:39;tgraves;maybe I missed some discussions but I thought we were going to stop checkins without jira for hot fixes and things like that.  Other then release builds changing version and such.

Below is the text from the email discussion on dev list from 06/06/15.

I guess it is just in email and perhaps we need to put something on the wiki for this or perhaps I missed this being reverted?

{quote}
Hey All,

Just a request here - it would be great if people could create JIRA's
for any and all merged pull requests. The reason is that when patches
get reverted due to build breaks or other issues, it is very difficult
to keep track of what is going on if there is no JIRA. Here is a list
of 5 patches we had to revert recently that didn't include a JIRA:

    Revert ""[MINOR] [BUILD] Use custom temp directory during build.""
    Revert ""[SQL] [TEST] [MINOR] Uses a temporary log4j.properties in
HiveThriftServer2Test to ensure expected logging behavior""
    Revert ""[BUILD] Always run SQL tests in master build.""
    Revert ""[MINOR] [CORE] Warn users who try to cache RDDs with
dynamic allocation on.""
    Revert ""[HOT FIX] [YARN] Check whether `/lib` exists before
listing its files""

The cost overhead of creating a JIRA relative to other aspects of
development is very small. If it's *really* a documentation change or
something small, that's okay.

But anything affecting the build, packaging, etc. These all need to
have a JIRA to ensure that follow-up can be well communicated to all
Spark developers.

Hopefully this is something everyone can get behind, but opened a
discussion here in case others feel differently.

- Patrick
{quote};;;","07/Jul/16 18:47;tgraves;I'll pop this discussions up on dev list again.;;;","07/Jul/16 21:42;tgraves;do we know why maven 3.3.3 got removed?  I assume this is going to happen to 3.3.9 at some point so perhaps it makes sense for us to get his from a more reliable location.;;;","07/Jul/16 22:19;lresende;When new releases become available, old ones get moved to archive (e.g. https://archive.apache.org/dist/maven/maven-3/...) 
If we use the mirrors, we will always have this issue when new maven releases become available, if we use the archive we won't have this issue but we would be putting a little more load on the Apache Infrastructure. 
Anyway, if you guys think it's worth moving to use the archive release location I could provide a patch.;;;","08/Jul/16 13:38;tgraves;it looks like there is generally about 6 months between releases so at least it won't be a frequent change.  If there is concern over the load on the archive mirrors I'm fine with leaving it for now and then discussing more the next time we hit it.

The downside is any of the older branches also don't work unless you patch them, but the user can simply workaround by either going to archive and installing themselves or updating the dev build script.;;;","08/Jul/16 13:47;srowen;FWIW it also only matters if you don't already have a recent-enough Maven installed. So if you had 3.3.3 installed or later it would always work.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsafeShuffleWriter leaks compression streams with off-heap memory.,SPARK-16420,12987528,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rdblue,rdblue,rdblue,07/Jul/16 16:22,08/Jul/16 19:37,14/Jul/23 06:29,08/Jul/16 19:37,2.0.0,,,,,,,,2.0.0,,,,Spark Core,,,,,,,,0,,,,,,"When merging spill files using Java file streams, {{UnsafeShuffleWriter}} opens a decompression stream for each shuffle part and [never closes them|https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java#L352-357]. When the compression codec holds off-heap resources, these aren't cleaned up until the finalizer is called.",,apachespark,rdblue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 07 16:39:05 UTC 2016,,,,,,,,,,"0|i30nrz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/16 16:39;apachespark;User 'rdblue' has created a pull request for this issue:
https://github.com/apache/spark/pull/14093;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Logging in shutdown hook does not work properly with Log4j 2.x,SPARK-16416,12987462,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mikaelstaldal,mikaelstaldal,mikaelstaldal,07/Jul/16 12:09,26/Jul/16 09:56,14/Jul/23 06:29,24/Jul/16 10:16,1.6.2,,,,,,,,2.1.0,,,,Spark Core,,,,,,,,0,,,,,,"Spark registers some shutdown hooks, and they log messages during shutdown:

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala#L58

Since the {{Logging}} trait creates SLF4J loggers lazily:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/internal/Logging.scala#L47

a SLF4J logger is created during the execution of the shutdown hook.

This does not work when Log4j 2.x is used as SLF4J implementation:
https://issues.apache.org/jira/browse/LOG4J2-1222

Even though Log4j 2.6 handles this more gracefully than before, it still does emit a warning and will not be able to process the log message properly.

Proposed solution: make sure to eagerly create the SLF4J logger to be used in shutdown hooks when registering the hook.",,apachespark,mikaelstaldal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6305,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 25 08:27:57 UTC 2016,,,,,,,,,,"0|i30ndb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/16 12:39;srowen;Certainly not Major. Can you propose a change? I am not sure how to do this unless the loggers are always instantiated eagerly.;;;","08/Jul/16 09:26;mikaelstaldal;Maybe just add a log statement in {{ShutdownHookManager}} before adding any hook to force the logger to be created:

{code}
logDebug(""Adding shutdown hook"")
{code}

can be added here:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala#L56
;;;","08/Jul/16 10:17;srowen;OK, make a pull request?;;;","22/Jul/16 16:40;apachespark;User 'mikaelstaldal' has created a pull request for this issue:
https://github.com/apache/spark/pull/14320;;;","22/Jul/16 16:40;mikaelstaldal;https://github.com/apache/spark/pull/14320;;;","24/Jul/16 10:16;srowen;Issue resolved by pull request 14320
[https://github.com/apache/spark/pull/14320];;;","25/Jul/16 08:27;mikaelstaldal;Latest master works fine.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Spark][SQL] - Failed to create table due to catalog string error,SPARK-16415,12987456,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,adrian-wang,jameszhouyi,jameszhouyi,07/Jul/16 11:41,07/Jul/16 18:08,14/Jul/23 06:29,07/Jul/16 18:08,2.0.0,,,,,,,,2.0.0,,,,SQL,,,,,,,,0,,,,,,"When create  below table like below schema, Spark SQL error out for struct type

SQL:
{code}
CREATE EXTERNAL TABLE date_dim_temporary
  ( d_date_sk                 bigint              --not null
  , d_date_id                 string              --not null
  , d_date                    string
  , d_month_seq               int
  , d_week_seq                int
  , d_quarter_seq             int
  , d_year                    int
  , d_dow                     int
  , d_moy                     int
  , d_dom                     int
  , d_qoy                     int
  , d_fy_year                 int
  , d_fy_quarter_seq          int
  , d_fy_week_seq             int
  , d_day_name                string
  , d_quarter_name            string
  , d_holiday                 string
  , d_weekend                 string
  , d_following_holiday       string
  , d_first_dom               int
  , d_last_dom                int
  , d_same_day_ly             int
  , d_same_day_lq             int
  , d_current_day             string
  , d_current_week            string
  , d_current_month           string
  , d_current_quarter         string
  , d_current_year            string
  )
  ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
  STORED AS TEXTFILE LOCATION '/user/root/benchmarks/test/data/date_dim'

CREATE TABLE date_dim
STORED AS ORC
AS
SELECT * FROM date_dim_temporary
{code}

Error Message:
{code}
16/07/05 23:38:43 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 198.0 (TID 677, hw-node5): java.lang.IllegalArgumentException: Error: : expected at the position 400 of 'struct<d_date_sk:bigint,d_date_id:string,d_date:string,d_month_seq:int,d_week_seq:int,d_quarter_seq:int,d_year:int,d_dow:int,d_moy:int,d_dom:int,d_qoy:int,d_fy_year:int,d_fy_quarter_seq:int,d_fy_week_seq:int,d_day_name:string,d_quarter_name:string,d_holiday:string,d_weekend:string,d_following_holiday:string,d_first_dom:int,d_last_dom:int,d_same_day_ly:int,d_same_day_lq:int,d_current_day:string,... 4 more fields>' but ' ' is found.
	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.expect(TypeInfoUtils.java:360)
	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.expect(TypeInfoUtils.java:331)
	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseType(TypeInfoUtils.java:483)
	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseTypeInfos(TypeInfoUtils.java:305)
	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfoFromTypeString(TypeInfoUtils.java:770)
	at org.apache.spark.sql.hive.orc.OrcSerializer.<init>(OrcFileFormat.scala:184)
	at org.apache.spark.sql.hive.orc.OrcOutputWriter.<init>(OrcFileFormat.scala:220)
	at org.apache.spark.sql.hive.orc.OrcFileFormat$$anon$1.newInstance(OrcFileFormat.scala:93)
	at org.apache.spark.sql.execution.datasources.BaseWriterContainer.newOutputWriter(WriterContainer.scala:130)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:246)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,jameszhouyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 07 11:47:05 UTC 2016,,,,,,,,,,"0|i30nbz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/16 11:47;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/14089;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Can not get user config when calling SparkHadoopUtil.get.conf in other places, such as DataSourceStrategy",SPARK-16414,12987454,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sharkd,sharkd,sharkd,07/Jul/16 11:27,17/May/20 18:13,14/Jul/23 06:29,12/Jul/16 17:15,1.6.2,,,,,,,,2.0.0,,,,Spark Core,YARN,,,,,,,0,easyfix,,,,,"Fix bugs for ""Can not get user config when calling SparkHadoopUtil.get.conf in other places"".

The `SparkHadoopUtil` singleton was instantiated before `ApplicationMaster`, So the `sparkConf` and `conf` in the `SparkHadoopUtil` singleton didn't include user's configuration. 

see https://github.com/apache/spark/pull/14088",,apachespark,sharkd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 07 11:29:05 UTC 2016,,,,,,,,,,"0|i30nbj:",9223372036854775807,,,,,,,,,,,,,2.0.0,,,,,,,,,,,"07/Jul/16 11:29;apachespark;User 'sharkdtu' has created a pull request for this issue:
https://github.com/apache/spark/pull/14088;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
regexp_extract with optional groups causes NPE,SPARK-16409,12987386,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,mmoroz,mmoroz,07/Jul/16 06:51,07/Aug/16 11:20,14/Jul/23 06:29,07/Aug/16 11:20,2.0.0,,,,,,,,1.6.3,2.0.1,2.1.0,,Spark Core,,,,,,,,0,,,,,,"df = sqlContext.createDataFrame([['aaaac']], ['s'])
df.select(F.regexp_extract('s', r'(a+)(b)?(c)', 2)).collect()

causes NPE. Worse, in a large program it doesn't cause NPE instantly; it actually works fine, until some unpredictable (and inconsistent) moment in the future when (presumably) the invalid memory access occurs, and then it fails. For this reason, it took several hours to debug this.

Suggestion: either fill the group with null; or raise exception immediately after examining the argument with a message that optional groups are not allowed.

Traceback:

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-8-825292b569fc> in <module>()
----> 1 df.select(F.regexp_extract('s', r'(a+)(b)?(c)', 2)).collect()

C:\Users\me\Downloads\spark-2.0.0-preview-bin-hadoop2.7\python\pyspark\sql\dataframe.py in collect(self)
    294         """"""
    295         with SCCallSiteSync(self._sc) as css:
--> 296             port = self._jdf.collectToPython()
    297         return list(_load_from_socket(port, BatchedSerializer(PickleSerializer())))
    298 

C:\Users\me\Downloads\spark-2.0.0-preview-bin-hadoop2.7\python\lib\py4j-0.10.1-src.zip\py4j\java_gateway.py in __call__(self, *args)
    931         answer = self.gateway_client.send_command(command)
    932         return_value = get_return_value(
--> 933             answer, self.gateway_client, self.target_id, self.name)
    934 
    935         for temp_arg in temp_args:

C:\Users\me\Downloads\spark-2.0.0-preview-bin-hadoop2.7\python\pyspark\sql\utils.py in deco(*a, **kw)
     55     def deco(*a, **kw):
     56         try:
---> 57             return f(*a, **kw)
     58         except py4j.protocol.Py4JJavaError as e:
     59             s = e.java_exception.toString()

C:\Users\me\Downloads\spark-2.0.0-preview-bin-hadoop2.7\python\lib\py4j-0.10.1-src.zip\py4j\protocol.py in get_return_value(answer, gateway_client, target_id, name)
    310                 raise Py4JJavaError(
    311                     ""An error occurred while calling {0}{1}{2}.\n"".
--> 312                     format(target_id, ""."", name), value)
    313             else:
    314                 raise Py4JError(

Py4JJavaError: An error occurred while calling o51.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:210)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$7$$anon$1.hasNext(WholeStageCodegenExec.scala:357)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.to(SerDeUtil.scala:112)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toBuffer(SerDeUtil.scala:112)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toArray(SerDeUtil.scala:112)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:883)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:883)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1889)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1889)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1863)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1876)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1889)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:883)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:357)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:882)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)
	at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:2417)
	at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2417)
	at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2417)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2436)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:2416)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:211)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:210)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$7$$anon$1.hasNext(WholeStageCodegenExec.scala:357)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.to(SerDeUtil.scala:112)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toBuffer(SerDeUtil.scala:112)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toArray(SerDeUtil.scala:112)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:883)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:883)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1889)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1889)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more
",,apachespark,dongjoon,mmoroz,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16324,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 07 11:20:35 UTC 2016,,,,,,,,,,"0|i30mwf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/16 07:49;srowen;We're missing the stack trace here? that's an important piece of info. Also what is your data like that triggers this, if possible?;;;","08/Jul/16 17:49;mmoroz;[~srowen] So sorry I was sure I copied the entire code. I'm gonna update the issue with the full details.
;;;","04/Aug/16 15:45;mmoroz;Still causes NPE on the newly released Spark 2.0.0.;;;","04/Aug/16 22:47;rxin;[~srowen] do you have time to fix this one?
;;;","05/Aug/16 05:23;srowen;It's not quite my area, but I might know the answer. As to occurring ""randomly"", I suspect you mean that your example causes it immediately but in a large program, where an action isn't invoked until much later, it would execute and manifest when this statement was executed. That's normal for Spark programs, given the architecture.

In this case the matching group is missing. Looking at this implementation, it seems like this could be a problem already in 

{code}
  override def nullSafeEval(s: Any, p: Any, r: Any): Any = {
    if (!p.equals(lastRegex)) {
      // regex value changed
      lastRegex = p.asInstanceOf[UTF8String].clone()
      pattern = Pattern.compile(lastRegex.toString)
    }
    val m = pattern.matcher(s.toString)
    if (m.find) {
      val mr: MatchResult = m.toMatchResult
      UTF8String.fromString(mr.group(r.asInstanceOf[Int]))
    } else {
      UTF8String.EMPTY_UTF8
    }
  }
{code}

mr.group() returns null in this case and so the whole method does. Seems like it's not supposed to do that. I'll table a PR?;;;","05/Aug/16 06:23;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14504;;;","07/Aug/16 11:20;srowen;Issue resolved by pull request 14504
[https://github.com/apache/spark/pull/14504];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data Source APIs: Extending RelationProvider and CreatableRelationProvider Without SchemaRelationProvider,SPARK-16401,12987266,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,smilegator,smilegator,smilegator,06/Jul/16 19:52,14/Jul/16 16:50,14/Jul/23 06:29,09/Jul/16 12:39,2.0.0,,,,,,,,2.0.0,,,,SQL,,,,,,,,0,,,,,,"When users try to implement a data source API with extending only RelationProvider and CreatableRelationProvider, they will hit an error when resolving the relation.
{noformat}
spark.read
.format(""org.apache.spark.sql.test.DefaultSourceWithoutUserSpecifiedSchema"")
  .load()
  .write.
format(""org.apache.spark.sql.test.DefaultSourceWithoutUserSpecifiedSchema"")
  .save()
{noformat}

The error they hit is like
{noformat}
xyzDataSource does not allow user-specified schemas.;
org.apache.spark.sql.AnalysisException: xyzDataSource does not allow user-specified schemas.;
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:319)
	at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:494)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:211)
{noformat}",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 09 12:39:23 UTC 2016,,,,,,,,,,"0|i30m5r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Jul/16 19:57;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14075;;;","09/Jul/16 12:39;cloud_fan;Issue resolved by pull request 14075
[https://github.com/apache/spark/pull/14075];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove InSet filter pushdown from Parquet,SPARK-16400,12987256,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,06/Jul/16 19:00,07/Jul/16 10:18,14/Jul/23 06:29,07/Jul/16 10:18,,,,,,,,,2.1.0,,,,SQL,,,,,,,,1,,,,,,"Filter pushdown that needs to be evaluated per row is not useful to Spark, since parquet-mr own filtering is likely to be less performant than Spark's due to boxing and virtual function dispatches.

To simplify the code base, we should remove the InSet filters.
",,apachespark,dongjoon,lian cheng,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 07 10:18:06 UTC 2016,,,,,,,,,,"0|i30m3j:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"06/Jul/16 20:35;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14076;;;","07/Jul/16 10:18;lian cheng;Resolved by https://github.com/apache/spark/pull/14076;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove spark.sql.nativeView and spark.sql.nativeView.canonical config,SPARK-16388,12987026,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,06/Jul/16 01:28,06/Jul/16 09:42,14/Jul/23 06:29,06/Jul/16 09:42,,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"These two configs should not be relevant anymore after Spark 2.0.
",,apachespark,lian cheng,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 06 09:42:42 UTC 2016,,,,,,,,,,"0|i30ko7:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"06/Jul/16 01:33;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14061;;;","06/Jul/16 09:42;lian cheng;Issue resolved by pull request 14061
[https://github.com/apache/spark/pull/14061];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reserved SQL words are not escaped by JDBC writer,SPARK-16387,12986998,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,lev.numerify,lev.numerify,05/Jul/16 23:27,13/Feb/20 13:07,14/Jul/23 06:29,08/Jul/16 23:07,2.0.0,,,,,,,,2.0.0,,,,SQL,,,,,,,,1,,,,,,"Here is a code (imports are omitted)
object Main extends App {
  val sqlSession = SparkSession.builder().config(new SparkConf().
    setAppName(""Sql Test"").set(""spark.app.id"", ""SQLTest"").
    set(""spark.master"", ""local[2]"").
    set(""spark.ui.enabled"", ""false"")
    .setJars(Seq(""/mysql/mysql-connector-java-5.1.38.jar"" ))
  ).getOrCreate()

  import sqlSession.implicits._

  val localprops = new Properties
  localprops.put(""user"", ""xxxx"")
  localprops.put(""password"", ""xxxx"")

  val df = sqlSession.createDataset(Seq(""a"",""b"",""c"")).toDF(""order"")
  val writer = df.write
  .mode(SaveMode.Append)
  writer
  .jdbc(""jdbc:mysql://localhost:3306/test3"", s""jira_test"", localprops)
}


End error is :
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'order TEXT )' at line 1
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)

Clearly the reserved word <order> has to be quoted",,apachespark,dongjoon,igreenfi,lev.numerify,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-12437,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 13 13:07:25 UTC 2020,,,,,,,,,,"0|i30khz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Jul/16 05:30;lev.numerify;JdbcDialect class has a functionality that allows DB-dependent quotation. Please note that quotation has to be applied to all SQL statement generation code;;;","06/Jul/16 06:17;lev.numerify;I am not sure what PR means.;;;","08/Jul/16 20:56;dongjoon;You're right. I'll make a PR for this.;;;","08/Jul/16 21:11;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/14107;;;","12/Feb/20 14:28;igreenfi; 

[~dongjoon] 
I think that PR fix one issue but create other issue and should be fixed in other way.
not quotation of all but for each supported dialect check if the word is reserved and just in that case quote: like the scaladoc of the function said:

/**
 * Quotes the identifier. This is used to put quotes around the identifier in case the column
 * name is a reserved keyword, or in case it contains characters that require quotes (e.g. space).
 */

 

something like: [link|https://stackoverflow.com/a/60167628];;;","12/Feb/20 23:04;dongjoon;Hi, [~igreenfi]. What is the problem?
For case-sensitive database, the general rule is that the user should be consistent in both DBMS side and Spark side. Technically, quotes do not change the case sensitivity of the column names. If you have something to suggest, could you file a new JIRA with your idea with a reproducible example? We are open for the suggestion always.;;;","13/Feb/20 13:07;igreenfi;Hi [~dongjoon]

thanks for the quick response. we found that on oracle the addition of """" cause error in our other servers that try to read that table.

if the column name is `t_id` and you create it ""t_id"" if you will try to do `select T_ID from table` you will get an error from oracle.

from the docs on the method, it says that it will escape only reserved words but it actually escapes all... so I this it could be change to really escape only reserved words. 

Like I do in the link I put in the previous comment.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoSuchMethodException thrown by Utils.waitForProcess,SPARK-16385,12986939,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,05/Jul/16 20:20,11/Jul/16 21:04,14/Jul/23 06:29,05/Jul/16 23:56,2.0.0,,,,,,,,1.6.3,2.0.0,,,Spark Core,,,,,,,,0,,,,,,"The code in Utils.waitForProcess catches the wrong exception: when using reflection, {{NoSuchMethodException}} is thrown, but the code catches {{NoSuchMethodError}}.",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 05 20:25:05 UTC 2016,,,,,,,,,,"0|i30k4v:",9223372036854775807,,,,,,,,,,,,,2.0.0,,,,,,,,,,,"05/Jul/16 20:20;vanzin;Here's what I see when running unit tests on java 7:

{noformat}
Exception in thread ""ExecutorRunner for app-20160705131725-0000/1"" java.lang.NoSuchMethodException: java.lang.Process.waitFor(long, java.util.concurrent.TimeUnit)
        at java.lang.Class.getMethod(Class.java:1678)
        at org.apache.spark.util.Utils$.waitForProcess(Utils.scala:1812)
        at org.apache.spark.util.Utils$.terminateProcess(Utils.scala:1783)
        at org.apache.spark.deploy.worker.ExecutorRunner.org$apache$spark$deploy$worker$ExecutorRunner$$killProcess(ExecutorRunner.scala:101)
        at org.apache.spark.deploy.worker.ExecutorRunner.org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor(ExecutorRunner.scala:185)
        at org.apache.spark.deploy.worker.ExecutorRunner$$anon$1.run(ExecutorRunner.scala:73)
{noformat}
;;;","05/Jul/16 20:25;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14056;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark on mesos is broken due to race condition in Logging,SPARK-16379,12986787,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,srowen,skonto,skonto,05/Jul/16 11:24,06/Jul/16 21:21,14/Jul/23 06:29,06/Jul/16 20:36,2.0.0,,,,,,,,2.0.0,,,,Spark Core,,,,,,,,0,,,,,,"This commit introduced a transient lazy log val: https://github.com/apache/spark/commit/044971eca0ff3c2ce62afa665dbd3072d52cbbec

This has caused problems in the past:
https://github.com/apache/spark/pull/1004

One commit before that everything works fine.

I spotted that when my CI started to fail:
https://ci.typesafe.com/job/mit-docker-test-ref/191/

You can easily verify it by installing mesos on your machine and try to connect with spark shell from bin dir:

./spark-shell --master mesos://zk://localhost:2181/mesos --conf spark.executor.url=$(pwd)/../spark-2.0.0-SNAPSHOT-bin-test.tgz

It gets stuck at the point where it tries to create the SparkContext.

Logging gets stuck here:
I0705 12:10:10.076617  9303 group.cpp:700] Trying to get '/mesos/json.info_0000000152' in ZooKeeper
I0705 12:10:10.076920  9304 detector.cpp:479] A new leading master (UPID=master@127.0.1.1:5050) is detected
I0705 12:10:10.076956  9303 sched.cpp:326] New master detected at master@127.0.1.1:5050
I0705 12:10:10.077057  9303 sched.cpp:336] No credentials provided. Attempting to register without authentication
I0705 12:10:10.090709  9301 sched.cpp:703] Framework registered with 13553f8b-f42c-4f20-88cd-16f1cc153ede-0001

I verified it also by changing @transient lazy val log to def and it works as expected.
",,apachespark,clambert,drcrallen,mgummelt,skonto,skyluc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16131,,,,,,,,,,,,,,"05/Jul/16 12:10;skonto;out.txt;https://issues.apache.org/jira/secure/attachment/12816175/out.txt",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 06 21:21:41 UTC 2016,,,,,,,,,,"0|i30j7b:",9223372036854775807,,,,,,,,,,,,,2.0.0,,,,,,,,,,,"05/Jul/16 11:57;srowen;Hm, I wonder if the implicit lock present here that manages the 'lazy' aspect is involved in a deadlock now. Can you show a thread dump perhaps to verify that?;;;","05/Jul/16 12:10;skonto;Thread dump of repl added.
waiting here https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils.scala#L135

but before that:

""main"" #1 prio=5 os_prio=0 tid=0x00007f0bc8011800 nid=0x1269 waiting on condition [0x00007f0bcfa24000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000ed7b8558> (a java.util.concurrent.CountDownLatch$Sync)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
	at org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils$class.startScheduler(MesosSchedulerUtils.scala:135)
	- locked <0x00000000ed78bb68> (a org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend)


and blocked here:
Thread-15"" #67 prio=5 os_prio=0 tid=0x00007f0abc004000 nid=0x12b0 waiting for monitor entry [0x00007f0ad37fd000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.log$lzycompute(CoarseGrainedSchedulerBackend.scala:43)
	- waiting to lock <0x00000000ed78bb68> (a org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend)

The lock is on 0x00000000ed78bb68. 

MesosCoarseGrainedSchedulerBackend inherits from CoarseGrainedSchedulerBackend so the code cannot proceed.

We should have a rule here to avoid transient lazy vals etc. ;;;","05/Jul/16 12:52;srowen;This doesn't seem to have anything to do with Logging, though it is a legitimate deadlock, it seems. I don't think there's anything wrong with lazy vals (why would transient matter?) excepting of course that one has to be careful about initialization circularity.;;;","05/Jul/16 13:09;skonto;It is because of how logging is implemented there. Its a clear deadlock. This synchronized block here: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils.scala#L105
conflicts with the log lazy val. I suggested a rule or something because its tricky to use them sometimes, lazy val tries to lock the object, obviously was not caught at the review process. ;;;","05/Jul/16 13:09;srowen;Hm, but I note that in https://github.com/apache/spark/commit/044971eca0ff3c2ce62afa665dbd3072d52cbbec we removed a seemingly pointless override of {{log}} in {{CoarseGrainedSchedulerBackend}}. It certainly seems suspicious, and I'd be OK just reverting the commit ... but I'd love to figure out why this is actually the problem. It seems like the Logging change may be OK, but revealed some other problem?;;;","05/Jul/16 13:12;skonto;I dont think it reveals another problem. If you just replace it with a def everything is ok because a method does not lock anything.;;;","05/Jul/16 13:30;srowen;A little more context does show the role that Logging plays:

{code}
""Thread-15"" #67 prio=5 os_prio=0 tid=0x00007f0abc004000 nid=0x12b0 waiting for monitor entry [0x00007f0ad37fd000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.log$lzycompute(CoarseGrainedSchedulerBackend.scala:43)
	- waiting to lock <0x00000000ed78bb68> (a org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.log(CoarseGrainedSchedulerBackend.scala:43)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:48)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.logInfo(CoarseGrainedSchedulerBackend.scala:43)
	at org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend.registered(MesosCoarseGrainedSchedulerBackend.scala:247)
{code}

{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007f0bc8011800 nid=0x1269 waiting on condition [0x00007f0bcfa24000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000ed7b8558> (a java.util.concurrent.CountDownLatch$Sync)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
	at org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils$class.startScheduler(MesosSchedulerUtils.scala:135)
	- locked <0x00000000ed78bb68> (a org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend)
	at org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend.startScheduler(MesosCoarseGrainedSchedulerBackend.scala:48)
	at org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend.start(MesosCoarseGrainedSchedulerBackend.scala:157)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:155)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:500)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2256)
{code}

It's not a direct deadlock, but a problem of a callback holding a lock while waiting for its callback, which can't proceed without the lock.
I'd rather actually fix this problem. One simplistic solution is to remove the logInfo statement. Another is to remove the synchronized block in MesosSchedulerUtils.startScheduler; I'm not 100% clear why it's needed, when it seems like it's just protecting {{mesosDriver}}.;;;","05/Jul/16 14:29;skonto;Usually you should not lock on the object for synchronization that way i agree. But the reason the callback cannot proceed is the log, and the implicit lock it needs on the object. Actually it tries to log here: 
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosCoarseGrainedSchedulerBackend.scala#L247
I think for whatever reason the synch block is needed (correct or wrong) a new functionality or a refactoring should not break it.

And even after this is fixed (the probable wrong use of the synch block), i wanted to point out the tricky thing with lazy vals, there was another issue as i said in the past.
In other words i dont think using the lazy val there is the best way since it creates hidden issues. Thats what i am trying to say.

Redesign something that is not a good choice is fine, but redesigning it by breaking it is not something i agree on at the end of the day. 
I propose the commit to be reverted. Refactor-redesign the scheduler there (if needed) and then proceed with the commit if needed. What do you think?
[~mgummelt] what do you think? Can you provide some context for the synchronization there? 

;;;","05/Jul/16 18:55;mgummelt;I traced back the addition of the `synchronized` block, and it seems Matei added it a long time ago.  I can't prove that the method is thread-safe, so I'd rather not remove the synchronization block.  So we can either:

1) Remove the log statements (I'd like to keep them)
2) Revert the `lazy` commit
3) Introduce an explicit lock, and synchronize on that rather than `this`

2) is the ""correct"" thing to do, since it's the author's responsibility to not break existing code, but I'm OK with 3) as well.  [~srowen] what do you think?;;;","05/Jul/16 20:43;srowen;I don't agree with that logic; it's entirely possible that code has a bug that's only revealed when some other legitimate change happens, and the right subsequent change is to fix the bug. I don't think we'd ban lazy vals either. Arguably it's ""synchronized"" that is the issue here, really.

Indeed, reverting the last patch only 'fixes' it because the code contained a hack to avoid this condition. The previous code also involved acquiring a lock, and I'm guessing it _could_ still be a problem, though less likely to come up given that the locking only happens during the first call (well hopefully). Removing the logInfo actually removes the issue more directly than reintroducing the hack. Changing the startScheduler method is _probably_ the right-er fix, though that's less conservative.

I'm not against reverting the change just on the grounds that Logging is inherited lots of places and so there's a risk of a repeat of this problem elsewhere, even if it may ultimately also be due to some other coding problem. I'd just rather not also reintroduce the hack.;;;","05/Jul/16 21:09;mgummelt;> it's entirely possible that code has a bug that's only revealed when some other legitimate change happens

Of course, but I still don't see the bug that existed previously.  Perhaps `synchronized` was unnecessary, but I still see no race condition nor deadlock in the previous code.  Maybe following up on this will help:

> The previous code also involved acquiring a lock

Link? I don't see this. Or do you just mean the null check? https://github.com/apache/spark/commit/044971eca0ff3c2ce62afa665dbd3072d52cbbec#diff-bfd5810d8aa78ad90150e806d830bb78L45
;;;","05/Jul/16 21:15;srowen;I mean this: https://github.com/apache/spark/blob/044971eca0ff3c2ce62afa665dbd3072d52cbbec/core/src/main/scala/org/apache/spark/internal/Logging.scala#L94;;;","05/Jul/16 21:16;srowen;Oh I also meant this as the existing workaround: https://github.com/apache/spark/commit/044971eca0ff3c2ce62afa665dbd3072d52cbbec#diff-7d99a7c7a051e5e851aaaefb275a44a1L103;;;","05/Jul/16 21:16;skonto;> Arguably it's ""synchronized"" that is the issue here, really.
Is it forbidden to use a synchronized block if i know what i am doing? The same applies to the log lazy val.
If you know what you are doing i am sure its fine.
The problem here is that we have two incompatible code parts and we have to merge them somehow in order to proceed.;;;","05/Jul/16 21:18;srowen;Yes, but that is what I am arguing. Above you said it should be prohibited in all cases. I don't think it should be prohibited.;;;","05/Jul/16 21:21;skonto;Ok we can be flexible thats not the issue. A warning at least. Given it has caused an issue twice. Sometimes i prefer to be more strict, but its just a suggestion. Could be added as a warning in the project code guidelines for example. 

One other thing i hope it holds is no new commit should break the project even if it fixes something or reveals another issue etc.
;;;","05/Jul/16 21:29;mgummelt;I say we add a new lock to synchronize on and be done with it.

The root of the issue is that deadlock detection is hard.  The author of the breaking change added a critical region, and to do so safely, you have to ensure that all calling code paths haven't acquired the same lock, which is difficult (undecidable).

The only process change I can imagine to fix the higher level issue is running some sort of deadlock detection tool in the Spark tests.  I agree we shouldn't get rid of `lazy val` completely, but it is unfortunate that you can't use them in a `synchronized` block.  It's a leaky abstraction.  Seems to be addressed here: http://docs.scala-lang.org/sips/pending/improved-lazy-val-initialization.html

{quote}
One other thing i hope it holds is no new commit should break the project even if it fixes something or reveals another issue etc.
{quote}

Well I do agree with Sean that it's on us to fix bugs revealed by external changes.;;;","05/Jul/16 21:29;mgummelt;Hmmm, since that's a different lock, I don't see the possibility for deadlock in the previous code, but I'm content to relinquish the point.  Concurrency is hard :);;;","05/Jul/16 21:35;srowen;I don't think it's even a bad practice, any more than using {{synchronized}}. 
Ideally, if change A uncovers bug B then it needs to be expanded to address the bug and committed all at once. Nobody is suggesting knowingly making a change that triggers a bug, so I am not sure what this is arguing against in this context. We have a bug and need to address it in the best way we can see.;;;","05/Jul/16 21:39;skonto;There was no bug previously in the scheduler. It was working before i guess.
The project was not broken and the best practice is to keep it that way all the time. 
I think we can agree on that right?;;;","05/Jul/16 21:39;srowen;It's true, I can't say for sure the problem exists without that line. It's suspicious. In any event it seems worth doing away with it while fixing this up, which may still entail reverting the main change for safety but also trying to prove there's no similar problem still lurking in the previous Logging approach that needs any working-around anywhere.;;;","05/Jul/16 21:44;skonto;> Nobody is suggesting knowingly making a change that triggers a bug, so I am not sure what this is arguing against in this context.

I am just saying now that we know its an issue, we could revert the commit so we can do the improvements next. Its a blocker.;;;","05/Jul/16 21:51;srowen;Of course we'd all like to never have bugs. Nobody makes bugs on purpose. Bugs exist though, and everyone agrees something has to be fixed. This just states the obvious and is not actionable. What does that mean for _how_ to fix _this_ bug?  I will make a PR in any event since I think this issue is understood by now.;;;","06/Jul/16 09:22;skyluc;This problem is in 2.0.0-rc2. Making it impossible to test the release candidate with Mesos.;;;","06/Jul/16 09:25;srowen;Yeah there are other blockers. It won't be the last RC.;;;","06/Jul/16 10:54;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14069;;;","06/Jul/16 21:16;drcrallen;[~srowen] is there a list of blockers somewhere? I also want to get branch-2.0 tested from our side but would like to know what sort of caveats to expect.;;;","06/Jul/16 21:19;srowen;https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20Priority%20%3D%20Blocker%20AND%20%22Target%20Version%2Fs%22%20%3D%202.0.0%20AND%20Resolution%20%3D%20Unresolved ? you can filter JIRA how you like. Target Version should be pretty reliable.;;;","06/Jul/16 21:21;drcrallen;That's great, thanks a ton!;;;",,,,,,,,,,,,,,
"[Spark web UI]:HTTP ERROR 500 when using rest api ""/applications/[app-id]/jobs"" if array ""stageIds"" is empty",SPARK-16376,12986761,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,marymwu,marymwu,05/Jul/16 09:58,09/Jul/16 03:17,14/Jul/23 06:29,09/Jul/16 03:17,2.0.0,,,,,,,,2.0.0,,,,Web UI,,,,,,,,0,,,,,,"[Spark web UI]:HTTP ERROR 500 when using rest api ""/applications/[app-id]/jobs"" if array ""stageIds"" is empty

See attachment for reference.

HTTP ERROR 500

Problem accessing /api/v1/applications/application_1466239933301_175531/jobs. Reason:

    Server Error

Caused by:

java.lang.UnsupportedOperationException: empty.max
	at scala.collection.TraversableOnce$class.max(TraversableOnce.scala:216)
	at scala.collection.AbstractTraversable.max(Traversable.scala:105)
	at org.apache.spark.status.api.v1.AllJobsResource$.convertJobData(AllJobsResource.scala:71)
	at org.apache.spark.status.api.v1.AllJobsResource$$anonfun$2$$anonfun$apply$2.apply(AllJobsResource.scala:46)
	at org.apache.spark.status.api.v1.AllJobsResource$$anonfun$2$$anonfun$apply$2.apply(AllJobsResource.scala:44)
	at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply(TraversableLike.scala:722)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:721)
	at org.apache.spark.status.api.v1.AllJobsResource$$anonfun$2.apply(AllJobsResource.scala:44)
	at org.apache.spark.status.api.v1.AllJobsResource$$anonfun$2.apply(AllJobsResource.scala:43)
	at scala.collection.TraversableLike$WithFilter$$anonfun$flatMap$2.apply(TraversableLike.scala:753)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$WithFilter.flatMap(TraversableLike.scala:752)
	at org.apache.spark.status.api.v1.AllJobsResource.jobsList(AllJobsResource.scala:43)
	
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:134)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
	at org.spark-project.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at org.spark-project.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)
	at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:164)
	at org.spark-project.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.spark-project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)
	at org.spark-project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.spark-project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
	at org.spark-project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.spark-project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.spark-project.jetty.server.handler.GzipHandler.handle(GzipHandler.java:264)
	at org.spark-project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)
	at org.spark-project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.spark-project.jetty.server.Server.handle(Server.java:370)
	at org.spark-project.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.spark-project.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)
	at org.spark-project.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)
	at org.spark-project.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.spark-project.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.spark-project.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.spark-project.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
	at org.spark-project.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
",,apachespark,marymwu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 08 13:34:05 UTC 2016,,,,,,,,,,"0|i30j1j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/16 13:34;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14105;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Spark web UI]:The wrong value(numCompletedTasks) has been assigned to the variable numSkippedTasks,SPARK-16375,12986756,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ajbozarth,marymwu,marymwu,05/Jul/16 09:33,14/Jul/16 16:50,14/Jul/23 06:29,13/Jul/16 09:45,2.0.0,,,,,,,,1.6.3,2.0.0,,,Web UI,,,,,,,,0,,,,,,"[Spark web UI]:The wrong value(numCompletedTasks) has been assigned to the variable numSkippedTasks

See attachment for reference.",,ajbozarth,apachespark,marymwu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/16 09:41;marymwu;numSkippedTasksWrongValue.png;https://issues.apache.org/jira/secure/attachment/12816166/numSkippedTasksWrongValue.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 13 09:45:39 UTC 2016,,,,,,,,,,"0|i30j0f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/16 22:05;apachespark;User 'ajbozarth' has created a pull request for this issue:
https://github.com/apache/spark/pull/14141;;;","13/Jul/16 09:45;srowen;Issue resolved by pull request 14141
[https://github.com/apache/spark/pull/14141];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Retag RDD to tallSkinnyQR of RowMatrix,SPARK-16372,12986684,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yinxusen,yinxusen,yinxusen,04/Jul/16 22:53,14/Jul/16 16:50,14/Jul/23 06:29,07/Jul/16 10:28,,,,,,,,,2.0.0,,,,MLlib,,,,,,,,0,,,,,,"The following Java code because of type erasing:

{code}
JavaRDD<Vector> rows = jsc.parallelize(...);
RowMatrix mat = new RowMatrix(rows.rdd());
QRDecomposition<RowMatrix, Matrix> result = mat.tallSkinnyQR(true);
{code}

We should use retag to restore the type to prevent the following exception:

{code}
java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to [Lorg.apache.spark.mllib.linalg.Vector;
{code}",,apachespark,yinxusen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-13015,,,,,,,,,,,,,,,,SPARK-11497,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 07 10:28:46 UTC 2016,,,,,,,,,,"0|i30ikf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Jul/16 22:54;yinxusen;SPARK-11497 fixed this for PySpark.;;;","05/Jul/16 00:07;apachespark;User 'yinxusen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14051;;;","07/Jul/16 10:28;srowen;Issue resolved by pull request 14051
[https://github.com/apache/spark/pull/14051];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IS NOT NULL clause gives false for nested not empty column,SPARK-16371,12986683,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gurwls223,maver1ck,maver1ck,04/Jul/16 22:50,12/Dec/22 18:11,14/Jul/23 06:29,06/Jul/16 19:42,2.0.0,,,,,,,,2.0.0,,,,SQL,,,,,,,,0,,,,,,"I have df where column1 is struct type and there is 1M rows.
(sample data from https://issues.apache.org/jira/browse/SPARK-16320)

{code}
df.where(""column1 is not null"").count()
{code}
gives:
1M in Spark 1.6
*0* in Spark 2.0

Is there a change in IS NOT NULL behaviour in Spark 2.0 ?",,apachespark,maver1ck,proflin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 06 19:46:06 UTC 2016,,,,,,,,,,"0|i30ik7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/16 03:30;proflin;Hi [~maver1ck], but I couldn't reproduce this issue (see codes below) ? 

{code}
object SPARK_16371 extends App {

  import org.apache.spark.sql.SparkSession

  case class Parent(a: Child)
  case class Child(b: Long)

  val spark = SparkSession.builder().master(""local"").getOrCreate()

  import spark.implicits._

  // write
  spark.range(1000000).map(num => Parent(Child(num))).write.mode(""overwrite"").parquet(""1m_parquet"")

  // read
  // ---
  // Parquet form:
  // message spark_schema {
  //   optional group a {
  //     optional int64 b;
  //   }
  // }
  //
  // Catalyst form:
  // StructType(StructField(a,StructType(StructField(b,LongType,true)),true))
  // ---
  println(spark.read.parquet(""1m_parquet"").where(""a is not null"").count())
  // Spark 2.0 prints 1000000
}
{code};;;","05/Jul/16 08:50;maver1ck;I tried your example and it's working.
Could you try mine ?

Maybe the problem appears only in some conditions like very wide schema?;;;","05/Jul/16 09:51;gurwls223;Could you narrow down the case with a simpler code and data? I cannot reproduce this one as well. So, I tried your example and I get {{NameError: global name 'struct' is not defined}}.;;;","05/Jul/16 09:55;gurwls223;I somehow modified your codes and tested this by myself with smaller data and it seems working fine.

{code}
>>> df.where(""column1 is not null"").count()
100
>>> df.count()
100
{code};;;","05/Jul/16 09:57;maver1ck;I forget to add import.
I repaired this.;;;","06/Jul/16 00:39;gurwls223;[~maver1ck] [~proflin] I could reproduce this. I will try to narrow it down.;;;","06/Jul/16 04:31;gurwls223;Here is shorter codes

{code}
from pyspark.sql.functions import struct

child_df = spark.range(10)
parent_df = child_df.select(struct(""id"").alias(""id""))
parent_df.write.mode('overwrite').parquet('/tmp/test')
parent_df = spark.read.parquet('/tmp/test')

parent_df.where(""id is not null"").count() # 0
parent_df.count() # 10
{code};;;","06/Jul/16 04:46;gurwls223;Sorry for being noisy, here is the Scala version

{code}
case class Parent(a: Child)
case class Child(a: Long)
spark.range(10).map(num => Parent(Child(num))).write.mode(""overwrite"").parquet(""/tmp/test"")
spark.read.parquet(""/tmp/test"").where(""a is not null"").count() # 0
{code}

It seems it fails to apply the filter from Parquet when both inner column name and outer column name are the same.

I will look into this deeper.;;;","06/Jul/16 09:34;gurwls223;Right, it seems https://github.com/apache/spark/pull/9940 introduces this problem. In {{ParquetFilters}}, it is being pushed down when the names are name. I will create a PR for this.;;;","06/Jul/16 10:14;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/14067;;;","06/Jul/16 19:46;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14074;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tallSkinnyQR of RowMatrix should aware of empty partition,SPARK-16369,12986673,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yinxusen,yinxusen,yinxusen,04/Jul/16 21:11,14/Jul/16 16:50,14/Jul/23 06:29,08/Jul/16 13:24,,,,,,,,,2.0.0,,,,MLlib,,,,,,,,0,,,,,,"tallSkinnyQR of RowMatrix should aware of empty partition, which could cause exception from Breeze qr decomposition.

See the [archived dev mail|https://mail-archives.apache.org/mod_mbox/spark-dev/201510.mbox/%3CCAF7ADNrycvPL3qX-VZJhq4OYmiUUhoscut_tkOm63Cm18iK1tQ@mail.gmail.com%3E] for more details.",,apachespark,yinxusen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-13015,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 08 13:24:23 UTC 2016,,,,,,,,,,"0|i30ii7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Jul/16 22:28;apachespark;User 'yinxusen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14049;;;","08/Jul/16 13:24;srowen;Issue resolved by pull request 14049
[https://github.com/apache/spark/pull/14049];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Strange Errors When Creating View With Unmatched Column Num,SPARK-16368,12986659,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,04/Jul/16 18:44,07/Jul/16 07:07,14/Jul/23 06:29,07/Jul/16 07:07,2.0.0,,,,,,,,2.0.0,,,,SQL,,,,,,,,0,,,,,,"When creating a view, a common user error is the number of columns produced by the SELECT clause does not match the number of column names specified by CREATE VIEW.

For example, given Table {{t1}} only has 3 columns
{noformat}
create view v1(col2, col4, col3, col5) as select * from t1
{noformat}

Currently, Spark SQL reports the following error:
{noformat}
requirement failed
java.lang.IllegalArgumentException: requirement failed
	at scala.Predef$.require(Predef.scala:212)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:90)
{noformat}

This error is very confusing.",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 04 18:49:04 UTC 2016,,,,,,,,,,"0|i30if3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Jul/16 18:49;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14047;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect Statistics when Queries Containing LIMIT/TABLESAMPLE 0,SPARK-16355,12986454,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,03/Jul/16 05:56,11/Jul/16 08:29,14/Jul/23 06:29,11/Jul/16 08:29,2.0.0,,,,,,,,,,,,SQL,,,,,,,,0,,,,,,"When a query containing LIMIT/TABLESAMPLE 0, the statistics could be zero. Results are correct but it could cause a huge performance regression. For example,

{noformat}
      Seq((""one"", 1), (""two"", 2), (""three"", 3), (""four"", 4)).toDF(""k"", ""v"")
        .createOrReplaceTempView(""test"")
      val df1 = spark.table(""test"")
      val df2 = spark.table(""test"").limit(0)
      val df = df1.join(df2, Seq(""k""), ""left"")
{noformat}

The statistics of both {{df}} and {{df2}} are zero. The statistics values should never be zero; otherwise `sizeInBytes` of `BinaryNode` will also be zero (product of children). ",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 04 18:11:04 UTC 2016,,,,,,,,,,"0|i30h5j:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"04/Jul/16 18:11;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14034;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Illegal Inputs In LIMIT or TABLESAMPLE,SPARK-16354,12986453,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,03/Jul/16 05:51,11/Jul/16 08:29,14/Jul/23 06:29,11/Jul/16 08:29,2.0.0,,,,,,,,,,,,SQL,,,,,,,,0,,,,,,"{noformat}
SELECT * FROM testData TABLESAMPLE (-1 rows)
SELECT * FROM testData LIMIT -1
{noformat}

Negative values should not be allowed in {{TABLESAMPLE n ROWS}} and {{LIMIT n}}

In addition, Spark SQL follows the restriction of LIMIT clause in Hive. The argument to the LIMIT clause must evaluate to a constant value. It can be a numeric literal, or another kind of numeric expression involving operators, casts, and function return values. You cannot refer to a column or use a subquery. Currently, we do not detect whether the expression in LIMIT clause is foldable or not. If non-foldable, we might issue a strange error message. For example,
{noformat}
SELECT * FROM testData LIMIT rand() > 0.2
{noformat}

Then, a misleading error message is issued, like
{noformat}
assertion failed: No plan for GlobalLimit (_nondeterministic#203 > 0.2)
+- Project [key#11, value#12, rand(-1441968339187861415) AS _nondeterministic#203]
   +- LocalLimit (_nondeterministic#202 > 0.2)
      +- Project [key#11, value#12, rand(-1308350387169017676) AS _nondeterministic#202]
         +- LogicalRDD [key#11, value#12]

java.lang.AssertionError: assertion failed: No plan for GlobalLimit (_nondeterministic#203 > 0.2)
+- Project [key#11, value#12, rand(-1441968339187861415) AS _nondeterministic#203]
   +- LocalLimit (_nondeterministic#202 > 0.2)
      +- Project [key#11, value#12, rand(-1308350387169017676) AS _nondeterministic#202]
         +- LogicalRDD [key#11, value#12]
{noformat}
",,apachespark,smilegator,techaddict,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 05 06:32:45 UTC 2016,,,,,,,,,,"0|i30h5b:",9223372036854775807,,,,,,,,,,,,,2.0.1,2.1.0,,,,,,,,,,"03/Jul/16 07:03;techaddict;[~smilegator] What should be the expected behaviour, a Failure or 0 rows ?;;;","04/Jul/16 18:11;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/14034;;;","05/Jul/16 06:32;smilegator;Failure.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Intended javadoc options are not honored for Java unidoc,SPARK-16353,12986437,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,michael,michael,michael,02/Jul/16 19:52,06/Jul/16 01:41,14/Jul/23 06:29,04/Jul/16 20:16,1.6.2,2.0.0,2.0.1,2.1.0,,,,,1.6.3,2.0.0,,,Build,Documentation,,,,,,,0,,,,,,"{{project/SparkBuild.scala}} specifies

{noformat}
javacOptions in doc := Seq(
  ""-windowtitle"", ""Spark "" + version.value.replaceAll(""-SNAPSHOT"", """") + ""JavaDoc"",
  ""-public"",
  ""-noqualifier"", ""java.lang""
)
{noformat}

However, {{sbt javaunidoc:doc}} ignores these options. To wit, the title of http://spark.apache.org/docs/latest/api/java/index.html is {{Generated Documentation (Untitled)}}, not {{Spark 1.6.2 JavaDoc}} as it should be.

(N.B. the Spark 1.6.2 build defines several javadoc groups as well, which are also missing from the official docs.)",,apachespark,michael,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 04 20:16:56 UTC 2016,,,,,,,,,,"0|i30h1r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/16 20:13;apachespark;User 'mallman' has created a pull request for this issue:
https://github.com/apache/spark/pull/14031;;;","04/Jul/16 20:16;srowen;Issue resolved by pull request 14031
[https://github.com/apache/spark/pull/14031];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Complete output mode does not output updated aggregated value in Structured Streaming,SPARK-16350,12986384,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,proflin,arnaud.bailly,arnaud.bailly,02/Jul/16 07:36,01/Nov/16 22:26,14/Jul/23 06:29,07/Jul/16 17:42,2.0.0,,,,,,,,2.0.0,,,,Structured Streaming,,,,,,,,0,streaming,,,,,"Given the following program :

{code}
// A simple Order <-> Items model
case class Order (
  orderid: Int,
  customer: String
)

case class Item (
  orderid: Int,
  itemid: Int,
  amount: Float
)

import spark.implicits._

val sqlContext = new org.apache.spark.sql.SQLContext(sc)

// input data comes from CSV files
val INPUT_DIR: String =""""
val FILE_EXTENSION: String="".tbl""
val ORDERS_FILE : String = INPUT_DIR + ""orders"" + FILE_EXTENSION

// Items are added as a stream so input is a directory, not a file
val ITEM_DIR  : String = INPUT_DIR + ""item""

import org.apache.spark.sql.types._
import org.apache.spark.sql._

val ItemSchema = StructType (
  StructField(""orderid""        ,  IntegerType, false) ::
    StructField(""itemid""      ,  IntegerType, true)   ::
    StructField(""amount""     ,  FloatType, true)      ::
    Nil)

val OrderSchema = StructType (
  StructField(""orderid""  , IntegerType, false) ::
    StructField(""customer"" , StringType, true) ::
    Nil)

val csvOptions = Map(""sep""  -> ""|"")

val orders = sqlContext.read.format(""csv"").schema(OrderSchema).options(csvOptions).load(ORDERS_FILE).as[Order]
orders.registerTempTable(""orders"")

val itemsStream = sqlContext.readStream.format(""csv"").schema(ItemSchema).options(csvOptions).csv(ITEM_DIR).as[Item]
itemsStream.registerTempTable(""itemsStream"")

val sum_of_items_per_customer_streamed =
  sqlContext.sql(""SELECT customer, sum(amount) from orders d, itemsStream s where d.orderid = s.orderid group by customer"")

// print each computed value
val outwriter = new ForeachWriter[Row] {
  def open(partitionId: Long, version: Long): Boolean = true
  def process(value: Row): Unit = if (value != null) print(value)
  def close(errorOrNull: Throwable): Unit = if (errorOrNull != null) print(errorOrNull)
}

sum_of_items_per_customer_streamed.writeStream.outputMode(""complete"").foreach(outwriter).start
{code}

and the following data sets:

- {{orders.tbl}}
|| orderid || customer ||
|1|foo|
|2|bar|
|3|foo|

- {{items1.tbl}}
||orderid||itemid||amount||
|1|1|1.0|
|1|2|1.0|
|1|3|1.0|
|2|1|1.0|
|2|2|1.0|
|3|1|1.0|

- {{items2.tbl}}
||orderid||itemid||amount||
|1|4|1.0|
|2|5|1.0|

When I do the following actions:

- start bin/spark-shell
- {{:load complete-bug.scala}}
- {{cp items1.tbl item/}}
- {{cp items2.tbl item/}} 

*Then* the following results are printed in console:

{code}
[bar,2.0][foo,4.0]
[bar,1.0][foo,1.0]
{code}

I would expect the following:

{code}
[bar,2.0][foo,4.0]
[bar,3.0][foo,5.0]
{code}

","- Spark @c553976

{code}
$ java -version
java version ""1.8.0_20""
Java(TM) SE Runtime Environment (build 1.8.0_20-b26)
Java HotSpot(TM) 64-Bit Server VM (build 25.20-b23, mixed mode)
{code}

{code}
$ uname -a
Darwin mymac.local 14.5.0 Darwin Kernel Version 14.5.0: Tue Sep  1 21:23:09 PDT 2015; root:xnu-2782.50.1~1/RELEASE_X86_64 x86_64
{code}
",apachespark,arnaud.bailly,proflin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8360,,,,,,,SPARK-16264,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 03 14:59:59 UTC 2016,,,,,,,,,,"0|i30gpz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/16 17:06;apachespark;User 'lw-lin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14030;;;","02/Jul/16 17:06;proflin;[~arnaud.bailly] thanks for reporting this! I'll submit a patch shortly.;;;","03/Jul/16 14:59;arnaud.bailly;I tested provided PR and I can confirm it fixes the issue on the provided data set. I will test it more in depth tomorrow. 

TBH, I am a bit surprised of this bug: Is it due to the fact the `foreach` sink has not been as thoroughly tested as other sinks and is considered more as a testing tool? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IsolatedClientLoader ignores needed Hadoop classes not present in Spark's loader,SPARK-16349,12986347,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,01/Jul/16 22:30,11/Jul/16 22:21,14/Jul/23 06:29,11/Jul/16 22:21,2.0.0,,,,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"While trying to use a custom classpath for metastore jars (spark.sql.hive.metastore.jars pointing at some filesystem path), I ran into the following issue:

{noformat}
java.lang.ClassNotFoundException: java.lang.NoClassDefFoundError: org/apache/hadoop/mapred/MRVersion when creating Hive client using classpath
{noformat}

The issue here is that {{MRVersion}} is not packaged anywhere with Spark, and the code in {{IsolatedClientLoader}} only ever tries the parent class loader when loading hadoop classes in this configuration. So even though I had the class in the list of files in {{spark.sql.hive.metastore.jars}}, Spark never tries to load it.",,apachespark,vanzin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 11 22:21:07 UTC 2016,,,,,,,,,,"0|i30ghr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/16 22:44;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/14020;;;","11/Jul/16 22:21;yhuai;Issue resolved by pull request 14020
[https://github.com/apache/spark/pull/14020];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark.ml MLSerDe should be called using full classpath,SPARK-16348,12986323,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,josephkb,josephkb,josephkb,01/Jul/16 21:05,06/Jul/16 00:00,14/Jul/23 06:29,06/Jul/16 00:00,2.0.0,,,,,,,,2.0.0,,,,ML,PySpark,,,,,,,0,,,,,,"Depending on how Spark is set up, pyspark.ml may or may not be able to find the MLSerDe instance when referenced as {{sc._jvm.MLSerDe}}.  This can cause failures {{'JavaPackage' object is not callable}} when trying to access Vector or Matrix values from pyspark, such as retrieving the coefficients of a LinearRegressionModel.

Proposal: Whenever we reference a class in the _jvm from pyspark, we should use the full classpath: {{sc._jvm.org.apache.spark.ml.python.MLSerDe}}.  This fixes the bug in my case.",,apachespark,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 06 00:00:51 UTC 2016,,,,,,,,,,"0|i30gcf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/16 21:06;josephkb;Note: The bug does not seem to affect pyspark.mllib, though I'm not sure why in my case.;;;","01/Jul/16 21:07;josephkb;(Thanks [~timhunter] for figuring this out!);;;","02/Jul/16 01:24;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/14023;;;","06/Jul/16 00:00;josephkb;Issue resolved by pull request 14023
[https://github.com/apache/spark/pull/14023];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Array of struct with a single field name ""element"" can't be decoded from Parquet files written by Spark 1.6+",SPARK-16344,12986130,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,01/Jul/16 10:37,04/Feb/21 06:57,14/Jul/23 06:29,20/Jul/16 23:50,1.6.0,1.6.1,1.6.2,2.0.0,,,,,2.1.0,,,,SQL,,,,,,,,0,,,,,,"This is a weird corner case. Users may hit this issue if they have a schema that

# has an array field whose element type is a struct, and
# the struct has one and only one field, and
# that field is named as ""element"".

The following Spark shell snippet for Spark 1.6 reproduces this bug:

{code}
case class A(element: Long)
case class B(f: Array[A])

val path = ""/tmp/silly.parquet""
Seq(B(Array(A(42)))).toDF(""f0"").write.mode(""overwrite"").parquet(path)

val df = sqlContext.read.parquet(path)
df.printSchema()
// root
//  |-- f0: array (nullable = true)
//  |    |-- element: struct (containsNull = true)
//  |    |    |-- element: long (nullable = true)

df.show()
{code}

Exception thrown:

{noformat}
org.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file file:/tmp/silly.parquet/part-r-00007-e06db7b0-5181-4a14-9fee-5bb452e883a0.gz.parquet
        at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:228)
        at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:201)
        at org.apache.spark.rdd.SqlNewHadoopRDD$$anon$1.hasNext(SqlNewHadoopRDD.scala:194)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:308)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
        at scala.collection.AbstractIterator.to(Iterator.scala:1157)
        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(SparkPlan.scala:212)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(SparkPlan.scala:212)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
        at org.apache.spark.scheduler.Task.run(Task.scala:89)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassCastException: Expected instance of group converter but got ""org.apache.spark.sql.execution.datasources.parquet.CatalystPrimitiveConverter""
        at org.apache.parquet.io.api.Converter.asGroupConverter(Converter.java:37)
        at org.apache.parquet.io.RecordReaderImplementation.<init>(RecordReaderImplementation.java:266)
        at org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:134)
        at org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:99)
        at org.apache.parquet.filter2.compat.FilterCompat$NoOpFilter.accept(FilterCompat.java:154)
        at org.apache.parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:99)
        at org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:137)
        at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:208)
        ... 26 more
{noformat}

Spark 2.0.0-SNAPSHOT and Spark master also suffer this issue. To reproduce it using these versions, just replace {{sqlContext}} in the above snippet with {{spark}}.

The reason behind is related to Parquet backwards-compatibility rules for LIST types defined in [parquet-format spec|https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#lists].

The Spark SQL schema shown above

{noformat}
root
 |-- f0: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- element: long (nullable = true)
{noformat}

is equivalent to the following SQL type:

{noformat}
STRUCT<
  f: ARRAY<
    STRUCT<element: BIGINT>
  >
>
{noformat}

According to the parquet-format spec, the standard layout of a LIST-like structure is a 3-level layout:

{noformat}
<list-repetition> group <name> (LIST) {
  repeated group list {
    <element-repetition> <element-type> element;
  }
}
{noformat}

Thus, the standard representation of the aforementioned SQL type should be:

{noformat}
message root {
  optional group f (LIST) {
    repeated group list {
      optional group element {    (1)
        optional int64 element;   (2)
      }
    }
  }
}
{noformat}

Note that the two ""element"" fields are different:

- The {{group}} field ""element"" at (1) is a ""container"" of list element type. This is defined as part of the parquet-format spec.
- The {{int64}} field ""element"" at (2) corresponds to the {{element}} field of case class {{A}} we defined above.

However, due to historical reasons, various existing systems do not conform to the parquet-format spec and may write LIST structures in a non-standard layout. For example, parquet-avro and parquet-thrift use a 2-level layout like

{noformat}
// parquet-avro style
<list-repetition> group <name> (LIST) {
  repeated <element-type> array;
}

// parquet-thrift style
<list-repetition> group <name> (LIST) {
  repeated <element-type> <name>_tuple;
}
{noformat}

To keep backwards-compatibility, the parquet-format spec defined a set of [backwards-compatibility rules|https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#backward-compatibility-rules] to also recognize these patterns.

Unfortunately, these backwards-compatibility rules makes the Parquet schema we mentioned above ambiguous:

{noformat}
message root {
  optional group f (LIST) {
    repeated group list {
      optional group element {
        optional int64 element;
      }
    }
  }
}
{noformat}

When interpreted using the standard 3-level layout, it is the expected type:

{noformat}
STRUCT<
  f: ARRAY<
    STRUCT<element: BIGINT>
  >
>
{noformat}

When interpreted using the legacy 2-level layout, it is the unexpected type

{noformat}
// When interpreted as legacy 2-level layout
STRUCT<
  f: ARRAY<
    STRUCT<element: STRUCT<element: BIGINT>>
  >
>
{noformat}

This is because the nested struct field name happens to be ""element"", which is used as a dedicated name of the element type ""container"" group in the standard 3-level layout, and lead to the ambiguity.

Currently, Spark 1.6.x, 2.0.0-SNAPSHOT, and master chose the 2nd one. We can fix this issue by giving the standard 3-level layout a higher priority when trying to match schema patterns.
",,apachespark,lian cheng,rdblue,sekiforever,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PARQUET-651,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 04 06:57:29 UTC 2021,,,,,,,,,,"0|i30f5j:",9223372036854775807,,,,,,,,,,,,,2.1.0,,,,,,,,,,,"01/Jul/16 11:03;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/14013;;;","01/Jul/16 11:42;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/14014;;;","06/Jul/16 17:35;rdblue;[~lian cheng], I'm looking at this today.;;;","06/Jul/16 20:53;rdblue;It looks like the main change is to specifically catch the 3-level name structure, {{list-name (LIST) -> ""list"" -> ""element""}}. The problem with this approach is that it doesn't solve the problem entirely either.

Let me try to give a bit more background. In parquet-avro, there are two {{isElementType}} methods; one in the schema converter and one in the record converter. The one in the schema converter will guess whether the Parquet type uses a 3-level list or a 2-level list when it can't be determined according to the spec's backward-compatibility rules. That guess assumes a 2-level structure by default and at the next major release will guess a 3-level structure. (This can be controlled by a property.) But this is only used when the reader doesn't supply a read schema / expected schema and the code has to convert from Parquet's type to get one. Ideally, we always have a read schema from the file, from the reader's expected class (if using Java objects), or from the reader passing in the expected schema. That's why the other {{isElementType}} method exists: it looks at the expected schema and the file schema to determine whether the caller has passed in a schema with the extra single-field list/element struct.

That code has to distinguish between two cases for a 3-level list:
1. When the caller expects {{List<OneTuple<ElementType>>}}, with the extra record layer that was originally returned when Avro only knew about 2-level lists.
2. When the caller expects {{List<ElementType>}}, without an extra layer.

The code currently assumes that if the element schema appears to match the repeated type that the caller has passed a schema indicating case 1. This issue points out that the matching isn't perfect and an element with a single field named ""element"" will incorrectly match case 1 when it was really case 2. The problem with the solution in PR #14013, if it were applied to Avro, is that it breaks if the caller is actually passing a schema for case 1.

I'm not sure whether Spark works like Avro and has two {{isElementType}} methods. If Spark can guarantee that the table schema is never case 1, then it is correct to use the logic in the PR. I don't think that's always the case because the table schema may come from user objects in a Dataset or from the Hive MetaStore. But, this may be a reasonable heuristic if you think case 2 is far more common than case 1. For parquet-avro, I think the user supplying a single-field record with the inner field named ""element"" is rare enough that it doesn't really matter, but it's up to you guys in the Spark community on this issue.

One last thing: based on the rest of the schema structure, there should be only one way to match the expected schema to the file schema. You could always try both and fall back to the other case, or have a more complicated {{isElementType}} method that recurses down the sub-trees to find a match. I didn't implement this in parquet-avro because I think it's a rare problem and not worth the time.;;;","07/Jul/16 13:13;lian cheng;Thanks for the detailed response!

Spark SQL also has two {{isElementType}} methods. Actually, I was following parquet-avro when writing that part of code.

For the two cases you listed, from the perspective of application-level data model, it's true that a type like {{List<OneTuple<ElementType>>}} can be rare. However, we did hit this kind of schema in production, which is the reason why I noticed this issue.

On the other hand, to the best of my knowledge, no well-known Parquet data model writes a two-level LIST structure with a repeated field named ""list"". That's why I thought this heuristic can be relatively safe.

I agree that recursing down the data type tree is the proper way to fix this issue, but it's probably not worth the extra complexity introduced.;;;","07/Jul/16 15:44;rdblue;I agree, using list/element is reasonable. I just want to note that this doesn't solve the problem, it changes the heuristic to do the right thing for one case and the wrong thing for another.

I won't make this change in parquet-avro before a major release because it isn't backward-compatible. I know of cases where applications were written expecting the additional layer and pass it in as the expected schema. Making this change would break those applications to make a rare struct work. For Spark, it's up to you guys because this is a major release (assuming it makes 2.0) and the need for backward-compatibility is different.;;;","09/Jul/16 09:25;lian cheng;I was re-thinking about [~rdblue]'s comment above, and tried to build some more corner cases that [PR #14014|https://github.com/apache/spark/pull/14014] can't handle. Here is a similar case constructed using Hive 1.2.1:

{code:sql}
CREATE TABLE s
STORED AS PARQUET
AS SELECT ARRAY(NAMED_STRUCT('array_element', 1)) AS f;
{code}

When writing to Parquet, Hive encodes array fields into the following non-standard 3-level layout:

{noformat}
optional group <name> (LIST) {
  repeated group bag {
    optional <element-type> array_element;
  }
}
{noformat}

According to this template layout, the above SQL DDL write a Parquet file with the following schema:

{noformat}
$ parquet-schema $WAREHOUSE_DIR/s/000000_0
message hive_schema {
  optional group f (LIST) {
    repeated group bag {
      optional group array_element {
        optional int32 array_element;
      }
    }
  }
}
{noformat}

Reading this file using Spark patched with PR #14014 results in the same exception described in the ticket description. This is not surprising since the case above is exactly the same with the tracked one except that the actual field names are different.
;;;","10/Jul/16 08:06;lian cheng;Thanks to [~rdblue]'s comment about why there're two different {{isElementType}} methods parquet-avro, I finally realized that Spark SQL doesn't really need two {{isElementType}} methods as what parquet-avro does, and came up with a proper fix of this issue for Spark SQL (already pushed this to [PR #14014|https://github.com/apache/spark/pull/14014]).

Here I'm trying to write down my understanding for future reference.

One important difference between parquet-avro and Spark SQL 1.6+ is how the requested Parquet schema is set in {{ReadSupport.init()}}.

In parquet-avro, the requested Parquet schema can be set via two methods:

# If no requested Avro schema is specified, the full Parquet schema of the file to be read is used as requested schema.
# If a requested Avro schema is specified, parquet-avro converts the requested Avro schema into a Parquet schema using {{AvroSchemaConverter}}, and use the converted Parquet schema as requested Parquet schema.

The 2nd case is risky because the converted Parquet schema may not conform to the ""flavor"" of the actual schema of the physical Parquet file. For example, the file might be created using parquet-protobuf, and use {{repeated int32 f;}} to represent an integer array, while parquet-avro uses either a 2-level or a 3-level layout. This inconsistency limits interoperability of parquet-avro. Although the 2nd {{isElementType}} in {{AvroRecordConverter}} helps to reconcile part of the corner cases by comparing the requested schema and the expected Avro schema, this issue is still not completely resolved.

In Spark SQL, to provide better interoperability, the requested Parquet schema generated for each physical file is always tailored from the actual schema of the file to be read. The way we do the tailoring is like the following:

# The query execution engine always provides a requested schema {{cs}} in the form of a Catalyst {{StructType}}.
# {{ParquetReadSupport.init()}} calls {{ParquetReadSupport.clipParquetSchema()}} to tailor the Parquet schema {{ps}} of the physical file using {{cs}}
# All column paths that exist in {{cs}} but missing in {{ps}} are added to {{ps}}
# All column paths that exist in {{ps}} but missing in {{ps}} are removed from {{ps}}
# All backward-compatibility rules are properly considered while adding/removing column paths to/from {{ps}}

The above work was done in [PR #8509|https://github.com/apache/spark/pull/8509].

In this way, it's guaranteed that the tailored requested schema always fits the file to be read. Thus we don't really need the 2nd {{isElementType}} in {{ParquetRowConverter}} to do any further reconcilation. The real cause of this JIRA ticket is that the two {{ParquetRowConverter.isElementType}} makes a different decision from {{ParquetSchemaConverter.isElementType}}. For the schema in question

{noformat}
optional group f (LIST) {
  repeated group list {
    optional group element {
      optional int32 element;
    }
  }
}
{noformat}

{{ParquetSchemaConverter.isElementType}} thinks it's a standard 3-level layout, which is correct, while {{ParquetRowConverter.isElementType}} thinks it's a 2-level layout. Thus the generated row converter doesn't conform with the requested schema, and caused the problem.

By removing {{ParquetRowConverter.isElementType}}, we can avoid the inconsistent decisions. When trying to test whether a repeated field within a LIST-annotated field corresponds to the element type or not in {{ParquetRowConverter}}, we only need to try to convert the repeated type into a Catalyst type to see whether the converted type matches the actual Catalyst array element type.

The above fix is also equivalent to the proper fix [~rdblue] mentioned since the schema conversion process properly recurses the data type sub-tree. The Hive case mentioned above is also fixed by this approach.

cc [~yhuai]
;;;","11/Jul/16 15:44;rdblue;Sounds good to me! I like the idea of converting back to the expected type and then checking that it is correct. That's an easy way to solve the problem entirely. I think we could do the same thing in parquet-avro to solve more cases.

The problem you raise with parquet-avro is a separate issue because the schema in case 2 is only used to filter columns. Setting up the converters is done independently. Also, this is why there are two methods for setting Avro schemas: the projection schema and the read schema. That gives some additional flexibility to make the projection schema match and solves cases like renamed fields. But you're right that it's not complete right now and needs to be fixed.;;;","20/Jul/16 23:50;yhuai;Issue resolved by pull request 14014
[https://github.com/apache/spark/pull/14014];;;","04/Feb/21 04:45;sekiforever;[~lian cheng] I know this jira has been resolved years ago. I am currently using AvroParquetReader to read spark created parquet data which contains array values, and I ran into a problem that resolved schema is an array of a record type instead of an array of primitive type (which was used as the schema in spark code to generate parquet files). I think this has to do with the 2-level/3-level layout you mentioned in this ticket. I already asked this question in #spark slack channel of ASF workspace. Do you mind sharing your contact info so that we can discuss it quickly? Thanks so much for your help!;;;","04/Feb/21 06:57;sekiforever;Actually this ticket is very similar to the problem I am facing: https://issues.apache.org/jira/browse/PARQUET-651;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ScriptTransform does not print stderr when outstream is lost,SPARK-16339,12986062,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,tejasp,tejasp,tejasp,01/Jul/16 06:11,14/Jul/16 16:50,14/Jul/23 06:29,06/Jul/16 08:18,1.6.2,,,,,,,,2.0.0,,,,Spark Core,,,,,,,,0,,,,,,"Currently, if due to some failure, the `outstream` gets destroyed or closed and later `outstream.close()` leads to `IOException` in such case : https://github.com/apache/spark/blob/4f869f88ee96fa57be79f972f218111b6feac67f/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/ScriptTransformation.scala#L325

Due to this, the `stderrBuffer` does not get logged and there is no way for users to see why the job failed.",,apachespark,tejasp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 06 08:18:23 UTC 2016,,,,,,,,,,"0|i30eqf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/16 06:12;apachespark;User 'tejasapatil' has created a pull request for this issue:
https://github.com/apache/spark/pull/13834;;;","06/Jul/16 08:18;srowen;Issue resolved by pull request 13834
[https://github.com/apache/spark/pull/13834];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
