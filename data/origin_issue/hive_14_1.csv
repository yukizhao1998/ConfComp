Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Log Work,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Cloners),Inward issue link (Container),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Outward issue link (Incorporates),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Outward issue link (Regression),Inward issue link (Required),Outward issue link (Required),Outward issue link (Supercedes),Inward issue link (dependent),Inward issue link (dependent),Outward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (Hadoop Flags),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
TestMetrics fails intermittently on the trunk,HIVE-7154,12717685,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,szehon,szehon,szehon,30/May/14 23:21,13/Nov/14 19:43,14/Jul/23 06:14,02/Jun/14 15:46,,,,,,,,,,0.14.0,,Tests,,,,0,,,"TestMetrics fails intermittently on trunk, with:
{noformat}
java.io.IOException: No metrics scope named foo
	at org.apache.hadoop.hive.common.metrics.Metrics.getScope(Metrics.java:222)
	at org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency(TestMetrics.java:215)
{noformat}

Not able to prove it, but seems like it may depend on the test order.  For example, if I just run ""testScopeConcurrency"" by itself then it fails because the metric is not initialized.  If I run the entire test then it will succeed because the metric is initialized by the other tests.",,szehon,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/14 02:49;szehon;HIVE-7154.2.patch;https://issues.apache.org/jira/secure/attachment/12647849/HIVE-7154.2.patch","30/May/14 23:25;szehon;HIVE-7154.patch;https://issues.apache.org/jira/secure/attachment/12647720/HIVE-7154.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395889,,,,Thu Nov 13 19:43:15 UTC 2014,,,,,,,,,,"0|i1w5pr:",396006,,,,,,,,,,,,,,,,,,,,,"30/May/14 23:25;szehon;Attaching first attempt at the fix.  Ran test and it passed.;;;","31/May/14 18:51;ashutoshc;+1;;;","01/Jun/14 20:44;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647720/HIVE-7154.patch

{color:red}ERROR:{color} -1 due to 16 failed/errored test(s), 5571 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_21
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_metadata_only_queries
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_ptf
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_schema_evolution
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeSingleThread
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/360/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/360/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-360/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 16 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647720;;;","02/Jun/14 01:13;ashutoshc;Resulted in new failure of {{TestMetrics.testScopeSingleThread}};;;","02/Jun/14 02:49;szehon;Looked at the test report and found they are run in this order (that is different on my machine):  testMetricsMBean, testScopeConcurrency	, testScopeSingleThread. 

Adding ""endScope"" in #testScopeConcurrency in addition to ""startScope"", so the next test has a clean state.

I manually ran the tests in this order, and in several other orders, to verify.  Hope this works.;;;","02/Jun/14 09:47;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647849/HIVE-7154.2.patch

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 5510 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_display_colstats_tbllvl
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/367/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/367/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-367/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647849;;;","02/Jun/14 15:46;ashutoshc;Committed to trunk. Thanks, Szehon!;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveStreaming - Bug in TransactionBatch.toString() method,HIVE-7153,12717650,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,roshan_naik,roshan_naik,roshan_naik,30/May/14 20:56,02/Jun/14 15:50,14/Jul/23 06:14,02/Jun/14 15:50,0.13.0,,,,,,,,,,,HCatalog,,,,0,Streaming,,"The TransactionBatchImpl.toString() method currently returns :

{code}
return ""TxnIds=["" + txnIds.get(0) + ""src/gen/thrift"" + txnIds.get(txnIds.size()-1)
              + ""] on endPoint= "" + endPt;
{code}


The ""src/gen/thrift"" there is a typo and needs to replaced with  ""...""

",,roshan_naik,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/May/14 21:02;roshan_naik;HIVE-7153.patch;https://issues.apache.org/jira/secure/attachment/12647685/HIVE-7153.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395854,,,,Mon Jun 02 15:50:06 UTC 2014,,,,,,,,,,"0|i1w5hz:",395971,,,,,,,,,,,,,,,,,,,,,"30/May/14 21:02;roshan_naik;uploading patch;;;","31/May/14 18:52;ashutoshc;+1;;;","01/Jun/14 18:06;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647685/HIVE-7153.patch

{color:red}ERROR:{color} -1 due to 18 failed/errored test(s), 5571 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_union
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_metadata_only_queries
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_ptf
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_schema_evolution
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/359/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/359/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-359/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 18 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647685;;;","02/Jun/14 15:50;ashutoshc;Test failures are unrelated. Committed to trunk. Thanks, Roshan!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileInputStream is not closed in HiveConnection#getHttpClient(),HIVE-7150,12717599,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,apivovarov,yuzhihong@gmail.com,yuzhihong@gmail.com,30/May/14 18:01,16/Feb/16 23:50,14/Jul/23 06:14,29/Jun/15 05:28,,,,,,,,,,1.3.0,2.0.0,JDBC,,,,0,,,"Here is related code:

{code}
            sslTrustStore.load(new FileInputStream(sslTrustStorePath),
                sslTrustStorePassword.toCharArray());
{code}
The FileInputStream is not closed upon returning from the method.",,apivovarov,gliptak,sushanth,xuefuz,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/15 19:01;gliptak;HIVE-7150.1.patch;https://issues.apache.org/jira/secure/attachment/12727994/HIVE-7150.1.patch","28/May/15 01:16;gliptak;HIVE-7150.2.patch;https://issues.apache.org/jira/secure/attachment/12735760/HIVE-7150.2.patch","27/Jun/15 03:25;apivovarov;HIVE-7150.3.patch;https://issues.apache.org/jira/secure/attachment/12742291/HIVE-7150.3.patch","27/Jun/15 20:19;apivovarov;HIVE-7150.4.patch;https://issues.apache.org/jira/secure/attachment/12742348/HIVE-7150.4.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395803,,,,Mon Jun 29 05:28:30 UTC 2015,,,,,,,,,,"0|i1w56n:",395920,,,,,,,,,,,,,,,,,,,,,"11/Apr/15 01:48;gliptak;Please code review. Thanks;;;","11/Apr/15 11:20;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12724404/HIVE-7150.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3376/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3376/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-3376/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]
+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-TRUNK-Build-3376/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
++ egrep -v '^X|^Performing status on external'
++ awk '{print $2}'
++ svn status --no-ignore
+ rm -rf
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1672859.

At revision 1672859.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12724404 - PreCommit-HIVE-TRUNK-Build;;;","25/Apr/15 15:36;gliptak;I uploaded an upodated patch (but the QA build didn't run ...);;;","28/Apr/15 01:17;gliptak;Is there a way to force a Hive QA build? Thanks;;;","09/May/15 00:11;sushanth;Removing fix version of 1.2.0 in preparation of release, since this is not a blocker for 1.2.0.;;;","27/May/15 23:13;yuzhihong@gmail.com;lgtm
{code}
372	               } catch (IOException ioe) {
373	                 // IGNORE
{code}
You may want to log the exception.;;;","28/May/15 01:17;gliptak;[~tedyu] I modified to log the exception.;;;","28/May/15 21:49;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12735760/HIVE-7150.2.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 8976 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fold_case
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx_cbo_2
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4079/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4079/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4079/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12735760 - PreCommit-HIVE-TRUNK-Build;;;","28/May/15 21:51;yuzhihong@gmail.com;Patch v2 looks good.;;;","12/Jun/15 21:22;gliptak;Any other improvements needed before this can be considered for merging? Thanks;;;","27/Jun/15 03:25;apivovarov;patch #3
- close FileInputStream in 3 places in HiveConnection class;;;","27/Jun/15 09:40;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12742291/HIVE-7150.3.patch

{color:green}SUCCESS:{color} +1 9030 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4407/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4407/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4407/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12742291 - PreCommit-HIVE-TRUNK-Build;;;","27/Jun/15 14:53;gliptak;[~apivovarov] Thank you for extending the patch
;;;","27/Jun/15 20:19;apivovarov;patch #4
- using try-with-resources;;;","28/Jun/15 05:39;xuefuz;＋1;;;","28/Jun/15 05:50;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12742348/HIVE-7150.4.patch

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 9033 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_groupby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_groupby2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_join_pkfk
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_ql_rewrite_gbtoidx_cbo_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_explainuser_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union_multiinsert
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_assert_true
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_assert_true2
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4420/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4420/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4420/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12742348 - PreCommit-HIVE-TRUNK-Build;;;","29/Jun/15 05:28;apivovarov;Committed to master and branch-1. Thank you Gabor Liptak and Xuefu Zhang!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet not able to handle negative decimal numbers,HIVE-7149,12717571,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,xuefuz,xuefuz,xuefuz,30/May/14 16:27,13/Nov/14 19:40,14/Jul/23 06:14,01/Jun/14 14:29,0.14.0,,,,,,,,,0.14.0,,Serializers/Deserializers,,,,0,,,"Because of storing decimal numbers as fixed length bytes, Parquet Serde pads leading zeros for decimal numbers that has less bytes representation. This works fine for positive decimals, but messes up with negative ones, for which, leading 1 bits are needed.",,brocknoland,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/May/14 16:37;xuefuz;HIVE-7149.patch;https://issues.apache.org/jira/secure/attachment/12647632/HIVE-7149.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395775,,,,Thu Nov 13 19:40:34 UTC 2014,,,,,,,,,,"0|i1w50v:",395894,,,,,,,,,,,,,,,,,,,,,"30/May/14 16:37;xuefuz;[~brocknoland] Would you mind taking a quick look at this? Thanks.;;;","30/May/14 16:50;brocknoland;+1 pending tests;;;","01/Jun/14 11:39;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647632/HIVE-7149.patch

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 5496 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/356/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/356/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-356/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647632;;;","01/Jun/14 14:29;xuefuz;None of above test failures seems related to the patch. Patch committed to trunk. Thanks to Brock for the review.;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ORC PPD should handle CHAR/VARCHAR types,HIVE-7147,12717448,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,gopalv,gopalv,gopalv,30/May/14 00:03,13/Nov/14 19:40,14/Jul/23 06:14,02/Jun/14 06:33,0.14.0,,,,,,,,,0.14.0,,File Formats,,,,0,,,"ORC PPD does not handle Varchar/Char data types.

{code}
Caused by: java.lang.IllegalArgumentException: Unknown type VARCHAR
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.getIndexPosition(RecordReaderImpl.java:2671)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.planReadPartialDataStreams(RecordReaderImpl.java:2736)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readPartialDataStreams(RecordReaderImpl.java:2911)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:2526)
{code}
The handling for PPD for both those types should be inline with the String type as the encoding mechanism is the same for all three types.",,erwaman,gopalv,jtschei,krisden,prasanth_j,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/May/14 01:16;gopalv;HIVE-7147.1.patch;https://issues.apache.org/jira/secure/attachment/12647503/HIVE-7147.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395652,,,,Thu Nov 13 19:40:59 UTC 2014,,,,,,,,,,"0|i1w4a7:",395774,ORC PPD should recognize CHAR/VARCHAR index entries,,,,,,,,,,,,,,,,,,,,"30/May/14 01:17;gopalv;[~prasanth_j]: can you review this change?;;;","30/May/14 01:18;prasanth_j;+1;;;","01/Jun/14 02:31;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647503/HIVE-7147.1.patch

{color:red}ERROR:{color} -1 due to 11 failed/errored test(s), 5496 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_6
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_binary_storage_queries
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.conf.TestHiveConf.testConfProperties
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/351/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/351/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-351/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 11 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647503;;;","02/Jun/14 06:33;prasanth_j;Committed to trunk. Thanks [~gopalv]!;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
posexplode() UDTF fails with a NullPointerException on NULL columns,HIVE-7146,12717380,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,sveiss,sveiss,sveiss,29/May/14 21:12,13/Nov/14 19:41,14/Jul/23 06:14,01/Jun/14 17:50,0.13.0,,,,,,,,,0.14.0,,Query Processor,,,,0,,,"posexplode() fails with a NullPointerException in GenericUDTFPosExplode.process if run against a column containing NULL values. 

explode() has a null check covering this case. I've attached a patch adding the equivalent check to posexplode(), and updating the test case to include a NULL value.",,richardatcloudera,sveiss,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/May/14 21:15;sveiss;HIVE-7146.patch;https://issues.apache.org/jira/secure/attachment/12647451/HIVE-7146.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395584,,,,Thu Nov 13 19:41:03 UTC 2014,,,,,,,,,,"0|i1w3v3:",395706,,,,,,,,,,,,,,,,,,,,,"29/May/14 21:15;sveiss;Minor patch for review. Thank you!;;;","31/May/14 18:53;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647451/HIVE-7146.patch

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 5467 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/346/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/346/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-346/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647451;;;","31/May/14 18:56;ashutoshc;+1;;;","01/Jun/14 17:50;ashutoshc;Committed to trunk. Thanks, Stephen!;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove dependence on apache commons-lang,HIVE-7145,12717375,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,dlavati,omalley,omalley,29/May/14 21:01,17/Nov/22 08:54,14/Jul/23 06:14,08/Oct/19 13:05,,,,,,,,,,4.0.0-alpha-1,,,,,,0,pull-request-available,,"We currently depend on both Apache commons-lang and commons-lang3. They are the same project, just at version 2.x vs 3.x. I propose that we move all of the references in Hive to commons-lang3 and remove the v2 usage.
",,abstractdog,dlavati,kgyrtkirk,omalley,,,,,,,,,,,,,,,,,,,,,,,,"dlavati commented on pull request #795: HIVE-7145 Remove dependence on apache commons-lang
URL: https://github.com/apache/hive/pull/795
 
 
   Change-Id: I0a71c77d736860ad2dbb9a89b10d92745c7c0137
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Oct/19 08:11;githubbot;600","kgyrtkirk commented on pull request #795: HIVE-7145 Remove dependence on apache commons-lang
URL: https://github.com/apache/hive/pull/795
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Oct/19 13:06;githubbot;600",,0,1200,,,0,1200,,,,,,,,,,,,,,,,HIVE-22653,,,HIVE-8040,HIVE-8139,,,,,,,,,,,,,,,,,,,,"01/Oct/19 08:11;dlavati;HIVE-7145.patch;https://issues.apache.org/jira/secure/attachment/12981871/HIVE-7145.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395579,,,,Tue Oct 08 13:05:04 UTC 2019,,,,,,,,,,"0|i1w3uf:",395703,,,,,,,,,,,,,,,,,,,,,"17/Sep/14 21:33;omalley;This is harder than I thought because the generated Thrift code depends on commons lang 2. To completely remove the dependence on commons lang 2, you would need to fix Thrift...;;;","27/Sep/19 13:24;dlavati;Thrift switched to commons-lang3 in 0.9.1 according to THRIFT-1956, and we're using 0.9.3-1, so I'm giving this another shot.;;;","01/Oct/19 18:48;hiveqa;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 58s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  7m 43s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 12m 13s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 11m 27s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 24s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 13m  0s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 11m 25s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 11m 25s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m 16s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 10m 45s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 13s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 71m  3s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  xml  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.43-2+deb8u5 (2017-09-19) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-18813/dev-support/hive-personality.sh |
| git revision | master / 8111ede |
| Default Java | 1.8.0_111 |
| modules | C: storage-api shims/common shims/0.23 common serde metastore vector-code-gen ql llap-server service accumulo-handler beeline cli druid-handler hbase-handler hplsql . U: . |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-18813/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

;;;","01/Oct/19 18:56;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12981871/HIVE-7145.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:green}SUCCESS:{color} +1 due to 17014 tests passed

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/18813/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/18813/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-18813/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12981871 - PreCommit-HIVE-Build;;;","02/Oct/19 12:39;abstractdog;+1
;;;","07/Oct/19 08:56;kgyrtkirk;I also noticed this a few times, its good to see it going away... :)
+1 ;;;","08/Oct/19 13:05;kgyrtkirk;pushed to master. Thank you [~dlavati]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GC pressure during ORC StringDictionary writes ,HIVE-7144,12717372,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,gopalv,gopalv,gopalv,29/May/14 20:55,13/Nov/14 19:44,14/Jul/23 06:14,07/Jul/14 22:31,0.14.0,,,,,,,,,0.14.0,,File Formats,,,,0,ORC,Performance,"When ORC string dictionary writes data out, it suffers from bad GC performance due to a few allocations in-loop.

!orc-string-write.png!

The conversions are as follows

StringTreeWriter::getStringValue() causes 2 conversions

LazyString -> Text (LazyString::getWritableObject)
Text -> String (LazyStringObjectInspector::getPrimitiveJavaObject)

Then StringRedBlackTree::add() does one conversion

String -> Text

This causes some GC pressure with un-necessary String and byte[] array allocations.",ORC Table ~ 12 string columns,gopalv,hagleitn,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7364,,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/14 03:07;gopalv;HIVE-7144.1.patch;https://issues.apache.org/jira/secure/attachment/12651799/HIVE-7144.1.patch","24/Jun/14 17:17;gopalv;HIVE-7144.2.patch;https://issues.apache.org/jira/secure/attachment/12652232/HIVE-7144.2.patch","02/Jul/14 18:59;gopalv;HIVE-7144.3.patch;https://issues.apache.org/jira/secure/attachment/12653660/HIVE-7144.3.patch","29/May/14 20:56;gopalv;orc-string-write.png;https://issues.apache.org/jira/secure/attachment/12647448/orc-string-write.png",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395576,,,,Thu Nov 13 19:44:33 UTC 2014,,,,,,,,,,"0|i1w3tr:",395700,Use Text writables directly in ORC dictionaries to avoid String allocations.,,,,,,,,,,,,,,,,,,,,"21/Jun/14 07:50;gopalv;Benchmark insert of TPC-H 1Tb scale data

|| Table || Before || After || Diff ||
| part | 46.497 | 45.508 | 0.989 |
| partsupp | 145.031 | 144.841 | 0.19 |
| customer | 55.315 | 55.745 | -0.430 |
| orders | 246.692 | 217.834 | 28.858 |
| lineitem | 959.995 | 875.659 | 84.336 |

This makes negligible difference to the smaller tables, but gives a ~10% boost for orders (50.8Gb ORC) and lineitem (224.5Gb ORC).

The optimization does not benefit the sorted string columns, because for every row read there's a data-copy to populate either the minimum or maximum in the column statistics section (Strings are immutable, Text is Writable).;;;","21/Jun/14 07:51;gopalv;Previous comment, all table items are in seconds.;;;","22/Jun/14 12:18;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12651799/HIVE-7144.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5653 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/550/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/550/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-550/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12651799;;;","24/Jun/14 07:31;hagleitn;Failures are unrelated. Minor comments on rb. Otherwise: LGTM +1;;;","24/Jun/14 17:17;gopalv;Address rb comments - renamed methods for clarity, refactored duplicate String/Text codepaths into one.;;;","24/Jun/14 23:45;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12652232/HIVE-7144.2.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5669 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part2
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/581/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/581/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-581/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12652232;;;","02/Jul/14 19:56;gopalv;Re-run tests with trunk.;;;","02/Jul/14 22:35;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12653660/HIVE-7144.3.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5672 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/660/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/660/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-660/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12653660;;;","07/Jul/14 22:31;gopalv;Committed to trunk, thanks [~hagleitn]!;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add Streaming support in Windowing mode for more UDAFs (min/max, lead/lag, fval/lval)",HIVE-7143,12717294,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,rhbutani,rhbutani,rhbutani,29/May/14 16:49,13/Nov/14 19:40,14/Jul/23 06:14,07/Jun/14 16:07,,,,,,,,,,0.14.0,,,,,,0,TODOC14,,"Provided implementations for Streaming for the above fns.

Min/Max based on Alg by Daniel Lemire: http://www.archipel.uqam.ca/309/1/webmaximinalgo.pdf",,leftyl,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7062,,,,,,,,,,,,,,,,,,,,,"30/May/14 20:59;rhbutani;HIVE-7143.1.patch;https://issues.apache.org/jira/secure/attachment/12647683/HIVE-7143.1.patch","02/Jun/14 18:35;rhbutani;HIVE-7143.3.patch;https://issues.apache.org/jira/secure/attachment/12647955/HIVE-7143.3.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395498,,,,Thu Nov 13 19:40:06 UTC 2014,,,,,,,,,,"0|i1w3db:",395626,,,,,,,,,,,,,,,,,,,,,"30/May/14 20:59;rhbutani;patch for min/max
Based on changes in HIVE-7062;;;","02/Jun/14 18:35;rhbutani;patch includes lead/lag also;;;","04/Jun/14 04:55;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647955/HIVE-7143.3.patch

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 5530 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_display_colstats_tbllvl
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/382/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/382/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-382/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647955;;;","06/Jun/14 16:53;ashutoshc;+1;;;","07/Jun/14 16:07;ashutoshc;Committed to trunk. Thanks, Harish!;;;","13/Jun/14 09:32;leftyl;What user doc does this need?

* [Language Manual -- Windowing and Analytics Functions | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics];;;","07/Jul/14 22:22;leftyl;See user doc comments on HIVE-7062.;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix test fail of TestTezTask.testSubmit,HIVE-7135,12717091,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,navis,vikram.dixit,vikram.dixit,28/May/14 17:31,13/Nov/14 19:42,14/Jul/23 06:14,06/Jun/14 16:20,0.14.0,,,,,,,,,0.14.0,,Tez,,,,0,,,HIVE-7043 broke a tez test case.,,thejas,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/May/14 17:32;vikram.dixit;HIVE-7135.1.patch;https://issues.apache.org/jira/secure/attachment/12647172/HIVE-7135.1.patch","03/Jun/14 05:40;navis;HIVE-7135.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12648079/HIVE-7135.2.patch.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395299,,,,Thu Nov 13 19:42:51 UTC 2014,,,,,,,,,,"0|i1w25z:",395430,,,,,,,,,,,,,,,,,,,,,"28/May/14 17:32;vikram.dixit;Patch provided by [~navis];;;","29/May/14 21:27;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647172/HIVE-7135.1.patch

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 5466 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.conf.TestHiveConf.testConfProperties
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/329/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/329/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-329/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647172;;;","31/May/14 19:15;ashutoshc;If it was meant to fix {{TestTezTask}} that didnt get fixed as evident in test results of Hive QA.;;;","04/Jun/14 01:06;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12648079/HIVE-7135.2.patch.txt

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 5510 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_display_colstats_tbllvl
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/381/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/381/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-381/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12648079;;;","05/Jun/14 17:42;ashutoshc;+1;;;","06/Jun/14 16:20;ashutoshc;Committed to trunk. Thanks, Navis!;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dependencies of fetch task for tez are not shown properly,HIVE-7131,12716983,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,Fixed,navis,navis,navis,28/May/14 06:31,13/Nov/14 19:40,14/Jul/23 06:14,05/Jun/14 17:39,,,,,,,,,,0.14.0,,Query Processor,,,,0,,,HIVE-3925 made dependencies for fetch task. But missed that for Tez tasks.,,navis,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/May/14 02:13;navis;HIVE-7131.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12647282/HIVE-7131.1.patch.txt","03/Jun/14 06:09;navis;HIVE-7131.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12648082/HIVE-7131.2.patch.txt","04/Jun/14 13:42;navis;HIVE-7131.3.patch.txt;https://issues.apache.org/jira/secure/attachment/12648328/HIVE-7131.3.patch.txt",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395191,,,,Thu Nov 13 19:40:47 UTC 2014,,,,,,,,,,"0|i1w1hz:",395322,,,,,,,,,,,,,,,,,,,,,"30/May/14 20:03;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647282/HIVE-7131.1.patch.txt

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/339/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/339/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-339/

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
U    ql/src/test/results/clientpositive/list_bucket_dml_12.q.out
U    ql/src/test/results/clientpositive/union_date.q.out
U    ql/src/test/results/clientpositive/join_nullsafe.q.out
U    ql/src/test/results/clientpositive/avro_evolved_schemas.q.out
U    ql/src/test/results/clientpositive/groupby1_limit.q.out
U    ql/src/test/results/clientpositive/auto_sortmerge_join_13.q.out
U    ql/src/test/results/clientpositive/index_auto_mult_tables.q.out
U    ql/src/test/results/clientpositive/index_compact_3.q.out
U    ql/src/test/results/clientpositive/index_bitmap1.q.out
U    ql/src/test/results/clientpositive/mapjoin_memcheck.q.out
U    ql/src/test/results/clientpositive/index_auto.q.out
U    ql/src/test/results/clientpositive/join_1to1.q.out
U    ql/src/test/results/clientpositive/groupby2_map.q.out
U    ql/src/test/results/clientpositive/merge3.q.out
U    ql/src/test/results/clientpositive/create_like_view.q.out
U    ql/src/test/results/clientpositive/input_part9.q.out
U    ql/src/test/results/clientpositive/tez/load_dyn_part2.q.out
U    ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out
U    ql/src/test/results/clientpositive/tez/bucket3.q.out
U    ql/src/test/results/clientpositive/tez/load_dyn_part3.q.out
U    ql/src/test/results/clientpositive/tez/tez_dml.q.out
U    ql/src/test/results/clientpositive/tez/subquery_in.q.out
U    ql/src/test/results/clientpositive/tez/custom_input_output_format.q.out
U    ql/src/test/results/clientpositive/tez/insert1.q.out
U    ql/src/test/results/clientpositive/tez/leftsemijoin.q.out
U    ql/src/test/results/clientpositive/tez/bucket2.q.out
U    ql/src/test/results/clientpositive/groupby5_noskew.q.out
U    ql/src/test/results/clientpositive/join28.q.out
U    ql/src/test/results/clientpositive/index_auto_unused.q.out
U    ql/src/test/results/clientpositive/join37.q.out
U    ql/src/test/results/clientpositive/groupby_sort_4.q.out
U    ql/src/test/results/clientpositive/load_dyn_part1.q.out
U    ql/src/test/results/clientpositive/correlationoptimizer6.q.out
U    ql/src/test/results/clientpositive/join_filters_overlap.q.out
U    ql/src/test/results/clientpositive/join32.q.out
U    ql/src/test/results/clientpositive/orc_empty_strings.q.out
U    ql/src/test/results/clientpositive/merge_dynamic_partition.q.out
U    ql/src/test/results/clientpositive/filter_numeric.q.out
U    ql/src/test/results/clientpositive/list_bucket_dml_5.q.out
U    ql/src/test/results/clientpositive/union32.q.out
U    ql/src/test/results/clientpositive/groupby1_map_skew.q.out
U    ql/src/test/results/clientpositive/subquery_in.q.out
U    ql/src/test/results/clientpositive/alter_char1.q.out
U    ql/src/test/results/clientpositive/input1_limit.q.out
U    ql/src/test/results/clientpositive/load_dyn_part14.q.out
U    ql/src/test/results/clientpositive/newline.q.out
U    ql/src/test/results/clientpositive/subquery_in_having.q.out
U    ql/src/test/results/clientpositive/ctas_colname.q.out
U    ql/src/test/results/clientpositive/auto_join26.q.out
U    ql/src/test/results/clientpositive/avro_joins.q.out
U    ql/src/test/results/clientpositive/groupby1_noskew.q.out
U    ql/src/test/results/clientpositive/load_dyn_part8.q.out
U    ql/src/test/results/clientpositive/orc_diff_part_cols.q.out
U    ql/src/test/results/clientpositive/groupby6_noskew.q.out
U    ql/src/test/results/clientpositive/groupby_rollup1.q.out
U    ql/src/test/results/clientpositive/join39.q.out
U    ql/src/test/results/clientpositive/parquet_partitioned.q.out
U    ql/src/test/results/clientpositive/groupby_sort_6.q.out
U    ql/src/test/results/clientpositive/groupby7_noskew_multi_single_reducer.q.out
U    ql/src/test/results/clientpositive/index_bitmap_auto_partitioned.q.out
U    ql/src/test/results/clientpositive/bucket2.q.out
U    ql/src/test/results/clientpositive/load_dyn_part3.q.out
U    ql/src/test/results/clientpositive/groupby8_map_skew.q.out
U    ql/src/test/results/clientpositive/join25.q.out
U    ql/src/test/results/clientpositive/auto_sortmerge_join_9.q.out
U    ql/src/test/results/clientpositive/correlationoptimizer8.q.out
U    ql/src/test/results/clientpositive/stats4.q.out
U    ql/src/test/results/clientpositive/groupby_sort_1_23.q.out
U    ql/src/test/results/clientpositive/join_casesensitive.q.out
U    ql/src/test/results/clientpositive/join34.q.out
U    ql/src/test/results/clientpositive/ba_table2.q.out
U    ql/src/test/results/clientpositive/groupby_sort_1.q.out
U    ql/src/test/results/clientpositive/join18_multi_distinct.q.out
U    ql/src/test/results/clientpositive/groupby7_map.q.out
U    ql/src/test/results/clientpositive/columnarserde_create_shortcut.q.out
U    ql/src/test/results/clientpositive/list_bucket_dml_7.q.out
U    ql/src/test/results/clientpositive/parallel.q.out
U    ql/src/test/results/clientpositive/index_auto_mult_tables_compact.q.out
U    ql/src/test/results/clientpositive/list_bucket_query_oneskew_1.q.out
U    ql/src/test/results/clientpositive/groupby_map_ppr_multi_distinct.q.out
U    ql/src/test/results/clientpositive/list_bucket_dml_2.q.out
U    ql/src/test/results/clientpositive/mapjoin_subquery.q.out
U    ql/src/test/results/clientpositive/multi_insert_gby.q.out
U    ql/src/test/results/clientpositive/join_filters.q.out
U    ql/src/test/results/clientpositive/groupby1_map.q.out
U    ql/src/test/results/clientpositive/input_lazyserde.q.out
U    ql/src/test/results/clientpositive/combine1.q.out
U    ql/src/test/results/clientpositive/char_1.q.out
U    ql/src/test/results/clientpositive/index_compact_2.q.out
U    ql/src/test/results/clientpositive/groupby7_map_multi_single_reducer.q.out
U    ql/src/test/results/clientpositive/join_nulls.q.out
U    ql/src/test/results/clientpositive/orc_diff_part_cols2.q.out
U    ql/src/test/results/clientpositive/multi_insert_lateral_view.q.out
U    ql/src/test/results/clientpositive/groupby2_noskew.q.out
U    ql/src/test/results/clientpositive/orc_create.q.out
U    ql/src/test/results/clientpositive/join32_lessSize.q.out
U    ql/src/test/results/clientpositive/alter_varchar1.q.out
U    ql/src/test/results/clientpositive/groupby7_noskew.q.out
U    ql/src/test/results/clientpositive/join27.q.out
U    ql/src/test/results/clientpositive/innerjoin.q.out
U    ql/src/test/results/clientpositive/union_top_level.q.out
U    ql/src/test/results/clientpositive/join36.q.out
U    ql/src/test/results/clientpositive/groupby_sort_3.q.out
U    ql/src/test/results/clientpositive/leftsemijoin.q.out
U    ql/src/test/results/clientpositive/input40.q.out
U    ql/src/test/results/clientpositive/list_bucket_dml_9.q.out
U    ql/src/test/results/clientpositive/join31.q.out
U    ql/src/test/results/clientpositive/char_join1.q.out
U    ql/src/test/results/clientpositive/union22.q.out
U    ql/src/test/results/clientpositive/list_bucket_query_oneskew_3.q.out
U    ql/src/test/results/clientpositive/join_map_ppr.q.out
U    ql/src/test/results/clientpositive/list_bucket_dml_4.q.out
U    ql/src/test/results/clientpositive/union31.q.out
U    ql/src/test/results/clientpositive/groupby7_map_skew.q.out
U    ql/src/test/results/clientpositive/index_auto_self_join.q.out
U    ql/src/test/results/clientpositive/auto_smb_mapjoin_14.q.out
U    ql/src/test/results/clientpositive/merge_dynamic_partition3.q.out
U    ql/src/test/results/clientpositive/list_bucket_dml_13.q.out
U    ql/src/test/results/clientpositive/insert1.q.out
U    ql/src/test/results/clientpositive/column_access_stats.q.out
U    ql/src/test/results/clientpositive/index_bitmap2.q.out
U    ql/src/test/results/clientpositive/orc_predicate_pushdown.q.out
U    ql/src/test/results/clientpositive/mi.q.out
U    ql/src/test/results/clientpositive/groupby_multi_single_reducer.q.out
U    ql/src/test/results/clientpositive/merge4.q.out
U    ql/src/test/results/clientpositive/combine2_hadoop20.q.out
U    ql/src/test/results/clientpositive/groupby_multi_insert_common_distinct.q.out
U    ql/src/test/results/clientpositive/index_bitmap.q.out
U    ql/src/test/results/clientpositive/join29.q.out
U    ql/src/test/results/clientpositive/groupby6_map.q.out
U    ql/src/test/results/clientpositive/groupby8_noskew.q.out
U    ql/src/test/results/clientpositive/multi_insert.q.out
U    ql/src/test/results/clientpositive/groupby_sort_5.q.out
U    ql/src/test/results/clientpositive/groupby_grouping_sets4.q.out
U    ql/src/test/results/clientpositive/index_bitmap_rc.q.out
U    ql/src/test/results/clientpositive/bucket1.q.out
U    ql/src/test/results/clientpositive/load_dyn_part2.q.out
U    ql/src/test/results/clientpositive/index_auto_partitioned.q.out
U    ql/src/test/results/clientpositive/input42.q.out
U    ql/src/test/results/clientpositive/groupby2_noskew_multi_distinct.q.out
U    ql/src/test/results/clientpositive/correlationoptimizer7.q.out
U    ql/src/test/results/clientpositive/join33.q.out
U    ql/src/test/results/clientpositive/groupby_map_ppr.q.out
U    ql/src/test/results/clientpositive/index_auto_file_format.q.out
U    ql/src/test/results/clientpositive/ba_table1.q.out
U    ql/src/test/results/clientpositive/union24.q.out
U    ql/src/test/results/clientpositive/list_bucket_dml_6.q.out
U    ql/src/test/results/clientpositive/avro_partitioned.q.out
U    ql/src/test/results/clientpositive/index_auto_multiple.q.out
U    ql/src/test/results/clientpositive/groupby_sort_skew_1.q.out
U    ql/src/test/results/clientpositive/custom_input_output_format.q.out
U    ql/src/test/results/clientpositive/groupby6_map_skew.q.out
U    ql/src/test/results/clientpositive/load_dyn_part10.q.out
U    ql/src/test/results/clientpositive/list_bucket_query_multiskew_2.q.out
U    ql/src/test/results/clientpositive/groupby_complex_types_multi_single_reducer.q.out
U    ql/src/test/results/clientpositive/index_compact_1.q.out
U    ql/src/test/results/clientpositive/database.q.out
U    ql/src/test/results/clientpositive/index_bitmap_compression.q.out
U    ql/src/test/results/clientpositive/index_serde.q.out
U    ql/src/test/results/clientpositive/input11_limit.q.out
U    ql/src/test/results/clientpositive/load_dyn_part9.q.out
U    ql/src/test/results/clientpositive/multi_insert_move_tasks_share_dependencies.q.out
U    ql/src/test/results/clientpositive/groupby4_noskew.q.out
U    ql/src/test/results/clientpositive/groupby_sort_7.q.out
U    ql/src/test/results/clientpositive/bucket3.q.out
U    ql/src/test/results/clientpositive/load_dyn_part4.q.out
U    ql/src/test/results/clientpositive/groupby_cube1.q.out
U    ql/src/test/results/clientpositive/join26.q.out
U    ql/src/test/results/clientpositive/correlationoptimizer9.q.out
U    ql/src/test/results/clientpositive/avro_sanity_test.q.out
U    ql/src/test/results/clientpositive/join35.q.out
U    ql/src/test/results/clientpositive/groupby_sort_2.q.out
U    ql/src/test/results/clientpositive/correlationoptimizer15.q.out
U    ql/src/test/results/clientpositive/subquery_views.q.out
U    ql/src/test/results/clientpositive/union26.q.out
U    ql/src/test/results/clientpositive/groupby2_map_multi_distinct.q.out
U    ql/src/test/results/clientpositive/join30.q.out
U    ql/src/test/results/clientpositive/correlationoptimizer10.q.out
U    ql/src/test/results/clientpositive/orc_split_elimination.q.out
U    ql/src/test/results/clientpositive/index_compression.q.out
U    ql/src/test/results/clientpositive/nonblock_op_deduplicate.q.out
U    ql/src/test/results/clientpositive/list_bucket_query_oneskew_2.q.out
U    ql/src/test/results/clientpositive/ba_table_udfs.q.out
U    ql/src/test/results/clientpositive/groupby_sort_skew_1_23.q.out
U    ql/src/test/results/clientpositive/index_compact.q.out
U    ql/src/java/org/apache/hadoop/hive/ql/hooks/PreExecutePrinter.java
U    ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java

Fetching external item into 'hcatalog/src/test/e2e/harness'
Updated external to revision 1598725.

Updated to revision 1598725.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647282;;;","04/Jun/14 08:43;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12648082/HIVE-7131.2.patch.txt

{color:red}ERROR:{color} -1 due to 14 failed/errored test(s), 5510 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_display_colstats_tbllvl
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view_cp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin2
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/383/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/383/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-383/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 14 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12648082;;;","04/Jun/14 15:27;ashutoshc;+1;;;","04/Jun/14 23:14;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12648328/HIVE-7131.3.patch.txt

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 5510 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_union_remove_2
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/389/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/389/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-389/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12648328;;;","05/Jun/14 17:39;ashutoshc;Committed to trunk. Thanks, Navis!;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
schematool is broken for minor version upgrades (eg 0.13.x),HIVE-7130,12716956,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,thejas,thejas,thejas,28/May/14 02:16,09/Jun/14 06:39,14/Jul/23 06:14,30/May/14 07:30,0.13.1,,,,,,,,,0.13.1,0.14.0,,,,,0,,,"The schema initialization fails with schema tool in 0.13.1, with the message that ""Unknown version specified for initialization: 0.13.1"".
The upgrade with schema tool works (ie it runs the upgrade scripts),
but then errors out at the end, when it finds that the version in
metastore is 0.13.0 (not 0.13.1).
",,leftyl,raviprak,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/May/14 20:06;thejas;HIVE-7130.1.patch;https://issues.apache.org/jira/secure/attachment/12647198/HIVE-7130.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,395164,,,,Mon Jun 09 06:39:36 UTC 2014,,,,,,,,,,"0|i1w1c7:",395295,,,,,,,,,,,,,,,,,,,,,"28/May/14 02:17;thejas;Schema tool when used for schema initialization - 

{code}
[apache-hive-0.13.1-bin18:52]$ bin/schematool -dbType derby   -initSchema
Picked up JAVA_TOOL_OPTIONS: -Djava.awt.headless=true
Picked up JAVA_TOOL_OPTIONS: -Djava.awt.headless=true
Metastore connection URL:        jdbc:derby:;databaseName=metastore_db;create=true
Metastore Connection Driver :    org.apache.derby.jdbc.EmbeddedDriver
Metastore connection User:       APP
Starting metastore schema initialization to 0.13.1
org.apache.hadoop.hive.metastore.HiveMetaException: Unknown version specified for initialization: 0.13.1
*** schemaTool failed ***

{code}

Schema tool when used for upgrade - 
{code}

[apache-hive-0.13.1-bin18:35]$ bin/schematool -dbType derby -initSchemaTo  0.12.0
Picked up JAVA_TOOL_OPTIONS: -Djava.awt.headless=true
Picked up JAVA_TOOL_OPTIONS: -Djava.awt.headless=true
Metastore connection URL:        jdbc:derby:;databaseName=metastore_db;create=true
Metastore Connection Driver :    org.apache.derby.jdbc.EmbeddedDriver
Metastore connection User:       APP
Starting metastore schema initialization to 0.12.0
Initialization script hive-schema-0.12.0.derby.sql
Initialization script completed
schemaTool completeted
[apache-hive-0.13.1-bin18:35]$ bin/schematool -dbType derby -upgradeSchema
Picked up JAVA_TOOL_OPTIONS: -Djava.awt.headless=true
Picked up JAVA_TOOL_OPTIONS: -Djava.awt.headless=true
Metastore connection URL:        jdbc:derby:;databaseName=metastore_db;create=true
Metastore Connection Driver :    org.apache.derby.jdbc.EmbeddedDriver
Metastore connection User:       APP
Starting upgrade metastore schema from version 0.12.0 to 0.13.1
Upgrade script upgrade-0.12.0-to-0.13.0.derby.sql
Completed upgrade-0.12.0-to-0.13.0.derby.sql
org.apache.hadoop.hive.metastore.HiveMetaException: Found unexpected schema version 0.13.0
*** schemaTool failed ***
{code};;;","28/May/14 02:19;thejas;Also after upgrade with schematool, the command fails if you turn on hive.metastore.schema.verification
{code}
bin/hive -hiveconf hive.metastore.schema.verification=true
..
..
Caused by: MetaException(message:Hive Schema version 0.13.1 does not match metastore's schema version 0.13.0 Metastore is not upgraded or corrupt)
        at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6306)
        at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6277)
{code};;;","28/May/14 20:30;thejas;HIVE-7130.1.patch adds the concept of equivalent versions. New versions of hive that don't have any updates to metastore schema, can use this to map to an equivalent version that has a schema file.
;;;","28/May/14 22:35;thejas;Ran tests locally and they passed.
;;;","29/May/14 04:16;ashutoshc;+1;;;","30/May/14 04:05;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647198/HIVE-7130.1.patch

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 5541 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/331/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/331/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-331/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647198;;;","30/May/14 07:30;thejas;Patch committed to trunk. Thanks for the review Ashutosh.

[~sushanth] Can you please include this in 0.13.1 release ?
;;;","30/May/14 19:46;sushanth;Will do, this is a release blocker.;;;","02/Jun/14 19:02;sushanth;Committed to 0.13.1, Thanks, Thejas!;;;","09/Jun/14 06:39;thejas;This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Follow-up of HIVE-6367,HIVE-7123,12716525,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,xuefuz,xuefuz,xuefuz,25/May/14 00:30,13/Nov/14 19:43,14/Jul/23 06:14,30/May/14 01:35,0.14.0,,,,,,,,,0.14.0,,Serializers/Deserializers,,,,0,,,"HIVE-6367 provides initial decimal support in Parquet serde. The are a few minor items left over:

1. parquet_decimal.q seems failing
2. will use fixed length binary to encode decimal instead of variable length binary.",,brocknoland,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6367,,,,,,,,,,,,,,,,,,,,,"26/May/14 13:37;xuefuz;HIVE-7123.1.patch;https://issues.apache.org/jira/secure/attachment/12646789/HIVE-7123.1.patch","27/May/14 17:55;xuefuz;HIVE-7123.2.patch;https://issues.apache.org/jira/secure/attachment/12646954/HIVE-7123.2.patch","27/May/14 22:49;xuefuz;HIVE-7123.3.patch;https://issues.apache.org/jira/secure/attachment/12647010/HIVE-7123.3.patch","28/May/14 18:15;xuefuz;HIVE-7123.4.patch;https://issues.apache.org/jira/secure/attachment/12647181/HIVE-7123.4.patch","29/May/14 01:53;xuefuz;HIVE-7123.5.patch;https://issues.apache.org/jira/secure/attachment/12647279/HIVE-7123.5.patch","25/May/14 00:42;xuefuz;HIVE-7123.patch;https://issues.apache.org/jira/secure/attachment/12646698/HIVE-7123.patch",,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394733,,,,Thu Nov 13 19:43:53 UTC 2014,,,,,,,,,,"0|i1vypb:",394868,,,,,,,,,,,,,,,,,,,,,"26/May/14 09:17;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646698/HIVE-7123.patch

{color:red}ERROR:{color} -1 due to 12 failed/errored test(s), 5464 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_serde
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.testArrayDecimal
org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.testDecimalType
org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.testMapDecimal
org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.testStruct
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/297/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/297/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-297/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 12 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646698;;;","26/May/14 13:37;xuefuz;Patch #1 fixes a few parquet related test failures. Others seem unrelated to the patch.;;;","27/May/14 16:08;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646789/HIVE-7123.1.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 5465 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/310/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/310/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-310/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646789;;;","27/May/14 17:04;xuefuz;The above test failures are unrelated. Patch is ready for review. [~brocknoland] Would you mind doing so?;;;","27/May/14 17:41;xuefuz;Patch #2 slightly changed the way that the number of bytes needed to hold a decimal value of a given precision.;;;","27/May/14 17:56;xuefuz;RB: https://reviews.apache.org/r/21933/;;;","27/May/14 18:58;brocknoland;LGTM. The only item is that precisionToBytes is a constant and thus should be final and all caps.;;;","27/May/14 22:49;xuefuz;Patch #3 addressed above Brock's comment.;;;","28/May/14 02:50;brocknoland;Thank you!!

+1 pending tests;;;","28/May/14 13:38;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647010/HIVE-7123.3.patch

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 5467 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_stats_counter
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/318/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/318/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-318/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647010;;;","28/May/14 18:15;xuefuz;Patch #4 addressed the above two parquet test failures.;;;","29/May/14 01:27;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647181/HIVE-7123.4.patch

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 5467 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/322/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/322/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-322/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647181;;;","29/May/14 01:53;xuefuz;Patch #5 updated test output that was caused by other commits.;;;","30/May/14 01:22;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647279/HIVE-7123.5.patch

{color:red}ERROR:{color} -1 due to 11 failed/errored test(s), 5467 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nestedvirtual
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/330/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/330/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-330/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 11 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647279;;;","30/May/14 01:29;xuefuz;None of above test failures appears related to the patch.;;;","30/May/14 01:31;brocknoland;+1;;;","30/May/14 01:35;xuefuz;Patch committed to trunk. Thanks Brock for the review.;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use murmur hash to distribute HiveKey,HIVE-7121,12716447,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,gopalv,gopalv,gopalv,23/May/14 21:34,10/Jun/14 02:44,14/Jul/23 06:14,10/Jun/14 02:41,,,,,,,,,,,,Query Processor,,,,0,,,"The current hashCode implementation produces poor parallelism when dealing with single integers or doubles.

And for partitioned inserts into a 1 bucket table, there is a significant hotspot on Reducer #31.

Removing the magic number 31 and using a more normal hash algorithm would help fix these hotspots.",,appodictic,gopalv,h_o,hagleitn,sershe,wzheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6924,HIVE-7074,,,,,,,,,,,,,,,,,,HIVE-7074,HIVE-7158,HIVE-7148,,,"24/May/14 01:12;gopalv;HIVE-7121.1.patch;https://issues.apache.org/jira/secure/attachment/12646646/HIVE-7121.1.patch","29/May/14 05:02;gopalv;HIVE-7121.2.patch;https://issues.apache.org/jira/secure/attachment/12647296/HIVE-7121.2.patch","30/May/14 04:42;hagleitn;HIVE-7121.3.patch;https://issues.apache.org/jira/secure/attachment/12647534/HIVE-7121.3.patch","23/May/14 21:36;gopalv;HIVE-7121.WIP.patch;https://issues.apache.org/jira/secure/attachment/12646612/HIVE-7121.WIP.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394655,,,,Tue Jun 10 02:41:37 UTC 2014,,,,,,,,,,"0|i1vy8v:",394794,,,,,,,,,,,,,,,,,,,,,"25/May/14 09:47;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646646/HIVE-7121.1.patch

{color:red}ERROR:{color} -1 due to 15 failed/errored test(s), 5462 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket5
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_bucketed_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_dyn_part
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_num_buckets
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_quotedid_smb
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_truncate_column_buckets
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testDropTable
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testListPartitions
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testNameMethods
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/288/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/288/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-288/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 15 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646646;;;","25/May/14 16:58;appodictic;Does this effect bucketed tables? I think it does and then we can not just change the hash code because that would break assumptions of what is in the bucket. IE i create a bucket in hive 12, and in hive 13 different data would be in the bucket.

I think this is why:

org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_num_buckets
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_quotedid_smb

These tests are failing. If this is the case we need a way or recording the hashcode in the metadata for the table.;;;","27/May/14 19:14;hagleitn;[~appodictic] I think you're right. This definitely affects bucketing. 

Options I see are:

- Only do it for queries that do not enter into bucketed tables, i.e.: leave the bucketing hash function as badly distributed as it is, but fix shuffle joins, group bys and inserts into other tables.
- Remember the hash function in table metadata. This is slightly tricky because we probably don't want a mix of hash functions in the same table (different partitions have different bucketing schemes - that would probably destroy any chance of SMB on that table.) Maybe we even want only one function per DB to make sure different tables in a DB can be joined without looking at the hash function used for each.

How come though these unit tests are failing? I didn't think we changed the bucketing scheme between hive 12 and 13. Did we?;;;","27/May/14 20:01;gopalv;[~hagleitn]: The unit tests are failing because I'm applying the same insert mechanic for flat & partitioned tables.

The patch works correctly when the following code fragment is hit

{code}
      // replace bucketing columns with hashcode % numBuckets
      int buckNum = 0;
      if (bucketEval != null) {
        buckNum = computeBucketNumber(row, conf.getNumBuckets());
        cachedKeys[0][buckColIdxInKey] = new IntWritable(buckNum);
      }
{code}

This is indeed setup correctly when doing dynamic partitioned inserts. Looks like this optimization is missed for the flat table inserts.;;;","29/May/14 05:02;gopalv;Fix the table bucketed insert case by limiting the murmur hash to cases with variable number of reducers.

This neatly avoids using the new hash function where there are explicit bucket counts or reducer counts.;;;","30/May/14 04:42;hagleitn;Couldn't apply .2 cleanly. Not sure what happened. .3 should apply.;;;","30/May/14 04:43;hagleitn;Patch looks good. This doesn't affect bucketed tables, but we should still open another jira to handle that case too.

+1 assuming tests pass.;;;","01/Jun/14 07:52;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647534/HIVE-7121.3.patch

{color:red}ERROR:{color} -1 due to 14 failed/errored test(s), 5571 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input42
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_metadata_only_queries
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_ptf
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_schema_evolution
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/355/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/355/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-355/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 14 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647534;;;","02/Jun/14 20:37;gopalv;This has an additional test failure which was not present in HIVE-7158 runs (which includes this patch).

{code}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
{code}

are not failing in https://issues.apache.org/jira/browse/HIVE-7158?focusedCommentId=14015118&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14015118

flaky tests?;;;","10/Jun/14 02:22;hagleitn;Believe so. I've run all the failed tests locally and didn't see any new errors.;;;","10/Jun/14 02:41;hagleitn;Committed to trunk. Thanks [~gopalv]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extended ACL's should be inherited if warehouse perm inheritance enabled,HIVE-7119,12716255,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,szehon,szehon,szehon,23/May/14 03:11,18/Dec/14 07:59,14/Jul/23 06:14,02/Jun/14 03:10,,,,,,,,,,0.14.0,,,,,,0,,,"HDFS recently came out with support for extended ACL's, ie permission for specific group/user in addition to the general owner/group/other permission.

Hive permission inheritance should also inherit those as well, if user has set them at any point in the warehouse directory.",,hagleitn,leftyl,mdominguez@cloudera.com,singhashish,szehon,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6892,,,,,,,,,,,,,,,,,,HIVE-7247,,,,,,,,"29/May/14 22:51;szehon;HIVE-7119.2.patch;https://issues.apache.org/jira/secure/attachment/12647475/HIVE-7119.2.patch","30/May/14 19:19;szehon;HIVE-7119.3.patch;https://issues.apache.org/jira/secure/attachment/12647671/HIVE-7119.3.patch","01/Jun/14 01:09;szehon;HIVE-7119.4.patch;https://issues.apache.org/jira/secure/attachment/12647800/HIVE-7119.4.patch","29/May/14 08:02;szehon;HIVE-7119.patch;https://issues.apache.org/jira/secure/attachment/12647320/HIVE-7119.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394464,,,,Thu Dec 18 07:59:18 UTC 2014,,,,,,,,,,"0|i1vx33:",394605,,,,,,,,,,,,,,,,,,,,,"29/May/14 08:02;szehon;Attaching a patch.;;;","29/May/14 20:46;singhashish;Thanks for the patch Szehon! Left some comments on RB.;;;","29/May/14 22:51;szehon;Rebase + address review comments;;;","30/May/14 04:38;singhashish;LGTM!  
+1;;;","30/May/14 18:48;xuefuz;Patch looks good. It seems missing a license header. Otherwise, +1 pending on tests.;;;","30/May/14 19:19;szehon;Incorporate latest review comments.  Thanks Xuefu and Ashish for review!;;;","31/May/14 11:16;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647671/HIVE-7119.3.patch

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 5480 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/344/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/344/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-344/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647671;;;","01/Jun/14 00:20;xuefuz;Are the failures related?;;;","01/Jun/14 01:09;szehon;[~xuefuz] I looked and the failures are not related.

Unfortunately while running manually some of them I found that the patch needs rebase as HIVE-7116 also modified the shims file, so rebasing here.;;;","01/Jun/14 15:28;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647800/HIVE-7119.4.patch

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 5509 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_multi_single_reducer3
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/357/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/357/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-357/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647800;;;","02/Jun/14 02:24;szehon;Results of rebase.  These failures also do not seem related.;;;","02/Jun/14 03:10;xuefuz;Patch committed to trunk. Thanks Szehon for the contribution.;;;","17/Jun/14 20:06;hagleitn;After this patch ""itests"" cannot be compiled against hadoop-1 profile anymore. Could you take a look?

$cd itests
$mvn install -DskipTests -Phadoop-1
...
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hive-it-unit: Compilation failure: Compilation failure:
[ERROR] /Users/ghagleitner/Projects/hive-test-trunk/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/FolderPermissionBase.java:[31,39] cannot find symbol
[ERROR] symbol  : class AclStatus
[ERROR] location: package org.apache.hadoop.fs.permission
[ERROR] /Users/ghagleitner/Projects/hive-test-trunk/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestExtendedAcls.java:[20,46] cannot find symbol
[ERROR] symbol  : class AclEntryScope
[ERROR] location: package org.apache.hadoop.fs.permission
[ERROR] /Users/ghagleitner/Projects/hive-test-trunk/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestExtendedAcls.java:[20,1] static import only from classes and interfaces
[ERROR] /Users/ghagleitner/Projects/hive-test-trunk/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestExtendedAcls.java:[21,46] cannot find symbol
[ERROR] symbol  : class AclEntryType
[ERROR] location: package org.apache.hadoop.fs.permission
[ERROR] /Users/ghagleitner/Projects/hive-test-trunk/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestExtendedAcls.java:[21,1] static import only from classes and interfaces
[ERROR] /Users/ghagleitner/Projects/hive-test-trunk/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestExtendedAcls.java:[22,46] cannot find symbol
[ERROR] symbol  : class AclEntryType
...;;;","17/Jun/14 22:52;szehon;Sorry about that, I'll take a look, made HIVE-7247 ;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;","18/Dec/14 01:17;szehon;This is already doc'ed via [https://cwiki.apache.org/confluence/display/Hive/Permission+Inheritance+in+Hive|https://cwiki.apache.org/confluence/display/Hive/Permission+Inheritance+in+Hive];;;","18/Dec/14 07:59;leftyl;bq.  This is already doc'ed ...

So I deleted the release note, which was ""Document this addition."";;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Oracle upgrade schema scripts do not map Java long datatype columns correctly for transaction related tables,HIVE-7118,12716252,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,deepesh,deepesh,deepesh,23/May/14 02:35,13/Nov/14 19:42,14/Jul/23 06:14,24/Jun/14 17:32,0.14.0,,,,,,,,,0.14.0,,Database/Schema,,,,0,,,"In Transaction related tables, Java long column fields are mapped to NUMBER(10) which results in failure to persist the transaction ids which are incompatible. Following error is seen:
{noformat}
ORA-01438: value larger than specified precision allowed for this column
{noformat}
NO PRECOMMIT TESTS",Oracle DB,deepesh,gates,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7259,,,,,,,HIVE-8239,,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/14 18:41;deepesh;HIVE-7118-0.13.0.1.patch;https://issues.apache.org/jira/secure/attachment/12651706/HIVE-7118-0.13.0.1.patch","23/May/14 03:59;deepesh;HIVE-7118.1.patch;https://issues.apache.org/jira/secure/attachment/12646443/HIVE-7118.1.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394461,,,,Thu Nov 13 19:42:01 UTC 2014,,,,,,,,,,"0|i1vx2f:",394602,,,,,,,,,,,,,,,,,,,,,"23/May/14 03:59;deepesh;Attaching a patch that changes NUMBER(10) to NUMBER(19) datatype for long columns for Oracle metastore schema upgrade scripts.;;;","23/May/14 17:11;gates;We haven't built install scripts for 14 yet as Ashutosh and I were discussing if there was a better method than creating the tables directly in the scripts.  So we can't apply this patch until we make a decision on that.;;;","19/Jun/14 22:21;gates;Can we just take the upgrade parts of this (the 019-... script and the upgrade script) and leave the 0.14 versions of schema files for later?  That way we'll get this fix in where people have access to it.;;;","20/Jun/14 18:02;deepesh;Its not clear to me as to where would the upgrade script (019-HIVE-7118.oracle.sql) be invoked from. It may not be desirable to call this from upgrade-0.12.0-to-0.13.0.oracle.sql script as people will miss it as they are already on 0.13. What do you think?;;;","20/Jun/14 18:10;gates;Good point.  For the Oracle scripts we should fix the hive-schema-0.13 and hive-txn-schema-0.13 scripts so they don't need to call the upgrade.  If they've already installed 13 and need to fix this bug they can just call the 019-HIVE-7118 script manually.;;;","20/Jun/14 18:41;deepesh;I have attached a new patch which provides the following:
019-HIVE-7118.oracle.sql -> Intended for users who are already on Hive 0.13.0. Will need to run the script manually against their existing hive metastore schema.
hive-txn-schema-0.13.0.oracle.sql & hive-schema-0.13.0.oracle.sql -> For fresh installs.;;;","23/Jun/14 16:30;gates;+1;;;","24/Jun/14 17:32;gates;Patch committed.  Thank you Deepesh.;;;","24/Jun/14 18:24;deepesh;Thanks Alan for review and commit!;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partitions not inheriting table permissions after alter rename partition,HIVE-7117,12716243,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,singhashish,singhashish,singhashish,23/May/14 00:47,13/Nov/14 19:44,14/Jul/23 06:14,06/Jun/14 18:03,,,,,,,,,,0.14.0,,Security,,,,0,,,"On altering/renaming a partition it must inherit permission of the parent directory, if the flag hive.warehouse.subdir.inherit.perms is set.",,singhashish,swarnim,szehon,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6892,,,,,,,,,,,,,,,,,,,,,,,,,,"23/May/14 04:40;singhashish;HIVE-7117.2.patch;https://issues.apache.org/jira/secure/attachment/12646447/HIVE-7117.2.patch","23/May/14 05:44;singhashish;HIVE-7117.3.patch;https://issues.apache.org/jira/secure/attachment/12646455/HIVE-7117.3.patch","23/May/14 06:16;singhashish;HIVE-7117.4.patch;https://issues.apache.org/jira/secure/attachment/12646460/HIVE-7117.4.patch","27/May/14 18:40;singhashish;HIVE-7117.5.patch;https://issues.apache.org/jira/secure/attachment/12646960/HIVE-7117.5.patch","28/May/14 22:12;singhashish;HIVE-7117.6.patch;https://issues.apache.org/jira/secure/attachment/12647229/HIVE-7117.6.patch","03/Jun/14 03:35;singhashish;HIVE-7117.7.patch;https://issues.apache.org/jira/secure/attachment/12648065/HIVE-7117.7.patch","03/Jun/14 21:17;singhashish;HIVE-7117.8.patch;https://issues.apache.org/jira/secure/attachment/12648233/HIVE-7117.8.patch","23/May/14 01:37;singhashish;HIVE-7117.patch;https://issues.apache.org/jira/secure/attachment/12646428/HIVE-7117.patch",,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394452,,,,Thu Nov 13 19:44:28 UTC 2014,,,,,,,,,,"0|i1vx0f:",394593,,,,,,,,,,,,,,,,,,,,,"23/May/14 02:04;szehon;Thanks for the fix, can you please fix the formatting diffs and create a review board request with the patch?  Probably its clearer if the patch just contain code diffs.  Thanks!

[https://reviews.apache.org/dashboard/|https://reviews.apache.org/dashboard/]

;;;","23/May/14 04:40;singhashish;Removed code formatting changes.;;;","23/May/14 04:42;singhashish;[~szehon] Updated patch and posted rb at https://reviews.apache.org/r/21846/.;;;","23/May/14 05:13;swarnim;Left comments on RB.;;;","23/May/14 05:44;singhashish;Add checks for filesystem.rename return value;;;","23/May/14 05:46;singhashish;[~swarnim] updated patch and addressed you reviews at rb.;;;","23/May/14 05:55;swarnim;Thanks Ashish. Left one more minor comment after that.

+1(non-binding) after that.;;;","23/May/14 06:16;singhashish;Rename fileutils.rename to fileutils.renameAndInheritParentPermissions;;;","23/May/14 06:17;singhashish;[~swarnim] Done! The name now is bit long, but shouts out the intent. Thanks for reviewing!;;;","24/May/14 23:11;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646460/HIVE-7117.4.patch

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 5463 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hadoop.hive.ql.security.TestFolderPermissions.testAlterPartitionPerms
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/283/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/283/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-283/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646460;;;","27/May/14 18:40;singhashish;Avoid using same table in two different tests, which caused test error because of previous patch.;;;","27/May/14 18:42;singhashish;testAlterPartitionsPerms was using same table name as of testAlterSinglePartitionPerm, which led to failure while creating table in testAlterPartitionPerms. .5 patch fixes this. Updated RB.;;;","27/May/14 18:42;singhashish;Avoid using same table in two different tests.;;;","27/May/14 18:43;singhashish;Fixed test failure due to previous patch.;;;","28/May/14 21:33;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646960/HIVE-7117.5.patch

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 5467 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hadoop.hive.ql.security.TestFolderPermissions.testAlterSinglePartitionPerms
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/321/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/321/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-321/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646960;;;","28/May/14 22:12;singhashish;Change table name to have all lowercase letters. Looks like table name is toLower'ed by hive while creating file.;;;","30/May/14 17:12;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647229/HIVE-7117.6.patch

{color:red}ERROR:{color} -1 due to 11 failed/errored test(s), 5543 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/335/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/335/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-335/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 11 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647229;;;","30/May/14 23:33;szehon;+1 (non-binding)  The failures dont look related.  I filed HIVE-7154 to take a look at the TestMetrics one.  Looks like this will directly conflict with the test changes in HIVE-7119.;;;","03/Jun/14 03:35;singhashish;Rebase;;;","03/Jun/14 14:45;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12648065/HIVE-7117.7.patch

{color:red}ERROR:{color} -1 due to 20 failed/errored test(s), 5585 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_display_colstats_tbllvl
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_between_in
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_metadata_only_queries
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_ptf
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_schema_evolution
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testAlterPartition
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testDatabase
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testRenamePartition
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/378/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/378/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-378/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 20 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12648065;;;","03/Jun/14 21:17;singhashish;Addressed review comments after rebase. Replied at RB.;;;","04/Jun/14 05:21;szehon;Thanks.  +1 (non-binding);;;","04/Jun/14 05:26;swarnim;+1 from me as well (non-binding);;;","04/Jun/14 11:19;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12648233/HIVE-7117.8.patch

{color:red}ERROR:{color} -1 due to 19 failed/errored test(s), 5585 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_udf1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_metadata_only_queries
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_ptf
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_schema_evolution
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/385/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/385/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-385/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 19 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12648233;;;","04/Jun/14 23:33;singhashish;I do not think the test errors are related.;;;","04/Jun/14 23:35;singhashish;[~xuefuz] Could you commit the patch.;;;","05/Jun/14 13:32;xuefuz;+1;;;","06/Jun/14 18:03;xuefuz;Patch committed to trunk. Thanks to Ashish for the contribution.;;;","07/Jun/14 05:43;singhashish;Thanks [~szehon], [~xuefuz] and [~swarnim] for reviewing.;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HDFS FileSystem object cache causes permission issues in creating tmp directories,HIVE-7116,12716215,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vikram.dixit,vikram.dixit,vikram.dixit,22/May/14 22:20,09/Jun/14 06:39,14/Jul/23 06:14,31/May/14 22:53,0.13.0,,,,,,,,,0.13.1,0.14.0,HiveServer2,Tez,,,0,,,"We change permissions of the directory creation to 777 for HiveServer 2 operation and it turns out that because of HDFS caching, it does not reflect once created. We need to use the non-cached version of the API to get a FileSystem object to fix this.",,sushanth,swarnim,thejas,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/14 22:21;vikram.dixit;HIVE-7116.1.patch;https://issues.apache.org/jira/secure/attachment/12646398/HIVE-7116.1.patch","23/May/14 00:46;vikram.dixit;HIVE-7116.2.patch;https://issues.apache.org/jira/secure/attachment/12646422/HIVE-7116.2.patch","23/May/14 22:15;vikram.dixit;HIVE-7116.3.patch;https://issues.apache.org/jira/secure/attachment/12646619/HIVE-7116.3.patch","29/May/14 00:41;vikram.dixit;HIVE-7116.4.patch;https://issues.apache.org/jira/secure/attachment/12647266/HIVE-7116.4.patch","29/May/14 22:16;vikram.dixit;HIVE-7116.5.patch;https://issues.apache.org/jira/secure/attachment/12647465/HIVE-7116.5.patch","29/May/14 23:04;vikram.dixit;HIVE-7116.6.patch;https://issues.apache.org/jira/secure/attachment/12647478/HIVE-7116.6.patch","30/May/14 00:31;vikram.dixit;HIVE-7116.7.patch;https://issues.apache.org/jira/secure/attachment/12647495/HIVE-7116.7.patch","30/May/14 20:35;vikram.dixit;HIVE-7116.8.patch;https://issues.apache.org/jira/secure/attachment/12647680/HIVE-7116.8.patch","01/Jun/14 22:03;vikram.dixit;HIVE-7116.branch0.13.patch;https://issues.apache.org/jira/secure/attachment/12647832/HIVE-7116.branch0.13.patch",,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394424,,,,Mon Jun 09 06:39:36 UTC 2014,,,,,,,,,,"0|i1vwuf:",394565,,,,,,,,,,,,,,,,,,,,,"22/May/14 23:46;swarnim;Hi Vikram,

Couple of smaller comments on the patch.

{code}
LOG.info(""Create dirs "" + mkdirPath + "" with permission "" + fsPermission + "" recursive "" + recursive);
{code}

You probably don't need to manually concatenate the string but let log4j do it for you.

{code}
LOG.info(""Create dirs {} with permission {} recursive {}"", mkdirPath, fsPermission, recursive);
{code}

Secondly, I think that if you create a FileSystem using the newInstance method, it should be closed to avoid any resource leaks. Previosuly with the cached FS instance, the API was automatically handling that for you.;;;","23/May/14 00:44;vikram.dixit;Address Swarnim's comment.;;;","23/May/14 00:46;vikram.dixit;[~swarnim] Changing the log to that format affects readability as it is the way log has been done in Hive. I agree that is not a strong argument but, there is no advantage either ways.

The fs.close() is a good catch! Thanks for your quick review!;;;","23/May/14 04:51;swarnim;Should the fs.close() be in a finally so that an exception during mkdirs doesn't still cause a resource leak? We might as well use IOUtils.closeQuietly if we do unnecessarily want the fs.close() exception to propagate up the stack.;;;","23/May/14 22:17;vikram.dixit;Although not strictly required in this case, it makes sense to have the finally block do both the close and reset of the configuration flag. Updated.;;;","24/May/14 03:04;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646619/HIVE-7116.3.patch

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 5533 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/275/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/275/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-275/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646619;;;","29/May/14 00:41;vikram.dixit;The try/finally block will mask exceptions in case there is a double failure (create fails then close fails in the finally block). Fixing that.;;;","29/May/14 04:23;swarnim;Hey Vikram,

Looking deeper into the patch, in my opinion there could be some issues:

1. ""hadoop-1"" pulls in the 1.2.1 hadoop jar. The FileSystem object on this doesn't have the newInstance method on it. So with ""-P hadoop-1"" this would fail with a compilation error.
2. I think the latest patch is overly complicated for what it is trying to do. I think it could be simplified to just the following:

{code}
FileSystem fs = FileSystem.newInstance(mkdirPath.toUri(), conf);
    boolean retval = false;
    try {
      retval = fs.mkdirs(mkdirPath, fsPermission);
    } finally {
      org.apache.commons.io.IOUtils.closeQuietly(fs);
    }
{code}

3. Are we sure that it is the cached FileSystem instance that is causing the permissions to not get reflected? I wrote a simple test with cached FileSystem object[1] that sets permissions and it passed for me. Interestingly it only fails for ""777"" permissions so might be some issue specific to that additional ""executable"" sticky bit.

Just my 0.02 :)

[1] http://hadoop.apache.org/docs/r1.2.1/api/org/apache/hadoop/fs/FileSystem.html
[2] https://gist.github.com/swarnim87/afd29a06e6cc7fffae5c;;;","29/May/14 06:29;thejas;[~swarnim] Good catch about the hadoop-1 compilation issue. [~vikram.dixit] To work around that you can set """"fs.%s.impl.disable.cache""=true in conf, and then call FileSystem.get(URI uri, Configuration conf)

[~swarnim] Regarding 2,  I think it is important to re-set the conf to its old state in case of a failure as well.

[~vikram.dixit] You can use a function to avoid code duplication within the finally block and outside. I think this log message is better logged at DEBUG level as it is also being called for every vertex creation in Tez mode.

[~swarnim] Regarding 3, try your example with a path name where more than one level in dir structure needs to be created ""eg ""/tmp/a/b/c"". You will notice that only c gets created with FsPermission value. That is why the umask needs to be set in config so that ""a"" and ""b"" also get created with appropriate permissions. The cache becomes an issue only if you already have a FileSystem object in cache with old config. Your example is not resulting in a FileSystem object in cache with old config.





;;;","29/May/14 23:04;vikram.dixit;I think this is better solved by the shims instead of writing ugly hacks that we have to keep around forever. Refreshed with comments and shim changes.;;;","29/May/14 23:17;swarnim;+1 for shims. Very clean approach.

Should we have some tests to prove that this actually fixes the permissions issue? Also adding a new method to HadoopShims seems like would make it non-passive. Not sure how much we care about passivity in shims but just a thought. Since it is an interface, not sure if we can even do much about it anyways. :)

Much thanks for looking into this Vikram!;;;","30/May/14 00:31;vikram.dixit;Moved the 1.0 code to the appropriate shim and fixed an indentation error.;;;","30/May/14 01:12;thejas;+1 LGTM
;;;","30/May/14 13:50;swarnim;+1. Thanks again. :);;;","30/May/14 17:13;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647495/HIVE-7116.7.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/336/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/336/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-336/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]
+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-maven-3.0.5/bin:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-maven-3.0.5/bin:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-336/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java'
Reverted 'metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java'
Reverted 'itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java'
Reverted 'common/src/java/org/apache/hadoop/hive/common/FileUtils.java'
++ egrep -v '^X|^Performing status on external'
++ awk '{print $2}'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-minikdc/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/core/target hcatalog/streaming/target hcatalog/server-extensions/target hcatalog/hcatalog-pig-adapter/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hwi/target common/target common/src/gen service/target contrib/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1598675.

At revision 1598675.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647495;;;","30/May/14 20:35;vikram.dixit;Rebased to latest trunk.;;;","30/May/14 20:37;ashutoshc;+1 for 0.13.1;;;","30/May/14 20:42;thejas;+1 for 0.13.1 .
;;;","30/May/14 22:58;thejas;Ran tests locally and they passed.
;;;","31/May/14 22:42;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647680/HIVE-7116.8.patch

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 5496 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/347/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/347/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-347/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647680;;;","31/May/14 22:53;thejas;Patch committed to trunk. Thanks Vikram!
[~sushanth] Can you please include it in 0.13.1 ?
;;;","31/May/14 23:13;thejas;[~vikram.dixit] Can you please create a rebased patch for 0.13 ?
;;;","01/Jun/14 22:03;vikram.dixit;0.13 patch added.;;;","02/Jun/14 19:01;sushanth;Committed to 0.13.1, Thanks, Vikram!;;;","09/Jun/14 06:39;thejas;This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extra Tez session is started during HiveServer2 startup,HIVE-7114,12716092,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,thejas,deepesh,thejas,22/May/14 14:58,13/Nov/14 19:43,14/Jul/23 06:14,23/May/14 21:54,0.13.0,,,,,,,,,0.14.0,,HiveServer2,Tez,,,0,,,"When starting the HiveServer2 we are seeing an extra Tez AM launched.

This is where it is getting created .
{noformat}
2014-05-09 23:11:22,261 INFO  [main]: metastore.HiveMetaStore (HiveMetaStore.java:addAdminUsers(588)) - No user is added in admin role, since config is empty
java.lang.Exception: Opening session
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:134)
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:119)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:356)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:292)
        at org.apache.hive.service.cli.session.SessionManager.applyAuthorizationConfigPolicy(SessionManager.java:88)
        at org.apache.hive.service.cli.session.SessionManager.init(SessionManager.java:63)
        at org.apache.hive.service.CompositeService.init(CompositeService.java:59)
        at org.apache.hive.service.cli.CLIService.init(CLIService.java:110)
        at org.apache.hive.service.CompositeService.init(CompositeService.java:59)
        at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:68)
        at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:100)
        at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:149)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
{noformat}",,thejas,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/14 22:47;thejas;HIVE-7114.1.patch;https://issues.apache.org/jira/secure/attachment/12646405/HIVE-7114.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394301,,,,Thu Nov 13 19:43:10 UTC 2014,,,,,,,,,,"0|i1vw3j:",394439,,,,,,,,,,,,,,,,,,,,,"22/May/14 22:55;vikram.dixit;+1 LGTM.;;;","23/May/14 21:23;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646405/HIVE-7114.1.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5458 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/273/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/273/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-273/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646405;;;","23/May/14 21:54;thejas;Patch committed to trunk. Thanks for the review Vikram!
;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tez processor swallows errors,HIVE-7112,12715923,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,22/May/14 05:04,13/Nov/14 19:43,14/Jul/23 06:14,31/May/14 06:10,,,,,,,,,,0.14.0,,,,,,0,,,If a OOM or other occurs during initializing a TezProcessor the original stack trace/throwable is swallowed and replaced by a misleading NPE caused by trying to close an uninitialized processor.,,hagleitn,navis,thejas,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/14 05:11;hagleitn;HIVE-7112.1.patch;https://issues.apache.org/jira/secure/attachment/12646178/HIVE-7112.1.patch","22/May/14 18:03;hagleitn;HIVE-7112.2.patch;https://issues.apache.org/jira/secure/attachment/12646346/HIVE-7112.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394208,,,,Thu Nov 13 19:43:28 UTC 2014,,,,,,,,,,"0|i1vviv:",394346,,,,,,,,,,,,,,,,,,,,,"22/May/14 18:03;hagleitn;Addressing review comments.;;;","23/May/14 02:18;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646346/HIVE-7112.2.patch

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 5529 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/266/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/266/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-266/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646346;;;","30/May/14 04:51;hagleitn;Test failures are unrelated.;;;","30/May/14 05:03;navis;+1, always welcome of debuggability patches.;;;","30/May/14 17:10;vikram.dixit;+1.;;;","31/May/14 06:10;hagleitn;Committed to trunk. Thanks [~navis] and [~vikram.dixit]!;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resource leak in HBaseStorageHandler,HIVE-7109,12715818,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,swarnim,swarnim,swarnim,21/May/14 18:12,13/Nov/14 19:39,14/Jul/23 06:14,24/May/14 20:37,0.13.0,,,,,,,,,0.14.0,,HBase Handler,,,,0,,,"The ""preCreateTable"" method in the HBaseStorageHandler checks that the HBase table is still online by creating a new instance of HTable

{code}
// ensure the table is online
new HTable(hbaseConf, tableDesc.getName());
{code}

However this instance is never closed. So if this test succeeds, we would have a resource leak in the code.",,swarnim,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/May/14 05:48;swarnim;HIVE-7109.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12646456/HIVE-7109.1.patch.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394103,,,,Thu Nov 13 19:39:58 UTC 2014,,,,,,,,,,"0|i1vuvr:",394241,,,,,,,,,,,,,,,,,,,,,"23/May/14 05:48;swarnim;Patch attached.;;;","23/May/14 05:48;swarnim;This is now available for review.;;;","23/May/14 06:01;ashutoshc;+1;;;","24/May/14 19:18;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646456/HIVE-7109.1.patch.txt

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 5458 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_partitioned
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency
org.apache.hadoop.hive.metastore.TestRetryingHMSHandler.testRetryingHMSHandler
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/282/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/282/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-282/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646456;;;","24/May/14 20:37;ashutoshc;Committed to trunk. Thanks, Swarnim!;;;","13/Nov/14 19:39;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix HiveServer1 JDBC Driver spec compliancy issue,HIVE-7107,12715813,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,larsfrancke,larsfrancke,larsfrancke,21/May/14 18:02,13/Nov/14 19:43,14/Jul/23 06:14,25/Sep/14 23:32,0.14.0,,,,,,,,,0.14.0,,,,,,0,,,"The HiveServer1 driver does not adhere to the JDBC standard of returning null when it can't handle a connection URL. It instead throws an exception which leads to subsequent exceptions (from other drivers, i.e. the HiveServer2 one) being swallowed.

This is what you'd see:
{quote}
Error: Invalid URL: jdbc:hive2://localhost:10000/default;principal=hive/_HOST@EXAMPLE.COM (state=08S01,code=0)
java.sql.SQLException: Invalid URL: jdbc:hive2://localhost:10000/default;principal=hive/_HOST@EXAMPLE.COM
	at org.apache.hadoop.hive.jdbc.HiveConnection.<init>(HiveConnection.java:86)
{quote}

In addition this patch cleans up the drivers a bit.",,larsfrancke,leftyl,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/14 12:05;larsfrancke;HIVE-7107.2.patch;https://issues.apache.org/jira/secure/attachment/12670956/HIVE-7107.2.patch","21/May/14 18:07;larsfrancke;HIVE-7107.2.patch;https://issues.apache.org/jira/secure/attachment/12646059/HIVE-7107.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,394098,,,,Thu Nov 13 19:43:14 UTC 2014,,,,,,,,,,"0|i1vuun:",394236,,,,,,,,,,,,,,,,,,,,,"22/May/14 16:35;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646059/HIVE-7107.2.patch

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 5452 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/263/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/263/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-263/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646059;;;","22/May/14 20:14;larsfrancke;I'm pretty sure none of those tests are related to my patch. All other patches tested around the same time fail on the same tests.;;;","31/May/14 19:59;ashutoshc;HS1 is no longer actively maintained and will soon be removed. See, HIVE-6977 Please provide only HS2 patches.;;;","01/Jun/14 08:58;larsfrancke;This patch *does* actively maintain HS1 and there's no timeline or patch for HIVE-6977.

Who knows when HIVE-6977 will actually happen. It'd not be the first time that things slip because some company decides that they need a release of Hive.

Feel free to ignore this patch. I'm not going to provide a new one.;;;","29/Jul/14 23:12;larsfrancke;https://reviews.apache.org/r/24076/

I realize that HS1 is scheduled to be removed but I submitted this before 0.13.1 was released and maybe 0.13.2 will be released still. Also I think there's a chance that some distributions will keep HS1 in for a bit longer and this patch fixes an issue with it. I also don't see any harm in maintaining code that's scheduled to be removed at some point.;;;","23/Sep/14 20:35;thejas;Lars,
I usually prefer large cleanups (relative to the fix) to be done as part of separate jira. That way it is easier to determine what part was the actual fix, while looking at svn/git history. I feel around 10-20% of changes in a bug fix being cleanup is OK, but more than 80% of the changes for a bug fix being cleanup makes it little difficult to handle.
I can review a separate HiveServer* cleanup patch in another jira.

;;;","23/Sep/14 21:08;larsfrancke;Sure, I'll prepare a separate patch. Thanks for taking a look.;;;","24/Sep/14 12:05;larsfrancke;This is the patch for the mentioned JDBC issue and without any cleanup.;;;","24/Sep/14 12:07;larsfrancke;I have also updated Review Board. [~thejas] it should be ready for review now, thanks!;;;","24/Sep/14 18:40;thejas;+1;;;","24/Sep/14 20:55;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12670956/HIVE-7107.2.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6347 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.ql.parse.TestParse.testParse_union
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataPrimitiveTypes
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/966/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/966/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-966/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12670956;;;","24/Sep/14 21:01;thejas;The test failures above are unrelated to this change.
;;;","25/Sep/14 23:32;thejas;Patch committed to trunk and 0.14 branch.
Thanks for the contribution Lars!
;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable ReduceRecordProcessor to generate VectorizedRowBatches,HIVE-7105,12715701,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,gopalv,rajesh.balamohan,rajesh.balamohan,21/May/14 09:30,13/Nov/14 19:40,14/Jul/23 06:14,01/Jul/14 04:56,,,,,,,,,,0.14.0,,Tez,Vectorization,,,0,,,"Currently, ReduceRecordProcessor sends one key,value pair at a time to its operator pipeline.  It would be beneficial to send VectorizedRowBatch to downstream operators. ",,ehans,gopalv,hagleitn,jnp,mmccline,rajesh.balamohan,rusanu,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7029,,,,,,,"21/May/14 11:18;rajesh.balamohan;HIVE-7105.1.patch;https://issues.apache.org/jira/secure/attachment/12645991/HIVE-7105.1.patch","12/Jun/14 21:42;gopalv;HIVE-7105.2.patch;https://issues.apache.org/jira/secure/attachment/12650150/HIVE-7105.2.patch","01/Jul/14 02:00;gopalv;HIVE-7105.3.patch;https://issues.apache.org/jira/secure/attachment/12653294/HIVE-7105.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393986,Reviewed,,,Thu Nov 13 19:40:01 UTC 2014,,,,,,,,,,"0|i1vu67:",394126,Tez vectorized shuffle record reader,,,,,,,,,,,,,,,,,,,,"21/May/14 10:56;rusanu;Extending the vectorized processing to the reduce side is a complex undertaking. None of the vector mode operators are implemented in reduce side. The thinking is that the bulk of the CPU intensive processing occurs on the map side and our goal was to provide maximum feature coverage (ie. implement as many operators as needed to cover the most queries) but atm vectorization only works for map side of first stage. I'm not sure whether at this stage we can call the map side effort stable/mature/complete enough to warrant a focus shift to reduce side.;;;","21/May/14 11:18;rajesh.balamohan;[~rusanu] Agreed that there are no vectorized operators on the reducer side atm.  Purpose of this JIRA is to take the first step in enabling it on the reducer side for simple queries. ;;;","21/May/14 16:42;ehans;I agree with Remus. If you do want to get good performance with vectorization on the reduce side, you'll need to think carefully about how you can efficiently create full VectorizedRowBatches. Single-row or small VectorizedRowBatches will not give performance gains. Also, if it is expensive to load rows into the batches on the reduce side, that could dominate total runtime.;;;","12/Jun/14 21:42;gopalv;Rebased to trunk and with the additional changes to RowObjectInspectors.

This still respects tagging, but it might be almost impossible to tag vectorized row batches on the operator side.;;;","12/Jun/14 22:45;hagleitn;Comments on rb.;;;","13/Jun/14 06:49;rusanu;Can you share the rb link?;;;","14/Jun/14 07:24;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12650150/HIVE-7105.2.patch

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 5611 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_columnar
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/460/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/460/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-460/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12650150;;;","17/Jun/14 20:30;jnp;[~rusanu]  Here is the RB link: https://reviews.apache.org/r/22540/;;;","19/Jun/14 22:48;jnp;+1;;;","26/Jun/14 22:55;mmccline;Re-run tests, please.;;;","01/Jul/14 03:42;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12653294/HIVE-7105.3.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5671 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/640/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/640/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-640/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12653294;;;","01/Jul/14 04:40;gopalv;Test failure unrelated to Tez.;;;","01/Jul/14 04:52;gopalv;Committed to trunk, thanks [~mmccline], [~jnp] & [~rajesh.balamohan].;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit tests are disabled,HIVE-7104,12715648,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,davidzchen,davidzchen,davidzchen,21/May/14 00:45,13/Nov/14 19:43,14/Jul/23 06:14,21/May/14 21:03,,,,,,,,,,0.14.0,,,,,,0,,,"When I run {{mvn clean test -Phadoop-1|2}}, none of the unit tests are run. I did a binary search through the commit logs and found that the change that caused the unit tests to be disabled was the the change to the root pom.xml in the patch for HIVE-7067 (e77f38dc44de5a9b10bce8e0a2f1f5452f6921ed). Removing that change allowed the unit tests to be run again.",,cwsteinbach,davidzchen,erwaman,gautamkowshik,prasanth_j,szehon,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7067,,,,,,,,,,,,,,,,,,,,,,,,"21/May/14 00:49;davidzchen;HIVE-7104.1.patch;https://issues.apache.org/jira/secure/attachment/12645915/HIVE-7104.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393933,Reviewed,,,Thu Nov 13 19:43:12 UTC 2014,,,,,,,,,,"0|i1vtuf:",394073,,,,,,,,,,,,,,,,,,,,,"21/May/14 00:49;davidzchen;I have attached a patch.

RB: https://reviews.apache.org/r/21743/;;;","21/May/14 16:38;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12645915/HIVE-7104.1.patch

{color:red}ERROR:{color} -1 due to 12 failed/errored test(s), 5451 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_reduce_deduplicate_exclude_gby
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/255/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/255/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-255/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 12 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12645915;;;","21/May/14 18:50;szehon;NIce catch.  Fortunately the Hive build is running all tests individually so its not hitting this issue, but this affects devs trying to run unit-tests outside /itests.  [~prasanth_j] was that pom change committed by mistake ?;;;","21/May/14 18:56;prasanth_j;Oops! My bad. That was a mistake. I added that to run unit test locally in my environment. I forgot to revert that while committing. Sorry for the inconvenience. [~davidzchen] thanks for catching it!;;;","21/May/14 18:56;prasanth_j;+1;;;","21/May/14 21:03;cwsteinbach;Committed to trunk. Thanks David!;;;","16/Jul/14 21:25;gautamkowshik;can we  commit this back to 0.13.1 branch as well? hive 13.1 checkout doesn't run tests either.;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Decimal datatype support for Windowing,HIVE-7099,12715549,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,rhbutani,rhbutani,rhbutani,20/May/14 17:42,13/Nov/14 19:39,14/Jul/23 06:14,22/May/14 23:04,,,,,,,,,,0.14.0,,Query Processor,,,,0,TODOC14,,Decimal datatype is not handled by Windowing,,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/14 17:43;rhbutani;HIVE-7099.1.patch;https://issues.apache.org/jira/secure/attachment/12645831/HIVE-7099.1.patch","21/May/14 18:59;rhbutani;HIVE-7099.2.patch;https://issues.apache.org/jira/secure/attachment/12646077/HIVE-7099.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393835,,,,Thu Nov 13 19:39:35 UTC 2014,,,,,,,,,,"0|i1vtav:",393983,,,,,,,,,,,,,,,,,,,,,"21/May/14 06:07;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12645831/HIVE-7099.1.patch

{color:red}ERROR:{color} -1 due to 14 failed/errored test(s), 5527 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_decimal
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/251/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/251/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-251/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 14 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12645831;;;","22/May/14 19:29;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646077/HIVE-7099.2.patch

{color:red}ERROR:{color} -1 due to 12 failed/errored test(s), 5527 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/264/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/264/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-264/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 12 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646077;;;","22/May/14 20:38;ashutoshc;+1;;;","22/May/14 23:04;ashutoshc;Committed to trunk. Thanks, Harish!;;;","13/Nov/14 19:39;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support grouped splits in Tez partitioned broadcast join,HIVE-7096,12715354,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vikram.dixit,hagleitn,hagleitn,20/May/14 02:32,13/Nov/14 19:40,14/Jul/23 06:14,24/Sep/14 07:05,tez-branch,,,,,,,,,0.14.0,,Tez,,,,0,,,Same checks for schema + deser + file format done in HiveSplitGenerator need to be done in the CustomPartitionVertex.,,hagleitn,thejas,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7430,,,,,,,,,,,,,,HIVE-7071,,,,,,,,,,,,,,,,,,,,,"22/Jul/14 22:20;vikram.dixit;HIVE-7096.1.patch;https://issues.apache.org/jira/secure/attachment/12657207/HIVE-7096.1.patch","23/Jul/14 00:23;vikram.dixit;HIVE-7096.2.patch;https://issues.apache.org/jira/secure/attachment/12657234/HIVE-7096.2.patch","24/Jul/14 20:53;vikram.dixit;HIVE-7096.3.patch;https://issues.apache.org/jira/secure/attachment/12657670/HIVE-7096.3.patch","30/Jul/14 23:34;vikram.dixit;HIVE-7096.4.patch;https://issues.apache.org/jira/secure/attachment/12658773/HIVE-7096.4.patch","31/Jul/14 00:57;vikram.dixit;HIVE-7096.5.patch;https://issues.apache.org/jira/secure/attachment/12658805/HIVE-7096.5.patch","22/Jul/14 23:28;vikram.dixit;HIVE-7096.tez.branch.patch;https://issues.apache.org/jira/secure/attachment/12657220/HIVE-7096.tez.branch.patch",,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393640,,,,Thu Nov 13 19:40:50 UTC 2014,,,,,,,,,,"0|i1vs73:",393799,,,,,,,,,,,,,,,,,,,,,"22/Jul/14 23:28;vikram.dixit;tez branch specific patch because of the 0.5 upgrade on that branch.;;;","23/Jul/14 00:23;vikram.dixit;Make test results deterministic.;;;","23/Jul/14 03:16;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12657234/HIVE-7096.2.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5737 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_fail_8
org.apache.hive.jdbc.TestJdbcDriver2.testParentReferences
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/12/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/12/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-12/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12657234;;;","23/Jul/14 22:12;hagleitn;some minor comments on rb. otherwise +1;;;","24/Jul/14 20:53;vikram.dixit;Address review comments.;;;","25/Jul/14 00:33;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12657670/HIVE-7096.3.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5742 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_bmj_schema_evolution
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/47/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/47/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-47/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12657670;;;","30/Jul/14 23:00;vikram.dixit;This patch works with tez-0.5 only. Since only the tez branch has been upgraded to that version, this is only applicable to that hive branch.;;;","31/Jul/14 02:03;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12658805/HIVE-7096.5.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/112/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/112/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-112/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]
+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-maven-3.0.5/bin:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-maven-3.0.5/bin:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-TRUNK-Build-112/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'itests/qtest/testconfiguration.properties'
Reverted 'ql/src/test/results/clientpositive/vectorization_9.q.out'
Reverted 'ql/src/test/results/clientpositive/vectorization_14.q.out'
Reverted 'ql/src/test/results/clientpositive/vectorization_16.q.out'
Reverted 'ql/src/test/results/clientpositive/tez/vectorization_15.q.out'
Reverted 'ql/src/test/results/clientpositive/vectorization_15.q.out'
Reverted 'ql/src/test/org/apache/hadoop/hive/ql/optimizer/physical/TestVectorizer.java'
Reverted 'ql/src/test/queries/clientpositive/vectorization_15.q'
Reverted 'ql/src/test/queries/clientpositive/vectorization_9.q'
Reverted 'ql/src/test/queries/clientpositive/vectorization_14.q'
Reverted 'ql/src/test/queries/clientpositive/vectorization_16.q'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/plan/BaseWork.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorFactory.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordProcessor.java'
++ egrep -v '^X|^Performing status on external'
++ awk '{print $2}'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit-hadoop2/target itests/hive-minikdc/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/core/target hcatalog/streaming/target hcatalog/server-extensions/target hcatalog/hcatalog-pig-adapter/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hwi/target common/target common/src/gen contrib/target service/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target ql/src/test/results/clientpositive/tez/vectorized_shufflejoin.q.out ql/src/test/results/clientpositive/tez/vectorization_9.q.out ql/src/test/results/clientpositive/tez/vectorized_timestamp_funcs.q.out ql/src/test/results/clientpositive/tez/vectorization_13.q.out ql/src/test/results/clientpositive/tez/vectorization_part_project.q.out ql/src/test/results/clientpositive/tez/vectorized_nested_mapjoin.q.out ql/src/test/results/clientpositive/tez/vectorization_short_regress.q.out ql/src/test/results/clientpositive/tez/vectorization_12.q.out ql/src/test/results/clientpositive/tez/vectorization_14.q.out ql/src/test/results/clientpositive/tez/vector_decimal_aggregate.q.out ql/src/test/results/clientpositive/tez/vectorized_mapjoin.q.out ql/src/test/results/clientpositive/tez/vector_left_outer_join.q.out ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExtractOperator.java
+ svn update
U    itests/qtest/testconfiguration.properties
U    common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
A    ql/src/test/queries/clientnegative/orc_merge1.q
A    ql/src/test/queries/clientnegative/orc_merge2.q
A    ql/src/test/queries/clientnegative/orc_merge3.q
A    ql/src/test/queries/clientnegative/orc_merge4.q
A    ql/src/test/queries/clientnegative/orc_merge5.q
A    ql/src/test/queries/clientpositive/orc_merge1.q
A    ql/src/test/queries/clientpositive/orc_merge3.q
A    ql/src/test/queries/clientpositive/alter_merge_2_orc.q
A    ql/src/test/queries/clientpositive/orc_merge2.q
A    ql/src/test/queries/clientpositive/alter_merge_orc.q
A    ql/src/test/queries/clientpositive/orc_merge4.q
A    ql/src/test/queries/clientpositive/alter_merge_stats_orc.q
A    ql/src/test/results/clientnegative/orc_merge5.q.out
A    ql/src/test/results/clientnegative/orc_merge1.q.out
A    ql/src/test/results/clientnegative/orc_merge2.q.out
A    ql/src/test/results/clientnegative/orc_merge3.q.out
A    ql/src/test/results/clientnegative/orc_merge4.q.out
U    ql/src/test/results/clientpositive/rcfile_merge1.q.out
U    ql/src/test/results/clientpositive/list_bucket_dml_10.q.out
U    ql/src/test/results/clientpositive/list_bucket_dml_9.q.out
U    ql/src/test/results/clientpositive/union_remove_14.q.out
U    ql/src/test/results/clientpositive/list_bucket_dml_4.q.out
A    ql/src/test/results/clientpositive/orc_merge3.q.out
U    ql/src/test/results/clientpositive/merge_dynamic_partition5.q.out
U    ql/src/test/results/clientpositive/rcfile_merge3.q.out
U    ql/src/test/results/clientpositive/union_remove_9.q.out
U    ql/src/test/results/clientpositive/infer_bucket_sort_dyn_part.q.out
U    ql/src/test/results/clientpositive/union_remove_16.q.out
U    ql/src/test/results/clientpositive/list_bucket_dml_6.q.out
U    ql/src/test/results/clientpositive/union_remove_11.q.out
A    ql/src/test/results/clientpositive/alter_merge_2_orc.q.out
U    ql/src/test/results/clientpositive/union_remove_13.q.out
A    ql/src/test/results/clientpositive/alter_merge_stats_orc.q.out
A    ql/src/test/results/clientpositive/orc_merge2.q.out
U    ql/src/test/results/clientpositive/merge_dynamic_partition4.q.out
U    ql/src/test/results/clientpositive/orc_createas1.q.out
U    ql/src/test/results/clientpositive/rcfile_merge2.q.out
A    ql/src/test/results/clientpositive/alter_merge_orc.q.out
U    ql/src/test/results/clientpositive/union_remove_10.q.out
A    ql/src/test/results/clientpositive/orc_merge4.q.out
A    ql/src/test/results/clientpositive/tez/orc_merge1.q.out
A    ql/src/test/results/clientpositive/tez/orc_merge3.q.out
A    ql/src/test/results/clientpositive/tez/alter_merge_orc.q.out
A    ql/src/test/results/clientpositive/tez/alter_merge_stats_orc.q.out
A    ql/src/test/results/clientpositive/tez/alter_merge_2_orc.q.out
A    ql/src/test/results/clientpositive/tez/orc_merge2.q.out
A    ql/src/test/results/clientpositive/tez/orc_merge4.q.out
U    ql/src/test/results/clientpositive/list_bucket_dml_7.q.out
U    ql/src/test/results/clientpositive/union_remove_12.q.out
A    ql/src/test/results/clientpositive/orc_merge1.q.out
U    ql/src/test/results/clientpositive/rcfile_createas1.q.out
U    ql/src/java/org/apache/hadoop/hive/ql/parse/AlterTablePartMergeFilesDesc.java
U    ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezUtils.java
U    ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
U    ql/src/java/org/apache/hadoop/hive/ql/exec/TaskFactory.java
U    ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
U    ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
U    ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
A    ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileMergeMapper.java
A    ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileStripeMergeRecordReader.java
A    ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileValueWrapper.java
A    ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileKeyWrapper.java
A    ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileStripeMergeInputFormat.java
U    ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java
U    ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java
A    ql/src/java/org/apache/hadoop/hive/ql/io/merge
A    ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeTask.java
A    ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeWork.java
A    ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeMapper.java
A    ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeInputFormat.java
A    ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeOutputFormat.java
D    ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/merge/RCFileBlockMergeOutputFormat.java
D    ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/merge/BlockMergeTask.java
D    ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/merge/MergeWork.java
U    ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/merge/RCFileBlockMergeInputFormat.java
U    ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/merge/RCFileMergeMapper.java
U    ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
U    ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java

Fetching external item into 'hcatalog/src/test/e2e/harness'
Updated external to revision 1614801.

Updated to revision 1614801.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12658805;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Insert overwrite should not delete the original directory,HIVE-7092,12715336,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,szehon,szehon,szehon,20/May/14 00:36,13/Nov/14 19:44,14/Jul/23 06:14,22/May/14 22:18,,,,,,,,,,0.14.0,,,,,,0,,,"Today the implementation of ""insert overwrite"" table or partition deletes the entire directory of the table/partition recursively using the HDFS shell (-rmr) and then re-creates it.

This makes it get rid of certain user-set attributes of the directory, like permission, owner, group, and will become more important with introduction of HDFS extended-ACL's.",,szehon,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6892,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/14 00:49;szehon;HIVE-7092.patch;https://issues.apache.org/jira/secure/attachment/12645701/HIVE-7092.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393622,,,,Thu Nov 13 19:44:29 UTC 2014,,,,,,,,,,"0|i1vs3b:",393782,,,,,,,,,,,,,,,,,,,,,"20/May/14 00:49;szehon;Attaching a fix.;;;","20/May/14 10:02;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12645701/HIVE-7092.patch

{color:red}ERROR:{color} -1 due to 12 failed/errored test(s), 5452 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/240/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/240/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logsPreCommit-HIVE-Build-240/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 12 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12645701;;;","20/May/14 19:27;szehon;Hi [~xuefuz] [~brocknoland] Can one of you please help to review this?  Thanks;;;","21/May/14 13:47;xuefuz;+1;;;","22/May/14 17:04;xuefuz;[~szehon] Are those test failures above relevant?;;;","22/May/14 18:11;szehon;[~xuefuz] Thanks for the review.  Most of the failure is still in the latest pre-commit build, and I ran the one test not in there 'testDefaults' with my patch, and it is passing for me.;;;","22/May/14 22:18;xuefuz;Patch committed to trunk. Thanks Szehon for the contribution.;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move pre-commit trigger job to hive jenkins,HIVE-7091,12715324,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,szehon,szehon,szehon,19/May/14 23:41,21/May/14 00:33,14/Jul/23 06:14,21/May/14 00:33,,,,,,,,,,,,Testing Infrastructure,,,,0,,,"We have seen issues with the Apache Jenkins job [https://builds.apache.org/job/PreCommit-Admin/|https://builds.apache.org/job/PreCommit-Admin/] being down.  

This job is the one submitting tests for the Ptest framework.
[http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/|http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/]

Let's see if we can move this job to Hive jenkins for better reliability. ",,brocknoland,szehon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/14 22:41;szehon;HIVE-7091.patch;https://issues.apache.org/jira/secure/attachment/12645897/HIVE-7091.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393610,,,,Wed May 21 00:33:14 UTC 2014,,,,,,,,,,"0|i1vs0n:",393770,,,,,,,,,,,,,,,,,,,,,"20/May/14 00:15;szehon;Started some work in [http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/pre-commit-trigger|http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/pre-commit-trigger] , but it cannot get the first diff from the apache server as it is down.  Will try again after it comes up.;;;","20/May/14 22:41;szehon;Patch for testing;;;","20/May/14 22:47;szehon;This seems to work : [http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-Admin/|http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-Admin/].  I manually tested it but have disabled it for now as not to interfere with the existing one.

[~brocknoland] can you please disable the other build and enable this one when you get a chance, as you have the privilege on the apache jenkins?;;;","20/May/14 23:55;brocknoland;[~szehon] I have removed hive from the Apache Jenkins Precommit admin and enabled the new job.;;;","21/May/14 00:33;szehon;Thanks!  I'll resolve this JIRA then.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove lineage information after query completion,HIVE-7087,12715099,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,navis,navis,navis,19/May/14 06:31,20/Jul/16 00:04,14/Jul/23 06:14,28/May/14 02:54,,,,,,,,,,0.14.0,,Logging,,,,0,,,"Lineage information is stacked in session and is not cleared before the session is closed. That also makes redundant lineage logs in q.out files for all of the queries after any inserts, which should be available only for insert queries.",,navis,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-14275,,,,,,,,"19/May/14 06:38;navis;HIVE-7087.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12645515/HIVE-7087.1.patch.txt","23/May/14 02:33;navis;HIVE-7087.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12646434/HIVE-7087.2.patch.txt","27/May/14 02:31;navis;HIVE-7087.3.patch.txt;https://issues.apache.org/jira/secure/attachment/12646832/HIVE-7087.3.patch.txt","28/May/14 04:24;navis;HIVE-7087.3_2.patch.txt;https://issues.apache.org/jira/secure/attachment/12647049/HIVE-7087.3_2.patch.txt",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393385,,,,Thu Nov 13 19:40:21 UTC 2014,,,,,,,,,,"0|i1vqnb:",393547,,,,,,,,,,,,,,,,,,,,,"19/May/14 06:38;navis;Attaches sources only first. This makes huge diff file.;;;","19/May/14 06:53;ashutoshc;In DataContainer.toString() you may want to add delimiter between table name and partition values.  part.getDbName() + ""."" + part.getTableName() + ""@"" + part.getValues()  (or any other delimiter of your choice)

Other than look goods to me.;;;","21/May/14 01:52;navis;[~ashutoshc] Thanks for a quick review. I'll update the patch when HIVE-7095 is resolved.;;;","23/May/14 02:33;navis;with diff (partial);;;","23/May/14 05:31;ashutoshc;+1;;;","24/May/14 04:41;navis;Rebased to trunk;;;","24/May/14 19:46;ashutoshc;[~navis] Hive QA didnt run because patch is not appropriately named. Was that intentional ?;;;","26/May/14 04:20;navis;Ran test by manual. I've added 'a' for meaning that it contains a part of whole diff. But didn't know Hive QA accepts only numeric values, and I will take that in mind.;;;","26/May/14 18:24;ashutoshc;You want to upload full diff with correct file name, so Hive QA gets to run on it.;;;","27/May/14 20:07;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646832/HIVE-7087.3.patch.txt

{color:red}ERROR:{color} -1 due to 87 failed/errored test(s), 5464 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_numbuckets_partitioned_table2_h23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_numbuckets_partitioned_table_h23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_insert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_combine2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_like_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynpart_sort_opt_vectorization
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input39
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lb_fs_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_recursive_dir
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample_islocalmode_hook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_percentile_approx_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_fileformat_base64
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes2
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes3
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes4
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes5
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes_null
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_binary_storage_queries
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/312/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/312/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-312/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 87 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646832;;;","28/May/14 02:54;ashutoshc;Committed to trunk. Thanks, Navis!;;;","28/May/14 03:39;navis;[~ashutoshc] Oops, this needs one more diff file. I'm on it.;;;","28/May/14 05:22;ashutoshc;Thanks, Navis. I updated all other .q.out files where reported by Hive QA but missed tez files. ;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix test failures on trunk,HIVE-7083,12715088,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,19/May/14 05:28,13/Nov/14 19:40,14/Jul/23 06:14,19/May/14 16:57,0.14.0,,,,,,,,,0.14.0,,Tests,,,,0,,,"After move to jdk7 we need to update .q.out files for few tests. Also, in HIVE-6901 few .q.out updates were missed.
",,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/May/14 06:26;ashutoshc;HIVE-7083.1.patch;https://issues.apache.org/jira/secure/attachment/12645514/HIVE-7083.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393374,,,,Thu Nov 13 19:40:06 UTC 2014,,,,,,,,,,"0|i1vqkv:",393536,,,,,,,,,,,,,,,,,,,,,"19/May/14 06:27;ashutoshc;https://reviews.apache.org/r/21619/;;;","19/May/14 13:20;xuefuz;Thanks for looking into this, [~ashutoshc]. Just for my knowledge, with this patch, would tests still pass on JDK6? Or is JDK6 something that we don't care any more?;;;","19/May/14 15:44;ashutoshc;As far as I can tell, none of these fixes can make these tests to fail on jdk6. But we can be sure only after running the test suite on jdk6 once everything is fixed for jdk7. Also, to keep in mind none of these changes are in production code, all of these changes are exclusively in test code. So, even if we find some tests fail on jdk6 later, its not because of any changes in production code and thus won't be cause of concern. 
Not sure why ptest didnt paste result here. Pasting from :  http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/234/consoleFull
{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 5451 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
{noformat}
All the above failures are tracked in other jiras. So, this patch looks good to go. [~xuefuz] would you like to review?;;;","19/May/14 15:59;xuefuz;+1;;;","19/May/14 16:57;ashutoshc;Committed to trunk.;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Vectorized parquet reader should create assigners only for the columns it assigns, not for scratch columns",HIVE-7082,12715087,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,rusanu,rusanu,rusanu,19/May/14 05:06,13/Nov/14 19:43,14/Jul/23 06:14,19/May/14 18:10,0.13.0,0.14.0,,,,,,,,0.14.0,,Query Processor,,,,0,parquet,vectorization,The code in VectorizedParquetRecordReader.next and in VectorColumnAssignFactory.buildAssigners iterates for all columns in the vectorization context trying to build an assigner for it. Scratch columns do not require assigners. Fix is to simply use the writables.length instead of outputBatch.numCols.,,ashutoshc,jnp,rusanu,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-5771,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/May/14 05:18;rusanu;HIVE-7082.1.patch;https://issues.apache.org/jira/secure/attachment/12645509/HIVE-7082.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393373,,,,Thu Nov 13 19:43:42 UTC 2014,,,,,,,,,,"0|i1vqkn:",393535,,,,,,,,,,,,,,,,,,,,,"19/May/14 05:35;ashutoshc;LGTM +1 [~jnp] Would you like to take a look too?;;;","19/May/14 18:10;ashutoshc;Committed to trunk. Thanks, Remus!;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In PTest framework, Add logs URL to the JIRA comment",HIVE-7080,12714876,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,szehon,szehon,szehon,16/May/14 21:29,13/Nov/14 19:43,14/Jul/23 06:14,19/May/14 18:01,,,,,,,,,,0.14.0,,,,,,0,,,"Enhancement request to add the logs url to the JIRA report.  Currently it contains URL to the console output and jenkins reports only.

NO PRECOMMIT TESTS",,szehon,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6937,,,,,,,,,,,,,,,,,,,,,"16/May/14 23:00;szehon;HIVE-7080.patch;https://issues.apache.org/jira/secure/attachment/12645355/HIVE-7080.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393189,,,,Thu Nov 13 19:43:18 UTC 2014,,,,,,,,,,"0|i1vpgn:",393355,,,,,,,,,,,,,,,,,,,,,"16/May/14 21:31;szehon;Related to HIVE-6937;;;","16/May/14 23:00;szehon;Attaching a fix.  

The corresponding action on build machine is to modify the build properties (like /usr/local/hiveptest/etc/public/trunk-mr2.properties) to have the following property:

logsURL = http://ec2-174-129-184-35.compute-1.amazonaws.com/logs;;;","17/May/14 01:21;szehon;Seems like the server doesnt mind an extra argument, so already added to the properties.  Once this patch is committed, all we need to do is to update the latest code on the server.;;;","19/May/14 07:03;ashutoshc;+1;;;","19/May/14 18:01;ashutoshc;Committed to trunk. Thanks, Szehon!;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive logs errors about missing tables when parsing CTE expressions,HIVE-7079,12714870,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,navis,ccondit,ccondit,16/May/14 20:40,06/Nov/19 02:08,14/Jul/23 06:14,07/Jul/14 16:30,0.13.0,,,,,,,,,0.14.0,,Query Processor,,,,0,,,"Given a query co{color:#FF0000}ntain{color}ing common table expressions (CTE) such as:

WITH a AS (SELECT ...), b AS (SELECT ...)
 SELECT * FROM a JOIN b on a.col = b.col ...;

Hive CLI executes the query, but logs stack traces at ERROR level during query parsing:
{noformat}
ERROR metadata.Hive: NoSuchObjectException(message:ccondit.a table not found)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_result$get_table_resultStandardScheme.read(ThriftHiveMetastore.java:29338)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_result$get_table_resultStandardScheme.read(ThriftHiveMetastore.java:29306)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_result.read(ThriftHiveMetastore.java:29237)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_table(ThriftHiveMetastore.java:1036)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_table(ThriftHiveMetastore.java:1022)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:997)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at com.sun.proxy.$Proxy7.getTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:967)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:909)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1223)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1192)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9209)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:391)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:291)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:944)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1009)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:880)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:870)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
{noformat}
It looks like Hive is attempting to resolve the CTE aliases as physical tables.",,ccondit,navis,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-11427,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 07:31;navis;HIVE-7079.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12648927/HIVE-7079.1.patch.txt","07/Jul/14 02:31;navis;HIVE-7079.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12654246/HIVE-7079.2.patch.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393183,,,,Thu Nov 13 19:42:31 UTC 2014,,,,,,,,,,"0|i1vpfb:",393349,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 19:52;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12648927/HIVE-7079.1.patch.txt

{color:red}ERROR:{color} -1 due to 11 failed/errored test(s), 5607 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union31
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_joins
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.testPropertiesParsing
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/414/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/414/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-414/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 11 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12648927;;;","10/Jun/14 01:11;navis;Hmm.. cannot reproduce union31 and hbase_joins.;;;","10/Jun/14 03:07;ashutoshc;Can you create RB request for it?;;;","07/Jul/14 09:09;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12654246/HIVE-7079.2.patch.txt

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5692 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/688/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/688/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-688/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12654246;;;","07/Jul/14 15:22;ashutoshc;+1;;;","07/Jul/14 16:30;ashutoshc;Committed to trunk. Thanks, Navis!;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive contrib compilation maybe broken with removal of org.apache.hadoop.record,HIVE-7077,12714836,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,viraj,viraj,16/May/14 18:34,13/Nov/14 19:40,14/Jul/23 06:14,27/May/14 04:28,0.12.0,0.13.0,,,,,,,,0.14.0,,Contrib,,,,0,,,"Hadoop decided to move record to hadoop-streaming so the compilation of the contrib code will be broken if we do not include this jar.
{quote}
compile:
     [echo] Project: contrib
    [javac] Compiling 39 source files to /home/y/var/builds/thread2/workspace/Cloud-Hive-branch-0.12-Hadoop2-Component-JDK7/build/contrib/classes
    [javac] /home/y/var/builds/thread2/workspace/Cloud-Hive-branch-0.12-Hadoop2-Component-JDK7/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java:47: error: package org.apache.hadoop.record does not exist
    [javac] import org.apache.hadoop.record.Record;
    [javac]                                ^
    [javac] /home/y/var/builds/thread2/workspace/Cloud-Hive-branch-0.12-Hadoop2-Component-JDK7/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesOutput.java:30: error: package org.apache.hadoop.record does not exist
    [javac] import org.apache.hadoop.record.Buffer;
    [javac]                                ^
    [javac] /home/y/var/builds/thread2/workspace/Cloud-Hive-branch-0.12-Hadoop2-Component-JDK7/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java:224: error: cannot find symbol
    [javac]   public void writeRecord(Record r) throws IOException {
    [javac]                           ^
    [javac]   symbol:   class Record
    [javac]   location: class TypedBytesWritableOutput
    [javac] /home/y/var/builds/thread2/workspace/Cloud-Hive-branch-0.12-Hadoop2-Component-JDK7/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java:29: error: package org.apache.hadoop.record does not exist
    [javac] import org.apache.hadoop.record.Buffer;
    [javac]                                ^
    [javac] /home/y/var/builds/thread2/workspace/Cloud-Hive-branch-0.12-Hadoop2-Component-JDK7/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java:24: error: package org.apache.hadoop.record does not exist
    [javac] import org.apache.hadoop.record.Buffer;
    [javac]                                ^
{quote}

Besides this, https://issues.apache.org/jira/browse/HADOOP-10485 removes most of these classes. This Jira is being created to track this.

Viraj",Hadoop 2.4.0.5  and beyond,jlowe,navis,thejas,viraj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7008,,,,,,,,,,,,,,,,,,,,,,,,"24/May/14 22:40;ashutoshc;HIVE-7077.patch;https://issues.apache.org/jira/secure/attachment/12646695/HIVE-7077.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393149,,,,Thu Nov 13 19:40:37 UTC 2014,,,,,,,,,,"0|i1vp7z:",393315,,,,,,,,,,,,,,,,,,,,,"16/May/14 21:13;ashutoshc;I cleaned some of the related dead code in HIVE-7008 Seems like contrib/ still has some of it left.;;;","17/May/14 01:39;viraj;Hi Ashutosh, 
 Thanks for responding. It seems that a class RecordTestObj.java in /build/ql/test/classes  uses the org.apache.hadoop.record. Hadoop has decided to revert the change in HADOOP-10474 at the present moment.
Viraj;;;","19/May/14 20:43;jlowe;The move of org.apache.hadoop.record.* classes to the hadoop-streaming jar and the subsequent removal of some of those classes (HADOOP-10474 and HADOOP-10485, respectively) have been reverted from Hadoop's branch-2.  Note that these changes are still in Hadoop trunk, and they will be part of Hadoop 3.x and subsequent releases.  This gives users the Hadoop 2.x release to realize these classes are deprecated and a chance to migrate to an alternative (e.g.: Avro).
;;;","26/May/14 05:23;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646695/HIVE-7077.patch

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 5463 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_views
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/296/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/296/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-296/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646695;;;","26/May/14 07:37;ashutoshc;Test failures are unrelated. Patch is ready for review.;;;","26/May/14 07:51;navis;I've never used Record or RecordOutput before. But would it be safe to remove that in hive? Sill the most of clusters are based on hadoop-1(at least in my experience).;;;","26/May/14 08:12;ashutoshc;I think so. Patch just removes one of the type supported in TypedBytesSerDe, which lived in Hadoop code base but is deprecated for long time. TypedBytesSerDe itself is in contrib/ module (not in proper hive itself). TypedBytesSerDe is written mostly to be used with transform feature (less comonly used feature of Hive these days). 
So, patch is essentially removing a deprecated type from a serde from a contrib module which is used with a rarely used feature. Seems to me there will hardly be any users for it. If there are any users at all, they can still use this serde from previous release if they desire to do so.;;;","27/May/14 02:33;navis;+1;;;","27/May/14 04:28;ashutoshc;Committed to trunk.;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Plugin (exec hook) to log to application timeline data to Yarn,HIVE-7076,12714721,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,16/May/14 08:10,13/Nov/14 19:42,14/Jul/23 06:14,18/Jun/14 22:47,,,,,,,,,,0.14.0,,,,,,0,,,"See: https://issues.apache.org/jira/browse/YARN-1530

This is a simple pre/post exec hook to log query + plan information to yarn. This information can be used to build tools and UIs to monitor, track, debug and tune Hive queries.

Off by default, but can be enabled via:

hive.exec.pre.hooks=ql.src.java.org.apache.hadoop.hive.ql.hooks.ATSHook

hive.exec.post.hooks=ql.src.java.org.apache.hadoop.hive.ql.hooks.ATSHook

hive.exec.failure.hooks=ql.src.java.org.apache.hadoop.hive.ql.hooks.ATSHook

",,hagleitn,philip,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,YARN-1530,,,HIVE-7377,,,,,,,,,,,,,,,,,,,,,"16/May/14 08:14;hagleitn;HIVE-7076.1.patch;https://issues.apache.org/jira/secure/attachment/12645198/HIVE-7076.1.patch","17/Jun/14 19:49;hagleitn;HIVE-7076.2.patch;https://issues.apache.org/jira/secure/attachment/12650864/HIVE-7076.2.patch","17/Jun/14 20:45;hagleitn;HIVE-7076.3.patch;https://issues.apache.org/jira/secure/attachment/12650877/HIVE-7076.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393034,,,,Thu Nov 13 19:42:12 UTC 2014,,,,,,,,,,"0|i1voiv:",393201,,,,,,,,,,,,,,,,,,,,,"16/May/14 19:05;thejas;What version of hadoop does this need ?
;;;","17/May/14 00:15;hagleitn;2.4 - which is what's currently in the hive POM.;;;","17/May/14 02:48;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12645198/HIVE-7076.1.patch

{color:red}ERROR:{color} -1 due to 26 failed/errored test(s), 5525 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_java_method
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_reflect
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_nested_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_math_funcs
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_script_pipe
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_transform1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input20
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input5
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/217/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/217/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 26 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12645198;;;","20/May/14 17:48;hagleitn;Failures are unrelated.;;;","21/May/14 20:49;thejas;I have added some minor comments on review board.
We also need to figure out how to make this change work with 1.x build. ;;;","21/May/14 22:24;vgumashta;Other than [~thejas]' feedback, I did some verification with the patch. Without the patch and with ATSHooks set, I can see an ever growing bunch of ATS logger threads. With the patch, just one daemon thread. So looks good!
Test setup: I ran a test which creates 20 threads and fires up different short jdbc queries for few hours.;;;","22/May/14 17:58;hagleitn;Thanks [~vaibhavgumashta] and [~thejas]. On the 1.x front I propose the following: Let's change the hive build to always build against hadoop 2. 

Right now you can switch the dependency via -Phadoop-X flag, but that doesn't really match what we ship. There's only one hive-exec jar etc not hive-exec-h1, hive-exec-h2. What's more we're not catching binary incompatibilities the way it's set up. Ideally we should build once (hadoop 2) then run the q file tests with the -Phadoop-X flag. That way we will validate our build works correctly against multiple hadoop versions.

[~thejas] thoughts?

Let me see if I can figure this out in mvn.;;;","17/Jun/14 19:49;hagleitn;Different route. Patch .2 simply excludes the ATSHook from compilation on hadoop-1 profile;;;","17/Jun/14 20:45;hagleitn;.3 addresses review comments.;;;","18/Jun/14 00:22;thejas;+1;;;","18/Jun/14 18:09;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12650877/HIVE-7076.3.patch

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 5579 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_columnar
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/503/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/503/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-503/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12650877;;;","18/Jun/14 22:31;hagleitn;Failures are unrelated.;;;","18/Jun/14 22:47;hagleitn;Committed to trunk.;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JsonSerde raises NullPointerException when object key is not lower case,HIVE-7075,12714709,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,navis,Yibing,Yibing,16/May/14 06:29,13/Nov/14 19:41,14/Jul/23 06:14,09/Jun/14 20:28,0.12.0,,,,,,,,,0.14.0,,HCatalog,Serializers/Deserializers,,,1,,,"We have noticed that the JsonSerde produces a NullPointerException if a JSON object has a key value that is not lower case. For example. Assume we have the file ""one.json"": 

{ ""empId"" : 123, ""name"" : ""John"" } 
{ ""empId"" : 456, ""name"" : ""Jane"" } 

hive> CREATE TABLE emps (empId INT, name STRING) 
ROW FORMAT SERDE ""org.apache.hive.hcatalog.data.JsonSerDe""; 

hive> LOAD DATA LOCAL INPATH 'one.json' INTO TABLE emps; 

hive> SELECT * FROM emps; 

Failed with exception java.io.IOException:java.lang.NullPointerException 

-------- 

Notice, it seems to work if the keys are lower case. Assume we have the file 'two.json': 
{ ""empid"" : 123, ""name"" : ""John"" } 
{ ""empid"" : 456, ""name"" : ""Jane"" } 

hive> DROP TABLE emps; 

hive> CREATE TABLE emps (empId INT, name STRING) 
ROW FORMAT SERDE ""org.apache.hive.hcatalog.data.JsonSerDe""; 

hive> LOAD DATA LOCAL INPATH 'two.json' INTO TABLE emps;

hive> SELECT * FROM emps; 
OK 
123	John 
456	Jane",,ekoifman,jojspieg@gmail.com,navis,qwertymaniac,sushanth,thejas,Yibing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/May/14 07:21;navis;HIVE-7075.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12645962/HIVE-7075.1.patch.txt","23/May/14 01:01;navis;HIVE-7075.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12646424/HIVE-7075.2.patch.txt","26/May/14 04:54;navis;HIVE-7075.3.patch.txt;https://issues.apache.org/jira/secure/attachment/12646738/HIVE-7075.3.patch.txt",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,393022,,,,Thu Nov 13 19:41:47 UTC 2014,,,,,,,,,,"0|i1vog7:",393189,,,,,,,,,,,,,,,,,,,,,"16/May/14 06:35;Yibing;Suggest to fix this by:
- String fieldName = p.getText();
+ String fieldName = p.getText().toLowerCase();;;;","21/May/14 07:21;navis;Cannot sure it's right to use lower cased field names. But this is the patch.;;;","21/May/14 11:19;qwertymaniac;Can a test case be added as well, aside of just the fix, so this does not regress in future?

bq. Cannot sure it's right to use lower cased field names.

Hive explicitly appears to make them lower case though?;;;","21/May/14 23:18;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12645962/HIVE-7075.1.patch.txt

{color:red}ERROR:{color} -1 due to 12 failed/errored test(s), 5526 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hive.hcatalog.data.schema.TestHCatSchema.testCannotInstantiateSchemaWithRepeatedFieldNames
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/258/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/258/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-258/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 12 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12645962;;;","22/May/14 01:14;navis;Test fails seemed not related to this.

bq. Can a test case be added as well
Sure.
bq. Hive explicitly appears to make them lower case though
Yes, but this means all of the fields in json are regarded as lower case literal. Highly not realistic, but for example,
{ ""empid"" : 123, ""empId"" : 234 } 
'empId' will return 123, which was 234 before this patch.;;;","22/May/14 13:15;Yibing;I don't think we have better solution for a string like ""{ ""empid"" : 123, ""empId"" : 234 }"". For such a string, I believe either 123 or 234 is acceptable as the value.;;;","23/May/14 02:18;qwertymaniac;Thanks for clarifying Navis (and the test) and Yibing!;;;","24/May/14 05:53;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646424/HIVE-7075.2.patch.txt

{color:red}ERROR:{color} -1 due to 11 failed/errored test(s), 5534 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_stats_counter
org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency
org.apache.hadoop.hive.conf.TestHiveConf.testConfProperties
org.apache.hive.hcatalog.data.schema.TestHCatSchema.testCannotInstantiateSchemaWithRepeatedFieldNames
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/276/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/276/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-276/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 11 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646424;;;","27/May/14 04:22;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646738/HIVE-7075.3.patch.txt

{color:red}ERROR:{color} -1 due to 14 failed/errored test(s), 5540 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/305/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/305/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-305/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 14 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646738;;;","28/May/14 07:43;navis;Failed tests seemed not related to this.;;;","05/Jun/14 03:45;Yibing;Hi Navis, any update on this? ;;;","05/Jun/14 03:50;navis;Just need +1 from others.;;;","05/Jun/14 17:34;ashutoshc;cc: [~sushanth] Would you like to review this one?;;;","09/Jun/14 20:25;sushanth;Looks good to me. +1.

Will go ahead and commit.;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HCatLoader only loads first region of hbase table,HIVE-7072,12714602,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sushanth,sushanth,sushanth,15/May/14 20:25,13/Nov/14 19:44,14/Jul/23 06:14,07/Aug/14 23:57,0.14.0,,,,,,,,,0.14.0,,,,,,0,,,"Pig needs a config parameter 'pig.noSplitCombination' set to 'true' for it to be able to read HBaseStorageHandler-based tables.

This is done in the HBaseLoader at getSplits time, but HCatLoader does not do so, which results in only a partial data load.

Thus, we need one more special case definition in HCat, that sets this parameter in the job properties if we detect that we're loading a HBaseStorageHandler based table. (Note, also, that we should not depend directly on the HBaseStorageHandler class, and instead depend on the name of the class, since we do not want a mvn dependency on hive-hbase-handler to be able to compile HCatalog core, since it's conceivable that at some time, there might be a reverse dependency.) The primary issue is one of where this code should go, since it doesn't belong in pig (pig does not know what loader behaviour should be, and this parameter is its interface to a loader), and doesn't belong in the HBaseStorageHandler either, since that's implementing a HiveStorageHandler and is connecting up the two. Thus, this should belong to HCatLoader. Setting this parameter across the board results in poor performance for HCatLoader, so it must only be set when using with HBase.

Thus, it belongs in the SpecialCases definition as that was created specifically for these kinds of odd cases, and can be called from within HCatLoader.

",,daijy,leftyl,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/May/14 01:23;sushanth;HIVE-7072.2.patch;https://issues.apache.org/jira/secure/attachment/12647274/HIVE-7072.2.patch","09/Jun/14 22:02;sushanth;HIVE-7072.3.patch;https://issues.apache.org/jira/secure/attachment/12649468/HIVE-7072.3.patch","05/Aug/14 21:30;sushanth;HIVE-7072.4.patch;https://issues.apache.org/jira/secure/attachment/12659959/HIVE-7072.4.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392915,,,,Thu Nov 13 19:44:37 UTC 2014,,,,,,,,,,"0|i1vnt3:",393085,,,,,,,,,,,,,,,,,,,,,"28/May/14 23:44;sushanth;Note for future readers: pig actually has two parameters : pig.splitCombination and pig.noSplitCombination, which are inverses of each other. The workaround for this issue for people is to set ""pig.splitCombination"" to ""false"" inside the pig script before invoking HCatLoader. However, it does not work for HCatLoader to set that parameter inside the setLocation, for that is too late (that parameter seems to be evaluated prior to hcat being loaded, in order to set its opposite twin) - instead, it should set the ""pig.noSplitCombination"" to true, and then pig is able to pick that up from within the job.;;;","29/May/14 01:23;sushanth;Uploading patch. [~daijy], could you please review?;;;","29/May/14 01:33;daijy;+1;;;","30/May/14 18:34;sushanth;Canceling this patch, still needed one more modification to ensure non-null args.;;;","30/May/14 20:01;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647274/HIVE-7072.2.patch

{color:red}ERROR:{color} -1 due to 67 failed/errored test(s), 5542 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.mapreduce.TestHCatHiveCompatibility.testPartedRead
org.apache.hive.hcatalog.mapreduce.TestHCatHiveCompatibility.testUnpartedReadWrite
org.apache.hive.hcatalog.mapreduce.TestHCatHiveThriftCompatibility.testDynamicCols
org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite.testSequenceTableWriteRead
org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite.testSequenceTableWriteReadMR
org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite.testTextTableWriteRead
org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite.testTextTableWriteReadMR
org.apache.hive.hcatalog.pig.TestE2EScenarios.testReadOrcAndRCFromPig
org.apache.hive.hcatalog.pig.TestHCatLoader.testColumnarStorePushdown
org.apache.hive.hcatalog.pig.TestHCatLoader.testConvertBooleanToInt
org.apache.hive.hcatalog.pig.TestHCatLoader.testGetInputBytes
org.apache.hive.hcatalog.pig.TestHCatLoader.testProjectionsBasic
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataBasic
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadPartitionedBasic
org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.testMapWithComplexData
org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.testSyntheticComplexSchema
org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.testTupleInBagInTupleInBag
org.apache.hive.hcatalog.pig.TestHCatLoaderStorer.testReadWrite
org.apache.hive.hcatalog.pig.TestHCatLoaderStorer.testSmallTinyInt
org.apache.hive.hcatalog.pig.TestHCatStorer.testDateCharTypes
org.apache.hive.hcatalog.pig.TestHCatStorer.testPartColsInData
org.apache.hive.hcatalog.pig.TestHCatStorer.testStoreFuncAllSimpleTypes
org.apache.hive.hcatalog.pig.TestHCatStorer.testStoreInPartiitonedTbl
org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteChar
org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteDate
org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteDate2
org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteDate3
org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteDecimalXY
org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteSmallint
org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteTimestamp
org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteTinyint
org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteVarchar
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testColumnarStorePushdown
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testConvertBooleanToInt
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testGetInputBytes
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testProjectionsBasic
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataBasic
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadPartitionedBasic
org.apache.hive.hcatalog.pig.TestOrcHCatLoaderComplexSchema.testMapWithComplexData
org.apache.hive.hcatalog.pig.TestOrcHCatLoaderComplexSchema.testSyntheticComplexSchema
org.apache.hive.hcatalog.pig.TestOrcHCatLoaderComplexSchema.testTupleInBagInTupleInBag
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testDateCharTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testPartColsInData
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testStoreFuncAllSimpleTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testStoreInPartiitonedTbl
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteChar
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDate
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDate2
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDate3
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteSmallint
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteTimestamp
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteTinyint
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteVarchar
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/337/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/337/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-337/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 67 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647274;;;","09/Jun/14 22:02;sushanth;Updated patch.;;;","10/Jun/14 11:58;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12649468/HIVE-7072.3.patch

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 5608 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.testPropertiesParsing
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/422/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/422/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-422/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12649468;;;","03/Jul/14 21:50;sushanth;[~daijy], could you please review/commit the latest version of this patch?;;;","05/Aug/14 21:30;sushanth;Updated patch for current trunk. [~daijy], could you please review the latest version?;;;","06/Aug/14 16:27;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12659959/HIVE-7072.4.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5877 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_mixed_case
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_opt_vectorization
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.testCreateTableWithIndexAndPartitionsNonDefaultNameNode
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/189/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/189/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-189/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12659959;;;","06/Aug/14 20:56;sushanth;Failing tests are unrelated to this patch.;;;","07/Aug/14 23:03;daijy;+1;;;","07/Aug/14 23:57;sushanth;Committed to trunk. Thanks, Daniel!;;;","08/Aug/14 00:06;leftyl;Does this need user doc (particularly [~sushanth]'s ""Note for future readers"")?

* [Note for future readers | https://issues.apache.org/jira/browse/HIVE-7072?focusedCommentId=14011834&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14011834];;;","08/Aug/14 00:37;sushanth;That would be developer documentation for pig, not hive, and thus my intention was that it's helpful for other developers that try to make sense of what this jira did. :)

The parameter noted there was a workaround for an issue, but this patch makes using that workaround unnecessary, and so, should not need documentation.;;;","08/Aug/14 01:02;leftyl;Yay, less work for me.  Thanks Sushanth.  (With a whinny from the local screech owl:  http://www.allaboutbirds.org/guide/eastern_screech-owl/sounds.);;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use custom Tez split generator to support schema evolution,HIVE-7071,12714597,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,15/May/14 19:52,13/Nov/14 19:41,14/Jul/23 06:14,31/May/14 06:31,,,,,,,,,,0.14.0,,,,,,0,,,Right now we're falling back to combinehivefileinputformat and switch of am side grouping when there's different schemata in a single vertex. We need to handle this in a custom initializer so we can still group on the AM.,,hagleitn,thejas,vasanthkumar,vikram.dixit,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6826,HIVE-7096,,,,,,,,,,,,,,,,,,,,,,,"16/May/14 07:27;hagleitn;HIVE-7071.1.patch;https://issues.apache.org/jira/secure/attachment/12645190/HIVE-7071.1.patch","20/May/14 02:28;hagleitn;HIVE-7071.2.patch;https://issues.apache.org/jira/secure/attachment/12645714/HIVE-7071.2.patch","01/Jun/14 23:29;vikram.dixit;HIVE-7071.hadoop1.patch;https://issues.apache.org/jira/secure/attachment/12647839/HIVE-7071.hadoop1.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392910,,,,Thu Nov 13 19:41:29 UTC 2014,,,,,,,,,,"0|i1vnrz:",393080,,,,,,,,,,,,,,,,,,,,,"17/May/14 06:43;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12645190/HIVE-7071.1.patch

{color:red}ERROR:{color} -1 due to 18 failed/errored test(s), 5450 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_java_method
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_reflect
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_math_funcs
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input20
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input5
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/218/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/218/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 18 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12645190;;;","19/May/14 22:28;hagleitn;Failures seem unrelated. Build before this one showed the same errors. Will run locally to verify.;;;","20/May/14 02:28;hagleitn;.2 Addresses review comments;;;","20/May/14 16:43;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12645714/HIVE-7071.2.patch

{color:red}ERROR:{color} -1 due to 11 failed/errored test(s), 5451 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_17
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.conf.TestHiveConf.testConfProperties
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/242/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/242/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logsPreCommit-HIVE-Build-242/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 11 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12645714;;;","20/May/14 17:47;hagleitn;Failures are unrelated.;;;","30/May/14 17:06;vikram.dixit;+1 LGTM.;;;","31/May/14 06:31;hagleitn;Committed to trunk. Thanks [~vikram.dixit]!;;;","01/Jun/14 18:35;vasanthkumar;Getting compile error when running maven for hadoop-1;;;","01/Jun/14 22:09;xuefuz; [~hagleitn]  / [~vikram.dixit], could you please take a look at the build failure mentioned above?;;;","01/Jun/14 22:41;vikram.dixit;Looking into this.;;;","01/Jun/14 23:47;vikram.dixit;[~xuefuz] can you take a quick look?

https://reviews.apache.org/r/22122/

Raised HIVE-7162 for follow on.;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Zookeeper connection leak,HIVE-7069,12714497,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,,z1lv1n4s,z1lv1n4s,15/May/14 14:43,12/Feb/15 23:40,14/Jul/23 06:14,04/Feb/15 19:18,0.12.0,,,,,,,,,1.1.0,,HiveServer2,,,,0,,,"We're using CDH 5.0.0 which ships with HIVE 0.12.0. We're running HiveServer2 and connect to it via JDBC. We have zookeeper support enabled. If a connection is made to HS2 and not explicitly closed via JDBC then a connection made to zookeeper is never released. It reaches a point where HS2 hangs and stops executing any new queries. It's easy to replicate with a simple script that connects to HS2 via JDBC and runs a simple query like 'show tables'. At the same time run this on the hive server machine to monitor zookeeper connections: 'while sleep 1; do netstat -anlp | grep 2181 | wc -l; done' .. If you close the connection explicitly the count will go down soon after the program exits.",Linux 2.6.32-431.11.2.el6.x86_64 #1 SMP Tue Mar 25 19:59:55 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux,morfious902002,richardatcloudera,xuefuz,z1lv1n4s,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-9119,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392810,,,,Wed Feb 04 19:18:31 UTC 2015,,,,,,,,,,"0|i1vn6v:",392984,,,,,,,,,,,,,,,,,,,,,"04/Feb/15 16:43;morfious902002;We are having the same issue with hive2 jdbc connection on CDH 5.3 which comes with 0.13.1. If we keep hive concurrency on then simple select statement create a zookeeper connection that is never closed. We open the hive2 jdbc connection then run a number of select queries that in turn create a zookeeper connection for each of them. We never explicitly call zookeeper in our code. So, is there a way to force close zookeeper connection in this case? Any help would be appreciated. 
  ;;;","04/Feb/15 19:14;xuefuz;HIVE-9119 should have addressed the problem and thus I'm closing this one for now. Please feel free to reopen this issue if otherwise.;;;","04/Feb/15 19:18;xuefuz;Fixed via HIVE-9119.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Min() and Max() on Timestamp and Date columns for ORC returns wrong results,HIVE-7067,12714373,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,prasanth_j,prasanth_j,prasanth_j,15/May/14 00:47,09/Jun/14 06:39,14/Jul/23 06:14,16/May/14 18:55,,,,,,,,,,0.13.1,0.14.0,,,,,0,,,"min() and max() of timestamp and date columns of ORC table returns wrong results. The reason for that is when ORC creates object inspectors for date and timestamp it uses JAVA primitive objects as opposed to WRITABLE objects. When get() is performed on java primitive objects, a reference to the underlying object is returned whereas when get() is performed on writable objects, a copy of the underlying object is returned. 

Fix is to change the object inspector creation to return writable objects for timestamp and date.",,hagleitn,jdere,prasanth_j,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7104,,,,,,,,,,,,,,,,,,,,,"15/May/14 00:49;prasanth_j;HIVE-7067.1.patch;https://issues.apache.org/jira/secure/attachment/12644932/HIVE-7067.1.patch","15/May/14 07:41;prasanth_j;HIVE-7067.2.patch;https://issues.apache.org/jira/secure/attachment/12644976/HIVE-7067.2.patch","16/May/14 00:06;prasanth_j;HIVE-7067.branch-13.2.patch;https://issues.apache.org/jira/secure/attachment/12645129/HIVE-7067.branch-13.2.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392686,,,,Mon Jun 09 06:39:36 UTC 2014,,,,,,,,,,"0|i1vmgn:",392863,,,,,,,,,,,,,,,,,,,,,"15/May/14 00:52;prasanth_j;Attaching RB link;;;","15/May/14 07:41;prasanth_j;Addressed [~jdere]'s review comments.;;;","15/May/14 17:55;hagleitn;+1. Will commit if tests pass.;;;","15/May/14 18:03;jdere;+1;;;","15/May/14 18:19;prasanth_j;HIVE QA seems broken. It didn't run any of my previous patches. I will run all the tests locally and will report back if it passes.;;;","15/May/14 18:49;thejas;I think we should include this in 0.13.1 release.
;;;","15/May/14 20:04;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12644976/HIVE-7067.2.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/199/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/199/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12644976;;;","15/May/14 23:53;thejas;Ran the tests myself and results look good.
;;;","16/May/14 00:06;prasanth_j;Thanks Thejas for running the tests! Attaching patch that applies cleanly on branch-0.13.;;;","16/May/14 18:56;prasanth_j;Committed patch to trunk. [~sushanth] can you please commit this to 0.13 branch?;;;","16/May/14 19:09;sushanth;I'm okay with backporting this to 0.13.1, but I want to follow the process I set out for candidates for late inclusion for 0.13.1, and for that, I need two +1s for this. [~thejas] / [~hagleitn] / [~jdere] , would you please confirm that you're okay with this inclusion request?;;;","16/May/14 19:13;thejas;+1 for 0.13.1
;;;","16/May/14 19:32;jdere;+1 for 0.13.1;;;","16/May/14 21:54;sushanth;Thanks folks, I'll include this in 0.13.1 RC2.;;;","23/May/14 07:17;sushanth;Committed to 0.13 branch.;;;","09/Jun/14 06:39;thejas;This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hive-exec jar is missing avro core,HIVE-7066,12714343,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,davidzchen,davidzchen,davidzchen,14/May/14 23:05,13/Nov/14 19:44,14/Jul/23 06:14,21/May/14 21:18,0.13.0,0.13.1,,,,,,,,0.14.0,,Build Infrastructure,,,,0,,,"Running a simple query that reads an Avro table caused the following exception to be thrown on the cluster side:

{code}
java.lang.RuntimeException: org.apache.hive.com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Unable to create serializer ""org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer"" for class: org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat
Serialization trace:
outputFileFormatClass (org.apache.hadoop.hive.ql.plan.PartitionDesc)
aliasToPartnInfo (org.apache.hadoop.hive.ql.plan.MapWork)
	at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:365)
	at org.apache.hadoop.hive.ql.exec.Utilities.getMapWork(Utilities.java:276)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:254)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:445)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:438)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:587)
	at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.<init>(MapTask.java:191)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:412)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:366)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:394)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)
Caused by: org.apache.hive.com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Unable to create serializer ""org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer"" for class: org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat
Serialization trace:
outputFileFormatClass (org.apache.hadoop.hive.ql.plan.PartitionDesc)
aliasToPartnInfo (org.apache.hadoop.hive.ql.plan.MapWork)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:139)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:17)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:672)
	at org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo(Utilities.java:942)
	at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:850)
	at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:864)
	at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:334)
	... 13 more
Caused by: java.lang.IllegalArgumentException: Unable to create serializer ""org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer"" for class: org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat
	at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:45)
	at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:26)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:343)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:336)
	at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.registerImplicit(DefaultClassResolver.java:56)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.getRegistration(Kryo.java:476)
	at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:148)
	at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:115)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:656)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers$ClassSerializer.read(DefaultSerializers.java:238)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers$ClassSerializer.read(DefaultSerializers.java:226)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObjectOrNull(Kryo.java:745)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:113)
	... 25 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:32)
	... 37 more
Caused by: java.lang.NoClassDefFoundError: org/apache/avro/io/DatumWriter
	at java.lang.Class.getDeclaredFields0(Native Method)
	at java.lang.Class.privateGetDeclaredFields(Class.java:2348)
	at java.lang.Class.getDeclaredFields(Class.java:1779)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:150)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:109)
	... 42 more
Caused by: java.lang.ClassNotFoundException: org.apache.avro.io.DatumWriter
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
	... 47 more
{code}

I took a look at the hive-exec jar and found that the Avro core jar was not included, though avro-mapred is included.

I confirmed that Avro core was included in the Hive 0.12 hive-exec jar. Was there a reason why this was removed in trunk? It seems that this would break the AvroSerDe.",,cwsteinbach,davidzchen,erwaman,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/May/14 23:36;davidzchen;HIVE-7066.1.patch;https://issues.apache.org/jira/secure/attachment/12644913/HIVE-7066.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392656,Reviewed,,,Thu Nov 13 19:44:00 UTC 2014,,,,,,,,,,"0|i1vmav:",392836,,,,,,,,,,,,,,,,,,,,,"14/May/14 23:41;davidzchen;I have posted a patch for a fix. I have tested this on trunk by confirming that Avro core is in the hive-exec jar and successfully running a simple Hive query against a table registered with the AvroSerDe. The fix was a simple 1 line change. It looks like this issue was caused by the Ant -> Maven switch and the avro core jar was inadvertently left out when creating the hive-exec  jar.

I am not able to create an RB right now because RB is giving me a 502 error when I try to create a new review request, both using {{rbt post}} and manually via the RB web UI. I will try to create a RB later. ;;;","14/May/14 23:47;davidzchen;RB: https://reviews.apache.org/r/21471;;;","15/May/14 13:42;xuefuz;Thanks for the patch. What query are you running that demonstrates the problem? Recently I worked on HIVE-5823, and didn't hit the problem you described. I'd like to reproduce the problem in order to verify your fix.;;;","15/May/14 21:02;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12644913/HIVE-7066.1.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/204/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/204/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12644913;;;","16/May/14 23:00;davidzchen;Hi Xuefu, I simply ran {{select count(1) from avro-table;}}. I ran {{mvn clean install -DskipTests -Phadoop-1 -Pdist}} to build Hive, create the tarball, unzipped it and set up a local metastore and ran the query.;;;","19/May/14 23:53;davidzchen;[~xuefuz], were you able to reproduce the problem? Do you have any more feedback on this patch before it can be merged?;;;","20/May/14 16:20;xuefuz;[~davidzchen] I ran your mvn command and built a tarball. I unzipped it, and found in the lib directory there is a jar named avro-1.7.5.jar. Is this what you need? I thought all jars under /lib will be shipped to the cluster. BTW, I was able to run avro query from this build, though my cluster may already have the avro jar.

Your change seems to shade the avro jar in Hive libexec jar, but the JIRA's title mentiones avro-mapred. I'm little confused.;;;","20/May/14 19:32;davidzchen;[~xuefuz] Sorry for the confusion. The title should be referring to avro core and not avro-mapred. avro-mapred is already included but avro core is not.

From what I understand, only the hive-exec jar is shipped to the cluster. The jars under /lib are used by Hive locally before firing the MapReduce jobs. If your cluster already has the Avro jar, then the query would work. In this case, I was simply running a local instance; further, we do not install Avro on each node but rather expect jobs to provide the jars. Based on the testing that I have done, if avro core is not present in the hive-exec jar, then queries that read tables registered with the AvroSerDe fails with the above exception.;;;","20/May/14 21:23;xuefuz;Okay. +1;;;","21/May/14 21:18;cwsteinbach;Committed to trunk. Thanks David!;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive jobs in webhcat run in default mr mode even in Hive on Tez setup,HIVE-7065,12714325,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,14/May/14 22:12,13/Nov/14 19:40,14/Jul/23 06:14,14/Jun/14 02:10,0.13.0,,,,,,,,,0.14.0,,Tez,WebHCat,,,0,TODOC14,,"WebHCat config has templeton.hive.properties to specify Hive config properties that need to be passed to Hive client on node executing a job submitted through WebHCat (hive query, for example).

this should include ""hive.execution.engine""
",,ekoifman,leftyl,navis,szehon,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7268,,,,,,,,,,,,,,,,,,,,,"02/Jun/14 19:07;ekoifman;HIVE-7065.1.patch;https://issues.apache.org/jira/secure/attachment/12647963/HIVE-7065.1.patch","11/Jun/14 22:40;ekoifman;HIVE-7065.2.patch;https://issues.apache.org/jira/secure/attachment/12649923/HIVE-7065.2.patch","14/May/14 22:32;ekoifman;HIVE-7065.patch;https://issues.apache.org/jira/secure/attachment/12644900/HIVE-7065.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392638,,,,Thu Nov 13 19:40:58 UTC 2014,,,,,,,,,,"0|i1vm6v:",392818,,,,,,,,,,,,,,,,,,,,,"02/Jun/14 19:07;ekoifman;update based on feedback from @Thejas;;;","02/Jun/14 20:06;thejas;+1;;;","06/Jun/14 21:08;thejas;Patch committed to trunk. Thanks for the contribution Eugene!
;;;","08/Jun/14 03:22;leftyl;This looks like it should be documented in the wiki:

* [WebHCat Configuration -- Configuration Variables | https://cwiki.apache.org/confluence/display/Hive/WebHCat+Configure#WebHCatConfigure-ConfigurationVariables]

{quote}
*templeton.hive.properties* -- Properties to set when running Hive. To use it in a cluster with Kerberos security enabled, set hive.metastore.sasl.enabled=false and add hive.metastore.execute.setugi=true. Using localhost in metastore uri does not work with Kerberos security.
{quote}

Revise as ""Properties to set when running Hive (during job sumission)"" or perhaps that should be ""Properties set (during job submission) when running Hive.""

Should hive.execution.engine be mentioned for releases prior to 0.14?;;;","09/Jun/14 05:21;navis;Seemed caused fail of org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.testPropertiesParsing. Why no-precommit-test if it's not tested?;;;","09/Jun/14 18:07;ekoifman;good point, this should have pre-commit tests
;;;","09/Jun/14 18:13;ekoifman;[~leftylev] Is there a way to make this table in the wiki be autogenerated from the webhcat-default.xml?  It would ensure there is a single source of truth. Tez was shipped in 0.13, so yes I think hive.execution.engine can be mentioned for 0.13.;;;","09/Jun/14 19:34;ekoifman;java.lang.IllegalArgumentException: Illegal escaped string hive.some.fake.path=C:\foo\bar.txt\ unescaped \ 
  at 22at org.apache.hadoop.util.StringUtils.unEscapeString(StringUtils.java:565)
  at org.apache.hadoop.util.StringUtils.unEscapeString(StringUtils.java:547)
  at org.apache.hadoop.util.StringUtils.unEscapeString(StringUtils.java:533)
  at org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.testPropertiesParsing(TestTempletonUtils.java:308);;;","09/Jun/14 19:40;ekoifman;What's strange is that that is the test added for this this ticket.;;;","09/Jun/14 23:03;leftyl;[~eugene.koifman], I don't know if there's a way to autogenerate wiki docs from xml files, but it would certainly be useful for Hive configs as well as WebHCat.

Confluence has this documentation with links to various possibilities:  [Importing Content into Confluence -- Importing other non-wiki content | https://confluence.atlassian.com/display/DOC/Importing+Content+Into+Confluence#ImportingContentIntoConfluence-Importingothernon-wikicontent].

So we can follow those links and see if there's an easy way to autogenerate wikidocs, but I won't have much time for the research until I whittle down my backlog of 0.13 doc tasks.

In the meantime, to get a single source of truth I suggest adding a comparison of webhcat-default.xml vs. wikidoc to the release checklist.  Unlike Hive, WebHCat doesn't have a huge number of parameters so manually fixing the wiki for each release wouldn't be difficult.;;;","11/Jun/14 18:33;szehon;[~thejas] [~ekoifman] Hi, are we filing a JIRA to fix the broken test TestTempletonUtils?  It is still failing on trunk.;;;","11/Jun/14 20:02;ekoifman;I'm looking at it now.  Will make changes in this ticket;;;","11/Jun/14 22:23;ekoifman;HIVE-7065.2.patch is an ADDITIONAL patch to fix  the regression.;;;","12/Jun/14 21:22;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12649923/HIVE-7065.2.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 5610 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/447/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/447/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-447/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12649923;;;","12/Jun/14 21:24;ekoifman;none of the failed tests are related to WebHCat;;;","14/Jun/14 02:10;thejas;+1 for the unit test fix.
;;;","14/Jun/14 02:10;thejas;Unit test fix committed to trunk.
;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize for the Top N within a Group use case,HIVE-7063,12714318,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,rhbutani,rhbutani,rhbutani,14/May/14 21:44,13/Nov/14 19:43,14/Jul/23 06:14,30/Jun/14 15:16,,,,,,,,,,0.14.0,,,,,,0,,,"It is common to rank within a Group/Partition and then only return the Top N entries within each Group.
With Streaming mode for Windowing, we should push the post filter on the rank into the Windowing processing as a Limit expression.",,gopalv,leftyl,mmccline,rhbutani,sershe,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/14 18:50;rhbutani;HIVE-7063.1.patch;https://issues.apache.org/jira/secure/attachment/12648684/HIVE-7063.1.patch","18/Jun/14 20:23;rhbutani;HIVE-7063.2.patch;https://issues.apache.org/jira/secure/attachment/12651239/HIVE-7063.2.patch","30/Jun/14 01:34;rhbutani;HIVE-7063.3.patch;https://issues.apache.org/jira/secure/attachment/12653080/HIVE-7063.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392631,,,,Thu Nov 13 19:43:12 UTC 2014,,,,,,,,,,"0|i1vm5b:",392811,,,,,,,,,,,,,,,,,,,,,"15/May/14 02:24;gopalv;This would be exceptionally useful - I have seen at least two implementations of TOPN UDAFs for this.;;;","06/Jun/14 18:50;rhbutani;preliminary patch: this adds code to WdwTabFn to react to a rank limit.;;;","20/Jun/14 01:29;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12651239/HIVE-7063.2.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 5656 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_implicit_cast1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/519/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/519/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-519/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12651239;;;","28/Jun/14 02:23;ashutoshc;This is not going to optimize limit with rank like following :
{code}
select * from ( select p_mfgr, rank() over(..) from part) a limit 4;
{code}
Rather, this optimization is targeted for rank with filter predicates. It does seem like users are likely to write query with filter predicate given semantics of rank so this may not be an issue, but I think its good to note here so expectations are clear.;;;","28/Jun/14 02:33;rhbutani;Yes, in your case we can optimize as though 'rank < 5' was specified.  Though I cannot see a valid use case of writing a limit after a windowing expression, as you point out the more common case is a predicate on rank.;;;","28/Jun/14 03:35;ashutoshc;Make sense. I left few comments on RB.;;;","30/Jun/14 01:35;rhbutani;thanks [~ashutoshc]. Have uploaded a patch addressing the issues you raised;;;","30/Jun/14 04:46;ashutoshc;+1;;;","30/Jun/14 05:24;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12653080/HIVE-7063.3.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5671 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/630/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/630/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-630/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12653080;;;","30/Jun/14 15:16;ashutoshc;Committed to trunk. Thanks, Harish!;;;","01/Jul/14 02:53;leftyl;No user doc for this?;;;","11/Nov/14 20:44;leftyl;Pinging [~rhbutani]:  No doc needed for this optimization?;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Streaming mode in Windowing,HIVE-7062,12714317,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,rhbutani,rhbutani,rhbutani,14/May/14 21:42,03/Jun/16 00:59,14/Jul/23 06:14,31/May/14 18:41,,,,,,,,,,0.14.0,,,,,,0,TODOC14,,"1. Have the Windowing Table Function support streaming mode.
2. Have special handling for Ranking UDAFs.
3. Have special handling for Sum/Avg for fixed size Wdws.",,jtschei,leftyl,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-5381,,,,,,,,,,,HIVE-13936,HIVE-7143,HIVE-7344,,,,,,,,,,,,,,,,,,,,,,"20/May/14 20:11;rhbutani;HIVE-7062.1.patch;https://issues.apache.org/jira/secure/attachment/12645852/HIVE-7062.1.patch","28/May/14 05:35;rhbutani;HIVE-7062.4.patch;https://issues.apache.org/jira/secure/attachment/12647064/HIVE-7062.4.patch","29/May/14 16:28;rhbutani;HIVE-7062.5.patch;https://issues.apache.org/jira/secure/attachment/12647387/HIVE-7062.5.patch","31/May/14 02:23;rhbutani;HIVE-7062.6.patch;https://issues.apache.org/jira/secure/attachment/12647746/HIVE-7062.6.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392630,,,,Thu Nov 13 19:44:07 UTC 2014,,,,,,,,,,"0|i1vm53:",392810,,,,,,,,,,,,,,,,,,,,,"20/May/14 20:11;rhbutani;preliminary patch attached;;;","28/May/14 05:35;rhbutani;Has Framework changes + Streaming for Sum and Avg functions + Streaming for Ranking functions.
Still need to do Streaming for Min, Max, Lead, Lag, FirstVal, LastVal;;;","28/May/14 22:28;ashutoshc;Mostly looks good. Some minor comments on RB;;;","29/May/14 16:28;rhbutani;addressed [~ashutoshc] review comments;;;","29/May/14 16:33;rhbutani;[~leftylev] documentation note:

{quote}
One of the factors checked for processing Analytic functions in Streaming mode is the 'Window size'
For Streaming mode to kick in window size must be less than the config parameter 'hive.join.cache.size'. Default value for this parameter is 25000.
{quote};;;","29/May/14 17:22;ashutoshc;LGTM +1;;;","31/May/14 02:23;rhbutani;add check to not allow streaming when there are Lead/Lag invocations in arguments.;;;","31/May/14 15:04;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647746/HIVE-7062.6.patch

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 5496 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/345/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/345/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-345/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647746;;;","31/May/14 18:41;ashutoshc;Committed to trunk. Thanks, Harish!;;;","31/May/14 23:47;leftyl;[~rhbutani], will streaming have its own wikidoc for 0.14.0 or should this just be mentioned in a new section for Windowing & Analytics?

It could also be mentioned in Configuration Properties, but that buries the information.  It could go in hive-default.xml.template too.

By the way, Windowing & Analytics is a very skimpy doc.  It links to the spec but that information should be merged into the wiki and updated.

Quick ref:

* [Windowing and Analytics Functions | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics]
* [Configuration Properties:  hive.join.cache.size | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.join.cache.size]
* [Windowing Specifications in HQL | https://issues.apache.org/jira/secure/attachment/12575830/WindowingSpecification.pdf];;;","02/Jun/14 18:40;rhbutani;I don't see a need a separate page for Streaming. How about adding a note on the Windowing and Analytics page. 
This is an implementation improvement, not a functional change.

Yes agreed, windowing documentation can be expanded. The Oracle one is really nice: http://docs.oracle.com/cd/B14117_01/server.101/b10736/analysis.htm 
Don't know when I am going to get around to it though.;;;","05/Jun/14 08:40;leftyl;Okay, thanks [~rhbutani].  I've put this with my doc-by-0.14 tasks.;;;","11/Nov/14 22:09;leftyl;Doc note:  HIVE-7143 & HIVE-7344 also need documentation related to this issue -- min/max, lead/lag, fval/lval (HIVE-7143) & FirstVal, LastVal (HIVE-7344).;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sql std auth - insert queries without overwrite should not require delete privileges,HIVE-7061,12714316,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,thejas,thejas,thejas,14/May/14 21:33,13/Nov/14 19:44,14/Jul/23 06:14,23/May/14 15:46,0.13.0,,,,,,,,,0.14.0,,Authorization,SQLStandardAuthorization,,,0,TODOC14,,"Insert queries can do the equivalent of delete and insert of all rows of a table or partition, if the overwrite keyword is used. As a result DELETE privilege is applicable to such queries.
However, SQL Standard auth requires DELETE privilege even for queries that don't have the overwrite keyword.
",,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/14 22:31;thejas;HIVE-7061.1.patch;https://issues.apache.org/jira/secure/attachment/12645893/HIVE-7061.1.patch","21/May/14 01:47;thejas;HIVE-7061.2.patch;https://issues.apache.org/jira/secure/attachment/12645921/HIVE-7061.2.patch","22/May/14 18:37;thejas;HIVE-7061.3.patch;https://issues.apache.org/jira/secure/attachment/12646355/HIVE-7061.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392629,,,,Thu Nov 13 19:44:30 UTC 2014,,,,,,,,,,"0|i1vm4v:",392809,,,,,,,,,,,,,,,,,,,,,"14/May/14 21:40;thejas;The top level operation type for insert queries can't change for this, as we could have multiple inserts in a query, each with different behavior wrt OVERWRITE. 
WriteEntity will have to be enhanced to capture this information, to indicate if it is going to be overwritten or appended. Semantic analyzer needs to populate this information in the right places. The authorization mechanism has to change to consider this additional information. Right now it is just a mapping of Operation to Privileges required on input and output.;;;","14/May/14 22:24;thejas;WriteEntity types are already in hive, as part of  HIVE-5843.;;;","21/May/14 01:47;thejas;HIVE-7061.2.patch - Fix NPE and update a testcase output

;;;","21/May/14 01:50;thejas;Adding review board link (HIVE-7061.2.patch)
;;;","22/May/14 07:13;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12645921/HIVE-7061.2.patch

{color:red}ERROR:{color} -1 due to 13 failed/errored test(s), 5453 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_insertoverwrite_nodel
org.apache.hadoop.hive.conf.TestHiveConf.testConfProperties
org.apache.hadoop.hive.metastore.TestRetryingHMSHandler.testRetryingHMSHandler
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/260/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/260/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-260/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 13 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12645921;;;","22/May/14 18:37;thejas;HIVE-7061.3.patch - 
Addressing review comments.
Fixed the one related failed test case (user name was not set, so the test output had my name in 'show grants' output).
;;;","22/May/14 19:34;ashutoshc;+1;;;","23/May/14 14:09;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646355/HIVE-7061.3.patch

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 5458 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf2
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/271/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/271/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-271/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646355;;;","23/May/14 15:46;ashutoshc;Committed to trunk. Thanks, Thejas!;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
webhcat e2e deployment scripts don't have x bit set,HIVE-7057,12714080,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,14/May/14 01:10,13/Nov/14 19:40,14/Jul/23 06:14,30/May/14 21:22,,,,,,,,,,0.14.0,,WebHCat,,,,0,,,"also, update env.sh to use latest Pig release

NO PRECOMMIT TESTS
",,ekoifman,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/May/14 01:14;ekoifman;HIVE-7057.patch;https://issues.apache.org/jira/secure/attachment/12644751/HIVE-7057.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392393,,,,Thu Nov 13 19:40:45 UTC 2014,,,,,,,,,,"0|i1vkqn:",392578,,,,,,,,,,,,,,,,,,,,,"14/May/14 01:14;ekoifman;@Thejas could you review this?  When checking in please ""chmod u+x"" on all .sh files.  The patch files can't capture this.;;;","14/May/14 17:27;thejas;+1
;;;","30/May/14 21:22;thejas;Patch committed to trunk. Thanks for the contribution Eugene!
;;;","10/Jun/14 18:39;thejas;I forgot to update the permissions of the files. I have done that now.

chmod+svn-commit does not update the permissions in svn, so I did the following before commit -

cd  hcatalog/src/test/e2e/templeton/deployers/
svn propset svn:executable true *.sh
;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
config not propagating for PTFOperator,HIVE-7055,12714067,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,14/May/14 00:21,13/Nov/14 19:40,14/Jul/23 06:14,16/May/14 15:33,0.12.0,0.13.0,,,,,,,,0.14.0,,PTF-Windowing,,,,0,,,e.g. setting hive.join.cache.size has no effect and task nodes always got default value of 25000,,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/May/14 01:27;ashutoshc;HIVE-7055.1.patch;https://issues.apache.org/jira/secure/attachment/12645155/HIVE-7055.1.patch","15/May/14 07:59;ashutoshc;HIVE-7055.1.patch;https://issues.apache.org/jira/secure/attachment/12644977/HIVE-7055.1.patch","14/May/14 00:24;ashutoshc;HIVE-7055.patch;https://issues.apache.org/jira/secure/attachment/12644740/HIVE-7055.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392380,,,,Thu Nov 13 19:40:04 UTC 2014,,,,,,,,,,"0|i1vknz:",392566,,,,,,,,,,,,,,,,,,,,,"14/May/14 00:24;ashutoshc;https://reviews.apache.org/r/21420/;;;","14/May/14 16:09;rhbutani;+1;;;","14/May/14 18:13;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12644740/HIVE-7055.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/191/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/191/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12644740;;;","15/May/14 08:58;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12644977/HIVE-7055.1.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/197/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/197/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12644977;;;","16/May/14 07:44;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12645155/HIVE-7055.1.patch

{color:red}ERROR:{color} -1 due to 21 failed/errored test(s), 5524 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_java_method
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_reflect
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_math_funcs
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_script_pipe
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_transform1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_minimr_broken_pipe
org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input20
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input5
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/208/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/208/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 21 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12645155;;;","16/May/14 15:33;ashutoshc;Committed to trunk. Failures are due to recent switch to jdk7 and is consistent with other recent runs.;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to fetch column stats from decimal columns,HIVE-7053,12713815,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,xuefuz,xuefuz,xuefuz,13/May/14 03:43,13/Nov/14 19:39,14/Jul/23 06:14,08/Jun/14 18:16,0.13.0,,,,,,,,,0.14.0,,Statistics,,,,0,,,"After HIVE-6701, column stats for decimal columns can be computed. However, when the stats are fetched, nothing is returned. The problem was originally reproducible using thrift API. With the patch in HIVE-7050, the problem can be also produced using ""desc formatted table column"".

{code}
hive> desc formatted dec i;   
OK
# col_name            	data_type           	min                 	max                 	num_nulls           	distinct_count      	avg_col_len         	max_col_len         	num_trues           	num_falses          	comment             
	 	 	 	 	 	 	 	 	 	 
i                   	int                 	0                   	4                   	0                   	5                   	null                	null                	null                	null                	from deserializer   
hive> desc formatted dec d;
OK
# col_name            	data_type           	min                 	max                 	num_nulls           	distinct_count      	avg_col_len         	max_col_len         	num_trues           	num_falses          	comment             
	 	 	 	 	 	 	 	 	 	 
d                   	decimal(5,2)        	from deserializer   	 	 	 	 	 	 	 	 
{code}",,ishaan,nongli,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/14 05:00;xuefuz;HIVE-7053.patch;https://issues.apache.org/jira/secure/attachment/12648853/HIVE-7053.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392131,,,,Thu Nov 13 19:39:59 UTC 2014,,,,,,,,,,"0|i1vj53:",392325,,,,,,,,,,,,,,,,,,,,,"08/Jun/14 04:55;ashutoshc;+1;;;","08/Jun/14 05:00;xuefuz;Newly attached patch addressed:
1. Rebased with trunk
2. Added a test case.;;;","08/Jun/14 09:56;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12648853/HIVE-7053.patch

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 5530 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testListPartitions
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testNameMethods
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testPartition
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.testPropertiesParsing
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/409/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/409/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-409/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12648853;;;","08/Jun/14 18:16;ashutoshc;Committed to trunk. Thanks, Xuefu!;;;","13/Nov/14 19:39;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize split calculation time,HIVE-7052,12713814,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,rajesh.balamohan,rajesh.balamohan,rajesh.balamohan,13/May/14 03:02,17/Jun/15 22:14,14/Jul/23 06:14,02/Jun/14 22:59,,,,,,,,,,0.14.0,,,,,,0,performance,,"When running a TPC-DS query (query_27),  significant amount of time was spent in split computation on a dataset of size 200 GB (ORC format).

Profiling revealed that, 
1. Lot of time was spent in Config's subtitutevar (regex) in HiveInputFormat.getSplits() method.  
2. FileSystem was created repeatedly in OrcInputFormat.generateSplitsInfo(). 

I will attach the profiler snapshots soon.",hive + tez,prasanth_j,rajesh.balamohan,sershe,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-11035,,,,,,,,,,,,,,,,,,,,,"13/May/14 03:04;rajesh.balamohan;HIVE-7052-profiler-1.png;https://issues.apache.org/jira/secure/attachment/12644546/HIVE-7052-profiler-1.png","13/May/14 03:04;rajesh.balamohan;HIVE-7052-profiler-2.png;https://issues.apache.org/jira/secure/attachment/12644547/HIVE-7052-profiler-2.png","13/May/14 04:27;rajesh.balamohan;HIVE-7052-v3.patch;https://issues.apache.org/jira/secure/attachment/12644555/HIVE-7052-v3.patch","30/May/14 03:23;rajesh.balamohan;HIVE-7052-v7.patch;https://issues.apache.org/jira/secure/attachment/12647522/HIVE-7052-v7.patch","02/Jun/14 06:40;prasanth_j;HIVE-7052.7.patch;https://issues.apache.org/jira/secure/attachment/12647870/HIVE-7052.7.patch",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392130,Reviewed,,,Thu Nov 13 19:44:22 UTC 2014,,,,,,,,,,"0|i1vj4v:",392324,,,,,,,,,,,,,,,,,,,,,"13/May/14 03:17;rajesh.balamohan;https://reviews.apache.org/r/21357/diff/#index_header;;;","16/May/14 19:18;prasanth_j;Mostly looks good. Left a minor nit in RB.;;;","30/May/14 06:35;prasanth_j;+1;;;","30/May/14 06:36;prasanth_j;I think the patch is not in proper format. Hive QA does not seem to pick up this patch. Can you reupload with HIVE-XXXX.7.patch name?;;;","30/May/14 06:40;prasanth_j;Looks like the patch is already queued up. http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/
Renaming is not required. ;;;","02/Jun/14 06:40;prasanth_j;Renamed patch for hive QA to pickup;;;","02/Jun/14 13:37;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647870/HIVE-7052.7.patch

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 5510 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_display_colstats_tbllvl
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/368/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/368/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-368/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647870;;;","02/Jun/14 22:44;rajesh.balamohan;Failures are not related to this patch.;;;","02/Jun/14 22:59;prasanth_j;Patch committed to trunk. Thanks [~rajesh.balamohan] for the contribution.;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Display partition level column stats in DESCRIBE FORMATTED PARTITION,HIVE-7051,12713809,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,prasanth_j,prasanth_j,13/May/14 01:56,13/Nov/14 19:42,14/Jul/23 06:14,23/Jun/14 09:05,,,,,,,,,,0.14.0,,Statistics,,,,0,,,Same as HIVE-7050 but for partitions,,hagleitn,leftyl,prasanth_j,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7255,,,,,,,,,,,,,,HIVE-7050,,,,,,,,,,,,,,,,,,,,,"19/Jun/14 20:06;ashutoshc;HIVE-7051.1.patch;https://issues.apache.org/jira/secure/attachment/12651481/HIVE-7051.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392125,,,,Thu Nov 13 19:42:06 UTC 2014,,,,,,,,,,"0|i1vj3r:",392319,"Added ""Display column statistics"" section to https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-Describe",,,,,,,,,,,,,,,,,,,,"19/Jun/14 20:06;ashutoshc;Patch to implement the same.;;;","19/Jun/14 20:11;ashutoshc;https://reviews.apache.org/r/22782/;;;","19/Jun/14 20:46;hagleitn;+1;;;","21/Jun/14 06:59;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12651481/HIVE-7051.1.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5653 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/537/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/537/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-537/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12651481;;;","23/Jun/14 09:05;ashutoshc;Committed to trunk.;;;","11/Oct/14 07:35;leftyl;Documented in the wiki here:

* [Statistics in Hive -- Existing Tables | https://cwiki.apache.org/confluence/display/Hive/StatsDev#StatsDev-ExistingTables]
* [DDL -- Display Column Statistics | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-DisplayColumnStatistics];;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Display table level column stats in DESCRIBE FORMATTED TABLE,HIVE-7050,12713808,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,prasanth_j,prasanth_j,prasanth_j,13/May/14 01:56,13/Nov/14 19:41,14/Jul/23 06:14,01/Jun/14 22:13,,,,,,,,,,0.14.0,,Statistics,,,,0,,,There is currently no way to display the column level stats from hive CLI. It will be good to show them in DESCRIBE EXTENDED/FORMATTED TABLE,,leftyl,prasanth_j,thejas,wzheng,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7051,,,,,,,,,,,,,,,,,,,,,,,,"13/May/14 03:04;prasanth_j;HIVE-7050.1.patch;https://issues.apache.org/jira/secure/attachment/12644548/HIVE-7050.1.patch","14/May/14 05:45;prasanth_j;HIVE-7050.2.patch;https://issues.apache.org/jira/secure/attachment/12644776/HIVE-7050.2.patch","14/May/14 18:25;prasanth_j;HIVE-7050.3.patch;https://issues.apache.org/jira/secure/attachment/12644860/HIVE-7050.3.patch","15/May/14 08:16;prasanth_j;HIVE-7050.4.patch;https://issues.apache.org/jira/secure/attachment/12644978/HIVE-7050.4.patch","17/May/14 03:06;prasanth_j;HIVE-7050.5.patch;https://issues.apache.org/jira/secure/attachment/12645388/HIVE-7050.5.patch","30/May/14 07:49;prasanth_j;HIVE-7050.6.patch;https://issues.apache.org/jira/secure/attachment/12647552/HIVE-7050.6.patch",,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392124,,,,Thu Nov 13 19:41:13 UTC 2014,,,,,,,,,,"0|i1vj3j:",392318,"Added ""Display column statistics"" section to https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-Describe",,,,,,,,,,,,,,,,,,,,"13/May/14 03:07;prasanth_j;attaching RB link;;;","13/May/14 22:36;xuefuz;Thanks for the patch. Minor comments/questions on RB.

One clarification: are column stats shown when either EXTENDED or FORMATTED is specified? And only when column is specified? I think this is important for documentation purpose. It would be good if functional details can be put in the description area.;;;","14/May/14 05:45;prasanth_j;Addressed [~xuefuz]'s review comments.;;;","14/May/14 05:49;prasanth_j;Column stats are stored only when a column is specified and only when FORMATTED is specified. It does NOT show for EXTENDED because extended output does not show the column names at the top which makes it difficult to comprehend the column stats output.;;;","14/May/14 16:52;xuefuz;Patch looks good. I left a couple of minor comments on rb.;;;","14/May/14 18:25;prasanth_j;Addressed [~xuefuz]'s review comments. Left reply in RB. RB is flaky now will update the patch in RB later.;;;","15/May/14 08:16;prasanth_j;Addressed Xuefu's comments in RB.;;;","15/May/14 20:34;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12644978/HIVE-7050.4.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/201/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/201/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12644978;;;","17/May/14 03:06;prasanth_j;Addressed Xuefu's review comments;;;","17/May/14 05:03;xuefuz;+1, pending on test result.;;;","18/May/14 07:22;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12645388/HIVE-7050.5.patch

{color:red}ERROR:{color} -1 due to 19 failed/errored test(s), 5451 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_describe_syntax
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_describe_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_java_method
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_reflect
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_math_funcs
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_stats_counter
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input20
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input5
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/226/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/226/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 19 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12645388;;;","19/May/14 18:08;xuefuz;[~prashuj] Could you take a look at the test failures above to see if they are related to your patch?;;;","19/May/14 18:10;xuefuz;Sorry, but the above comment was intended for [~prasanth_j]. :);;;","20/May/14 21:30;xuefuz;[~prasanth_j] Could you give an analysis of the above test failures?;;;","20/May/14 22:00;prasanth_j;[~xuefuz] only describe_syntax and describe_table seems to be related. Other test failures are unrelated and are tracked else where. I quickly ran describe_syntax and describe_table and found that the diffs are showing additional spaces. I will analyze more to see if those spaces valid or added as a side effect. Will post a new patch later today with the fix.;;;","30/May/14 07:49;prasanth_j;This patch fixes the relevant test failures. Another change is when column stats is not available then empty string is displayed instead of ""null"" to be inline with how comments are handled (empty instead of null).

[~xuefuz] sorry for getting back late on this. Can you please take a look again at this patch?;;;","31/May/14 20:05;xuefuz;+1;;;","01/Jun/14 22:13;xuefuz;Patch committed to trunk. Thanks to Prasanth J for the contribution.;;;","05/Jun/14 19:30;ashutoshc;[~prasanth_j] Does this also support display of column stats for a particular partition of table? Test case doesnt cover it, so not sure. I was hoping following syntax to work, but seems like not supported yet.
{code}
describe formatted T partition (k1=v1) c1;
{code};;;","05/Jun/14 19:40;prasanth_j;No it is not supported yet. HIVE-7051 is created to support it. ;;;","06/Jun/14 04:03;ashutoshc;Ok..cool;;;","07/Oct/14 22:32;prasanth_j;[~leftylev] I added ""Display column statistics"" section here https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-Describe
which should cover the features added in this jira as well as HIVE-7051. Can you take a look to see if its good?;;;","07/Oct/14 22:57;leftyl;Thanks, [~prasanth_j], especially for HIVE-7051 which had slipped through my net.

Your new section is good, but needs version information -- I'll add that, plus a link to the ANALYZE syntax, and a bit of editorial tinkering.  Then you can verify my changes.

Would you please change the release note (which currently says ""Please document the new functionality"") and add one on HIVE-7051 too?

Q:  Does this only work for FORMATTED, not EXTENDED (although it's in both jira titles)?;;;","07/Oct/14 23:00;prasanth_j;Done. Updated release notes and title of both JIRAs to say only FORMATTED.
;;;","11/Oct/14 07:34;leftyl;Besides the doc changes already mentioned, I added the FOR COLUMNS option to ANALYZE in the Statistics doc for HIVE-1362:

* [Statistics in Hive -- Existing Tables | https://cwiki.apache.org/confluence/display/Hive/StatsDev#StatsDev-ExistingTables]
* [DDL -- Display Column Statistics | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-DisplayColumnStatistics];;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to deserialize AVRO data when file schema and record schema are different and nullable,HIVE-7049,12713757,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,daijy,kamrul,kamrul,12/May/14 21:46,23/Sep/15 17:00,14/Jul/23 06:14,06/Apr/15 21:06,,,,,,,,,,1.2.0,,,,,,0,,,"It mainly happens when 
1 )file schema and record schema are not same
2 ) Record schema is nullable  but file schema is not.

The potential code location is at class AvroDeserialize
 
{noformat}
 if(AvroSerdeUtils.isNullableType(recordSchema)) {
      return deserializeNullableUnion(datum, fileSchema, recordSchema, columnType);
    }
{noformat}

In the above code snippet, recordSchema is verified if it is nullable. But the file schema is not checked.

I tested with these values:
{noformat}
recordSchema= [""null"",""string""]
fielSchema= ""string""
{noformat}

And i got the following exception <line numbers might not be the same due to mu debugged code version>.

{noformat}
org.apache.avro.AvroRuntimeException: Not a union: ""string"" 
        at org.apache.avro.Schema.getTypes(Schema.java:272)
        at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserializeNullableUnion(AvroDeserializer.java:275)
        at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.worker(AvroDeserializer.java:205)
        at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.workerBase(AvroDeserializer.java:188)
        at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserialize(AvroDeserializer.java:174)
        at org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.verifyNullableType(TestAvroDeserializer.java:487)
        at org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.canDeserializeNullableTypes(TestAvroDeserializer.java:407)

{noformat}
",,brocknoland,daijy,erwaman,hagleitn,jonathan.bender,kamrul,mdominguez@cloudera.com,smallem,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-3159,,,,"13/May/14 00:30;kamrul;HIVE-7049.1.patch;https://issues.apache.org/jira/secure/attachment/12644526/HIVE-7049.1.patch","31/Mar/15 01:59;daijy;HIVE-7049.2.patch;https://issues.apache.org/jira/secure/attachment/12708290/HIVE-7049.2.patch","31/Mar/15 01:59;daijy;Statistic;https://issues.apache.org/jira/secure/attachment/12708288/Statistic","31/Mar/15 01:59;daijy;Statistics10Min.avsc;https://issues.apache.org/jira/secure/attachment/12708289/Statistics10Min.avsc",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,392073,,,,Mon Apr 06 21:06:02 UTC 2015,,,,,,,,,,"0|i1visf:",392268,,,,,,,,,,,,,,,,,,,,,"13/May/14 00:30;kamrul;patch uploaded;;;","13/May/14 00:57;kamrul;RB at: https://reviews.apache.org/r/21353/;;;","13/May/14 04:41;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12644526/HIVE-7049.1.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/188/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/188/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12644526;;;","13/May/14 17:22;xuefuz;Thanks for bringing this up.  I'm wondering if the situation you described is an issue of incompatibility of schemas rather than a bug. Record schema says that a field is union (nullable), while file schema says that the file is not a union, which seems suggesting that the data is not compatible with the schema. While we may need to provided a better error message for this, ignoring the file schema (by passing NULL down) will very likely break decimal support, which needs the file schema to read data correctly.;;;","14/May/14 08:11;kamrul;Thank [~xuefuz] for the comments.

I believe it is a valid Avro schema evolution.
Please see the following comments copied from  the link:
http://avro.apache.org/docs/1.7.6/spec.html#Schema+Resolution
{noformat}
* if reader's is a union, but writer's is not
The first schema in the reader's union that matches the writer's schema is recursively resolved against it. If none match, an error is signalled.
* if writer's is a union, but reader's is not
If the reader's schema matches the selected writer's schema, it is recursively resolved against it. If they do not match, an error is signalled.
{noformat}

Moreover, i tested a similar scenarios using pure avro code where i wrote using schema ""string"" and read it using [""null"",""string""].;;;","14/May/14 16:22;xuefuz;[~kamrul] If Hive can support the AVRO schema resolutions you mentioned, I don't see any obstacles. However, the fix in your patch seems having a problem with decimal, which may need more deliberation.;;;","15/May/14 01:53;kamrul;Thanks @xzhang.
>However, the fix in your patch seems having a problem with decimal, which may need more deliberation.

What is the (potential) problem in decimal?
Any proposal what to do to address the decimal problem?

;;;","16/May/14 08:21;kamrul;[~xuefuz] : can you please help me to understand the problem mentioned in the previous comment?
;;;","16/May/14 17:19;xuefuz;It seems that your patch tries to fix the issue by ignoring the file schema ( passing NULL down). File schema is needed to read decimal data correctly. Thus, we might need to fix in a different way.;;;","17/May/14 03:13;kamrul;Null is passed only if record schema is null but file schema is not null.
Do you see any use case for decimal too?

>Thus, we might need to fix in a different way.

Do you want me to fix it differently? or you are looking to address this for decimal differently?

;;;","17/May/14 05:11;xuefuz;{code}
+    if (AvroSerdeUtils.isNullableType(recordSchema)) {
+      Schema tmpFileSchema = fileSchema;
+      if (tmpFileSchema == null || !AvroSerdeUtils.isNullableType(tmpFileSchema)) {
+	tmpFileSchema = null;
+      }
+      return deserializeNullableUnion(datum, tmpFileSchema, recordSchema, columnType);
     }
{code}

If fileSchema is not null, but AvroSerdeUtils.isNullableType(tmpFileSchema) returns false, then tmpFileSchema = null. So you pass null as fileSchema in  deserializeNullableUnion(datum, tmpFileSchema, recordSchema, columnType).  This doesn't seem right.
;;;","10/Sep/14 23:47;smallem;I'm seeing a similar issue but with ""long"" datatype. 

Hive version: 0.13.1

Here is the error:
{code}
exception java.io.IOException:org.apache.avro.AvroRuntimeException: Not a union: ""long""
{code}

Here is the error from Log:
{code}
2014-09-10 23:45:05,679 ERROR CliDriver (SessionState.java:printError(545)) - Failed with exception java.io.IOException:org.apache.avro.AvroRuntimeException: Not a union: ""long""
java.io.IOException: org.apache.avro.AvroRuntimeException: Not a union: ""long""
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:636)
        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:534)
        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:137)
        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1519)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:285)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
Caused by: org.apache.avro.AvroRuntimeException: Not a union: ""long""
        at org.apache.avro.Schema.getTypes(Schema.java:266)
        at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserializeNullableUnion(AvroDeserializer.java:269)
        at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.worker(AvroDeserializer.java:200)
        at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.workerBase(AvroDeserializer.java:188)
        at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserialize(AvroDeserializer.java:174)
        at org.apache.hadoop.hive.serde2.avro.AvroSerDe.deserialize(AvroSerDe.java:99)
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:620)
{code}
;;;","18/Dec/14 18:24;jonathan.bender;Seems like we can get away with the following patch (confirm the fileSchema AKA writer's schema is actually a union type before trying to find the type that the reader schema expects).  If not, just use the schema as is (it should be promoted to a union by Avro).

This worked for me in local testing.

{noformat}
diff --git a/src/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java b/src/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java
index ce933ff..032761c 100644
--- a/src/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java
+++ b/src/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java
@@ -265,9 +265,12 @@ private Object deserializeNullableUnion(Object datum, Schema fileSchema, Schema
     if(schema.getType().equals(Schema.Type.NULL)) {
       return null;
     }
+    Schema writerSchema = fileSchema;
+    if (writerSchema != null && writerSchema.getType().equals(Schema.Type.UNION)) {
+      writerSchema = writerSchema.getTypes().get(tag);  
+    }
 
-    return worker(datum, fileSchema == null ? null : fileSchema.getTypes().get(tag), schema,
-        SchemaToTypeInfo.generateTypeInfo(schema));
+    return worker(datum, writerSchema, schema, SchemaToTypeInfo.generateTypeInfo(schema));
 
   }
{noformat};;;","31/Mar/15 01:59;daijy;Hit the same issue and here is the reproducible steps:
{code}
CREATE TABLE Statistics10Min 
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' 
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat' 
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' 
LOCATION '/user/daijy/data/Statistics10Min/' 
TBLPROPERTIES ('avro.schema.url'='/user/daijy/Statistics10Min/Statistics10Min.avsc');
{code}
select * from Statistics10Min fail with the message:
{code}
Failed with exception java.io.IOException:org.apache.avro.AvroRuntimeException: Not a record: [""null"",{""type"":""record"",""name"":""Statistic"",""namespace"":""SHARP"",""fields"":[{""name"":""WindFarm"",""type"":[""null"",""string""]},{""name"":""WTG"",""type"":[""null"",""string""]},{""name"":""LocalTS"",""type"":[""null"",""string""]},{""name"":""TS"",""type"":[""null"",""string""]},{""name"":""WindSpeedAvg"",""type"":""string""},{""name"":""WindSpeedMin"",""type"":""string""},{""name"":""WindSpeedMax"",""type"":""string""},{""name"":""WindSpeedStdDev"",""type"":""string""},{""name"":""ActPowAvg"",""type"":""string""},{""name"":""ActPowMin"",""type"":""string""},{""name"":""ActPowMax"",""type"":""string""},{""name"":""ActPowStdDev"",""type"":""string""},{""name"":""ReactPowAvg"",""type"":""string""},{""name"":""ReactPowMin"",""type"":""string""},{""name"":""ReactPowMax"",""type"":""string""},{""name"":""ReactPowStdDev"",""type"":""string""},{""name"":""GenSpeedAvg"",""type"":""string""},{""name"":""GenSpeedMin"",""type"":""string""},{""name"":""GenSpeedMax"",""type"":""string""},{""name"":""GenSpeedStdDev"",""type"":""string""},{""name"":""RotSpeedAvg"",""type"":""string""},{""name"":""RotSpeedMin"",""type"":""string""},{""name"":""RotSpeedMax"",""type"":""string""},{""name"":""RotSpeedStdDev"",""type"":""string""},{""name"":""YawDirAvg"",""type"":""string""},{""name"":""YawDirMin"",""type"":""string""},{""name"":""YawDirMax"",""type"":""string""},{""name"":""YawDirStdDev"",""type"":""string""},{""name"":""VibTowAvg"",""type"":""string""},{""name"":""VibTowMin"",""type"":""string""},{""name"":""VibTowMax"",""type"":""string""},{""name"":""VibTowStdDev"",""type"":""string""},{""name"":""PitchAvg"",""type"":""string""},{""name"":""PitchMin"",""type"":""string""},{""name"":""PitchMax"",""type"":""string""},{""name"":""PitchRateMin"",""type"":""string""},{""name"":""PitchRateMax"",""type"":""string""},{""name"":""PitchRateStdDev"",""type"":""string""},{""name"":""GridVoltAvg"",""type"":""int""},{""name"":""GridVoltMin"",""type"":""int""},{""name"":""GridVoltMax"",""type"":""int""},{""name"":""EnvTempAvg"",""type"":""string""},{""name"":""EnvTempMin"",""type"":""string""},{""name"":""EnvTempMax"",""type"":""string""},{""name"":""GbxTempAvg"",""type"":""string""},{""name"":""GbxTempMin"",""type"":""string""},{""name"":""GbxTempMax"",""type"":""string""},{""name"":""GenAccMin"",""type"":""string""},{""name"":""GenAccMax"",""type"":""string""},{""name"":""VibratAvg"",""type"":""string""},{""name"":""VibratMax"",""type"":""string""},{""name"":""GenU1TempAvg"",""type"":""string""},{""name"":""GenU1TempMin"",""type"":""string""},{""name"":""GenU1TempMax"",""type"":""string""},{""name"":""RotorSideLTempAvg"",""type"":""string""},{""name"":""RotorSideLTempMin"",""type"":""string""},{""name"":""RotorSideLTempMax"",""type"":""string""},{""name"":""WindBin"",""type"":""int""},{""name"":""FreqMean"",""type"":""string""}]}]
{code}
Stack:
{code}
        at org.apache.avro.Schema.getField(Schema.java:184)
        at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.workerBase(AvroDeserializer.java:191)
        at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserialize(AvroDeserializer.java:178)
        at org.apache.hadoop.hive.serde2.avro.AvroSerDe.deserialize(AvroSerDe.java:199)
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:488)
{code}

Note this is broken by HIVE-5823.

Attach HIVE-7049.2.patch. [~xuefuz], can you check if the new patch work with decimal?;;;","31/Mar/15 05:48;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12708290/HIVE-7049.2.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 8693 tests executed
*Failed tests:*
{noformat}
TestMinimrCliDriver-smb_mapjoin_8.q - did not produce a TEST-*.xml file
org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.testRevokeTimedOutWorkers
org.apache.hive.hcatalog.streaming.TestStreaming.testMultipleTransactionBatchCommits
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3213/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3213/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-3213/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12708290 - PreCommit-HIVE-TRUNK-Build;;;","01/Apr/15 23:15;ashutoshc;Daniel's approach of extracting actual type from a nullable type looks ok to me.
+1;;;","06/Apr/15 21:06;hagleitn;Tests passed locally. Committed to trunk. Thanks [~daijy] and [~ashutoshc]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong results in multi-table insert aggregating without group by clause,HIVE-7045,12713640,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,navis,dimamah,dimamah,12/May/14 14:08,13/Nov/14 19:43,14/Jul/23 06:14,07/Jul/14 22:07,0.10.0,0.12.0,,,,,,,,0.14.0,,,,,,0,,,"This happens whenever there are more than 1 reducers.

The scenario :

CREATE  TABLE t1 (a int, b int);
CREATE  TABLE t2 (cnt int) PARTITIONED BY (var_name string);

insert into table t1 select 1,1 from asd limit 1;
insert into table t1 select 2,2 from asd limit 1;

t1 contains :
1 1
2 2

from  t1
insert overwrite table t2 partition(var_name='a') select count(a) cnt 
insert overwrite table t2 partition(var_name='b') select count(b) cnt ;

select * from t2;
returns : 
2 a
2 b

as expected.

Setting the number of reducers higher than 1 :

set mapred.reduce.tasks=2;

from  t1
insert overwrite table t2 partition(var_name='a') select count(a) cnt
insert overwrite table t2 partition(var_name='b') select count(b) cnt;

select * from t2;
1	a
1	a
1	b
1	b

Wrong results.

This happens when ever t1 is big enough to automatically generate more than 1 reducers and without specifying it directly.

adding ""group by 1"" in the end of each insert solves the problem :

from  t1
insert overwrite table t2 partition(var_name='a') select count(a) cnt group by 1
insert overwrite table t2 partition(var_name='b') select count(b) cnt group by 1;

generates : 
2 a
2 b

This should work without the group by...
The number of rows for each partition will be the amount of reducers.
Each reducer calculated a sub total of the count.

",,dimamah,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jul/14 06:36;navis;HIVE-7045.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12654053/HIVE-7045.1.patch.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391956,,,,Thu Nov 13 19:43:09 UTC 2014,,,,,,,,,,"0|i1vi4n:",392159,,,,,,,,,,,,,,,,,,,,,"04/Jul/14 11:01;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12654053/HIVE-7045.1.patch.txt

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5676 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/679/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/679/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-679/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12654053;;;","07/Jul/14 16:55;ashutoshc;+1;;;","07/Jul/14 22:07;ashutoshc;Committed to trunk. Thanks, Navis!;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When using the tez session pool via hive, once sessions time out, all queries go to the default queue",HIVE-7043,12713422,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vikram.dixit,vikram.dixit,vikram.dixit,09/May/14 23:42,13/Nov/14 19:42,14/Jul/23 06:14,27/May/14 19:56,0.13.0,,,,,,,,,0.14.0,,HiveServer2,,,,0,,,"When using a tez session pool to run multiple queries, once the sessions time out, we always end up using the default queue to launch queries. The load balancing doesn't work in this case.",,dang@cloudera.com,hagleitn,navis,thejas,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/May/14 22:43;vikram.dixit;HIVE-7043.2.patch;https://issues.apache.org/jira/secure/attachment/12644506/HIVE-7043.2.patch","12/May/14 23:31;vikram.dixit;HIVE-7043.3.patch;https://issues.apache.org/jira/secure/attachment/12644516/HIVE-7043.3.patch","19/May/14 23:31;vikram.dixit;HIVE-7043.4.patch;https://issues.apache.org/jira/secure/attachment/12645677/HIVE-7043.4.patch","13/May/14 08:33;vikram.dixit;HIVE-7043.4.patch;https://issues.apache.org/jira/secure/attachment/12644588/HIVE-7043.4.patch","28/May/14 04:59;navis;HIVE-7043.followup.patch.txt;https://issues.apache.org/jira/secure/attachment/12647054/HIVE-7043.followup.patch.txt",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391738,,,,Thu Nov 13 19:42:31 UTC 2014,,,,,,,,,,"0|i1vgtb:",391945,,,,,,,,,,,,,,,,,,,,,"12/May/14 22:40;hagleitn;+1;;;","12/May/14 22:44;vikram.dixit;Resubmitting so that hive-qa picks it up.;;;","12/May/14 23:31;vikram.dixit;Minor nit fixed. Queue name could potentially be null.;;;","13/May/14 04:27;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12644516/HIVE-7043.3.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/186/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/186/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12644516;;;","13/May/14 08:33;vikram.dixit;Re-uploading as Hive QA failed to run tests.;;;","19/May/14 23:31;vikram.dixit;Re-uploading for jenkins to pick up this patch.;;;","20/May/14 06:09;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12645677/HIVE-7043.4.patch

{color:red}ERROR:{color} -1 due to 13 failed/errored test(s), 5451 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_partitions_filter2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_20
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/239/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/239/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logsPreCommit-HIVE-Build-239/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 13 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12645677;;;","24/May/14 01:09;vikram.dixit;Test failures are unrelated.;;;","27/May/14 19:56;vikram.dixit;Committed to trunk.;;;","27/May/14 19:57;vikram.dixit;Thanks [~hagleitn] for the review.;;;","28/May/14 04:59;navis;[~vikram.dixit] TestTezTask#testSubmit fails with NPE. Consider this patch. ;;;","28/May/14 05:43;vikram.dixit;+1

How should this be committed? Do you want to raise another jira or revert the commit and re-apply?

Thanks
Vikram.;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix stats_partscan_1_23.q and orc_createas1.q for hadoop-2,HIVE-7042,12713421,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,prasanth_j,prasanth_j,prasanth_j,09/May/14 23:39,13/Nov/14 19:41,14/Jul/23 06:14,12/May/14 18:47,0.14.0,,,,,,,,,0.14.0,,,,,,0,,,stats_partscan_1_23.q and orc_createas1.q should use HiveInputFormat as opposed to CombineHiveInputFormat. RCFile uses DefaultCodec for compression (uses DEFLATE) which is not splittable. Hence using CombineHiveIF will yield different results for these tests. ORC should use HiveIF to generate ORC splits.,,prasanth_j,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/May/14 23:41;prasanth_j;HIVE-7042.1.patch;https://issues.apache.org/jira/secure/attachment/12644222/HIVE-7042.1.patch","12/May/14 18:25;prasanth_j;HIVE-7042.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12644453/HIVE-7042.1.patch.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391737,,,,Thu Nov 13 19:41:41 UTC 2014,,,,,,,,,,"0|i1vgt3:",391944,,,,,,,,,,,,,,,,,,,,,"09/May/14 23:42;prasanth_j;[~ashutoshc] can you take a look at this patch?;;;","10/May/14 01:15;ashutoshc;Thanks [~prasanth_j] for taking this up. I assume you have tested this on both mac & linux. +1;;;","10/May/14 23:30;prasanth_j;[~ashutoshc] Yes. I tested it with mac and linux, the results are consistent.;;;","12/May/14 18:25;prasanth_j;Not sure why this patch was not picked up HIVE QA for days. Reuploading the patch again.;;;","12/May/14 18:47;ashutoshc;Tested this manually on mac os & on ubuntu. Results are consistent. Committed to trunk. Thanks, Prasanth!;;;","12/May/14 19:03;prasanth_j;Thanks Ashutosh!;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DoubleWritable/ByteWritable should extend their hadoop counterparts,HIVE-7041,12713407,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,09/May/14 22:33,13/Nov/14 19:39,14/Jul/23 06:14,20/May/14 02:04,,,,,,,,,,0.14.0,,,,,,0,,,"Hive has its own implementations of ByteWritable/DoubleWritable/ShortWritable.  We cannot replace usage of these classes since they will break 3rd party UDFs/SerDes, however we can at least extend from the Hadoop version of these classes when possible to avoid duplicate code.

When Hive finally moves to version 1.0 we might want to consider removing use of these Hive-specific writables and switching over to using the Hadoop version of these classes.

ShortWritable didn't exist in Hadoop until 2.x so it looks like we can't do it with this class until 0.20/1.x support is dropped from Hive.",,jdere,szehon,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/May/14 02:08;szehon;HIVE-7041.1.patch;https://issues.apache.org/jira/secure/attachment/12645379/HIVE-7041.1.patch","14/May/14 00:51;jdere;HIVE-7041.1.patch;https://issues.apache.org/jira/secure/attachment/12644746/HIVE-7041.1.patch","09/May/14 22:48;jdere;HIVE-7041.1.patch;https://issues.apache.org/jira/secure/attachment/12644205/HIVE-7041.1.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391723,,,,Thu Nov 13 19:39:58 UTC 2014,,,,,,,,,,"0|i1vgqf:",391931,,,,,,,,,,,,,,,,,,,,,"10/May/14 01:12;ashutoshc;+1;;;","13/May/14 16:38;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12644205/HIVE-7041.1.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/190/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/190/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12644205;;;","14/May/14 00:51;jdere;tests didn't run for some reason, re-upload patch.;;;","14/May/14 22:13;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12644746/HIVE-7041.1.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/193/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/193/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12644746;;;","17/May/14 02:08;szehon;Attaching it again.  The last failure was during java7 upgrade and tests were failing due to env.;;;","19/May/14 18:41;hiveqa;There was an error with test result posting on JIRA, posting manually.

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12645379/HIVE-7041.1.patch

{color:red}ERROR:{color} -1 due to 18 failed/errored test(s), 5450 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_binary_search
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_java_method
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_reflect
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_math_funcs
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input20
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input5
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/231/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/231/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 18 tests failed
{noformat};;;","20/May/14 02:04;ashutoshc;Committed to trunk. Thanks, Jason!;;;","13/Nov/14 19:39;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add additional tests for transform clauses with Tez,HIVE-7037,12713175,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,08/May/14 20:19,13/Nov/14 19:43,14/Jul/23 06:14,13/May/14 01:36,,,,,,,,,,0.14.0,,Tez,,,,0,,,Enabling some q tests for Tez wrt to ScriptOperator/Stream/Transform.,,hagleitn,thejas,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/May/14 20:22;hagleitn;HIVE-7037.1.patch;https://issues.apache.org/jira/secure/attachment/12644001/HIVE-7037.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391491,,,,Thu Nov 13 19:43:09 UTC 2014,,,,,,,,,,"0|i1vfbz:",391704,,,,,,,,,,,,,,,,,,,,,"08/May/14 20:50;vikram.dixit;LGTM +1.;;;","11/May/14 01:46;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12644001/HIVE-7037.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5436 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/168/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/168/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12644001;;;","12/May/14 22:10;hagleitn;Test failures are unrelated.;;;","13/May/14 01:36;hagleitn;Committed to trunk. Thanks Vikram for the review!;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Templeton returns 500 for user errors - when job cannot be found,HIVE-7035,12712999,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,08/May/14 01:41,03/Nov/15 20:51,14/Jul/23 06:14,12/May/14 19:12,0.13.0,,,,,,,,,0.14.0,,WebHCat,,,,0,,,"curl -i 'http://localhost:50111/templeton/v1/jobs/job_1399496111138_00011?user.name=ekoifman'
 should return HTTP Status code 4xx when no such job exists; it currently returns 500.

{noformat}
{""error"":""org.apache.hadoop.yarn.exceptions.ApplicationNotFoundException: Application with id 'application_201304291205_0015' doesn't exist in RM.\r\n\tat org.apache.hadoop.yarn.server.resourcemanager
.ClientRMService.getApplicationReport(ClientRMService.java:247)\r\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplicationReport(ApplicationClientProtocol
PBServiceImpl.java:120)\r\n\tat org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:241)\r\n\tat org.apache.hado
op.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\r\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Serve
r.java:2053)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.ja
va:415)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2047)\r\n""}
{noformat}

NO PRECOMMIT TESTS",,ekoifman,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-12327,,,,,,,,,,,,,,,,,,,,,"08/May/14 18:22;ekoifman;HIVE-7035.patch;https://issues.apache.org/jira/secure/attachment/12643985/HIVE-7035.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391315,,,,Thu Nov 13 19:40:39 UTC 2014,,,,,,,,,,"0|i1vean:",391535,,,,,,,,,,,,,,,,,,,,,"08/May/14 18:52;thejas;+1;;;","12/May/14 19:12;thejas;Patch committed to trunk. Thanks for the contribution Eugene!
;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
grant statements should check if the role exists,HIVE-7033,12712975,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,thejas,thejas,thejas,07/May/14 23:40,13/Nov/14 19:42,14/Jul/23 06:14,13/May/14 16:11,0.13.0,,,,,,,,,0.14.0,,Authorization,SQLStandardAuthorization,,,0,,,"The following grant statement that grants to a role that does not exist succeeds, but it should result in an error.

> grant all on t1 to role nosuchrole;

",,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/May/14 21:55;thejas;HIVE-7033.1.patch;https://issues.apache.org/jira/secure/attachment/12644195/HIVE-7033.1.patch","09/May/14 23:05;thejas;HIVE-7033.2.patch;https://issues.apache.org/jira/secure/attachment/12644209/HIVE-7033.2.patch","12/May/14 20:27;thejas;HIVE-7033.3.patch;https://issues.apache.org/jira/secure/attachment/12644474/HIVE-7033.3.patch","12/May/14 20:50;thejas;HIVE-7033.4.patch;https://issues.apache.org/jira/secure/attachment/12644480/HIVE-7033.4.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391291,,,,Thu Nov 13 19:42:19 UTC 2014,,,,,,,,,,"0|i1ve5b:",391511,,,,,,,,,,,,,,,,,,,,,"09/May/14 22:14;thejas;Patch also fixes the handling of role names in some cases to be case insensitive.
;;;","09/May/14 23:05;thejas;HIVE-7033.2.patch - updating comment in .q file;;;","10/May/14 01:32;ashutoshc;Left a comment on RB. Check of role name existence should be done within transaction to avoid TOCTOU bug.;;;","12/May/14 20:27;thejas;Thanks for pointing that out Ashutosh!
HIVE-7033.3.patch - changes to avoid TOCTOU issue.;;;","12/May/14 20:50;thejas;HIVE-7033.4.patch - q.out files didn't have the comment update.;;;","12/May/14 20:55;ashutoshc;+1;;;","13/May/14 04:00;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12644480/HIVE-7033.4.patch

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 5506 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/183/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/183/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12644480;;;","13/May/14 16:11;ashutoshc;Committed to trunk. Thanks, Thejas!;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Utiltites.createEmptyFile uses File.Separator instead of Path.Separator to create an empty file in HDFS,HIVE-7031,12712934,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hsubramaniyan,hsubramaniyan,hsubramaniyan,07/May/14 20:36,13/Nov/14 19:42,14/Jul/23 06:14,09/May/14 23:23,,,,,,,,,,0.14.0,,,,,,0,,,"This leads to inconsitent HDFS naming for empty partition/tables where a file might be named as  hdfs://headnode0:9000/hive/scratch/hive_2
014-04-07_22-39-52_649_4046112898053848089-1/-mr-10010\0 in windows operating system",,hsubramaniyan,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/14 20:47;hsubramaniyan;HIVE-7031.1.patch;https://issues.apache.org/jira/secure/attachment/12643845/HIVE-7031.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391250,,,,Thu Nov 13 19:42:04 UTC 2014,,,,,,,,,,"0|i1vdw7:",391470,,,,,,,,,,,,,,,,,,,,,"07/May/14 20:47;hsubramaniyan;cc-ing [~ashutoshc] [~xuefuz] for review.;;;","09/May/14 09:16;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12643845/HIVE-7031.1.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5433 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority2
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/150/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/150/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12643845;;;","09/May/14 18:22;ashutoshc;+1;;;","09/May/14 18:54;thejas;+1
testCliDriver_schemeAuthority2 is a flaky test, it passed when I ran locally. Other tests failures are unrelated.
;;;","09/May/14 23:23;ashutoshc;Committed to trunk. Thanks, Hari!;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove hive.hadoop.classpath from hiveserver2.cmd,HIVE-7030,12712933,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hsubramaniyan,hsubramaniyan,hsubramaniyan,07/May/14 20:32,13/Nov/14 19:42,14/Jul/23 06:14,09/May/14 18:49,0.14.0,,,,,,,,,0.14.0,,HiveServer2,,,,0,,,This parameter is not used anywhere and should be removed.,,hsubramaniyan,leftyl,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/14 22:42;hsubramaniyan;HIVE-7030.1.patch;https://issues.apache.org/jira/secure/attachment/12643867/HIVE-7030.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391249,,,,Thu Nov 13 19:42:25 UTC 2014,,,,,,,,,,"0|i1vdvz:",391469,,,,,,,,,,,,,,,,,,,,,"07/May/14 22:42;hsubramaniyan;cc-ing [~vaibhavgumashta] for review.;;;","07/May/14 22:49;vgumashta;+1;;;","09/May/14 13:02;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12643867/HIVE-7030.1.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5433 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input41
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/151/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/151/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12643867;;;","09/May/14 18:49;vgumashta;Committed to trunk.

Thanks for the contribution [~hsubramaniyan]! Thanks for pointing out the issue [~leftylev]!;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive job fails when referencing a view that explodes an array,HIVE-7027,12712840,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,navis,ctang,ctang,07/May/14 14:29,27/Oct/15 23:41,14/Jul/23 06:14,12/May/14 16:20,,,,,,,,,,0.14.0,,Query Processor,,,,0,,,"For a table created with following DDL
CREATE TABLE test_issue (fileid int, infos ARRAY<STRUCT<user:INT>>, 
test_c STRUCT<user_c:STRUCT<age:INT>>), 
create a view that lateral view explodes the array column like
CREATE VIEW v_test_issue AS SELECT fileid, i.user, test_c.user_c.age FROM test_issue LATERAL VIEW explode(infos) info AS i; 

Querying the view such as:
SELECT *  FROM v_test_issue WHERE age = 25; 

Will failed with following errors:
{code}
java.lang.Exception: java.lang.RuntimeException: Error in configuring object
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:354)
Caused by: java.lang.RuntimeException: Error in configuring object
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:426)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:366)
        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:223)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        at java.lang.Thread.run(Thread.java:695)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)
        ... 11 more
Caused by: java.lang.RuntimeException: Error in configuring object
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
        at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:34)
        ... 16 more
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)
        ... 19 more
Caused by: java.lang.RuntimeException: Map operator initialization failed
        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:154)
        ... 24 more
Caused by: java.lang.RuntimeException: cannot find field test_c from [0:_col0, 1:_col5]
        at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:415)
        at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef(StandardStructObjectInspector.java:150)
        at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ExprNodeColumnEvaluator.java:55)
        at org.apache.hadoop.hive.ql.exec.ExprNodeFieldEvaluator.initialize(ExprNodeFieldEvaluator.java:53)
        at org.apache.hadoop.hive.ql.exec.ExprNodeFieldEvaluator.initialize(ExprNodeFieldEvaluator.java:53)
        at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:934)
        at org.apache.hadoop.hive.ql.exec.Operator.initEvaluatorsAndReturnStruct(Operator.java:960)
        at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:65)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
        at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
        at org.apache.hadoop.hive.ql.exec.Operator.initializeOp(Operator.java:401)
        at org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.initializeOp(LateralViewJoinOperator.java:109)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
        at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
        at org.apache.hadoop.hive.ql.exec.Operator.initializeOp(Operator.java:401)
        at org.apache.hadoop.hive.ql.exec.UDTFOperator.initializeOp(UDTFOperator.java:94)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
        at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
        at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:67)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
        at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
        at org.apache.hadoop.hive.ql.exec.Operator.initializeOp(Operator.java:401)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
        at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
        at org.apache.hadoop.hive.ql.exec.FilterOperator.initializeOp(FilterOperator.java:83)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
        at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
        at org.apache.hadoop.hive.ql.exec.FilterOperator.initializeOp(FilterOperator.java:83)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
        at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
        at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:189)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
        at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:424)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:133)
        ... 24 more
{code}",,bradruderman,ctang,erwaman,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-12228,,,,,,,,,,,,,,,,,,,,,"08/May/14 05:21;navis;HIVE-7027.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12643906/HIVE-7027.1.patch.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391156,,,,Thu Nov 13 19:42:01 UTC 2014,,,,,,,,,,"0|i1vdbj:",391377,,,,,,,,,,,,,,,,,,,,,"10/May/14 15:42;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12643906/HIVE-7027.1.patch.txt

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5504 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/164/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/164/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12643906;;;","10/May/14 17:21;ashutoshc;+1;;;","12/May/14 16:20;ashutoshc;Committed to trunk. Thanks, Navis!;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Escape control characters for explain result,HIVE-7024,12712758,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,Fixed,navis,navis,navis,07/May/14 04:40,12/Feb/15 23:40,14/Jul/23 06:14,19/Dec/14 01:28,,,,,,,,,,1.1.0,,,,,,0,,,"Comments for columns are now delimited by 0x00, which is binary and make git refuse to make proper diff file.",,navis,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-8423,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/14 05:41;navis;HIVE-7024.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12643692/HIVE-7024.1.patch.txt","26/Jun/14 02:45;navis;HIVE-7024.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12652541/HIVE-7024.2.patch.txt","26/Jun/14 04:46;navis;HIVE-7024.3.patch.txt;https://issues.apache.org/jira/secure/attachment/12652550/HIVE-7024.3.patch.txt","16/Dec/14 04:28;navis;HIVE-7024.4.patch.txt;https://issues.apache.org/jira/secure/attachment/12687399/HIVE-7024.4.patch.txt","17/Dec/14 06:32;navis;HIVE-7024.5.patch.txt;https://issues.apache.org/jira/secure/attachment/12687684/HIVE-7024.5.patch.txt","18/Dec/14 02:32;navis;HIVE-7024.6.patch.txt;https://issues.apache.org/jira/secure/attachment/12687927/HIVE-7024.6.patch.txt",,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391074,,,,Fri Dec 19 02:03:00 UTC 2014,,,,,,,,,,"0|i1vctj:",391296,,,,,,,,,,,,,,,,,,,,,"07/May/14 05:42;navis;Will affect many files including explain extended but just updated result of ""auto_sortmerge_join_11.q"".;;;","26/Jun/14 04:22;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12652541/HIVE-7024.2.patch.txt

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 5669 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/593/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/593/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-593/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12652541;;;","26/Jun/14 08:53;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12652550/HIVE-7024.3.patch.txt

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5669 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/596/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/596/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-596/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12652550;;;","17/Nov/14 23:08;sershe;For getPropertiesExplain I wonder if it makes sense to just output the map manually, escaping strings.

+1;;;","16/Dec/14 04:28;navis;Missed +1. Reattaching patch for test.;;;","16/Dec/14 18:02;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12687399/HIVE-7024.4.patch.txt

{color:red}ERROR:{color} -1 due to 120 failed/errored test(s), 6705 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binary_output_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_constantPropagateForSubQuery
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_display_colstats_tbllvl
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_extrapolate_part_stats_full
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_extrapolate_part_stats_partial
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_map_ppr_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_ppr_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input42
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32_lessSize
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join35
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_filters_overlap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_louter_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_optimize_nullscan
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_outer_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pcr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_vc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_allchildsarenull
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_push_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_regexp_extract
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_router_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_serde_user_properties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_temp_table_display_colstats_tbllvl
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_partition_diff_num_cols
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_ptf
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_12
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_4
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_8
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket4
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_ctas
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_disable_merge_for_bucketing
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mapjoin_mapjoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_metadataonly1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_optimize_nullscan
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_sample1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_partition_diff_num_cols
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_ptf
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket4
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket5
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin7
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_disable_merge_for_bucketing
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_list_bucket_dml_10
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_reduce_deduplicate
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2094/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2094/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-2094/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 120 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12687399 - PreCommit-HIVE-TRUNK-Build;;;","17/Dec/14 06:32;navis;IntelliJ ate all trailing spaces. Resubmitting.;;;","17/Dec/14 14:03;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12687684/HIVE-7024.5.patch.txt

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2108/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2108/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-2108/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]
+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-TRUNK-Build-2108/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/SubQueryUtils.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java'
++ egrep -v '^X|^Performing status on external'
++ awk '{print $2}'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/scheduler/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit-hadoop2/target itests/hive-minikdc/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/core/target hcatalog/streaming/target hcatalog/server-extensions/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target accumulo-handler/target hwi/target common/target common/src/gen service/target contrib/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target ql/src/test/results/clientnegative/subquery_missing_from.q.out ql/src/test/queries/clientnegative/subquery_missing_from.q
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1646247.

At revision 1646247.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12687684 - PreCommit-HIVE-TRUNK-Build;;;","18/Dec/14 07:43;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12687927/HIVE-7024.6.patch.txt

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6713 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_optimize_nullscan
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket5
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2122/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2122/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-2122/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12687927 - PreCommit-HIVE-TRUNK-Build;;;","19/Dec/14 01:28;navis;Committed to trunk. Thanks Sergey!;;;","19/Dec/14 02:03;sershe;Heh. Why thank me, it was your patch :) Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bucket mapjoin is broken when the number of small aliases is two or more,HIVE-7023,12712754,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,navis,navis,navis,07/May/14 04:21,13/Nov/14 19:39,14/Jul/23 06:14,09/May/14 23:27,0.13.0,,,,,,,,,0.14.0,,Query Processor,,,,0,,,"From auto_sortmerge_join_11.q,
{noformat}
-- small 1 part, 2 bucket & big 2 part, 4 bucket

CREATE TABLE bucket_small (key string, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/smallsrcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/smallsrcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');

CREATE TABLE bucket_big (key string, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');

load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');

set hive.auto.convert.join=true;
set hive.ignore.mapjoin.hint=false;

set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin=true;
set hive.optimize.bucketmapjoin.sortedmerge=true;

select /* + MAPJOIN(a,b) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key JOIN bucket_big c ON a.key = c.key;
{noformat}
The last query produces 0 row, instead of 180 rows, which is correct.",,mdominguez@cloudera.com,navis,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-4790,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/14 04:36;navis;HIVE-7023.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12643687/HIVE-7023.1.patch.txt","08/May/14 02:27;navis;HIVE-7023.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12643890/HIVE-7023.2.patch.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,391070,,,,Thu Nov 13 19:39:41 UTC 2014,,,,,,,,,,"0|i1vcsn:",391292,,,,,,,,,,,,,,,,,,,,,"07/May/14 22:16;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12643687/HIVE-7023.1.patch.txt

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 5495 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/140/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/140/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12643687;;;","08/May/14 02:27;navis;added missing diff file;;;","09/May/14 15:36;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12643890/HIVE-7023.2.patch.txt

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 5500 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/152/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/152/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12643890;;;","09/May/14 18:19;ashutoshc;+1;;;","09/May/14 23:27;ashutoshc;Committed to trunk. Thanks, Navis!;;;","13/Nov/14 19:39;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table and Partition tables have column LINK_TARGET_ID in Mysql scripts but not others,HIVE-7018,12712506,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ychena,brocknoland,brocknoland,06/May/14 06:29,16/Feb/16 23:50,14/Jul/23 06:14,18/Jun/15 20:21,,,,,,,,,,1.3.0,2.0.0,,,,,0,,,It appears that at least postgres and oracle do not have the LINK_TARGET_ID column while mysql does.,,aihuaxu,brocknoland,ctang,hsubramaniyan,sushanth,thejas,xuefuz,ychena,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-10659,,,,,,,,,,,,,,HIVE-10635,,,,,,,,,,,,,,,,HIVE-10614,,,,,,,,"16/Mar/15 18:40;ychena;HIVE-7018.1.patch;https://issues.apache.org/jira/secure/attachment/12704842/HIVE-7018.1.patch","18/Mar/15 20:02;ychena;HIVE-7018.2.patch;https://issues.apache.org/jira/secure/attachment/12705423/HIVE-7018.2.patch","21/May/15 03:01;ychena;HIVE-7018.3.patch;https://issues.apache.org/jira/secure/attachment/12734314/HIVE-7018.3.patch","14/Jun/15 21:34;ychena;HIVE-7018.4.patch;https://issues.apache.org/jira/secure/attachment/12739520/HIVE-7018.4.patch","17/Jun/15 12:54;ychena;HIVE-7018.5.patch;https://issues.apache.org/jira/secure/attachment/12740104/HIVE-7018.5.patch",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390822,,,,Thu Jun 18 20:21:02 UTC 2015,,,,,,,,,,"0|i1vbdb:",391060,,,,,,,,,,,,,,,,,,,,,"16/Mar/15 17:57;ychena;The LINK_TARGET_ID is no use, it should be removed. ;;;","16/Mar/15 18:40;ychena;Need code review. ;;;","16/Mar/15 19:39;aihuaxu;+1. Looks good to me. The code is not used and causes confusing. ;;;","16/Mar/15 23:23;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12704842/HIVE-7018.1.patch

{color:green}SUCCESS:{color} +1 7769 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3045/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3045/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-3045/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12704842 - PreCommit-HIVE-TRUNK-Build;;;","17/Mar/15 13:28;ychena;[~xuefuz] , [~csun] could you review the change and commit it if looks fine? Thanks;;;","17/Mar/15 13:51;xuefuz;Patch looks fine. However, I don't quite understand why we are also removing the following:
{code}
-  CONSTRAINT `PARTITIONS_FK2` FOREIGN KEY (`SD_ID`) REFERENCES `SDS` (`SD_ID`),
...
-  CONSTRAINT `TBLS_FK2` FOREIGN KEY (`DB_ID`) REFERENCES `DBS` (`DB_ID`),
{code}

This doesn't seem related to LINK_TARGET_ID.;;;","17/Mar/15 13:57;aihuaxu;I think it's just removing the ending ',' , not removing the complete line.;;;","17/Mar/15 14:05;xuefuz;Got it. +1;;;","17/Mar/15 14:34;ctang;I wonder if a upgrade script is needed for this change as well. For an example, if the user is using HIVE-1.1.0 with Mysql and his TBLS and PARTITIONS table definition have this column, though the data for these columns might be null. But when he upgrades to HIVE-1.20 with this change and migrates the existing data of these tables to the new ones, will you see the issue?;;;","17/Mar/15 22:30;ychena;Here is my plan: I will find a way to check if the two tables have LINK_TARGE_ID column or not, if have it, I will drop the column. Is that a safe plan? Thanks;;;","18/Mar/15 13:45;xuefuz;[~ychena], do you know when this column was introduced in the script? If we leave as it's, what's the side-effect of it? Thanks.;;;","18/Mar/15 18:42;ychena;This column is introduced by 0.10.0 only for mysql, this inconsistency cause some customers that upgrade from 0.9 versions worried. The
side-effect other than uncomfortable is some migrate issues as Chaoyu said. For better supportability, we'd better fix it in the future releases. ;;;","18/Mar/15 20:03;ychena;New patch with drop columns from old version if exist. Please review.  Thanks;;;","18/Mar/15 23:44;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12705423/HIVE-7018.2.patch

{color:green}SUCCESS:{color} +1 7771 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3073/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3073/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-3073/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12705423 - PreCommit-HIVE-TRUNK-Build;;;","19/Mar/15 13:11;ychena;[~ctang.ma] ,[~aihuaxu] or [~xuefuz] could you review the changes? Thanks;;;","19/Mar/15 13:58;xuefuz;[~ctang.ma], what's your thought on the latest patch?;;;","19/Mar/15 15:42;ctang;+1 (non-binding). Now Hive 1.2.0, whether is freshly installed or upgraded from previous version, should be in sync in term of LINK_TARGET_ID column in TBLS/PARTITIONS tables.;;;","19/Mar/15 15:45;xuefuz;+1, Thanks, Chaoyu!;;;","20/Mar/15 18:10;xuefuz;Committed to trunk. Thanks, Yongzhi.;;;","20/Mar/15 21:03;ychena;Thank you [~xuefuz] & [~ctang.ma];;;","05/May/15 22:00;thejas;This change breaks schematool upgrade - See HIVE-10614;;;","06/May/15 00:54;thejas;I think the change here was in the right direction, however it breaks the preferred way to upgrade hive (using schematool). This is a release blocker for 1.2.0. .
A patch to revert the changes here has been uploaded to  HIVE-10614 . I think we should go ahead with that, and reopen this jira after it is committed. Once the schematool/beeline breakage is fixed, this change can go back into hive. 

;;;","06/May/15 21:47;thejas;As this patch has not gone into a release, it is easier to track the issue by reopening this jira.
Closing HIVE-10635;;;","06/May/15 21:47;thejas;As mentioned in HIVE-10635 -
In HIVE-10614, we had to revert HIVE-7018 because it was not schematool compatible and it would prevent upgrade from 0.14.0 to 1.3.0 when run via schematool. We need to redo HIVE-7018 work once the script introduced for HIVE-7018 is schematool compliant.;;;","11/May/15 18:01;ychena;Need code review. ;;;","11/May/15 18:11;ychena;From what is described in HIVE-10614, the schematool can not support stored procedure with compound statements which is the only way to support if statement. For mysql does not support drop column if exists, so now what I can do is assuming the LINK_TARGET_ID is in the old version DB(which should be true). Attached HIVE-7018.3.patch is the best I can do. How do you think  [~thejas] and [~hsubramaniyan] ? Thanks;;;","11/May/15 18:11;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12731966/HIVE-7018.3.patch

{color:green}SUCCESS:{color} +1 8 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-METASTORE-Test/48/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-METASTORE-Test/48/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-METASTORE-Test-48/

Messages:
{noformat}
LXC mysql found.
LXC mysql is not started. Starting container...
Container started.
Preparing mysql container...
Container prepared.
Calling /hive/testutils/metastore/dbs/mysql/prepare.sh ...
Server prepared.
Calling /hive/testutils/metastore/dbs/mysql/execute.sh ...
Tests executed.
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12731966 - PreCommit-HIVE-METASTORE-Test;;;","11/May/15 18:22;hsubramaniyan;Hi [~ychena] I have made changes to support mysql Stored procedures via schematool in HIVE-10659 . Once HIVE-10659 goes in, I believe your initial fix for this jira can be rebased and merged into master. I am not sure if patch#3 is sufficient here.

Thanks
Hari;;;","11/May/15 18:46;ychena;[~hsubramaniyan], thanks for the information. I will remove patch#3. ;;;","11/May/15 19:48;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12731966/HIVE-7018.3.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 8921 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_percentile_approx_23
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver_encryption_insert_partition_static
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3850/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3850/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-3850/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12731966 - PreCommit-HIVE-TRUNK-Build;;;","20/May/15 23:34;hsubramaniyan; Now that HIVE-10659 is committed, it should be possible to get this one in without breaking schematool upgrade. [~ychena] Can you please rebase the original patch and provide it for master, 1.2.1 branches. [~sushanth] Can you please make note of this jira for 1.2.1 inclusion.

Thanks
Hari;;;","20/May/15 23:41;sushanth;I think we should not have this for branch-1.2, since there are metastore schema changes (even if it is mysql-only), so as to not have schema differences between 1.2.0 and 1.2.1.

It should be okay for master though.;;;","21/May/15 03:01;ychena;[~hsubramaniyan] and [~sushanth], I attached HIVE-7018.3.patch for review. This is for master. Thanks.;;;","21/May/15 03:13;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12734314/HIVE-7018.3.patch

{color:green}SUCCESS:{color} +1 24 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-METASTORE-Test/51/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-METASTORE-Test/51/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-METASTORE-Test-51/

Messages:
{noformat}
LXC derby found.
LXC derby is not started. Starting container...
Container started.
Preparing derby container...
Container prepared.
Calling /hive/testutils/metastore/dbs/derby/prepare.sh ...
Server prepared.
Calling /hive/testutils/metastore/dbs/derby/execute.sh ...
Tests executed.
LXC mysql found.
LXC mysql is not started. Starting container...
Container started.
Preparing mysql container...
Container prepared.
Calling /hive/testutils/metastore/dbs/mysql/prepare.sh ...
Server prepared.
Calling /hive/testutils/metastore/dbs/mysql/execute.sh ...
Tests executed.
LXC postgres found.
LXC postgres is not started. Starting container...
Container started.
Preparing postgres container...
Container prepared.
Calling /hive/testutils/metastore/dbs/postgres/prepare.sh ...
Server prepared.
Calling /hive/testutils/metastore/dbs/postgres/execute.sh ...
Tests executed.
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12734314 - PreCommit-HIVE-METASTORE-Test;;;","05/Jun/15 21:19;ychena;[~ctang.ma], could you review the change? Thanks;;;","11/Jun/15 16:29;ctang;[~ychena] have you verified that it works with SchemaTool with [~hsubramaniyan] HIVE-10659 fix?;;;","12/Jun/15 04:17;ychena;[~ctang.ma], I tested my script with schematool, it works fine. And the patch passed the precommit build for metastore, so I think it is well tested. Thanks;;;","12/Jun/15 12:43;ctang;+1;;;","13/Jun/15 13:12;ctang;Thanks [~ychena] for the patch, it has been committed to 1.3.0 (branch-1). But I think you also need create a separate patch for 2.0.0 (master branch).;;;","14/Jun/15 21:34;ychena;Patch 4 for master (2.0.0);;;","14/Jun/15 21:37;ychena;Hive Chaoyu, I attached HIVE-7018.4.patch for 2.0.0  Thanks;;;","14/Jun/15 21:43;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12739520/HIVE-7018.4.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 27 tests executed
*Failed tests:*
{noformat}
Test failed: mysql/upgrade-1.2.0-to-2.0.0.mysql.sql
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-METASTORE-Test/53/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-METASTORE-Test/53/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-METASTORE-Test-53/

Messages:
{noformat}
LXC derby found.
LXC derby is not started. Starting container...
Container started.
Preparing derby container...
Container prepared.
Calling /hive/testutils/metastore/dbs/derby/prepare.sh ...
Server prepared.
Calling /hive/testutils/metastore/dbs/derby/execute.sh ...
Tests executed.
LXC mysql found.
LXC mysql is not started. Starting container...
Container started.
Preparing mysql container...
Container prepared.
Calling /hive/testutils/metastore/dbs/mysql/prepare.sh ...
Server prepared.
Calling /hive/testutils/metastore/dbs/mysql/execute.sh ...
Test failed: mysql/upgrade-1.2.0-to-2.0.0.mysql.sql
Tests executed.
LXC postgres found.
LXC postgres is not started. Starting container...
Container started.
Preparing postgres container...
Container prepared.
Calling /hive/testutils/metastore/dbs/postgres/prepare.sh ...
Server prepared.
Calling /hive/testutils/metastore/dbs/postgres/execute.sh ...
Tests executed.
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12739520 - PreCommit-HIVE-METASTORE-Test;;;","14/Jun/15 23:11;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12739520/HIVE-7018.4.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 9007 tests executed
*Failed tests:*
{noformat}
TestCustomAuthentication - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join28
org.apache.hadoop.hive.ql.TestMTQueries.testMTQueries1
org.apache.hive.beeline.TestSchemaTool.testSchemaInit
org.apache.hive.beeline.TestSchemaTool.testSchemaUpgrade
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4267/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4267/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4267/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12739520 - PreCommit-HIVE-TRUNK-Build;;;","15/Jun/15 14:23;ctang;[~ychena] Looks like the HMS upgrade test failed, do you know the reason?;;;","16/Jun/15 13:19;ychena;The scripts are called in sequence, I should not put same script in both
upgrade-1.2.0-to-1.3.0.mysql.sql
and 
upgrade-1.2.0-to-2.0.0.mysql.sql
;;;","17/Jun/15 12:55;ychena;Do not know why there is no pre-commit build, delete and reattach last patch. ;;;","17/Jun/15 14:54;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12740104/HIVE-7018.5.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 9006 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join28
org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4285/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4285/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4285/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12740104 - PreCommit-HIVE-TRUNK-Build;;;","17/Jun/15 15:26;ychena;The 3 failures are not related, but I do not know why schematool build has not run. ;;;","17/Jun/15 18:03;ctang;[~ychena] Actually the HMS precommit tests passed, see http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-METASTORE-Test/54/ (Build #54 (Jun 17, 2015 8:59:00 AM) )
+1;;;","18/Jun/15 20:21;ctang;Patch has been committed to Hive 2.0.0 and 1.3.0. Thanks [~ychena].;;;",,,,,,,,,,,
Insertion into Parquet tables fails under Tez,HIVE-7017,12712493,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ccondit,ccondit,ccondit,06/May/14 03:45,13/Nov/14 19:41,14/Jul/23 06:14,28/Jul/14 01:57,0.13.0,,,,,,,,,0.14.0,,Tez,,,,0,,,"It seems Parquet tables cannot be written to in Tez mode. CREATE TABLE foo STORED AS PARQUET SELECT ... queries fail with:

{noformat}
	java.lang.IllegalArgumentException: TaskAttemptId string : task1396892688715_80817_m_000076_3 is not properly formed
		at org.apache.hadoop.mapreduce.TaskAttemptID.forName(TaskAttemptID.java:201)
		at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.<init>(ParquetRecordWriterWrapper.java:49)
{noformat}

The same queries work fine after setting hive.execution.engine=mr.

","Hive 0.13.0, CentOS 6",ccondit,gopalv,NathanHowell,navis,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/14 06:07;navis;HIVE-7017.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12657543/HIVE-7017.1.patch.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390809,,,,Thu Nov 13 19:41:20 UTC 2014,,,,,,,,,,"0|i1vbaf:",391047,,,,,,,,,,,,,,,,,,,,,"06/May/14 04:35;gopalv;Looks like it comes from Parquet code assuming MR only?

https://github.com/apache/hive/blob/trunk/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/ParquetRecordWriterWrapper.java#L49;;;","06/May/14 18:58;ccondit;It's not obvious what the proper thing to do in this case is. The existing ID could be parsed and reformatted, or Tez could be modified to generate TaskAttemptID-compatible identifiers. I have created https://issues.apache.org/jira/browse/TEZ-1104 to track the issue on that end.;;;","06/May/14 19:23;ccondit;I mistakenly assumed that code came from TEZ, when it in fact exists in Hive...

https://github.com/apache/hive/blob/022ee59b8cb9161996310861d4fbf59801d4b9fe/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java#L103

Should probably be:

{noformat}
StringBuilder taskAttemptIdBuilder = new StringBuilder(""attempt_"");
{noformat}

instead of:

{noformat}
StringBuilder taskAttemptIdBuilder = new StringBuilder(""task"");
{noformat}


;;;","24/Jul/14 17:22;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12657543/HIVE-7017.1.patch.txt

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5756 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_join_hash
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/41/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/41/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-41/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12657543;;;","24/Jul/14 18:41;ccondit;Attached patch seems to fix the issue.;;;","25/Jul/14 05:45;thejas;+1;;;","28/Jul/14 01:57;navis;Committed to trunk. Thanks Craig.;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive returns wrong results when execute UDF on top of DISTINCT column,HIVE-7016,12712478,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,navis,selinazh,selinazh,06/May/14 02:00,13/Nov/14 19:39,14/Jul/23 06:14,09/May/14 23:12,0.12.0,0.13.1,,,,,,,,0.14.0,,Query Processor,,,,0,,,"The following query returns wrong result:
select hash(distinct value) from table;

This kind of query should be identified as syntax error. However, Hive ignores DISTINCT and returns the result. 

",,azuryy,navis,selinazh,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/14 07:59;navis;HIVE-7016.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12643717/HIVE-7016.1.patch.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390794,,,,Thu Nov 13 19:39:50 UTC 2014,,,,,,,,,,"0|i1vb7r:",391034,,,,,,,,,,,,,,,,,,,,,"07/May/14 21:24;selinazh;[~Navis] Thanks for the fix! Just wonder in the ErrorMsg.java, the error code seems like a duplicate?;;;","08/May/14 21:49;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12643717/HIVE-7016.1.patch.txt

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 5503 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.testRevokeTimedOutWorkers
org.apache.hadoop.hive.ql.TestErrorMsg.testUniqueErrorCode
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/147/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/147/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12643717;;;","09/May/14 06:42;navis;[~selinazh] You are right. Fail of testUniqueErrorCode is caused by that. Thanks.;;;","09/May/14 19:03;ashutoshc;+1;;;","09/May/14 23:12;ashutoshc;Committed to trunk. Thanks, Navis!;;;","13/Nov/14 19:39;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failing to inherit group/permission should not fail the operation,HIVE-7015,12712432,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,szehon,szehon,szehon,05/May/14 22:05,13/Nov/14 19:43,14/Jul/23 06:14,12/May/14 15:05,0.14.0,,,,,,,,,0.14.0,,Security,,,,0,,,"In the previous changes, chgrp and chmod were put on the critical path of directory creation and file copy/mv

These should not be, for instance existing users may not have hive-users in the same group as hive group, so chgrp would fail if they turn on the flag """"hive.warehouse.subdir.inherit.perms"".",,brocknoland,szehon,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6892,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/14 01:38;szehon;HIVE-7015.patch;https://issues.apache.org/jira/secure/attachment/12643672/HIVE-7015.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390748,,,,Thu Nov 13 19:43:16 UTC 2014,,,,,,,,,,"0|i1vaxj:",390988,,,,,,,,,,,,,,,,,,,,,"07/May/14 12:55;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12643672/HIVE-7015.patch

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 5495 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/137/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/137/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12643672;;;","08/May/14 03:15;brocknoland;+1;;;","12/May/14 15:05;brocknoland;Committed to trunk! Thank you Szehon!;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong RS de-duplication in the ReduceSinkDeDuplication Optimizer,HIVE-7012,12712105,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,,sunrui,sunrui,03/May/14 09:20,09/Aug/22 06:03,14/Jul/23 06:14,13/May/14 16:18,0.13.0,,,,,,,,,0.14.0,,Query Processor,,,,0,,,"With HIVE 0.13.0, run the following test case:
{code:sql}
create table src(key bigint, value string);

select  
   count(distinct key) as col0
from src
order by col0;
{code}

The following exception will be thrown:
{noformat}
java.lang.RuntimeException: Error in configuring object
	at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:485)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:420)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)
	... 9 more
Caused by: java.lang.RuntimeException: Reduce operator initialization failed
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.configure(ExecReducer.java:173)
	... 14 more
Caused by: java.lang.RuntimeException: cannot find field _col0 from [0:reducesinkkey0]
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:415)
	at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef(StandardStructObjectInspector.java:150)
	at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ExprNodeColumnEvaluator.java:79)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.initializeOp(GroupByOperator.java:288)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.configure(ExecReducer.java:166)
	... 14 more
{noformat}

This issue is related to HIVE-6455. When hive.optimize.reducededuplication is set to false, then this issue will be gone.

Logical plan when hive.optimize.reducededuplication=false;
{noformat}
src 
  TableScan (TS_0)
    alias: src
    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
    Select Operator (SEL_1)
      expressions: key (type: bigint)
      outputColumnNames: key
      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
      Group By Operator (GBY_2)
        aggregations: count(DISTINCT key)
        keys: key (type: bigint)
        mode: hash
        outputColumnNames: _col0, _col1
        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
        Reduce Output Operator (RS_3)
          istinctColumnIndices:
          key expressions: _col0 (type: bigint)
          DistributionKeys: 0
          sort order: +
          OutputKeyColumnNames: _col0
          Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
          Group By Operator (GBY_4)
            aggregations: count(DISTINCT KEY._col0:0._col0)
            mode: mergepartial
            outputColumnNames: _col0
            Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
            Select Operator (SEL_5)
              expressions: _col0 (type: bigint)
              outputColumnNames: _col0
              Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
              Reduce Output Operator (RS_6)
                key expressions: _col0 (type: bigint)
                DistributionKeys: 1
                sort order: +
                OutputKeyColumnNames: reducesinkkey0
                OutputVAlueColumnNames: _col0
                Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
                value expressions: _col0 (type: bigint)
                Extract (EX_7)
                  Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator (FS_8)
                    compressed: false
                    Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
{noformat}
You will see that RS_3 and RS_6 are not merged.

Logical plan when hive.optimize.reducededuplication=true;
{noformat}
src 
  TableScan (TS_0)
    alias: src
    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
    Select Operator (SEL_1)
      expressions: key (type: bigint)
      outputColumnNames: key
      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
      Group By Operator (GBY_2)
        aggregations: count(DISTINCT key)
        keys: key (type: bigint)
        mode: hash
        outputColumnNames: _col0, _col1
        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
        Reduce Output Operator (RS_3)
          istinctColumnIndices:
          key expressions: _col0 (type: bigint)
          DistributionKeys: 1
          sort order: +
          OutputKeyColumnNames: reducesinkkey0
          Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
          Group By Operator (GBY_4)
            aggregations: count(DISTINCT KEY._col0:0._col0)
            mode: mergepartial
            outputColumnNames: _col0
            Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
            Select Operator (SEL_5)
              expressions: _col0 (type: bigint)
              outputColumnNames: _col0
              Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
              File Output Operator (FS_8)
                compressed: false
                Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
                table:
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
{noformat}
You will see that RS_6 has been merged into RS_3. However, Obviously the merge is incorrect because RS_3 and RS_6 have different sort keys. (The sort key for RS_3 is
key and the sort key for RS_6 is count(distinct key)).

The problem is that the method sameKeys() returns the result that both RS have same keys. sameKeys() depends ExprNodeDescUtils.backtrack() to backtrack a key expr of cRS to pRS.

I don't understand the logical behind the following logic in ExprNodeDescUtils: 
  Why still backtrack when there is no mapping for the column of the current operator?
{code}
  private static ExprNodeDesc backtrack(ExprNodeColumnDesc column, Operator<?> current,
      Operator<?> terminal) throws SemanticException {
    ...
    if (mapping == null || !mapping.containsKey(column.getColumn())) {
      return backtrack((ExprNodeDesc)column, current, terminal);
    }
    ...
  }
{code}
The process of backtracking _col0 of cRS to pRS:
RS_6:_col0 --> SEL_5:_col0 --> GBY_4:_col0 (because the colExprMap is null for GBY_4) --> RS_3:_col0 (No mapping for output column _col0), which is a wrong backtrack.",,navis,sunrui,thejas,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6455,,,,,,,,,,,,,,,,,,,,,,,,"07/May/14 08:45;navis;HIVE-7012.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12643724/HIVE-7012.1.patch.txt","09/May/14 05:55;navis;HIVE-7012.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12644076/HIVE-7012.2.patch.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390424,,,,Thu Nov 13 19:43:34 UTC 2014,,,,,,,,,,"0|i1v90n:",390677,,,,,,,,,,,,,,,,,,,,,"03/May/14 09:28;sunrui;I am thinking about the following fix, but not sure if right:

sameKeys():

        ExprNodeDesc pexpr = pexprs.get(i);
        ExprNodeDesc cexpr = ExprNodeDescUtils.backtrack(cexprs.get(i), child, parent);
        // check if cexpr is from the parent
        if (cexpr == null || (cexpr not contained in the colExprMap of the parent operator) || !pexpr.isSame(cexpr)) {
          return null;
        };;;","07/May/14 08:48;navis;[~sunrui] You're right. It's wrong assumption. I think there was some historical issue for things done like that. Let's see what will happen with the patch.;;;","09/May/14 01:42;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12643724/HIVE-7012.1.patch.txt

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 5433 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fetch_aggregation
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_reduce_deduplicate_extended
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input20
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input5
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/148/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/148/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12643724;;;","09/May/14 18:07;ashutoshc;reduce_deduplicate_extended.q, ppd.q, fetch_aggregation.q failures might be relevant. [~navis] can you take a look?;;;","09/May/14 21:32;ashutoshc;Please ignore my previous comment, it seems your new patch takes care of those failures.;;;","09/May/14 21:36;ashutoshc;In ppd2.q.out looks like new MR stage got added, looks like RS-dedup optimization got disabled for it. That looks like performance regression. Was that intentional ?;;;","11/May/14 01:30;sunrui;[~navis] I verified that your patch solved my problem. 

[~navis] and [~yhuai] However, I suspect that the optimizer may still have bug when there are distinct expressions. It seems that the optimizer has not taken support for distinct keys into consideration when it was being implemented. Note that keyCols in ReduceSinkDesc is composed of groupby keys and possibly distinct keys. For example, assume cRS and pRS both have KeyCols as (a, b, c, d) and numDistributionKeys=2. cRS may have distinct expressions like distinct(c, d) while pRS may have distinct expressions like distinct(c), distinct(d). In this case, they have different sort keys while their KeyCols are same. [~yhuai] what do you think?
;;;","11/May/14 15:30;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12644076/HIVE-7012.2.patch.txt

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5503 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/174/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/174/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12644076;;;","12/May/14 07:41;navis;[~ashutoshc] Yes, it's intended. In the query ppd2.q
{code}
select a.*
  from (
    select key, count(value) as cc
    from srcpart a
    where a.ds = '2008-04-08' and a.hr = '11'
    group by key
  )a
  distribute by a.key
  sort by a.key,a.cc desc
{code}
cc is generated field by GBY operator, so It's semantically wrong to merge the RS for GBY with any following RS. But the same time, sort on ""a.cc"" is meaningless so it can be removed in optimizing, but not in here (maybe in SemanticAnalyzer?).

[~sunrui] Yes, RS for distinct should be avoided from any dedup process. Could you take this issue? I think you knows better than me.;;;","12/May/14 16:15;ashutoshc;+1 Issue raised by [~sunrui] if exists will probably require a different fix, which we shall take up in separate jira. ;;;","13/May/14 00:09;sunrui;For the issue about distinct, I will investigate it later and if I can find a real test case, I will submit a separate jira.;;;","13/May/14 16:18;ashutoshc;Committed to trunk. Thanks, Navis!;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveInputFormat's split generation isn't thread safe,HIVE-7011,12712087,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,03/May/14 01:33,13/Nov/14 19:41,14/Jul/23 06:14,12/May/14 22:08,0.13.0,0.14.0,,,,,,,,0.14.0,,Tez,,,,0,,,Tez will do split generation in parallel. Need to protect the inputformat cache against concurrent access.,,hagleitn,qwertymaniac,thejas,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7393,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/May/14 01:37;hagleitn;HIVE-7011.1.patch;https://issues.apache.org/jira/secure/attachment/12643168/HIVE-7011.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390406,,,,Thu Nov 13 19:41:32 UTC 2014,,,,,,,,,,"0|i1v8wv:",390659,,,,,,,,,,,,,,,,,,,,,"04/May/14 06:56;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12643168/HIVE-7011.1.patch

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 5430 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/119/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/119/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12643168;;;","08/May/14 22:37;vikram.dixit;+1 changes LGTM. Failed tests are unrelated.;;;","12/May/14 22:08;hagleitn;Committed to trunk. Thanks for the review [~vikram.dixit]!;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HIVE_USER_INSTALL_DIR could not bet set to non-HDFS filesystem,HIVE-7009,12712038,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,chuanliu,chuanliu,chuanliu,02/May/14 20:50,13/Nov/14 19:40,14/Jul/23 06:14,08/May/14 01:52,0.13.0,,,,,,,,,0.14.0,,Tez,,,,0,,,"In {{hive/ql/exec/tez/DagUtils.java}}, we enforce the user path get from {{HIVE_USER_INSTALL_DIR}} to be HDFS. This makes it impossible to run Hive+Tez jobs on non-HDFS filesystem, e.g. WASB. Relevant code are as follows:
{noformat}
  public Path getDefaultDestDir(Configuration conf) throws LoginException, IOException {
    UserGroupInformation ugi = ShimLoader.getHadoopShims().getUGIForConf(conf);
    String userName = ShimLoader.getHadoopShims().getShortUserName(ugi);
    String userPathStr = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_USER_INSTALL_DIR);
    Path userPath = new Path(userPathStr);
    FileSystem fs = userPath.getFileSystem(conf);
    if (!(fs instanceof DistributedFileSystem)) {
      throw new IOException(ErrorMsg.INVALID_HDFS_URI.format(userPathStr));
    }
{noformat}

Exceptions running jobs with defaultFs configured to WASB.
{noformat}
2014-05-01 00:21:39,847 ERROR exec.Task (TezTask.java:execute(192)) - Failed to execute tez graph.
java.io.IOException: wasb://hdi31-chuanliu@clhdistorage.blob.core.windows.net/user is not a hdfs uri
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.getDefaultDestDir(DagUtils.java:662)
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.getHiveJarDirectory(DagUtils.java:759)
	at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.createJarLocalResource(TezSessionState.java:321)
	at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:159)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:154)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1504)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1271)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1089)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:912)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:793)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
{noformat}",,chuanliu,mattf,sershe,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/May/14 21:17;chuanliu;HIVE-7009.patch;https://issues.apache.org/jira/secure/attachment/12643120/HIVE-7009.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390357,,,,Thu Nov 13 19:40:24 UTC 2014,,,,,,,,,,"0|i1v8lz:",390610,,,,,,,,,,,,,,,,,,,,,"02/May/14 21:17;chuanliu;Attach a patch that remove HDFS checks in tez/DagUtils.java.;;;","05/May/14 10:50;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12643120/HIVE-7009.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5428 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/127/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/127/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12643120;;;","06/May/14 15:16;sershe;Sounds reasonable to me. I wonder what was the rationale for HDFS check in the first place and if less strict check could be added instead. [~hagleitn] can you comment;;;","08/May/14 01:19;sershe;Gunther agrees (discussed offline), not clear why these checks are here. Will commit;;;","08/May/14 01:52;sershe;committed to trunk;;;","08/May/14 21:28;mattf;Can this patch also be applied to branch-2 ?;;;","08/May/14 23:23;mattf;Sorry, please ignore previous comment.;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ql_rewrite_gbtoidx.q output file,HIVE-7006,12711873,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,rhbutani,rhbutani,rhbutani,02/May/14 01:06,13/Nov/14 19:43,14/Jul/23 06:14,07/May/14 15:51,,,,,,,,,,0.14.0,,,,,,0,,,"HIVE-4904 moved GroupBy Optimization to happen before ColumnPruner
Now, the RewriteGBUsingIndex happens after GroupByOptimization.
So  setting of bucketGroup flag doesn't happen for Index tables.

Temporary fix is to update .q.out file.
Will file a bug to look into moving RewriteGBUsingIndex before GroupByOptimizer. ",,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7007,,,,,,,,,,,,,,,,,,,,,,,,"02/May/14 01:13;rhbutani;HIVE-7006.1.patch;https://issues.apache.org/jira/secure/attachment/12642980/HIVE-7006.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390192,,,,Thu Nov 13 19:43:39 UTC 2014,,,,,,,,,,"0|i1v7jb:",390429,,,,,,,,,,,,,,,,,,,,,"02/May/14 15:53;ashutoshc;+1;;;","02/May/14 22:44;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642980/HIVE-7006.1.patch

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 5429 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/107/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/107/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642980;;;","07/May/14 15:48;rhbutani;test failures not related to this patch;;;","07/May/14 15:51;rhbutani;thanks Ashutosh.;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MiniTez tests have non-deterministic explain plans,HIVE-7005,12711872,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,jdere,jdere,02/May/14 00:46,13/Nov/14 19:40,14/Jul/23 06:14,15/Jun/14 01:53,,,,,,,,,,0.14.0,,Tests,,,,0,,,"TestMiniTezCliDriver has a few test failures where there is a diff in the explain plan generated. According to Vikram, the plan generated is correct, but the plan can be generated in a couple of different ways and so sometimes the plan will not diff against the expected output. We should probably come up with a way to validate this explain plan in a reproducible way.",,hagleitn,jdere,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/14 07:05;hagleitn;HIVE-7005.1.patch;https://issues.apache.org/jira/secure/attachment/12650238/HIVE-7005.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390191,,,,Thu Nov 13 19:40:39 UTC 2014,,,,,,,,,,"0|i1v7j3:",390428,,,,,,,,,,,,,,,,,,,,,"13/Jun/14 07:05;hagleitn;I believe the problem was that the filesinkoperators were kept in a hashset in tez. I've ran the tests a few times with the patch and didn't get any non-deterministic output.;;;","13/Jun/14 07:08;hagleitn;rb: https://reviews.apache.org/r/22547;;;","13/Jun/14 17:16;jdere;+1 if tests pass;;;","15/Jun/14 00:14;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12650238/HIVE-7005.1.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 5611 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_columnar
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/466/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/466/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-466/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12650238;;;","15/Jun/14 01:44;hagleitn;No new/related test failures. And the typically non-deterministic test didn't show up as failures.;;;","15/Jun/14 01:53;hagleitn;Committed to trunk. Thanks for the review [~jdere]!;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix more unit test failures on hadoop-2,HIVE-7004,12711864,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,01/May/14 23:22,13/Nov/14 19:39,14/Jul/23 06:14,04/May/14 16:14,,,,,,,,,,0.14.0,,Tests,,,,0,,,"Still a number of precommit failures with hadoop-2, will try to fix some of them.
",,jdere,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/May/14 00:16;jdere;HIVE-7004.1.patch;https://issues.apache.org/jira/secure/attachment/12642963/HIVE-7004.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390183,,,,Thu Nov 13 19:39:57 UTC 2014,,,,,,,,,,"0|i1v7hb:",390420,,,,,,,,,,,,,,,,,,,,,"01/May/14 23:36;jdere;org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew: Needs order-by to make results deterministic

org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket: Output changed after HIVE-6968 (list bucketing changes), according to [~prasanth_j] this should be correct so the golden file should be regenerated.

These MiniTez tests all started failing after HIVE-4904. Most of these tests are also in TestCliDriver, and the TestCliDriver golden files were updated in HIVE-4904, so it is very likely that the MiniTez output just need to be updated as well:
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_count
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_limit_pushdown
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mrr
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union9

org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist: Looks like rows were not being processed in a deterministic order for some reason. Filtering input rows to just one value, which is still enough to show this test case.

org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether: RCFile looks like it uses some data compression so order of rows can affect the size.  Add order-by to make order of rows deterministic.;;;","02/May/14 00:13;jdere;Looks like the MiniTez tests were already updated by HIVE-6984.;;;","02/May/14 19:00;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642963/HIVE-7004.1.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 5495 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/105/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/105/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642963;;;","02/May/14 21:11;ashutoshc;+1;;;","04/May/14 16:14;ashutoshc;Committed to trunk. Thanks, Jason!;;;","13/Nov/14 19:39;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix typo in README,HIVE-7003,12711856,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,thejas,thejas,thejas,01/May/14 23:01,13/Nov/14 19:41,14/Jul/23 06:14,02/May/14 00:11,,,,,,,,,,0.14.0,,Documentation,,,,0,,,"From https://issues.apache.org/jira/browse/HIVE-6932?focusedCommentId=13987112&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13987112

 ""Users are free to swtich back....""",,leftyl,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6932,,,,,,,,,,,,,,,,,,,,,"01/May/14 23:03;thejas;HIVE-7003.1.patch;https://issues.apache.org/jira/secure/attachment/12642947/HIVE-7003.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390176,,,,Thu Nov 13 19:41:39 UTC 2014,,,,,,,,,,"0|i1v7fr:",390413,,,,,,,,,,,,,,,,,,,,,"01/May/14 23:03;thejas;HIVE-7003.1.patch - did the switch in ""swtich"" !
cc [~leftylev]
;;;","01/May/14 23:28;leftyl;Thanks Thejas.  (Big grin from the nitpicker.);;;","01/May/14 23:51;thejas;[~leftylev] Can you please do an explicit +1 :)
Thanks for catching this, having typos in README does not look good!
;;;","02/May/14 00:02;leftyl;+1  (a switch in time saves nine);;;","02/May/14 00:11;thejas;Patch committed to trunk. (Don't think its necessary to wait further for this typo fix!)
Thanks for the review and the humor Lefty!
;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fs.permissions.umask-mode is getting unset when Session is started,HIVE-7001,12711769,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,thejas,thejas,thejas,01/May/14 18:57,20/Aug/14 20:38,14/Jul/23 06:14,02/May/14 19:33,0.13.0,,,,,,,,,0.13.1,0.14.0,,,,,0,,,"{code}
hive> set fs.permissions.umask-mode;
fs.permissions.umask-mode=022
hive> show tables;
OK
t1
Time taken: 0.301 seconds, Fetched: 1 row(s)
hive> set fs.permissions.umask-mode;
fs.permissions.umask-mode is undefined

{code}",,thejas,vikram.dixit,vkorukanti,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6962,HIVE-7807,,,,,,,,,,,,,,,,,,,,"01/May/14 18:57;thejas;HIVE-7001.1.patch;https://issues.apache.org/jira/secure/attachment/12642878/HIVE-7001.1.patch","01/May/14 21:21;thejas;HIVE-7001.2.patch;https://issues.apache.org/jira/secure/attachment/12642919/HIVE-7001.2.patch","01/May/14 22:08;thejas;HIVE-7001.3.patch;https://issues.apache.org/jira/secure/attachment/12642930/HIVE-7001.3.patch","20/Aug/14 18:33;vkorukanti;TestUMask.patch;https://issues.apache.org/jira/secure/attachment/12663180/TestUMask.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,390090,,,,Wed Aug 20 20:38:59 UTC 2014,,,,,,,,,,"0|i1v6wn:",390327,,,,,,,,,,,,,,,,,,,,,"01/May/14 19:46;ashutoshc;+1;;;","01/May/14 21:21;thejas;HIVE-7001.2.patch - also includes a test. Checks if value is preserved.
;;;","01/May/14 22:08;thejas;HIVE-7001.3.patch - better 'unit' test 
;;;","01/May/14 22:39;thejas;[~sushanth] Apart from being a critical bug fix,  this also enables a workaround for pre-hadoop 1.2 versions. See HIVE-6962 . That is added value including this in hive 0.13.1. ;;;","02/May/14 01:01;vikram.dixit;+1 LGTM.;;;","02/May/14 05:25;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642930/HIVE-7001.3.patch

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 5429 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_ctas
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/100/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/100/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642930;;;","02/May/14 18:54;thejas;Ran parquet_ctas locally and it passed, rest of the test failures are unrelated.
;;;","02/May/14 19:33;thejas;Patch committed to trunk.
Thanks for the reviews Ashutosh & Vikram!
;;;","09/Jun/14 06:39;thejas;This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
;;;","20/Aug/14 18:33;vkorukanti;Hi [~thejas], One question regarding the ""fs.permissions.umask-mode"". Looks like ""fs.permissions.umask-mode"" doesn't exist in Hadoop 1.x and property ""dfs.umaskmode"" is used instead in 1.x for the same purpose. Also ""dfs.umaskmode"" was not deprecated in 1.x according to HADOOP-8727. Should we use FsPermission.UMASK_LABEL instead of ""fs.permissions.umask-mode"" which always points to proper property in latest Hadoop in each version (0.23.x, 1.x, 2.x)?

Attached a testcase to illustrate the problem. Test passes fine with -Phadoop-2, but not with -Phadoop-1.;;;","20/Aug/14 18:55;thejas;Using FsPermission.UMASK_LABEL sounds good to me .
Please open a new jira.
;;;","20/Aug/14 20:38;vkorukanti;Thanks [~thejas]. Created HIVE-7807 and attached patch. Please review.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FS based stats broken with indexed tables,HIVE-6996,12711619,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,30/Apr/14 22:52,09/Jun/14 06:39,14/Jul/23 06:14,04/May/14 16:06,0.13.0,,,,,,,,,0.13.1,0.14.0,Indexing,Statistics,,,0,,,,,daijy,prasanth_j,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/May/14 22:55;ashutoshc;HIVE-6996.2.patch;https://issues.apache.org/jira/secure/attachment/12643148/HIVE-6996.2.patch","30/Apr/14 22:57;ashutoshc;HIVE-6996.patch;https://issues.apache.org/jira/secure/attachment/12642753/HIVE-6996.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389940,,,,Mon Jun 09 06:39:37 UTC 2014,,,,,,,,,,"0|i1v607:",390181,,,,,,,,,,,,,,,,,,,,,"30/Apr/14 22:58;ashutoshc;https://reviews.apache.org/r/20912/;;;","30/Apr/14 23:22;prasanth_j;+1;;;","01/May/14 21:59;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642753/HIVE-6996.patch

{color:red}ERROR:{color} -1 due to 14 failed/errored test(s), 5427 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_index
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_database_drop
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_update
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_indexes_edge_cases
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_indexes_syntax
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_view
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_index_compact_entry_limit
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_index_compact_size_limit
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/97/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/97/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 14 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642753;;;","02/May/14 22:55;ashutoshc;Fixed failing tests.;;;","03/May/14 16:03;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12643148/HIVE-6996.2.patch

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 5496 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/113/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/113/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12643148;;;","04/May/14 16:06;ashutoshc;Committed to trunk.;;;","16/May/14 21:54;sushanth;Ashutosh pinged me regarding inclusion of this for 0.13.1, now that we're spinning an RC2 anyway. Without this patch, index tables are unusable with default config.

I give it my +1 for inclusion into 0.13.1. If there's another committer that can +1 its inclusion into 0.13.1 RC2, I'll include it.;;;","16/May/14 23:14;prasanth_j;+1 for 0.13.1;;;","17/May/14 00:21;daijy;+1 for 0.13.1.;;;","19/May/14 22:15;sushanth;Thanks folks, I'll include this in 0.13.1 RC2.;;;","23/May/14 07:18;sushanth;Committed to 0.13 branch.;;;","09/Jun/14 06:39;thejas;This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GenericUDFBridge should log exception when it is unable to instantiate UDF object,HIVE-6995,12711606,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,30/Apr/14 22:15,13/Nov/14 19:40,14/Jul/23 06:14,02/May/14 15:42,,,,,,,,,,0.14.0,,UDF,,,,0,,,"If GenericUDFBridge.initialize() is unable to create an instance of the UDF class, it logs a blanket error message of ""The UDF implementation class <udf> is not present in the class path"".  This can make it difficult to determine what the actual error is as there can be other issues besides the class not being in the class path.",,jdere,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/14 22:38;jdere;HIVE-6995.1.patch;https://issues.apache.org/jira/secure/attachment/12642748/HIVE-6995.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389927,,,,Thu Nov 13 19:40:44 UTC 2014,,,,,,,,,,"0|i1v5xb:",390168,,,,,,,,,,,,,,,,,,,,,"01/May/14 18:18;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642748/HIVE-6995.1.patch

{color:red}ERROR:{color} -1 due to 17 failed/errored test(s), 5492 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_count
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_limit_pushdown
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mrr
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/96/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/96/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 17 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642748;;;","01/May/14 19:50;ashutoshc;+1;;;","01/May/14 19:55;jdere;Test failures look unrelated, looks like the usual suspects of failing hadoop-2 tests.;;;","02/May/14 15:42;ashutoshc;Committed to trunk. Thanks, Jason!;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
History not able to disable/enable after session started,HIVE-6991,12711447,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,chinnalalam,chinnalalam,chinnalalam,30/Apr/14 12:17,16/Feb/16 23:50,14/Jul/23 06:14,09/Jun/15 20:19,,,,,,,,,,1.3.0,2.0.0,,,,,0,,,"By default history is disabled, after session started if enable history through this command set hive.session.history.enabled=true. It is not working.

I think it will help to this user query

http://mail-archives.apache.org/mod_mbox/hive-user/201404.mbox/%3CCAJqy7aFAPa_pjS6bUon0o8zYT2qwfN2WT-mtZnwfmuRav_8ZjA@mail.gmail.com%3E",,chinnalalam,erwaman,jxiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/15 12:06;chinnalalam;HIVE-6991.1.patch;https://issues.apache.org/jira/secure/attachment/12731163/HIVE-6991.1.patch","02/Jun/15 13:17;chinnalalam;HIVE-6991.2.patch;https://issues.apache.org/jira/secure/attachment/12736833/HIVE-6991.2.patch","30/Apr/14 12:20;chinnalalam;HIVE-6991.patch;https://issues.apache.org/jira/secure/attachment/12642636/HIVE-6991.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389768,Reviewed,,,Tue Jun 09 20:19:33 UTC 2015,,,,,,,,,,"0|i1v4yf:",390010,,,,,,,,,,,,,,,,,,,,,"30/Apr/14 12:20;chinnalalam;Added check method before starting the query, to check whether history is enabled or disabled.;;;","01/May/14 04:38;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642636/HIVE-6991.patch

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 5426 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/90/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/90/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642636;;;","07/May/15 11:47;chinnalalam;Patch need to be update.;;;","26/May/15 12:00;chinnalalam;After session started if set ""hive.session.history.enabled"" property, it won't take effect because creating the history file is done while starting the session only.

Added new method updateHistory(), it will call if set ""hive.session.history.enabled"" property.;;;","26/May/15 16:45;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12735306/HIVE-6991.2.patch

{color:red}ERROR:{color} -1 due to 637 failed/errored test(s), 8973 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_add_part_multiple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alias_casted_column
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_char1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_numbuckets_partitioned_table2_h23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_numbuckets_partitioned_table_h23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_coltype
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_protect_mode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_rename_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_rename_partition_authorization
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_table_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_table_serde2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_varchar1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_reordering_values
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_add_column
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_add_column2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_add_column3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_change_schema
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_comments
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_compression_enabled
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_compression_enabled_native
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_deserialize_map_null
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_evolved_schemas
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_joins
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_joins_native
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_native
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_nullable_fields
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_partitioned_native
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_sanity_test
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_schema_evolution_native
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_timestamp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_type_evolution
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ba_table1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ba_table2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ba_table_udfs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binary_output_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_columnarserde_create_shortcut
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_columnstats_part_coltype
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_constantPropagateForSubQuery
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_convert_enum_to_string
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_count
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_like2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_like_tbl_props
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_like_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cross_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas_colname
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_custom_input_output_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_database_drop
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_describe_comment_indent
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_describe_comment_nonascii
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_describe_pretty
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_describe_xpath
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_diff_part_input_formats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_database_removes_partition_dirs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_index_removes_partition_dirs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_partition_with_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_table_removes_partition_dirs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_table_with_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape_clusterby1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape_distributeby1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape_orderby1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape_sortby1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_explain_dependency
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_explain_dependency2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_explain_logical
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_explain_rearrange
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_complex_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_distinct_samekey
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_sets3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_multi_single_reducer3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_ppd
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_empty
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables_compact
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_self_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_update
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_skewtable
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_init_file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input11_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input1_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input39
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input40
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input42
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_lazyserde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_inputddl4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_inputddl5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_inputddl6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_inputddl7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into_with_schema
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_interval_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_interval_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join18_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join27
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join29
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32_lessSize
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join35
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join36
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join37
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join39
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_1to1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_casesensitive
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_filters
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_filters_overlap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_literals
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_nulls
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_nullsafe
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_on_varchar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_thrift
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_vc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_keyword_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view_cp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_leadlag
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_leadlag_queries
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_leftsemijoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_file_with_space_in_the_name
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_fs_overwrite
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mergejoins_mixed
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_gby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_gby2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_gby3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_lateral_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nomore_ambiguous_table_col
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonblock_op_deduplicate
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatCTAS
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatdir
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_create
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_diff_part_cols
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_diff_part_cols2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_empty_strings
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge_incompat1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge_incompat2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_predicate_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_split_elimination
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_vectorization_ppd
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_of_multi_field_struct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_of_optional_elements
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_of_required_elements
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_of_single_field_struct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_of_structs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_of_unannotated_groups
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_of_unannotated_primitives
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_avro_array_of_primitives
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_avro_array_of_single_field_struct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_map_null
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_map_of_arrays_of_ints
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_map_of_maps
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_nested_complex
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_read_backward_compatible_files
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_schema_evolution
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_table_with_subschema
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_thrift_array_of_primitives
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_thrift_array_of_single_field_struct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_boolexpr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_type_in_plan
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_plan_json
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_constant_where
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_vc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_protectmode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf_general_queries
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf_streaming
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_recursive_dir
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rename_partition_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rename_table_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_script_env_var1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_semicolon
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_semijoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_serde_user_properties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_alter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_db_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_delimited
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_aggregator_error_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_empty_dyn_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_empty_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_publisher_error_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_exists
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_in
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_in_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_notin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_notin_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_views
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_temp_table_display_colstats_tbllvl
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_temp_table_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_temp_table_options1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_comparison2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_merge
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_percentile_approx_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_sum_list
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_crc32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_sha1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_substring
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unicode_notation
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union27
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unionDistinct_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_null
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_top_level
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_join30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_rcfile_columnar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_timestamp_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_view_inputs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_columnPruning
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_udaf2
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udf_row_sequence
org.apache.hadoop.hive.cli.TestContribNegativeCliDriver.testNegativeCliDriver_invalid_row_sequence
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver_encryption_insert_values
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver_encryption_join_unencrypted_tbl
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver_encryption_join_with_different_encryption_keys
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver_encryption_select_read_only_encrypted_tbl
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver_encryption_select_read_only_unencrypted_tbl
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver_encryption_unencrypted_nonhdfs_external_tables
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_handler_bulk
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_ppd_join
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_single_sourced_multi_insert
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats_empty_partition
org.apache.hadoop.hive.cli.TestHBaseNegativeCliDriver.testCliDriver_generatehfiles_require_family_path
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_load_fs2
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_truncate_column_buckets
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_12
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_4
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_count
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cross_join
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_ctas
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_custom_input_output_format
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_having
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_hybridgrace_hashjoin_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_hybridgrace_hashjoin_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_join0
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_join_nullsafe
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_leftsemijoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_lvj_mapjoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mrr
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_orc_merge5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_orc_merge6
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_orc_merge7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_orc_merge_incompat1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_orc_merge_incompat2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_orc_vectorization_ppd
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_ptf
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_ptf_streaming
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_script_env_var1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_subquery_exists
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_subquery_in
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_join_tests
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_joins_explain
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union_multiinsert
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_transform2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union8
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union9
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_unionDistinct_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_null_projection
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_rcfile_columnar
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_timestamp_funcs
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_load_fs2
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_truncate_column_buckets
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_coltype_2columns
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_coltype_invalidcolname
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_coltype_invalidtype
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_invalidspec
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_nodrop
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_nodrop_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_offline
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_view_failure3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_view_failure8
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_fail_3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_fail_4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_fail_5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_fail_6
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_fail_7
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_part
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_bucket_mapjoin_wrong_table_metadata_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_bucket_mapjoin_wrong_table_metadata_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_columnstats_partlvl_invalid_values
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_or_replace_view1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_or_replace_view2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_or_replace_view3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_or_replace_view4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_or_replace_view5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_or_replace_view6
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_or_replace_view7
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_or_replace_view8
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_view_failure10
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_date_literal2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_date_literal3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exchange_partition_neg_table_missing
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_illegal_partition_type
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into_with_schema
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into_with_schema1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into_with_schema2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into_with_schema3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into_with_schema4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_interval_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_interval_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_interval_3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_select_column
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_select_column_with_subquery
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_select_column_with_tablename
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lateral_view_alias
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_load_wrong_fileformat
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_load_wrong_fileformat_rc_seq
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_load_wrong_fileformat_txt_seq
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_protectmode_part
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_protectmode_part1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_protectmode_part2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_protectmode_part_no_drop
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_protectmode_part_no_drop2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_protectmode_tbl1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_protectmode_tbl2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_protectmode_tbl3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_protectmode_tbl4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_protectmode_tbl5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_protectmode_tbl6
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_protectmode_tbl7
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_protectmode_tbl8
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_protectmode_tbl_no_drop
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_AggrFuncsWithNoGBYNoPartDef
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_AmbiguousWindowDefn
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_DistributeByOrderBy
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_DuplicateWindowAlias
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_HavingLeadWithNoGBYNoWindowing
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_HavingLeadWithPTF
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_InvalidValueBoundary
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_JoinWithAmbigousAlias
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_PartitionBySortBy
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_WhereWithRankCond
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_recursive_view
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_selectDistinctStarNeg_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_selectDistinctStarNeg_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_select_charliteral
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_select_star_suffix
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_select_udtf_alias
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_semijoin1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_semijoin2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_semijoin3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_semijoin4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_set_hiveconf_validation0
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_set_hiveconf_validation1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_set_hiveconf_validation2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_aggregator_error_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_aggregator_error_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_publisher_error_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_publisher_error_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_subquery_windowing_corr
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_temp_table_partitions
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_timestamp_literal
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_bucketed_column
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_column_indexed_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_column_seqfile
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_nonexistant_column
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_partition_column
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_partition_column2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_table_failure1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_table_failure2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_array_contains_wrong1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_array_contains_wrong2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_concat_ws_wrong1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_concat_ws_wrong2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_concat_ws_wrong3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_format_number_wrong1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_format_number_wrong2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_format_number_wrong3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_format_number_wrong4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_format_number_wrong5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_format_number_wrong6
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_format_number_wrong7
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_printf_wrong1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_printf_wrong2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_printf_wrong3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_printf_wrong4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_sort_array_wrong1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_sort_array_wrong2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_sort_array_wrong3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unionClusterBy
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unionDistributeBy
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unionLimit
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unionOrderBy
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unionSortBy
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_cluster_tasklog_retrieval
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_add_part_multiple
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_join26
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_join_reordering_values
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_12
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_5
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_7
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_avro_compression_enabled_native
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_avro_joins
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_avro_joins_native
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_count
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_cross_join
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ctas
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_custom_input_output_format
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_decimal_join
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_escape_clusterby1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_escape_distributeby1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_escape_orderby1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_escape_sortby1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby9
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_complex_types
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_multi_single_reducer3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_having
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_index_auto_self_join
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_input1_limit
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join0
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join10
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join11
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join12
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join13
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join14
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join15
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join17
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join18
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join18_multi_distinct
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join19
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join20
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join21
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join23
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join25
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join26
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join27
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join28
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join29
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join30
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join31
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join32
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join32_lessSize
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join33
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join34
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join35
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join36
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join37
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join39
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join5
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join8
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join9
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_1to1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_casesensitive
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_filters_overlap
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_literals
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_map_ppr
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_nullsafe
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_rc
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_thrift
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_vc
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_leftsemijoin
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part10
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part14
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part8
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part9
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_mergejoins_mixed
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_multi_insert
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_multi_insert_gby
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_multi_insert_gby2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_multi_insert_gby3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_multi_insert_lateral_view
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ptf
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ptf_decimal
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ptf_general_queries
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ptf_streaming
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_sample3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_sample8
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_script_env_var1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_semijoin
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_sort
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_subquery_exists
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_subquery_in
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_temp_table_join1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_tez_join_tests
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_tez_joins_explain
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_transform2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union13
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union16
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union19
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union20
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union21
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union22
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union24
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union26
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union27
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union31
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union32
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union8
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union9
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_date
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_null
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_top_level
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorized_rcfile_columnar
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorized_timestamp_funcs
org.apache.hadoop.hive.ql.TestLocationQueries.testAlterTablePartitionLocation_alter5
org.apache.hadoop.hive.ql.TestMTQueries.testMTQueries1
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4042/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4042/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4042/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 637 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12735306 - PreCommit-HIVE-TRUNK-Build;;;","27/May/15 08:22;chinnalalam;Re run the tests with updated trunk.;;;","02/Jun/15 15:17;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12736833/HIVE-6991.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 8996 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx_cbo_2
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4144/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4144/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4144/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12736833 - PreCommit-HIVE-TRUNK-Build;;;","02/Jun/15 17:28;chinnalalam;Failed test case is not related to this patch.;;;","03/Jun/15 15:53;jxiang;Can the user change the setting and start a new session? There are several other settings that don't take effect automatically.;;;","04/Jun/15 10:16;chinnalalam;Hi [~jxiang],

Can the user change the setting and start a new session?
bq.Yes.. But after starting the session if he change the configuration through ""SET"" command it won't take effect .

There are several other settings that don't take effect automatically
bq.May be need to list out the configurations which won't take effect after setting through ""SET"" command and we need address;;;","08/Jun/15 23:45;jxiang;+1. You tested it on a live cluster and it works for both disable and enable, right?;;;","09/Jun/15 19:24;chinnalalam;Hi [~jxiang],

Yes.. I have tested it  in my live cluster by disable and enable this property, in CLI and HiveServer2. It is working as expected.
Thanks for the review.;;;","09/Jun/15 20:19;jxiang;Integrated into trunk and branch 1. Thanks Chinna for the patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Direct SQL fails when the explicit schema setting is different from the default one,HIVE-6990,12711380,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sershe,libing,libing,30/Apr/14 04:24,22/May/18 23:57,14/Jul/23 06:14,07/Jul/17 15:46,0.12.0,0.14.0,1.2.1,,,,,,,3.0.0,,Query Processor,,,,0,,,"I got the following ERROR in hive.log
2014-04-23 17:30:23,331 ERROR metastore.ObjectStore (ObjectStore.java:handleDirectSqlError(1756)) - Direct SQL failed, falling back to ORM
javax.jdo.JDODataStoreException: Error executing SQL query ""select PARTITIONS.PART_ID from PARTITIONS  inner join TBLS on PARTITIONS.TBL_ID = TBLS.TBL_ID   inner join DBS on TBLS.DB_ID = DBS.DB_ID inner join PARTITION_KEY_VALS as FILTER0 on FILTER0.PART_ID = PARTITIONS.PART_ID and FILTER0.INTEGER_IDX = 0 where TBLS.TBL_NAME = ? and DBS.NAME = ? and ((FILTER0.PART_KEY_VAL = ?))"".
        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)
        at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:181)
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:98)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilterInternal(ObjectStore.java:1833)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilter(ObjectStore.java:1806)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:94)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)
        at java.lang.reflect.Method.invoke(Method.java:619)
        at org.apache.hadoop.hive.metastore.RetryingRawStore.invoke(RetryingRawStore.java:124)
        at com.sun.proxy.$Proxy11.getPartitionsByFilter(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_filter(HiveMetaStore.java:3310)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:94)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)
        at java.lang.reflect.Method.invoke(Method.java:619)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:103)
        at com.sun.proxy.$Proxy12.get_partitions_by_filter(Unknown Source)


Reproduce steps:
1. set the following properties in hive-site.xml
 <property>
  <name>javax.jdo.mapping.Schema</name>
  <value>HIVE</value>
 </property>
 <property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>user1</value>
 </property>

2. execute hive queries
hive> create table mytbl ( key int, value string);
hive> load data local inpath 'examples/files/kv1.txt' overwrite into table mytbl;
hive> select * from mytbl;
hive> create view myview partitioned on (value) as select key, value from mytbl where key=98;
hive> alter view myview add partition (value='val_98') partition (value='val_xyz');
hive> alter view myview drop partition (value='val_xyz');",hive + derby,libing,lizhang,sershe,vgarg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/17 23:47;sershe;HIVE-6990.06.patch;https://issues.apache.org/jira/secure/attachment/12875152/HIVE-6990.06.patch","30/Apr/14 08:06;libing;HIVE-6990.1.patch;https://issues.apache.org/jira/secure/attachment/12642612/HIVE-6990.1.patch","06/May/14 09:32;libing;HIVE-6990.2.patch;https://issues.apache.org/jira/secure/attachment/12643520/HIVE-6990.2.patch","06/May/14 14:18;libing;HIVE-6990.3.patch;https://issues.apache.org/jira/secure/attachment/12643577/HIVE-6990.3.patch","01/Jun/15 16:55;libing;HIVE-6990.4.patch;https://issues.apache.org/jira/secure/attachment/12736606/HIVE-6990.4.patch","14/Sep/15 17:16;libing;HIVE-6990.5.patch;https://issues.apache.org/jira/secure/attachment/12755772/HIVE-6990.5.patch",,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389701,,,,Tue May 22 23:57:50 UTC 2018,,,,,,,,,,"0|i1v4jj:",389943,,,,,,,,,,,,,,,,,,,,,"30/Apr/14 04:25;libing;The error message is similar like the one in HIVE-5128;;;","30/Apr/14 08:06;libing;The patch is generated based on trunk;;;","30/Apr/14 21:13;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642612/HIVE-6990.1.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/88/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/88/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
Reverted 'ql/src/test/results/clientpositive/ppd_outer_join3.q.out'
Reverted 'ql/src/test/results/clientpositive/input42.q.out'
Reverted 'ql/src/test/results/clientpositive/having.q.out'
Reverted 'ql/src/test/results/clientpositive/quote1.q.out'
Reverted 'ql/src/test/results/clientpositive/index_auto_unused.q.out'
Reverted 'ql/src/test/results/clientpositive/join_vc.q.out'
Reverted 'ql/src/test/results/clientpositive/vector_non_string_partition.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_join23.q.out'
Reverted 'ql/src/test/results/clientpositive/union15.q.out'
Reverted 'ql/src/test/results/clientpositive/udf5.q.out'
Reverted 'ql/src/test/results/clientpositive/sort_merge_join_desc_4.q.out'
Reverted 'ql/src/test/results/clientpositive/join16.q.out'
Reverted 'ql/src/test/results/clientpositive/nullgroup2.q.out'
Reverted 'ql/src/test/results/clientpositive/vectorized_bucketmapjoin1.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby_grouping_sets5.q.out'
Reverted 'ql/src/test/results/clientpositive/limit_pushdown_negative.q.out'
Reverted 'ql/src/test/results/clientpositive/bucketmapjoin10.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_join32.q.out'
Reverted 'ql/src/test/results/clientpositive/union24.q.out'
Reverted 'ql/src/test/results/clientpositive/index_auto.q.out'
Reverted 'ql/src/test/results/clientpositive/ppd_join.q.out'
Reverted 'ql/src/test/results/clientpositive/join_reorder4.q.out'
Reverted 'ql/src/test/results/clientpositive/metadata_only_queries.q.out'
Reverted 'ql/src/test/results/clientpositive/input2.q.out'
Reverted 'ql/src/test/results/clientpositive/vectorization_div0.q.out'
Reverted 'ql/src/test/results/clientpositive/udtf_parse_url_tuple.q.out'
Reverted 'ql/src/test/results/clientpositive/join11.q.out'
Reverted 'ql/src/test/results/clientpositive/smb_mapjoin_15.q.out'
Reverted 'ql/src/test/results/clientpositive/join_cond_pushdown_3.q.out'
Reverted 'ql/src/test/results/clientpositive/bucketmapjoin1.q.out'
Reverted 'ql/src/test/results/clientpositive/correlationoptimizer14.q.out'
Reverted 'ql/src/test/results/clientpositive/metadata_only_queries_with_filters.q.out'
Reverted 'ql/src/test/results/clientpositive/join20.q.out'
Reverted 'ql/src/test/results/clientpositive/correlationoptimizer9.q.out'
Reverted 'ql/src/test/results/clientpositive/sample10.q.out'
Reverted 'ql/src/test/results/clientpositive/list_bucket_dml_5.q.out'
Reverted 'ql/src/test/results/clientpositive/smb_mapjoin_10.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby_rollup1.q.out'
Reverted 'ql/src/test/results/clientpositive/ppd_gby2.q.out'
Reverted 'ql/src/test/results/clientpositive/list_bucket_dml_13.q.out'
Reverted 'ql/src/test/results/clientpositive/vectorized_context.q.out'
Reverted 'ql/src/test/results/clientpositive/union_view.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby_ppd.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_sortmerge_join_5.q.out'
Reverted 'ql/src/test/results/clientpositive/udf_current_database.q.out'
Reverted 'ql/src/test/results/clientpositive/skewjoinopt9.q.out'
Reverted 'ql/src/test/results/clientpositive/input39_hadoop20.q.out'
Reverted 'ql/src/test/results/clientpositive/correlationoptimizer4.q.out'
Reverted 'ql/src/test/results/clientpositive/vectorized_date_funcs.q.out'
Reverted 'ql/src/test/results/clientpositive/list_bucket_query_oneskew_3.q.out'
Reverted 'ql/src/test/results/clientpositive/louter_join_ppr.q.out'
Reverted 'ql/src/test/results/clientpositive/join_reorder.q.out'
Reverted 'ql/src/test/results/clientpositive/list_bucket_query_multiskew_3.q.out'
Reverted 'ql/src/test/results/clientpositive/join_cond_pushdown_unqual3.q.out'
Reverted 'ql/src/test/results/clientpositive/annotate_stats_join.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_join0.q.out'
Reverted 'ql/src/test/results/clientpositive/quotedid_skew.q.out'
Reverted 'ql/src/test/results/clientpositive/subquery_notin.q.out'
Reverted 'ql/src/test/results/clientpositive/subquery_notexists_having.q.out'
Reverted 'ql/src/test/results/clientpositive/skewjoinopt4.q.out'
Reverted 'ql/src/test/results/clientpositive/show_indexes_edge_cases.q.out'
Reverted 'ql/src/test/results/clientpositive/keyword_1.q.out'
Reverted 'ql/src/test/results/clientpositive/order.q.out'
Reverted 'ql/src/test/results/clientpositive/ppd_join3.q.out'
Reverted 'ql/src/test/results/clientpositive/skewjoin.q.out'
Reverted 'ql/src/test/results/clientpositive/mapjoin_distinct.q.out'
Reverted 'ql/src/test/results/clientpositive/vector_coalesce.q.out'
Reverted 'ql/src/test/results/clientpositive/bucketcontext_8.q.out'
Reverted 'ql/src/test/results/clientpositive/udf_inline.q.out'
Reverted 'ql/src/test/results/clientpositive/skewjoinopt16.q.out'
Reverted 'ql/src/test/results/clientpositive/bucket_groupby.q.out'
Reverted 'ql/src/test/results/clientpositive/alias_casted_column.q.out'
Reverted 'ql/src/test/results/clientpositive/udf_explode.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby_sort_11.q.out'
Reverted 'ql/src/test/results/clientpositive/ppd_udtf.q.out'
Reverted 'ql/src/test/results/clientpositive/input26.q.out'
Reverted 'ql/src/test/results/clientpositive/join_merging.q.out'
Reverted 'ql/src/test/results/clientpositive/smb_mapjoin_2.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_sortmerge_join_12.q.out'
Reverted 'ql/src/test/results/clientpositive/skewjoin_union_remove_1.q.out'
Reverted 'ql/src/test/results/clientpositive/bucketcontext_3.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_join_without_localtask.q.out'
Reverted 'ql/src/test/results/clientpositive/skewjoinopt11.q.out'
Reverted 'ql/src/test/results/clientpositive/vector_decimal_aggregate.q.out'
Reverted 'ql/src/test/results/clientpositive/index_auto_self_join.q.out'
Reverted 'ql/src/test/results/clientpositive/vector_decimal_mapjoin.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_join16.q.out'
Reverted 'ql/src/test/results/clientpositive/bucket3.q.out'
Reverted 'ql/src/test/results/clientpositive/vector_between_in.q.out'
Reverted 'ql/src/test/results/clientpositive/input21.q.out'
Reverted 'ql/src/test/results/clientpositive/ppd_outer_join5.q.out'
Reverted 'ql/src/test/results/clientpositive/semijoin.q.out'
Reverted 'ql/src/test/results/clientpositive/skewjoinopt20.q.out'
Reverted 'ql/src/test/results/clientpositive/show_columns.q.out'
Reverted 'ql/src/test/results/clientpositive/udf_count.q.out'
Reverted 'ql/src/test/results/clientpositive/partition_wise_fileformat2.q.out'
Reverted 'ql/src/test/results/clientpositive/subq_where_serialization.q.out'
Reverted 'ql/src/test/results/clientpositive/udf7.q.out'
Reverted 'ql/src/test/results/clientpositive/sort_merge_join_desc_6.q.out'
Reverted 'ql/src/test/results/clientpositive/join18.q.out'
Reverted 'ql/src/test/results/clientpositive/decimal_precision.q.out'
Reverted 'ql/src/test/results/clientpositive/nullgroup4.q.out'
Reverted 'ql/src/test/results/clientpositive/bucketmapjoin8.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_join11.q.out'
Reverted 'ql/src/test/results/clientpositive/subq2.q.out'
Reverted 'ql/src/test/results/clientpositive/annotate_stats_filter.q.out'
Reverted 'ql/src/test/results/clientpositive/bucketmapjoin12.q.out'
Reverted 'ql/src/test/results/clientpositive/union26.q.out'
Reverted 'ql/src/test/results/clientpositive/ppd1.q.out'
Reverted 'ql/src/test/results/clientpositive/cross_join.q.out'
Reverted 'ql/src/test/results/clientpositive/ppd_random.q.out'
Reverted 'ql/src/test/results/clientpositive/input4.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_join20.q.out'
Reverted 'ql/src/test/results/clientpositive/udf2.q.out'
Reverted 'ql/src/test/results/clientpositive/sort_merge_join_desc_1.q.out'
Reverted 'ql/src/test/results/clientpositive/join13.q.out'
Reverted 'ql/src/test/results/clientpositive/smb_mapjoin_17.q.out'
Reverted 'ql/src/test/results/clientpositive/ppd_repeated_alias.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby_grouping_sets2.q.out'
Reverted 'ql/src/test/results/clientpositive/union21.q.out'
Reverted 'ql/src/test/results/clientpositive/join22.q.out'
Reverted 'ql/src/test/results/clientpositive/nonblock_op_deduplicate.q.out'
Reverted 'ql/src/test/results/clientpositive/vector_left_outer_join.q.out'
Reverted 'ql/src/test/results/clientpositive/annotate_stats_part.q.out'
Reverted 'ql/src/test/results/clientpositive/symlink_text_input_format.q.out'
Reverted 'ql/src/test/results/clientpositive/truncate_column_list_bucket.q.out'
Reverted 'ql/src/test/results/clientpositive/list_bucket_dml_7.q.out'
Reverted 'ql/src/test/results/clientpositive/index_stale.q.out'
Reverted 'ql/src/test/results/clientpositive/vectorized_timestamp_funcs.q.out'
Reverted 'ql/src/test/results/clientpositive/metadataonly1.q.out'
Reverted 'ql/src/test/results/clientpositive/type_widening.q.out'
Reverted 'ql/src/test/results/clientpositive/escape_orderby1.q.out'
Reverted 'ql/src/test/results/clientpositive/union8.q.out'
Reverted 'ql/src/test/results/clientpositive/vectorized_math_funcs.q.out'
Reverted 'ql/src/test/results/clientpositive/correlationoptimizer11.q.out'
Reverted 'ql/src/test/results/clientpositive/multiMapJoin1.q.out'
Reverted 'ql/src/test/results/clientpositive/join40.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_sortmerge_join_7.q.out'
Reverted 'ql/src/test/results/clientpositive/correlationoptimizer6.q.out'
Reverted 'ql/src/test/results/clientpositive/subquery_exists_having.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_join18_multi_distinct.q.out'
Reverted 'ql/src/test/results/clientpositive/subquery_exists.q.out'
Reverted 'ql/src/test/results/clientpositive/ppd_gby.q.out'
Reverted 'ql/src/test/results/clientpositive/list_bucket_dml_2.q.out'
Reverted 'ql/src/test/results/clientpositive/select_transform_hint.q.out'
Reverted 'ql/src/test/results/clientpositive/outer_join_ppr.q.out'
Reverted 'ql/src/test/results/clientpositive/union_top_level.q.out'
Reverted 'ql/src/test/results/clientpositive/subquery_unqualcolumnrefs.q.out'
Reverted 'ql/src/test/results/clientpositive/udf_in_file.q.out'
Reverted 'ql/src/test/results/clientpositive/union3.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby_sort_9.q.out'
Reverted 'ql/src/test/results/clientpositive/index_bitmap_auto.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_sortmerge_join_2.q.out'
Reverted 'ql/src/test/results/clientpositive/skewjoinopt6.q.out'
Reverted 'ql/src/test/results/clientpositive/correlationoptimizer1.q.out'
Reverted 'ql/src/test/results/clientpositive/transform_ppr1.q.out'
Reverted 'ql/src/test/results/clientpositive/join0.q.out'
Reverted 'ql/src/test/results/clientpositive/escape_sortby1.q.out'
Reverted 'ql/src/test/results/clientpositive/regexp_extract.q.out'
Reverted 'ql/src/test/results/clientpositive/select_as_omitted.q.out'
Reverted 'ql/src/test/results/clientpositive/join_view.q.out'
Reverted 'ql/src/test/results/clientpositive/udaf_number_format.q.out'
Reverted 'ql/src/test/results/clientpositive/bucket_map_join_2.q.out'
Reverted 'ql/src/test/results/clientpositive/skewjoinopt18.q.out'
Reverted 'ql/src/test/results/clientpositive/skewjoinopt1.q.out'
Reverted 'ql/src/test/results/clientpositive/smb_mapjoin_4.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_sortmerge_join_14.q.out'
Reverted 'ql/src/test/results/clientpositive/disable_merge_for_bucketing.q.out'
Reverted 'ql/src/test/results/clientpositive/ppd_union_view.q.out'
Reverted 'ql/src/test/results/clientpositive/lateral_view_outer.q.out'
Reverted 'ql/src/test/results/clientpositive/mergejoins.q.out'
Reverted 'ql/src/test/results/clientpositive/ppd_gby_join.q.out'
Reverted 'ql/src/test/results/clientpositive/bucketcontext_5.q.out'
Reverted 'ql/src/test/results/clientpositive/escape_clusterby1.q.out'
Reverted 'ql/src/test/results/clientpositive/skewjoinopt13.q.out'
Reverted 'ql/src/test/results/clientpositive/orc_predicate_pushdown.q.out'
Reverted 'ql/src/test/results/clientpositive/cte_1.q.out'
Reverted 'ql/src/test/results/clientpositive/nullscript.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_join18.q.out'
Reverted 'ql/src/test/results/clientpositive/input23.q.out'
Reverted 'ql/src/test/results/clientpositive/filter_numeric.q.out'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/ExplainSemanticAnalyzer.java'
++ awk '{print $2}'
++ egrep -v '^X|^Performing status on external'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-minikdc/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/core/target hcatalog/streaming/target hcatalog/server-extensions/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target hwi/target common/target common/src/gen contrib/target service/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1591480.

At revision 1591480.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642612;;;","30/Apr/14 21:55;sershe;Some tests appear to fail. Related?
There are tabs in the patch (as well as trailing whitespace in some places).
I wonder if names of tables could just be pre-generated into string fields when schema is set, and fields with names like DBS/TBLS/... used. That will make queries easier to read.
;;;","06/May/14 09:32;libing;patch based on the latest trunk;;;","06/May/14 09:39;libing;Hi, [~sershe]
The failures in build#88 are not related to this patch.

If we don't set javax.jdo.mapping.Schema in hive-site.xml, then the value of the schema is empty, and I can't get the table schema info from the database either.

Do you have some good method to get this info?

Thank you!;;;","06/May/14 09:51;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12643520/HIVE-6990.2.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/129/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/129/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-common ---
[INFO] Compiling 39 source files to /data/hive-ptest/working/apache-svn-trunk-source/common/target/classes
[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/common/src/java/org/apache/hadoop/hive/common/FileUtils.java uses or overrides a deprecated API.
[WARNING] Note: Recompile with -Xlint:deprecation for details.
[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/common/src/java/org/apache/hadoop/hive/common/ObjectPair.java uses unchecked or unsafe operations.
[WARNING] Note: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-common ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 4 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-common ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/common/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/common/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/common/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/common/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-common ---
[INFO] Compiling 13 source files to /data/hive-ptest/working/apache-svn-trunk-source/common/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-common ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-common ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/common/target/hive-common-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-common ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-common ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/common/target/hive-common-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-common/0.14.0-SNAPSHOT/hive-common-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/common/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-common/0.14.0-SNAPSHOT/hive-common-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Serde 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-serde ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/serde (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-source (add-source) @ hive-serde ---
[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/serde/src/gen/protobuf/gen-java added.
[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/serde/src/gen/thrift/gen-javabean added.
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-serde ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-serde ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/serde/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-serde ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-serde ---
[INFO] Compiling 351 source files to /data/hive-ptest/working/apache-svn-trunk-source/serde/target/classes
[WARNING] Note: Some input files use or override a deprecated API.
[WARNING] Note: Recompile with -Xlint:deprecation for details.
[WARNING] Note: Some input files use unchecked or unsafe operations.
[WARNING] Note: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-serde ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/serde/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-serde ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/serde/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/serde/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/serde/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/serde/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-serde ---
[INFO] Compiling 43 source files to /data/hive-ptest/working/apache-svn-trunk-source/serde/target/test-classes
[WARNING] Note: Some input files use or override a deprecated API.
[WARNING] Note: Recompile with -Xlint:deprecation for details.
[WARNING] Note: Some input files use unchecked or unsafe operations.
[WARNING] Note: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-serde ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-serde ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/serde/target/hive-serde-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-serde ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-serde ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/serde/target/hive-serde-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-serde/0.14.0-SNAPSHOT/hive-serde-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/serde/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-serde/0.14.0-SNAPSHOT/hive-serde-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Metastore 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-metastore ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/metastore (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-source (add-source) @ hive-metastore ---
[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/model added.
[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/gen/thrift/gen-javabean added.
[INFO] 
[INFO] --- antlr3-maven-plugin:3.4:antlr (default) @ hive-metastore ---
[INFO] ANTLR: Processing source directory /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/java
ANTLR Parser Generator  Version 3.4
org/apache/hadoop/hive/metastore/parser/Filter.g
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-metastore ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-metastore ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-metastore ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-metastore ---
[INFO] Compiling 195 source files to /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/classes
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java:[461,29] illegal character: \92
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java:[464,9] not a statement
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java:[908,52] illegal character: \92
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java:[909,24] not a statement
[INFO] 4 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [9.567s]
[INFO] Hive Ant Utilities ................................ SUCCESS [6.443s]
[INFO] Hive Shims Common ................................. SUCCESS [4.375s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.661s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.838s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.462s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [8.894s]
[INFO] Hive Shims ........................................ SUCCESS [1.158s]
[INFO] Hive Common ....................................... SUCCESS [20.102s]
[INFO] Hive Serde ........................................ SUCCESS [11.517s]
[INFO] Hive Metastore .................................... FAILURE [12.046s]
[INFO] Hive Query Language ............................... SKIPPED
[INFO] Hive Service ...................................... SKIPPED
[INFO] Hive JDBC ......................................... SKIPPED
[INFO] Hive Beeline ...................................... SKIPPED
[INFO] Hive CLI .......................................... SKIPPED
[INFO] Hive Contrib ...................................... SKIPPED
[INFO] Hive HBase Handler ................................ SKIPPED
[INFO] Hive HCatalog ..................................... SKIPPED
[INFO] Hive HCatalog Core ................................ SKIPPED
[INFO] Hive HCatalog Pig Adapter ......................... SKIPPED
[INFO] Hive HCatalog Server Extensions ................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client ................. SKIPPED
[INFO] Hive HCatalog Webhcat ............................. SKIPPED
[INFO] Hive HCatalog Streaming ........................... SKIPPED
[INFO] Hive HWI .......................................... SKIPPED
[INFO] Hive ODBC ......................................... SKIPPED
[INFO] Hive Shims Aggregator ............................. SKIPPED
[INFO] Hive TestUtils .................................... SKIPPED
[INFO] Hive Packaging .................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1:28.938s
[INFO] Finished at: Tue May 06 05:50:23 EDT 2014
[INFO] Final Memory: 42M/315M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-metastore: Compilation failure: Compilation failure:
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java:[461,29] illegal character: \92
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java:[464,9] not a statement
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java:[908,52] illegal character: \92
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java:[909,24] not a statement
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-metastore
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12643520;;;","06/May/14 14:22;sershe;Can you elaborate on your question? not sure I understand. If schema is empty then table names are default and no prefix is needed in queries, right?;;;","06/May/14 18:37;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12643577/HIVE-6990.3.patch

{color:red}ERROR:{color} -1 due to 212 failed/errored test(s), 5428 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_add_part_exist
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_merge
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_merge_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_merge_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_clusterby_sortby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_coltype
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_format_loc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_protect_mode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_rename_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_rename_partition_authorization
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_table_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_analyze_table_null_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_archive_excludeHadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_parts
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_groupby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_columnstats_partlvl
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_combine2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_combine3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_concatenate_inherit_table_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_or_replace_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_view_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dbtxnmgr_compact2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dbtxnmgr_query3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dbtxnmgr_query4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dbtxnmgr_query5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_describe_formatted_view_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_describe_formatted_view_partitioned_json
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_partitions_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_partitions_filter2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_partitions_filter3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_table2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_table_removes_partition_dirs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_02_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_04_all_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_04_evolved_parts
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_05_some_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_06_one_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_07_all_part_over_nonoverlap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_08_nonpart_rename
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_09_part_spec_nonoverlap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_15_external_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_16_part_external
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_17_part_managed
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_18_part_external
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_19_00_part_external_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_19_part_external_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_20_part_managed_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_23_import_part_authsuccess
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_hidden_files
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_explain_dependency
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_explain_dependency2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_explain_logical
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_global_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_hook_context_cs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auth
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_unused
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_stale_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_inputddl6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert1_overwrite_partitions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32_lessSize
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_leftsemijoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_limit_partition_metadataonly
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_subquery
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadata_only_queries
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadata_only_queries_with_filters
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_null_column
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_analyze
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_create
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_date2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_decode_name
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_special_char
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_type_check
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_varchar1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_varchar2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partitions_json
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pcr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_constant_where
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_vc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_protectmode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_protectmode2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_merge1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_merge2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_merge3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_merge4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rename_partition_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_repair
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_unquote_and
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_unquote_not
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_unquote_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_noscan_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_noscan_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_only_null
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_statsfs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_touch
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_updateAccessTime
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats2
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_nodrop_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_bucket_mapjoin_wrong_table_metadata_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_drop_table_failure3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_02_all_part_over_overlap
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_15_part_nonpart
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_16_part_noncompat_schema
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_17_part_spec_underspec
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_18_part_spec_missing
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_21_part_managed_external
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_24_import_part_authfail
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_join28
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_join32
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partscan_norcfile
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/130/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/130/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 212 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12643577;;;","06/May/14 20:41;lizhang;Would the patch cause a problem if the table name is in a format of ""schema1.tblname"" within a query? I'd think it will incorrectly return table not found because schema1 will be ignored?;;;","01/Jun/15 16:55;libing;This patch is generated based on the latest Hive code in master branch.;;;","01/Jun/15 18:32;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12736606/HIVE-6990.4.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 8994 tests executed
*Failed tests:*
{noformat}
TestUtil - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fold_case
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx_cbo_2
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4128/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4128/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4128/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12736606 - PreCommit-HIVE-TRUNK-Build;;;","14/Sep/15 17:16;libing;The patch is created based on the latest code in master branch;;;","14/Sep/15 17:17;libing;Re-generated the patch based on master branch;;;","14/Sep/15 22:59;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12755772/HIVE-6990.5.patch

{color:red}ERROR:{color} -1 due to 2775 failed/errored test(s), 9439 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_acid_vectorization
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_acid_vectorization_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_acid_vectorization_project
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_add_part_exist
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_add_part_multiple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_char2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_concatenate_indexed_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_file_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_index
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_merge
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_merge_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_merge_2_orc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_merge_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_merge_orc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_merge_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_merge_stats_orc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_change_col
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_clusterby_sortby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_coltype
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_format_loc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_update_status
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_with_whitelist
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_rename_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_rename_partition_authorization
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_rename_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_table_cascade
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_table_invalidate_column_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_table_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_table_partition_drop
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_table_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_varchar2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_view_rename
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_analyze_table_null_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_analyze_tbl_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_select
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_archive_excludeHadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_archive_multi
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_array_map_access_nonconstant
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_1_sql_std
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_cli_createtab
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_explain
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_grant_public_role
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_owner_actions_db
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_parts
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_update_own_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join27
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_reordering_values
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_add_column3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_change_schema
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_compression_enabled
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_compression_enabled_native
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_decimal_native
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_evolved_schemas
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_native
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_nullable_fields
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_partitioned_native
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_sanity_test
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_schema_evolution_native
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_timestamp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_type_evolution
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ba_table3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binary_output_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_groupby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_spark1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_spark2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_spark3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketizedhiveinputformat_auto
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin_negative
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin_negative2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cast1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cast_qualified_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_gby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_gby_empty
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_auto_join0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_auto_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_cross_product_check_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_gby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_gby_empty
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_join0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_outer_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_semijoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_simple_select
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_subq_exists
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_subq_in
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_subq_not_in
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_udf_udaf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_views
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_windowing
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_windowing_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_semijoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_simple_select
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_subq_exists
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_subq_in
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_subq_not_in
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_udf_udaf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_views
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_windowing
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_cast
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_comparison
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_pad_convert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_udf1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_union1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cluster
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_colstats_all_nulls
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_column_access_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_columnarserde_create_shortcut
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_columnstats_part_coltype
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_columnstats_partlvl
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_columnstats_partlvl_dp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_combine2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_combine2_hadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_combine2_win
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_combine3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_complex_alias
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_compute_stats_binary
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_compute_stats_boolean
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_compute_stats_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_compute_stats_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_compute_stats_double
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_compute_stats_long
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_compute_stats_string
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_concatenate_inherit_table_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_confirm_initial_tbl_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_constprog2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_constprog_dp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_constprog_type
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_count
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cp_sel
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_alter_list_bucketing_table1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_default_prop
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_escape
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_func1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_genericudaf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_genericudf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_insert_outputformat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_like
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_like2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_like_tbl_props
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_nested_type
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_or_replace_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_view_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_view_translate
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cross_product_check_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas_colname
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas_uses_database_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cte_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cte_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_current_date_timestamp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_database_drop
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_database_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_database_properties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_comparison
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dbtxnmgr_compact1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dbtxnmgr_compact2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dbtxnmgr_ddl1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dbtxnmgr_query3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dbtxnmgr_query4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dbtxnmgr_query5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ddltime
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_precision
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_precision2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_trailing
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_udf2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_delete_all_non_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_delete_all_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_delete_orig_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_delete_tmp_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_delete_where_no_match
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_delete_where_non_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_delete_where_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_delete_whole_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_desc_non_existent_tbl
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_describe_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_describe_database_json
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_describe_formatted_view_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_describe_formatted_view_partitioned_json
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_describe_pretty
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_describe_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_describe_table_json
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_disallow_incompatible_type_change_off
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_display_colstats_tbllvl
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_distinct_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_driverhook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_database_removes_partition_dirs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_function
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_index
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_index_removes_partition_dirs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_multi_partitions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_partition_with_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_partitions_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_partitions_filter2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_partitions_filter3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_table2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_table_removes_partition_dirs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_table_with_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_with_concurrency
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynamic_partition_insert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynamic_partition_skip_default
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynamic_rdd_cache
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynpart_merge
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynpart_sort_opt_bucketing
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynpart_sort_opt_vectorization
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynpart_sort_optimization2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynpart_sort_optimization_acid
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_encoding_nonutf8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_enforce_order
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape_clusterby1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape_orderby1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape_sortby1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exchange_partition2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_00_nonpart_empty
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_02_00_part_empty
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_02_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_04_all_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_04_evolved_parts
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_05_some_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_06_one_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_07_all_part_over_nonoverlap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_08_nonpart_rename
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_09_part_spec_nonoverlap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_10_external_managed
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_11_managed_external
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_12_external_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_15_external_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_16_part_external
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_17_part_managed
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_18_part_external
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_19_00_part_external_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_19_part_external_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_20_part_managed_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_21_export_authsuccess
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_22_import_exist_authsuccess
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_23_import_part_authsuccess
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_24_import_nonexist_authsuccess
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_hidden_files
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_explain_dependency
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_explain_dependency2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_explain_logical
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_explode_null
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_extrapolate_part_stats_full
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_extrapolate_part_stats_partial
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_extrapolate_part_stats_partial_ndv
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fetch_aggregation
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fileformat_mix
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fileformat_text
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_cond_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fold_case
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fold_eq_with_case_when
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fold_when
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_folder_predicate
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fouter_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_gby_star
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_global_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3_map_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3_noskew_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby4_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby4_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby4_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby5_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby5_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby6_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby6_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby6_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7_map_multi_single_reducer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7_noskew_multi_single_reducer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby8_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby8_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby8_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_bigdata
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_distinct_samekey
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_id1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_id2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_sets2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_sets3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_sets4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_sets6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_window
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_map_ppr_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_multi_insert_common_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_multi_single_reducer2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_multi_single_reducer3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_neg_float
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_position
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_ppd
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_ppr_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_resolution
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_rollup1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_test_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_having2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_hook_context_cs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_hook_order
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_implicit_cast1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_implicit_cast_during_insert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auth
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_empty
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables_compact
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_multiple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_self_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_unused
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_update
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_binary_search
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_creation
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_in_db
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_skewtable
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_stale
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_stale_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_convert_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_const_type
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_init_file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_innerjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_inoutdriver
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input14_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input1_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input36
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input38
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input39
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input39_hadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input3_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input40
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input41
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input42
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input43
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input44
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input46
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input49
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input4_cb_delim
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input4_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_columnarserde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_dynamicserde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_lazyserde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part10_win
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_testsequencefile
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_testxpath
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_testxpath2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_testxpath4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_inputddl2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_inputddl4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_inputddl5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_inputddl6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_inputddl7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_inputddl8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert1_overwrite_partitions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert2_overwrite_partitions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_acid_dynamic_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_acid_not_bucketed
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_compressed
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into_with_schema
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into_with_schema2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_orig_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_overwrite_local_directory_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_update_delete
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_values_dynamic_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_values_non_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_values_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_values_tmp_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insertexternal1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insertoverwrite_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_interval_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_interval_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_interval_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_interval_arithmetic
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_interval_comparison
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_interval_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ivyDownload
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join14_hadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join18_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join27
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join29
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32_lessSize
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join35
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join36
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join37
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join38
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join40
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join41
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join42
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join43
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_1to1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_alt_syntax
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_array
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_casesensitive
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_unqual1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_unqual2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_unqual3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_unqual4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_empty
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_filters
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_merge_multi_expressions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_merging
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_nulls
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_nullsafe
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_reorder
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_reorder2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_reorder3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_star
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_thrift
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_json_serde_tsformat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_keyword_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view_explode2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view_ppd
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_leadlag
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_leadlag_queries
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_leftsemijoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_limit0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_limit_partition_metadataonly
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_limit_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_limit_pushdown_negative
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lineage1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lineage2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lineage3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_literal_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_literal_double
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_literal_ints
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_literal_string
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part14_win
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_exist_part_authsuccess
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_file_with_space_in_the_name
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_fs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_fs_overwrite
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_nonpart_authsuccess
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_orc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_orc_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_overwrite
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_part_authsuccess
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_loadpart1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_loadpart2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_loadpart_err
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_louter_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_macro_duplicate
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_addjar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_filter_on_outerjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_hook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_subquery
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_subquery2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mergejoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mergejoins
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mergejoins_mixed
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadataOnlyOptimizer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadata_export_drop
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadata_only_queries
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadata_only_queries_with_filters
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadataonly1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mi
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_misc_json
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_column_in
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_gby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_gby2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_lateral_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_mixed
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_move_tasks_share_dependencies
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_union_src
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_join_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_sahooks
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nested_complex
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nested_complex_additional
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nestedvirtual
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_newline
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_noalias_subq1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nomore_ambiguous_table_col
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonblock_op_deduplicate
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonmr_fetch
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonmr_fetch_threshold
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonreserved_keywords_input37
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonreserved_keywords_insert_into1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_notable_alias1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_notable_alias2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_notable_alias3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_null_cast
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_null_column
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatCTAS
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatdir
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup4_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullinput2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullscript
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_num_op_type_conv
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ops_comparison
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_optimize_nullscan
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_optional_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_analyze
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_create
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_dictionary_threshold
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_diff_part_cols
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_diff_part_cols2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_empty_strings
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ends_with_nulls
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_file_dump
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_int_type_promotion
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge_incompat2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_min_max
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ppd_char
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ppd_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ppd_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ppd_timestamp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_predicate_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_split_elimination
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_vectorization_ppd
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_wide_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_order
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_order2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_order_within_subquery
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_outer_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_overridden_confs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parallel
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parallel_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parenthesis_star_by
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_null_element
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_of_multi_field_struct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_of_optional_elements
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_of_required_elements
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_of_single_field_struct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_of_structs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_of_unannotated_groups
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_avro_array_of_primitives
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_columnar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_create
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_ctas
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_external_time
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_map_of_arrays_of_ints
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_map_of_maps
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_mixed_case
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_mixed_partition_formats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_nested_complex
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_ppd_boolean
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_ppd_char
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_ppd_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_ppd_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_ppd_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_ppd_varchar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_read_backward_compatible_files
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_schema_evolution
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_table_with_subschema
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_thrift_array_of_primitives
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_thrift_array_of_single_field_struct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_write_correct_definition_levels
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partInit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_part_inherit_tbl_props
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_part_inherit_tbl_props_empty
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_part_inherit_tbl_props_with_star
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partcols1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_boolexpr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_char
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_coltype_literals
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_date2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_decode_name
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_multilevels
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_schema1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_serde_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_special_char
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_timestamp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_timestamp2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_timestamp2_win
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_timestamp_win
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_type_check
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_type_in_plan
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_varchar1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_varchar2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_vs_table_metadata
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partitions_json
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pcr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_plan_json
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pointlookup
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pointlookup2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pointlookup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_clusterby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_constant_expr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_constant_where
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_field_garbage
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_gby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_gby_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_multi_insert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_outer_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_outer_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_outer_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_outer_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_random
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_repeated_alias
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_transform
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_udf_case
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_udf_col
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_udtf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_vc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_allchildsarenull
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_pushdown2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_pushdown3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_progress_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf_general_queries
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf_matchpath
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf_rcfile
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf_register_tblfn
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf_seqfile
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf_streaming
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_push_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_query_properties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_query_result_fileformat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_query_with_semi
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quote1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quote2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_basic
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_tblproperty
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_bigdata
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_default_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_lazydecompress
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_merge1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_merge2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_merge3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_merge4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_recursive_dir
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_reduce_deduplicate_exclude_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_reducesink_dedup
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_regex_col
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rename_column
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rename_external_partition_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rename_partition_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rename_table_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_repair
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_repl_1_drop
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_repl_2_exim_basic
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_repl_3_exim_metadata
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_reset_conf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_router_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_runtime_skewjoin_mapjoin_spark
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample_islocalmode_hook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample_islocalmode_hook_hadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_script_env_var1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_script_env_var2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_script_pipe
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_selectDistinctStar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_same_col
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_transform_hint
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_unquote_and
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_unquote_not
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_unquote_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_semicolon
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_semijoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_serde_opencsv
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_serde_regex
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_serde_user_properties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_set_metaconf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_set_processor_namespaces
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_set_variable_sub
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_conf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_alter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_db_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_delimited
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_temp_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_describe_func_quotes
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_functions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_indexes_edge_cases
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_indexes_syntax
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_partitions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_roles
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_tables
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_tablestatus
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_tblproperties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_shutdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_mapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_mapjoin10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_mapjoin11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_mapjoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_mapjoin3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_mapjoin4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_mapjoin5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_mapjoin6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_mapjoin7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_mapjoin8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_mapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_join_partition_key
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_split
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_aggregator_error_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_empty_dyn_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_empty_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_invalidation
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_noscan_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_noscan_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_only_null
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_ppr_all
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_publisher_error_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_statsfs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_storage_format_descriptor
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_str_to_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_structin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subq
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subq2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subq_where_serialization
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_exists
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_exists_explain_rewrite
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_exists_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_in
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_in_explain_rewrite
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_in_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_notexists
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_notexists_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_notin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_notin_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_unqual_corr_expr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_unqualcolumnrefs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_views
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sum_expr_with_order
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_table_access_keys_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_tablename_with_select
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_temp_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_temp_table_display_colstats_tbllvl
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_temp_table_gb1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_temp_table_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_temp_table_names
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_temp_table_options1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_temp_table_precedence
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_temp_table_subquery1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_temp_table_windowing_expressions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_test_boolean_whereclause
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_comparison
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_comparison2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_formats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_ints_casts
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_lazy
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_literal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_null
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_touch
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_acid
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_merge
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_type_cast_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_type_conversions_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_collect_set
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_collect_set_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_context_ngrams
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_corr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_covar_pop
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_covar_samp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_histogram_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_ngrams
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_percentile_approx_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_percentile_approx_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_sum_list
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_10_trims
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_PI
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_abs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_acos
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_add
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_add_months
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_aes_decrypt
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_aes_encrypt
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_array
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_array_contains
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_ascii
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_asin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_atan
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_avg
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_bigint
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_bin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_bitmap_and
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_bitmap_empty
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_bitwise_and
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_bitwise_not
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_bitwise_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_bitwise_shiftleft
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_bitwise_shiftright
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_bitwise_shiftrightunsigned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_boolean
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_bround
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_case
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_case_column_pruning
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_case_thrift
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_cbrt
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_ceil
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_coalesce
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_compare_java_string
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_concat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_concat_insert1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_concat_insert2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_concat_ws
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_context_aware
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_conv
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_count
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_crc32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_user
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_date_add
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_date_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_date_sub
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_datediff
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_day
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_dayofmonth
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_decode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_degrees
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_div
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_divide
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_double
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_elt
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_equal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_example_add
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_exp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_factorial
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_field
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_find_in_set
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_float
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_floor
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_folder_constants
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_format_number
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_from_unixtime
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_from_utc_timestamp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_get_json_object
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_greaterthan
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_greaterthanorequal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_greatest
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_hash
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_hex
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_hour
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_if
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_in
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_in_file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_index
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_initcap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_inline
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_instr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_isnotnull
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_isnull
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_isnull_isnotnull
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_java_method
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_last_day
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_lcase
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_least
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_lessthan
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_lessthanorequal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_levenshtein
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_like
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_ln
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_locate
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_log
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_log10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_log2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_logic_java_boolean
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_lower
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_lpad
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_ltrim
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_map_keys
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_map_values
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_max
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_md5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_min
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_minute
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_modulo
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_month
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_named_struct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_negative
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_next_day
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_not
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_notequal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_notop
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_parse_url
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_percentile
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_pmod
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_positive
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_pow
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_power
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_printf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_quarter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_radians
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_rand
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_reflect
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_reflect2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_regexp_extract
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_regexp_replace
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_repeat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_reverse
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_rlike
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_round
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_round_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_rpad
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_second
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_sentences
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_sha1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_sha2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_sign
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_sin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_size
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_smallint
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_sort_array
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_soundex
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_space
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_split
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_sqrt
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_std
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_stddev
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_stddev_pop
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_stddev_samp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_string
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_struct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_substr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_substring
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_substring_index
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_subtract
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_sum
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_tan
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_testlength
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_testlength2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_tinyint
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_to_byte
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_to_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_to_float
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_to_long
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_to_short
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_to_string
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_to_unix_timestamp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_translate
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_trim
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_trunc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_ucase
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_unhex
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_unix_timestamp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_upper
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_var_pop
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_var_samp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_variance
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_weekofyear
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_when
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_xpath
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_xpath_boolean
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_xpath_float
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_xpath_int
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_xpath_long
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_xpath_short
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_xpath_string
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_json_tuple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_parse_url_tuple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_posexplode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_stack
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unicode_notation
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union27
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union29
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union35
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unionDistinct_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unionDistinct_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_date_trim
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_lateralview
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_null
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_6_subq
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_script
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_top_level
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unionall_unbalancedppd
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_uniquejoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unset_table_view_property
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_updateAccessTime
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_update_after_multiple_inserts
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_update_all_non_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_update_all_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_update_all_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_update_orig_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_update_tmp_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_update_two_cols
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_update_where_no_match
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_update_where_non_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_update_where_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_cast
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_comparison
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_nested_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_udf1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_acid3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_aggregate_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_aggregate_without_gby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_between_in
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_binary_join_groupby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_bround
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_cast_constant
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_char_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_char_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_char_mapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_char_simple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_coalesce
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_coalesce_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_count_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_data_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_date_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_10_0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_aggregate
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_cast
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_expressions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_math_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_precision
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_round
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_round_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_trailing
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_udf2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_distinct_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_elt
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_groupby_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_groupby_reduce
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_if_expr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_inner_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_interval_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_interval_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_join30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_join_nulls
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_left_outer_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_left_outer_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_leftsemi_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_mapjoin_reduce
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_mr_diff_schema_alias
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_multi_insert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_non_string_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_null_projection
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_nullsafe_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_orderby_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_outer_join0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_outer_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_outer_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_outer_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_outer_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_outer_join5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_partition_diff_num_cols
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_partitioned_date_time
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_partitioned_date_time_win
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_reduce_groupby_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_string_concat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_varchar_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_varchar_mapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_varchar_simple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_decimal_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_div0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_nested_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_not
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_part_project
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_part_varchar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_short_regress
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_bucketmapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_case
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_casts
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_context
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_date_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_distinct_gby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_math_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_nested_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_parquet
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_parquet_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_ptf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_rcfile_columnar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_shufflejoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_string_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_timestamp_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_timestamp_ints_casts
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_view_cast
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_view_inputs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_virtual_column
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_adjust_rowcontainer_sz
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_columnPruning
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_expressions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_multipartitioning
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_navfn
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_ntile
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_rank
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_streaming
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_udaf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_udaf2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_windowspec
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_windowspec2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_windowspec3
org.apache.hadoop.hive.cli.TestCompareCliDriver.testCompareCliDriver_vectorized_math_funcs
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats2
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats3
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats_empty_partition
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_ppd_key_ranges
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_acid_overwrite
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_concatenate_indexed_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_file_format
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_non_native
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_change_col_dup_col
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_change_col_nonexist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_coltype_2columns
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_coltype_invalidcolname
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_coltype_invalidtype
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_partial_spec_dyndisabled
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_rename_partition_failure
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_rename_partition_failure2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_rename_partition_failure3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_table_add_partition
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_table_wrong_location
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_table_wrong_regex
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_view_as_select_not_exist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_view_as_select_with_partition
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_view_failure
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_view_failure2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_view_failure3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_view_failure4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_view_failure5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_view_failure6
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_view_failure7
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_view_failure8
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_view_failure9
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_altern1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ambiguous_col
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_analyze
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_analyze1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_analyze_non_existent_tbl
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_analyze_view
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_corrupt
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_insert1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_insert2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_insert3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_insert4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi6
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi7
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_partspec1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_partspec2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_partspec3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_partspec4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_partspec5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_addjar
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_addpartition
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_alter_db_owner
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_alter_db_owner_default
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_cannot_create_all_role
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_cannot_create_default_role
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_cannot_create_none_role
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_caseinsensitivity
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_cli_auth_enable
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_compile
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_create_func1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_create_func2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_create_index
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_create_macro1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_create_role_no_admin
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_create_tbl
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_create_view
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_createview
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_delete_nodeletepriv
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_deletejar
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_desc_table_nosel
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_dfs
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_disallow_transform
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_drop_admin_role
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_drop_db_cascade
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_drop_db_empty
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_drop_index
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_drop_role_no_admin
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_droppartition
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_fail_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_fail_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_fail_3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_fail_4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_fail_5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_fail_6
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_fail_7
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_fail_8
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_fail_create_db
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_fail_drop_db
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_grant_group
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_grant_server
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_grant_table_allpriv
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_grant_table_dup
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_grant_table_fail1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_grant_table_fail_nogrant
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_grant_uri
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_insert_noinspriv
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_insert_noselectpriv
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_insertoverwrite_nodel
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_invalid_priv_v1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_invalid_priv_v2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_not_owner_alter_tab_rename
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_not_owner_alter_tab_serdeprop
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_not_owner_drop_tab
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_not_owner_drop_tab2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_not_owner_drop_view
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_part
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_priv_current_role_neg
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_public_create
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_public_drop
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_revoke_table_fail1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_revoke_table_fail2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_role_case
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_role_cycles1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_role_cycles2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_role_grant
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_role_grant2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_role_grant_nosuchrole
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_role_grant_otherrole
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_role_grant_otheruser
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_rolehierarchy_privs
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_sba_drop_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_select
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_select_view
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_set_invalidconf
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_set_nonexistent_conf
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_set_role_neg1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_set_role_neg2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_show_columns
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_show_grant_otherrole
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_show_grant_otheruser_all
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_show_grant_otheruser_alltabs
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_show_grant_otheruser_wtab
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_show_parts_nosel
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_show_role_principals_no_admin
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_show_roles_no_admin
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_table_grant_nosuchrole
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_truncate
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_update_noupdatepriv
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_add_partition
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_alterpart_loc
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_altertab_setloc
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_create_table1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_create_table_ext
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_createdb
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_export
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_import
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_index
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_insert
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_insert_local
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_load_data
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorize_grant_public
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorize_revoke_public
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_autolocal1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_bad_exec_hooks
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_bad_indextype
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_bad_sample_clause
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_bucket_mapjoin_mismatch1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_bucket_mapjoin_wrong_table_metadata_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_bucket_mapjoin_wrong_table_metadata_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_cachingprintstream
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_char_pad_convert_fail0
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_char_pad_convert_fail1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_char_pad_convert_fail2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_char_pad_convert_fail3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_clusterbydistributeby
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_clusterbyorderby
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_clusterbysortby
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_clustern2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_clustern3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_clustern4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_column_change_skewedcol_type1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_column_rename1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_column_rename2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_column_rename3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_column_rename4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_column_rename5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_columnstats_partlvl_invalid_values
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_columnstats_partlvl_multiple_part_clause
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_columnstats_tbllvl
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_columnstats_tbllvl_complex_type
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_columnstats_tbllvl_incorrect_column
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_compare_double_bigint
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_compare_string_bigint
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_compile_processor
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_compute_stats_long
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_function_nonexistent_class
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_function_nonexistent_db
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_function_nonudf_class
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_insert_outputformat
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_or_replace_view1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_or_replace_view2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_or_replace_view3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_or_replace_view4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_or_replace_view5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_or_replace_view6
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_or_replace_view7
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_or_replace_view8
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_skewed_table_col_name_value_no_mismatch
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_skewed_table_dup_col_name
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_skewed_table_failure_invalid_col_name
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_table_failure1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_table_failure2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_table_failure3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_table_failure4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_table_wrong_regex
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_udaf_failure
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_unknown_genericudf
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_unknown_udf_udaf
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_view_failure1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_view_failure10
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_view_failure2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_view_failure3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_view_failure4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_view_failure5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_view_failure6
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_view_failure7
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_view_failure8
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_create_view_failure9
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ctas
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ctas_noemptyfolder
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_cte_recursion
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_cte_with_in_subquery
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_database_create_already_exists
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_database_create_invalid_name
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_database_drop_does_not_exist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_database_drop_not_empty
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_database_drop_not_empty_restrict
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_database_switch_does_not_exist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_date_literal2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_date_literal3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dbtxnmgr_nodblock
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dbtxnmgr_nodbunlock
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dbtxnmgr_notablelock
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dbtxnmgr_notableunlock
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ddltime
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_decimal_precision
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_decimal_precision_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_default_partition_name
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_delete_non_acid_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_delete_not_acid
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_delete_not_bucketed
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_delete_sorted
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_deletejar
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_desc_failure1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_desc_failure2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_desc_failure3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_describe_xpath1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_describe_xpath2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_describe_xpath3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_describe_xpath4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_disallow_incompatible_type_change_on1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_disallow_incompatible_type_change_on2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_drop_func_nonexistent
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_drop_function_failure
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_drop_index_failure
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_drop_native_udf
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_drop_partition_failure
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_drop_partition_filter_failure
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_drop_table_failure1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_drop_table_failure2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_drop_view_failure1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_drop_view_failure2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_duplicate_alias_in_transform
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_duplicate_alias_in_transform_schema
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_duplicate_insert1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_duplicate_insert2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_duplicate_insert3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dyn_part1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dyn_part2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dyn_part3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dyn_part4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dyn_part_max
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dyn_part_max_per_node
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exchange_partition
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exchange_partition_neg_incomplete_partition
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exchange_partition_neg_partition_exists
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exchange_partition_neg_partition_exists2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exchange_partition_neg_partition_exists3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exchange_partition_neg_partition_missing
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exchange_partition_neg_table_missing
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exchange_partition_neg_table_missing2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exchange_partition_neg_test
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_00_unsupported_schema
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_01_nonpart_over_loaded
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_02_all_part_over_overlap
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_03_nonpart_noncompat_colschema
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_04_nonpart_noncompat_colnumber
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_05_nonpart_noncompat_coltype
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_06_nonpart_noncompat_storage
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_07_nonpart_noncompat_ifof
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_08_nonpart_noncompat_serde
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_09_nonpart_noncompat_serdeparam
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_10_nonpart_noncompat_bucketing
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_11_nonpart_noncompat_sorting
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_12_nonnative_export
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_13_nonnative_import
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_14_nonpart_part
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_15_part_nonpart
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_16_part_noncompat_schema
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_17_part_spec_underspec
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_18_part_spec_missing
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_19_external_over_existing
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_20_managed_location_over_existing
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_21_part_managed_external
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_22_export_authfail
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_23_import_exist_authfail
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_24_import_part_authfail
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_exim_25_import_nonexist_authfail
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_external1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_external2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_fetchtask_ioexception
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_fileformat_bad_class
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_fileformat_void_input
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_fileformat_void_output
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_fs_default_name1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_fs_default_name2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_gby_star
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_gby_star2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_genericFileFormat
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_groupby2_map_skew_multi_distinct
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_groupby2_multi_distinct
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_groupby3_map_skew_multi_distinct
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_groupby3_multi_distinct
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_groupby_cube1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_groupby_cube2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_groupby_grouping_id1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_groupby_grouping_sets1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_groupby_grouping_sets2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_groupby_grouping_sets3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_groupby_grouping_sets4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_groupby_grouping_sets5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_groupby_grouping_sets6
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_groupby_grouping_sets7
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_groupby_invalid_position
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_groupby_key
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_groupby_rollup1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_groupby_rollup2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_having1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_illegal_partition_type
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_illegal_partition_type2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_illegal_partition_type3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_illegal_partition_type4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_index_bitmap_no_map_aggr
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_index_compact_entry_limit
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_index_compact_size_limit
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_input1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_input2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_input4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_input41
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_input_part0_neg
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into6
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into_with_schema
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into_with_schema1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into_with_schema2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into_with_schema3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into_with_schema4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_sorted
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_values_sorted
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_view_failure
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insertexternal1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insertover_dynapart_ifnotexists
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insertsel_fail
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_interval_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_interval_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_interval_3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_avg_syntax
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_from_binary_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_from_binary_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_from_binary_3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_from_binary_4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_from_binary_5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_from_binary_6
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_to_binary_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_to_binary_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_to_binary_3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_to_binary_4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_to_binary_5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_to_binary_6
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_char_length_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_char_length_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_char_length_3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_config1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_config2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_create_tbl1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_create_tbl2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_distinct1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_distinct2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_distinct3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_mapjoin1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_max_syntax
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_min_syntax
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_select_column
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_select_column_with_subquery
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_select_column_with_tablename
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_select_expression
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_std_syntax
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_stddev_samp_syntax
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_sum_syntax
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_t_alter1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_t_alter2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_t_create2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_t_transform
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_tbl_name
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_var_samp_syntax
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_varchar_length_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_varchar_length_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_varchar_length_3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_variance_syntax
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalidate_view1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ivyDownload
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_join2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_join28
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_join29
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_join32
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_join35
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_join_alt_syntax_comma_on
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_join_cond_unqual_ambiguous
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_join_cond_unqual_ambiguous_vc
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_join_nonexistent_part
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_joinneg
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lateral_view_alias
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lateral_view_join
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_limit_partition
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_limit_partition_stats
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_line_terminator
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_load_exist_part_authfail
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_load_non_native
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_load_nonpart_authfail
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_load_orc_negative1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_load_orc_negative2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_load_orc_negative3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_load_orc_negative_part
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_load_part_authfail
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_load_part_nospec
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_load_stored_as_dirs
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_load_view_failure
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_load_wrong_fileformat
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_load_wrong_fileformat_rc_seq
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_load_wrong_fileformat_txt_seq
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_load_wrong_noof_part
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg_query_tbl_in_locked_db
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg_try_db_lock_conflict
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg_try_drop_locked_db
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg_try_lock_db_in_use
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_macro_unused_parameter
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_merge_negative_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_merge_negative_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_merge_negative_3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_mismatch_columns_insertion
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_nested_complex_neg
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_no_matching_udf
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_nonkey_groupby
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_nopart_insert
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_nopart_load
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_notable_alias4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_nvl_mismatch_type
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_orderby_invalid_position
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_orderby_position_unsupported
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_orderbysortby
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_part_col_complex_type
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_AggrFuncsWithNoGBYNoPartDef
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_AmbiguousWindowDefn
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_DistributeByOrderBy
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_DuplicateWindowAlias
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_HavingLeadWithNoGBYNoWindowing
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_HavingLeadWithPTF
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_InvalidValueBoundary
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_JoinWithAmbigousAlias
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_NoWindowDefn
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_PartitionBySortBy
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_negative_WhereWithRankCond
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_window_boundaries
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_ptf_window_boundaries2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_recursive_view
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_regex_col_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_regex_col_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_regex_col_groupby
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_sample
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_script_broken_pipe2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_script_broken_pipe3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_script_error
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_selectDistinctStarNeg_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_selectDistinctStarNeg_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_select_charliteral
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_select_star_suffix
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_select_udtf_alias
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_semijoin1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_semijoin2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_semijoin3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_semijoin4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_serde_regex
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_serde_regex2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_serde_regex3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_set_hiveconf_validation0
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_set_hiveconf_validation1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_set_hiveconf_validation2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_set_table_property
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_show_columns1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_show_columns2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_show_columns3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_show_create_table_does_not_exist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_show_create_table_index
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_show_partitions1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_show_tableproperties1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_show_tables_bad1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_show_tables_bad2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_show_tables_bad_db1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_show_tables_bad_db2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_show_tablestatus
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_show_tablestatus_not_existing_part
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_shutdown
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_smb_bucketmapjoin
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_sortmerge_mapjoin_mismatch_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_split_sample_out_of_range
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_split_sample_wrong_format
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_split_sample_wrong_format2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_aggregator_error_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_aggregator_error_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_noscan_non_native
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_non_external
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_non_native
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partscan_norcfile
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_publisher_error_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_publisher_error_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_strict_join
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_strict_orderby
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_strict_pruning
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_subq_insert
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_subquery_exists_implicit_gby
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_subquery_in_groupby
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_subquery_in_select
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_subquery_missing_from
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_subquery_multiple_cols_in_select
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_subquery_nested_subquery
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_subquery_notexists_implicit_gby
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_subquery_shared_alias
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_subquery_subquery_chain
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_subquery_windowing_corr
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_subquery_with_or_cond
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_temp_table_authorize_create_tbl
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_temp_table_create_like_partitions
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_temp_table_index
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_temp_table_partitions
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_temp_table_rename
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_timestamp_literal
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_touch1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_touch2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_bucketed_column
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_column_indexed_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_column_list_bucketing
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_column_seqfile
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_nonexistant_column
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_partition_column
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_partition_column2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_table_failure1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_table_failure2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_table_failure3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_table_failure4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udaf_collect_set_unsupported
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udaf_invalid_place
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_add_months_error_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_add_months_error_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_array_contains_wrong1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_array_contains_wrong2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_assert_true
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_assert_true2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_coalesce
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_concat_ws_wrong1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_concat_ws_wrong2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_concat_ws_wrong3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_elt_wrong_args_len
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_elt_wrong_type
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_field_wrong_args_len
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_field_wrong_type
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_format_number_wrong1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_format_number_wrong2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_format_number_wrong3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_format_number_wrong4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_format_number_wrong5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_format_number_wrong6
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_format_number_wrong7
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_function_does_not_implement_udf
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_greatest_error_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_greatest_error_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_greatest_error_3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_greatest_error_4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_if_not_bool
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_if_wrong_args_len
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_in
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_instr_wrong_args_len
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_instr_wrong_type
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_invalid
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_last_day_error_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_last_day_error_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_locate_wrong_args_len
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_locate_wrong_type
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_map_keys_arg_num
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_map_keys_arg_type
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_map_values_arg_num
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_map_values_arg_type
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_max
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_min
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_next_day_error_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_next_day_error_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_nonexistent_resource
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_printf_wrong1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_printf_wrong2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_printf_wrong3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_printf_wrong4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_qualified_name
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_reflect_neg
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_size_wrong_args_len
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_size_wrong_type
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_sort_array_wrong1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_sort_array_wrong2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_sort_array_wrong3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_test_error
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_test_error_reduce
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_trunc_error1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_trunc_error2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_when_type_wrong
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udtf_explode_not_supported1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udtf_explode_not_supported2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udtf_explode_not_supported3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udtf_explode_not_supported4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udtf_invalid_place
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udtf_not_supported1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udtf_not_supported3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_union2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_union22
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unionClusterBy
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unionDistributeBy
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unionLimit
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unionOrderBy
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unionSortBy
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_uniquejoin
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_uniquejoin2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_uniquejoin3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unset_table_property
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unset_view_property
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_update_bucket_col
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_update_no_such_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_update_non_acid_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_update_not_acid
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_update_not_bucketed
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_update_partition_col
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_update_sorted
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_windowing_invalid_udaf
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_windowing_leadlag_in_udaf
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_windowing_ll_no_neg
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_windowing_ll_no_over
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_wrong_column_type
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_add_part_multiple
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_alter_merge_orc
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_alter_merge_stats_orc
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_join13
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_join14
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_join19
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_join5
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_join7
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_join9
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_12
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_15
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_6
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_7
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_8
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucket3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucket_map_join_spark1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucket_map_join_spark2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucket_map_join_spark3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucket_map_join_tez1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucket_map_join_tez2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucketmapjoin1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucketmapjoin10
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucketmapjoin11
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucketmapjoin12
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucketmapjoin13
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucketmapjoin2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucketmapjoin3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucketmapjoin5
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucketmapjoin7
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucketmapjoin8
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucketmapjoin9
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucketmapjoin_negative
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucketmapjoin_negative2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucketsortoptimize_insert_2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucketsortoptimize_insert_4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucketsortoptimize_insert_6
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucketsortoptimize_insert_7
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_bucketsortoptimize_insert_8
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_cbo_gby
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_cbo_gby_empty
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_cbo_limit
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_cbo_semijoin
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_cbo_simple_select
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_cbo_stats
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_cbo_subq_in
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_cbo_subq_not_in
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_cbo_udf_udaf
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_cbo_union
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_column_access_stats
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_cross_join
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_cross_product_check_2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_date_join1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_date_udf
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_disable_merge_for_bucketing
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_enforce_order
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_escape_orderby1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_escape_sortby1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_filter_join_breaktask2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby10
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby11
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby1_map_nomap
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby1_noskew
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby2_map
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby2_map_multi_distinct
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby2_noskew_multi_distinct
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby3_map_skew
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby3_noskew
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby3_noskew_multi_distinct
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby4_map
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby4_map_skew
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby4_noskew
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby5_map
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby5_noskew
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby6
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby6_map
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby6_map_skew
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby7
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby7_map
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby7_map_multi_single_reducer
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby7_map_skew
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby7_noskew
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby8
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby8_map
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby8_map_skew
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_bigdata
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_complex_types
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_grouping_id2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_map_ppr
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_map_ppr_multi_distinct
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_multi_insert_common_distinct
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_multi_single_reducer3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_position
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_ppr
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_ppr_multi_distinct
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_resolution
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_rollup1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_sort_1_23
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_sort_skew_1_23
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_having
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_index_auto_self_join
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_input12
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_input13
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_input14
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_input17
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_input1_limit
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_input_part2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_insert_into1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_insert_into2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_insert_into3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join0
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join12
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join13
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join14
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join18
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join18_multi_distinct
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join19
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join22
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join24
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join26
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join27
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join28
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join31
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join32
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join32_lessSize
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join33
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join35
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join36
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join37
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join6
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join8
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join9
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_1to1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_alt_syntax
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_array
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_casesensitive
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_cond_pushdown_1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_cond_pushdown_2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_cond_pushdown_3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_cond_pushdown_4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_cond_pushdown_unqual1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_cond_pushdown_unqual2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_cond_pushdown_unqual3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_cond_pushdown_unqual4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_hive_626
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_literals
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_map_ppr
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_merge_multi_expressions
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_merging
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_star
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_thrift
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_vc
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_view
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_lateral_view_explode2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_leftsemijoin
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_limit_partition_metadataonly
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_limit_pushdown
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_list_bucket_dml_2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part10
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part11
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part12
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part13
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part14
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part15
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part5
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part6
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part7
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part8
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part9
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_louter_join_ppr
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_mapjoin1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_mapjoin_distinct
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_mapjoin_filter_on_outerjoin
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_mapjoin_mapjoin
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_mapjoin_memcheck
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_mapjoin_subquery
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_mapjoin_subquery2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_mapreduce1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_merge1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_merge2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_mergejoins
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_mergejoins_mixed
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_metadata_only_queries
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_metadata_only_queries_with_filters
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_multi_insert
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_multi_insert_gby
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_multi_insert_gby2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_multi_insert_lateral_view
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_multi_insert_mixed
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_multi_insert_move_tasks_share_dependencies
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_multigroupby_singlemr
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_nullgroup
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_nullgroup2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_nullgroup4_multi_distinct
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_optimize_nullscan
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_order
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_order2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_outer_join_ppr
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_parallel
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_parallel_join0
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_parquet_join
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_pcr
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ppd_gby_join
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ppd_join
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ppd_join2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ppd_join3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ppd_join5
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ppd_join_filter
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ppd_multi_insert
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ppd_outer_join1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ppd_outer_join2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ppd_outer_join3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ppd_outer_join4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ptf
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ptf_decimal
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ptf_general_queries
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ptf_matchpath
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ptf_rcfile
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ptf_register_tblfn
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ptf_streaming
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_router_join_ppr
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_runtime_skewjoin_mapjoin_spark
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_sample1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_sample10
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_sample2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_sample3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_sample4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_sample5
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_sample7
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_sample8
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_script_env_var1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_script_env_var2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_semijoin
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_shutdown
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoin
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoin_noskew
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoin_union_remove_2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoinopt10
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoinopt11
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoinopt12
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoinopt13
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoinopt14
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoinopt15
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoinopt16
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoinopt17
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoinopt19
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoinopt2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoinopt20
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoinopt3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoinopt4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoinopt5
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoinopt6
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoinopt7
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoinopt8
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_10
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_11
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_12
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_13
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_15
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_16
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_17
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_18
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_19
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_20
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_21
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_22
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_25
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_5
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_6
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_8
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_sort
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats0
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats10
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats12
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats13
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats14
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats15
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats16
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats18
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats20
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats5
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats6
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats7
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats8
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats_counter
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats_counter_partitioned
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats_noscan_1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats_noscan_2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats_only_null
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_statsfs
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_subquery_exists
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_subquery_in
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_subquery_multiinsert
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_table_access_keys_stats
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_temp_table
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_temp_table_gb1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_temp_table_join1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_tez_join_tests
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_tez_joins_explain
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_timestamp_1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_timestamp_2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_timestamp_comparison
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_timestamp_lazy
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_timestamp_null
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_timestamp_udf
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_transform1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_transform2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_udaf_collect_set
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_udf_example_add
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_udf_in_file
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_udf_max
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_udf_min
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_udf_percentile
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union10
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union11
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union12
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union13
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union14
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union15
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union17
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union18
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union19
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union20
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union21
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union22
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union24
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union25
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union26
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union28
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union29
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union30
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union31
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union32
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union33
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union34
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union5
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union6
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union7
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union8
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union9
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_date
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_date_trim
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_lateralview
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_11
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_15
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_16
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_17
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_18
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_25
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_6_subq
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_script
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_view
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_uniquejoin
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_varchar_join1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vector_between_in
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vector_cast_constant
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vector_count_distinct
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vector_data_types
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vector_decimal_aggregate
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vector_decimal_mapjoin
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vector_distinct_2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vector_elt
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vector_groupby_3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vector_left_outer_join
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vector_mapjoin_reduce
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vector_orderby_5
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vector_string_concat
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vector_varchar_4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_0
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_10
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_11
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_12
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_13
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_14
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_15
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_16
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_17
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_5
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_6
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_9
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_decimal_date
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_div0
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_nested_udf
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_not
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_part
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_part_project
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_pushdown
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_short_regress
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorized_case
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorized_mapjoin
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorized_math_funcs
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorized_nested_mapjoin
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorized_ptf
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorized_rcfile_columnar
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorized_shufflejoin
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorized_string_funcs
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorized_timestamp_funcs
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_windowing
org.apache.hadoop.hive.metastore.TestEmbeddedHiveMetaStore.testPartitionFilter
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testPartitionFilter
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testPartitionFilter
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testPartitionFilter
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testPartitionFilter
org.apache.hadoop.hive.ql.parse.TestParseNegative.testParseNegative_ambiguous_join_col
org.apache.hadoop.hive.ql.parse.TestParseNegative.testParseNegative_insert_wrong_number_columns
org.apache.hadoop.hive.ql.parse.TestParseNegative.testParseNegative_invalid_dot
org.apache.hadoop.hive.ql.parse.TestParseNegative.testParseNegative_invalid_function_param2
org.apache.hadoop.hive.ql.parse.TestParseNegative.testParseNegative_invalid_index
org.apache.hadoop.hive.ql.parse.TestParseNegative.testParseNegative_nonkey_groupby
org.apache.hadoop.hive.ql.parse.TestParseNegative.testParseNegative_unknown_column1
org.apache.hadoop.hive.ql.parse.TestParseNegative.testParseNegative_unknown_column2
org.apache.hadoop.hive.ql.parse.TestParseNegative.testParseNegative_unknown_column3
org.apache.hadoop.hive.ql.parse.TestParseNegative.testParseNegative_unknown_column4
org.apache.hadoop.hive.ql.parse.TestParseNegative.testParseNegative_unknown_column5
org.apache.hadoop.hive.ql.parse.TestParseNegative.testParseNegative_unknown_column6
org.apache.hadoop.hive.ql.parse.TestParseNegative.testParseNegative_unknown_function1
org.apache.hadoop.hive.ql.parse.TestParseNegative.testParseNegative_unknown_function2
org.apache.hadoop.hive.ql.parse.TestParseNegative.testParseNegative_unknown_function3
org.apache.hadoop.hive.ql.parse.TestParseNegative.testParseNegative_unknown_function4
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation
org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Delimited
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5275/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5275/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5275/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2775 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12755772 - PreCommit-HIVE-TRUNK-Build;;;","09/Nov/15 21:11;ashutoshc;Failed tests are likely relevant. Also, as Sergey said if this is not specified in config, then default (empty string for schema) will be used, which should be ok, correct?;;;","29/Jun/17 23:11;sershe;Rebased, removed tabs, changed to use pre-set strings, also updated the newly added queries since the last patch and I think a few places that were missed. [~ashutoshc] can you please take a look?;;;","30/Jun/17 01:42;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12875152/HIVE-6990.06.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 11 failed/errored test(s), 10832 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=144)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=141)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=99)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=233)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testBootstrapFunctionReplication (batchId=217)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testCreateFunctionIncrementalReplication (batchId=217)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testCreateFunctionWithFunctionBinaryJarsOnHDFS (batchId=217)
org.apache.hadoop.hive.ql.txn.compactor.TestWorker2.minorPartitionWithBase (batchId=258)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=178)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=178)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=178)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5839/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5839/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5839/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 11 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12875152 - PreCommit-HIVE-Build;;;","30/Jun/17 04:33;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12875152/HIVE-6990.06.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 11 failed/errored test(s), 10832 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[insert_overwrite_local_directory_1] (batchId=238)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=144)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=141)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=146)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=99)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testBootstrapFunctionReplication (batchId=217)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testCreateFunctionIncrementalReplication (batchId=217)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testCreateFunctionWithFunctionBinaryJarsOnHDFS (batchId=217)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=178)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=178)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=178)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5842/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5842/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5842/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 11 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12875152 - PreCommit-HIVE-Build;;;","30/Jun/17 05:31;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12875152/HIVE-6990.06.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 10832 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=144)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=141)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testBootstrapFunctionReplication (batchId=217)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testCreateFunctionIncrementalReplication (batchId=217)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testCreateFunctionWithFunctionBinaryJarsOnHDFS (batchId=217)
org.apache.hive.beeline.TestBeeLineWithArgs.testQueryProgressParallel (batchId=221)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=178)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=178)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=178)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5843/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5843/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5843/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12875152 - PreCommit-HIVE-Build;;;","06/Jul/17 14:16;ashutoshc;+1;;;","07/Jul/17 15:46;ashutoshc;Pushed to master. Thanks, Sergey!;;;","22/May/18 23:57;vgarg;Hive 3.0.0 has been released so closing this jira.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error with arithmetic operators with javaXML serialization,HIVE-6989,12711339,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,29/Apr/14 23:49,13/Nov/14 19:42,14/Jul/23 06:14,13/May/14 16:22,0.13.0,,,,,,,,,0.14.0,,,,,,0,,,"A couple of members in GenericUDFBaseNumeric do not have getters/setters, which prevents them from being serialized as part of the query plan when using javaXML serialization. As a result, the following query:
{noformat}
select key + key from src limit 5;
{noformat}

fails with the following error:

{noformat}
java.lang.Exception: java.lang.RuntimeException: Error in configuring object
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:401)
Caused by: java.lang.RuntimeException: Error in configuring object
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:425)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:233)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)
        ... 11 more
Caused by: java.lang.RuntimeException: Error in configuring object
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
        at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38)
        ... 16 more
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)
        ... 19 more
Caused by: java.lang.RuntimeException: Map operator initialization failed
        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:154)
        ... 24 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.initialize(GenericUDFBaseNumeric.java:109)
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(GenericUDF.java:116)
        at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.initialize(ExprNodeGenericFuncEvaluator.java:127)
        at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorHead.initialize(ExprNodeEvaluatorHead.java:39)
        at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:931)
        at org.apache.hadoop.hive.ql.exec.Operator.initEvaluatorsAndReturnStruct(Operator.java:957)
        at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:65)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
        at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
        at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:189)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
        at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:425)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:133)
        ... 24 more
{noformat}",,jdere,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/14 23:51;jdere;HIVE-6989.1.patch;https://issues.apache.org/jira/secure/attachment/12642562/HIVE-6989.1.patch","30/Apr/14 20:33;jdere;HIVE-6989.2.patch;https://issues.apache.org/jira/secure/attachment/12642713/HIVE-6989.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389660,,,,Thu Nov 13 19:42:41 UTC 2014,,,,,,,,,,"0|i1v4af:",389902,,,,,,,,,,,,,,,,,,,,,"29/Apr/14 23:51;jdere;Add getters/setters necessary for javaXML serialization.;;;","30/Apr/14 07:36;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642562/HIVE-6989.1.patch

{color:red}ERROR:{color} -1 due to 18 failed/errored test(s), 5424 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.ql.parse.TestParse.testParse_cast1
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input20
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input8
org.apache.hadoop.hive.ql.parse.TestParse.testParse_join2
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample1
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample2
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample3
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample5
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample6
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample7
org.apache.hadoop.hive.ql.parse.TestParse.testParse_udf4
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/83/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/83/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 18 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642562;;;","30/Apr/14 19:39;jdere;Looks like the TestParse failures need to be addressed by updating the golden files used in the diff.;;;","30/Apr/14 20:33;jdere;Fix TestParse failures by updating output xml files.;;;","01/May/14 15:48;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642713/HIVE-6989.2.patch

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 5426 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority2
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_stats_counter
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/95/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/95/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642713;;;","02/May/14 01:42;jdere;Just ran schemaAuthority2.q,stats_counter.q locally and they pass.  They also didn't show up as failures during the test from the previous patch, so I'm guessing something in MiniMR was flaky.;;;","12/May/14 20:01;ashutoshc;+1;;;","13/May/14 16:22;ashutoshc;Committed to trunk. Thanks, Jason!;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MatchPath fails with small resultExprString,HIVE-6986,12711213,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,Fixed,fpin,fpin,fpin,29/Apr/14 16:46,13/Nov/14 19:43,14/Jul/23 06:14,23/May/14 06:19,,,,,,,,,,0.14.0,,UDF,,,,0,,,"When using MatchPath, a query like this:
select year
from matchpath(on 
        flights_tiny 
        sort by fl_num, year, month, day_of_month  
      arg1('LATE.LATE+'), 
      arg2('LATE'), arg3(arr_delay > 15), 
    arg4('year') 
   )
;

will fail with error message 
""FAILED: StringIndexOutOfBoundsException String index out of range: 6""

",,fpin,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/14 17:16;fpin;HIVE-6986.1.patch;https://issues.apache.org/jira/secure/attachment/12642490/HIVE-6986.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389534,,,,Thu Nov 13 19:43:17 UTC 2014,,,,,,,,,,"0|i1v3if:",389776,,,,,,,,,,,,,,,,,,,,,"29/Apr/14 17:16;fpin;Please review, 
(this is my first time submitting a patch to an Apache Project);;;","12/May/14 20:40;ashutoshc;Thanks [~fpin] for patch. I think better fix might be to just do r.startsWith(""select"") Can you try that?;;;","14/May/14 12:41;fpin;Hi Ashutosh,

r.startsWith(""select"") wouldn't match upper cases, and unfortunately there is no built-in String.startsWithIgnoreCase in Java I believe.

r.toLowerCase().equals(""select"") would work of course, but I wanted to preserve the initial (futile) optimisation with the r.substring(0,6), so that
we don't cast the whole query to lowercase, but just its prefix.;;;","14/May/14 16:05;ashutoshc;OK. +1;;;","23/May/14 06:19;ashutoshc;Committed to trunk. Thanks, Furcy!;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sql std auth - privileges grants to public role not being honored,HIVE-6985,12711072,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,thejas,thejas,thejas,29/Apr/14 01:03,09/Jun/14 06:39,14/Jul/23 06:14,01/May/14 17:58,0.13.0,,,,,,,,,0.13.1,0.14.0,Authorization,SQLStandardAuthorization,,,0,,,"When a privilege is granted to public role, the privilege is supposed to be applicable for all users.
However, the privilege check fails for users, even if the have public role in the list of current roles.

Note that the issue is only with public role. Grant of privileges of other role are not affected.
",,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/14 20:41;thejas;HIVE-6985.1.patch;https://issues.apache.org/jira/secure/attachment/12642717/HIVE-6985.1.patch","30/Apr/14 22:13;thejas;HIVE-6985.2.patch;https://issues.apache.org/jira/secure/attachment/12642737/HIVE-6985.2.patch","30/Apr/14 22:25;thejas;HIVE-6985.3.patch;https://issues.apache.org/jira/secure/attachment/12642743/HIVE-6985.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389393,,,,Mon Jun 09 06:39:37 UTC 2014,,,,,,,,,,"0|i1v2nb:",389636,,,,,,,,,,,,,,,,,,,,,"30/Apr/14 22:13;thejas;HIVE-6985.2.patch - includes test;;;","30/Apr/14 22:15;thejas;HIVE-6985.2.patch uploaded to review board.
;;;","30/Apr/14 22:25;thejas;HIVE-6985.3.patch - this one really includes the tests !;;;","01/May/14 00:09;ashutoshc;+1;;;","01/May/14 12:04;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642743/HIVE-6985.3.patch

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 5423 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/93/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/93/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642743;;;","01/May/14 17:58;thejas;Test failures are unrelated to this change.
Patch committed to trunk.
Thanks for the review Ashutosh!
;;;","01/May/14 18:00;thejas;[~sushanth] Can you please include this for 0.13.1 ?
This bug is critical.
;;;","01/May/14 18:20;sushanth;Yup, this bug is good to add for 0.13.1 :);;;","09/Jun/14 06:39;thejas;This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Analyzing partitioned table with NULL values for the partition column failed with NPE,HIVE-6984,12711055,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,xuefuz,xuefuz,xuefuz,28/Apr/14 23:08,13/Nov/14 19:40,14/Jul/23 06:14,01/May/14 23:07,0.14.0,,,,,,,,,0.14.0,,Statistics,,,,1,,,"The following describes how to produce the bug:
{code}
hive> desc test2;
name                	string              	                    
age                 	int                 	                    

hive> select * from test2;
6666666666666666666	NULL
5555555555555555555	NULL
tom	15
john	NULL
mayr	40
	30
	NULL

hive> create table test3(name string) partitioned by (age int);

hive> from test2 insert overwrite table test3 partition(age) select test2.name, test2.age;
Loading data to table default.test3 partition (age=null)
	Loading partition {age=40}
	Loading partition {age=__HIVE_DEFAULT_PARTITION__}
	Loading partition {age=30}
	Loading partition {age=15}
Partition default.test3{age=15} stats: [numFiles=1, numRows=1, totalSize=4, rawDataSize=3]
Partition default.test3{age=30} stats: [numFiles=1, numRows=1, totalSize=1, rawDataSize=0]
Partition default.test3{age=40} stats: [numFiles=1, numRows=1, totalSize=5, rawDataSize=4]
Partition default.test3{age=__HIVE_DEFAULT_PARTITION__} stats: [numFiles=1, numRows=4, totalSize=46, rawDataSize=42]

hive> analyze table test3 partition(age) compute statistics;
...
Task with the most failures(4): 
-----
Diagnostic Messages for this Task:
java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""name"":""6666666666666666666"",""age"":null,""raw__data__size"":19}
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:195)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:417)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:268)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1499)
	at org.apache.hadoop.mapred.Child.main(Child.java:262)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""name"":""6666666666666666666"",""age"":null,""raw__data__size"":19}
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:549)
	at org.apache.hado

FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
{code}

The following is the stack trace in mapper log:
{code}
2014-04-28 15:39:25,073 FATAL org.apache.hadoop.hive.ql.exec.mr.ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""name"":""6666666666666666666"",""age"":null,""raw__data__size"":19}
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:549)
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:417)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:268)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1499)
	at org.apache.hadoop.mapred.Child.main(Child.java:262)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.gatherStats(TableScanOperator.java:149)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:90)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:539)
	... 9 more
{code}",,sershe,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/14 23:24;xuefuz;HIVE-6984.1.patch;https://issues.apache.org/jira/secure/attachment/12642553/HIVE-6984.1.patch","30/Apr/14 17:45;xuefuz;HIVE-6984.2.patch;https://issues.apache.org/jira/secure/attachment/12642690/HIVE-6984.2.patch","29/Apr/14 22:23;xuefuz;HIVE-6984.patch;https://issues.apache.org/jira/secure/attachment/12642543/HIVE-6984.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389376,,,,Thu Nov 13 19:40:02 UTC 2014,,,,,,,,,,"0|i1v2k7:",389622,,,,,,,,,,,,,,,,,,,,,"29/Apr/14 22:26;sershe;+1; one question - should q file have stats output, to verify they are correct in the output?;;;","29/Apr/14 22:30;ashutoshc;Also, instead of adding new data file consider reusing one from data/files/;;;","29/Apr/14 22:37;xuefuz;{quote}
one question - should q file have stats output, to verify they are correct in the output?
{quote}

I tried ""desc extended test2;"" but it shows no stats (it shows ""parameters : {}"").  So, I skipped that. (I think there is a JIRA about that, but wasn't able to find it.) ;;;","29/Apr/14 23:04;sershe;maybe some bogus explain query can be used, I think they output stats;;;","29/Apr/14 23:24;xuefuz;Patch #1 updated based on feedback.;;;","30/Apr/14 10:05;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642553/HIVE-6984.1.patch

{color:red}ERROR:{color} -1 due to 19 failed/errored test(s), 5491 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_count
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_limit_pushdown
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mrr
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/84/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/84/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 19 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642553;;;","30/Apr/14 17:45;xuefuz;Patch #2 updated with regenerated test output.;;;","30/Apr/14 23:10;sershe;+1;;;","01/May/14 08:21;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642690/HIVE-6984.2.patch

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 5427 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/92/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/92/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642690;;;","01/May/14 23:05;xuefuz;The above test failures don't seem related to the patch. They appeared in other test runs.;;;","01/May/14 23:07;xuefuz;Patch committed to trunk. Thanks Sergey for the review.;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop-2 test failures related to quick stats not being populated correctly,HIVE-6979,12710694,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,prasanth_j,prasanth_j,prasanth_j,26/Apr/14 00:31,13/Nov/14 19:44,14/Jul/23 06:14,28/Apr/14 15:41,0.14.0,,,,,,,,,0.14.0,,,,,,0,,,"The test failures that are currently reported by Hive QA running on hadoop-2 (https://issues.apache.org/jira/browse/HIVE-6968?focusedCommentId=13980570&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13980570) are related to difference in the way hadoop FileSystem.globStatus() api behaves. For a directory structure like below
{code}
dir1/file1
dir1/file2
{code}
Two level of path pattern like dir1/*/* will return both files in hadoop 1.x but will return empty result in hadoop 2.x (in fact it will say no such file or directory and return empty file status array). Hadoop 2.x seems to be compliant to linux behaviour (ls dir1/*/*) but hadoop 1.x is not.

As a result of this, the fast statistics (NUM_FILES and TOTAL_SIZE) are populated wrongly causing diffs in qfile tests for hadoop-1 and hadoop-2.",,prasanth_j,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/14 00:37;prasanth_j;HIVE-6979.1.patch;https://issues.apache.org/jira/secure/attachment/12642049/HIVE-6979.1.patch","27/Apr/14 07:52;prasanth_j;HIVE-6979.2.patch;https://issues.apache.org/jira/secure/attachment/12642117/HIVE-6979.2.patch","28/Apr/14 03:52;prasanth_j;HIVE-6979.3.patch;https://issues.apache.org/jira/secure/attachment/12642181/HIVE-6979.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389015,,,,Thu Nov 13 19:44:26 UTC 2014,,,,,,,,,,"0|i1v0bz:",389261,,,,,,,,,,,,,,,,,,,,,"26/Apr/14 00:40;prasanth_j;attaching RB link;;;","26/Apr/14 00:40;prasanth_j;[~ashutoshc]/[~jdere] Can anyone take a look at the test failures fix?;;;","26/Apr/14 01:42;ashutoshc;+1 Left couple of comments on RB, if you prefer to fix them in follow-up, thats fine with me.;;;","26/Apr/14 19:32;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642049/HIVE-6979.1.patch

{color:red}ERROR:{color} -1 due to 32 failed/errored test(s), 5418 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_like
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_database_drop
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_10_external_managed
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_12_external_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_13_managed_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_14_managed_location_over_existing
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_overwrite_local_directory_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_fs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_fs_overwrite
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatCTAS
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_alter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_tblproperties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_noscan_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unset_table_view_property
org.apache.hadoop.hive.cli.TestHBaseMinimrCliDriver.testCliDriver_hbase_bulk
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_external_table_with_space_in_location_path
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_file_with_header_footer
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unset_table_property
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_file_with_header_footer_negative
org.apache.hadoop.hive.ql.security.TestFolderPermissions.testExternalTable
org.apache.hive.hcatalog.api.TestHCatClient.testCreateTableLike
org.apache.hive.hcatalog.pig.TestHCatLoader.testConvertBooleanToInt
org.apache.hive.hcatalog.pig.TestHCatLoaderStorer.testReadWrite
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testConvertBooleanToInt
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/54/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/54/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 32 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642049;;;","26/Apr/14 23:02;ashutoshc;Seems like a progress all union_remove tests are now passing. But seems like some new failures got introduced. I cannot repro some, but able to repro few others, like create_like.q, database_drop.q etc.;;;","27/Apr/14 07:52;prasanth_j;Addressed [~ashutoshc]'s review comments. [~ashutoshc] I fixed the recent test failures. Can you please take a look at the changes in RB?;;;","27/Apr/14 12:24;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642117/HIVE-6979.2.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 5419 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_25
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/59/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/59/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642117;;;","27/Apr/14 19:09;ashutoshc;union_remove_25 wasn't expected to fail. Prashant, can you take a look?;;;","28/Apr/14 03:52;prasanth_j;union_remove_25 was failing as it wasn't able to find partition. So increased the limit count so that the specific partition value is always present. Updated stats_partscan_1_23 test as well. Other tests seems to pass in my local setup (Mac OS X).;;;","28/Apr/14 05:41;ashutoshc;+1;;;","28/Apr/14 08:56;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642181/HIVE-6979.3.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 5420 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/62/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/62/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642181;;;","28/Apr/14 15:41;ashutoshc;Committed this to trunk. Thanks, Prasanth! Lets track remaining failures in follow-up.;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"beeline always exits with 0 status, should exit with non-zero status on error",HIVE-6978,12710679,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,navis,gwenshap,gwenshap,25/Apr/14 22:31,04/Mar/15 15:10,14/Jul/23 06:14,01/Sep/14 01:55,0.12.0,,,,,,,,,0.14.0,,HiveServer2,,,,0,,,"Was supposed to be fixed in Hive 0.12 (HIVE-4364). Doesn't look fixed from here.

[i@p sqoop]$ beeline -u 'jdbc:hive2://p:10000/k;principal=hive/p@L' -e ""select * from MEMBERS"" --outputformat=vertical
scan complete in 3ms
Connecting to jdbc:hive2://p:10000/k;principal=hive/p@L
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/lib/avro/avro-tools-1.7.5-cdh5.0.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Connected to: Apache Hive (version 0.12.0-cdh5.0.0)
Driver: Hive JDBC (version 0.12.0-cdh5.0.0)
Transaction isolation: TRANSACTION_REPEATABLE_READ
-hiveconf (No such file or directory)
hive.aux.jars.path=[redacted]
Error: Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 1:14 Table not found 'MEMBERS' (state=42S02,code=10001)
Beeline version 0.12.0-cdh5.0.0 by Apache Hive
Closing: org.apache.hive.jdbc.HiveConnection
[inter@p sqoop]$ echo $?
0",,gwenshap,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-9263,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/14 06:37;navis;HIVE-6978.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12657554/HIVE-6978.1.patch.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,389000,,,,Thu Nov 13 19:41:52 UTC 2014,,,,,,,,,,"0|i1v08n:",389246,,,,,,,,,,,,,,,,,,,,,"24/Jul/14 18:28;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12657554/HIVE-6978.1.patch.txt

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5756 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/42/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/42/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-42/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12657554;;;","01/Sep/14 00:58;ashutoshc;+1;;;","01/Sep/14 01:07;gwenshap;Thanks for fixing my bug :)

I may be missing something, but it looks like the only error condition covered by unit-tests is an error involving unmatched args.
Can we also add a tests that validates that we get an error code when the query fails (for example as result of SemanticException)? Otherwise this issue may return in the future and we won't know about it.;;;","01/Sep/14 01:55;ashutoshc;Committed to trunk. Thanks, Navis!;;;","01/Sep/14 01:56;ashutoshc;[~gwenshap] Sorry missed your comment. [~navis] Gwen's request is legit, it will be good to add such a test case. Can be done in a follow-up.;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Show query id only when there's jobs on the cluster,HIVE-6976,12710598,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,hagleitn,hagleitn,hagleitn,25/Apr/14 17:29,13/Nov/14 19:44,14/Jul/23 06:14,12/May/14 21:52,,,,,,,,,,0.14.0,,,,,,0,,,No need to print the query id for local-only execution.,,hagleitn,sershe,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/14 17:32;hagleitn;HIVE-6976.1.patch;https://issues.apache.org/jira/secure/attachment/12641963/HIVE-6976.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388919,,,,Thu Nov 13 19:44:07 UTC 2014,,,,,,,,,,"0|i1uzr3:",389165,,,,,,,,,,,,,,,,,,,,,"25/Apr/14 21:28;sershe;+1... perhaps printInfos can be combined;;;","11/May/14 04:17;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641963/HIVE-6976.1.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5503 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/169/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/169/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641963;;;","12/May/14 21:49;hagleitn;Failures unrelated. Happened in the run before as well.;;;","12/May/14 21:52;hagleitn;Committed to trunk. Thanks for the review Sergey!;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jdbc HTTP configuration options should be part of sessionConf part of connection string,HIVE-6972,12710402,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vgumashta,thejas,thejas,24/Apr/14 19:54,08/Dec/15 15:08,14/Jul/23 06:14,11/Oct/14 20:16,0.13.0,,,,,,,,,0.14.0,,HiveServer2,JDBC,,,0,,,"The http connection parameters are currently part of the HiveConf section of the connection string. It should ideally be part of SessionConf section, as that is where rest of the connection parameters are.
HTTP transport parameters are not part of the hiveconfiguration parameters that need to be set.
ie
instead of 
jdbc:hive2://<host>:<port>/<db>?hive.server2.transport.mode=http;hive.server2.thrift.http.path=<http_endpoint>
it should be -
jdbc:hive2://<host>:<port>/<db>;transportMode=http;httpPath=<http_endpoint>

*Release Note:*

The JDBC Uri for HiveServer2 has the form:
{code}
jdbc:hive2://host1:port1,host2:port2/dbName;sess_var_list?hive_conf_list#hive_var_list
{code}

Ideally, sess_var_list is supposed to be used to pass parameters that will be used within the JDBC Driver. However, some of the http mode parameters were being passed in the hive_conf_list which we've fixed now (in a backward compatible way).
Additionally, JDBC Driver has a naming convention for the parameters in the JDBC Uri which were not followed to the sasl.qop parameter. We've renamed it in a backward compatible way.

Details:
*1. HTTP Mode params:*
*Old example uri:*
{code}
jdbc:hive2://host:port/dbName;user=username;password=password?hive.server2.transport.mode=http;hive.server2.thrift.http.path=httpEndpoint
{code}

*New example uri:*
{code}
jdbc:hive2://host:port/dbName;user=username;password=password;transportMode=http;httpPath=httpEndpoint?hive.server2.logging.operation.enabled=false
{code}

As you can see above, {{hive.server2.transport.mode}} has been renamed to {{transportMode}}, {{hive.server2.thrift.http.path}} has been renamed to {{httpPath}} and both have been moved from {{hive_conf_list}} to {{sess_var_list}}.

*2. Sasl qop renaming:*
*Old example uri:*
{code}
jdbc:hive2://host:port/dbName;principal=hiveserver2KerberosPrincipal;sasl.qop=qopValue
{code}

*New example uri:*
{code}
jdbc:hive2://host:port/dbName;principal=hiveserver2KerberosPrincipal;saslQop=qopValue
{code}

As you can see {{sasl.qop}} has been renamed to {{saslQop}}.

Note: all changes are backward compatible and print a deprecation message like the following:
{code}
0: jdbc:hive2://localhost:10001> !connect jdbc:hive2://localhost:10001/;sasl.qop=auth?hive.server2.transport.mode=http;hive.server2.thrift.http.path=cliservice username password org.apache.hive.jdbc.HiveDriver
Connecting to jdbc:hive2://localhost:10007/;sasl.qop=auth?hive.server2.transport.mode=http;hive.server2.thrift.http.path=cliservice
14/10/07 16:22:24 INFO jdbc.Utils: Supplied authorities: localhost:10001
14/10/07 16:22:24 WARN jdbc.Utils: ***** JDBC param deprecation *****
14/10/07 16:22:24 WARN jdbc.Utils: The use of sasl.qop is deprecated.
14/10/07 16:22:24 WARN jdbc.Utils: Please use saslQop like so: jdbc:hive2://<host>:<port>/dbName;saslQop=<qop_value>
14/10/07 16:22:24 WARN jdbc.Utils: ***** JDBC param deprecation *****
14/10/07 16:22:24 WARN jdbc.Utils: The use of hive.server2.transport.mode is deprecated.
14/10/07 16:22:24 WARN jdbc.Utils: Please use transportMode like so: jdbc:hive2://<host>:<port>/dbName;transportMode=<transport_mode_value>
14/10/07 16:22:24 WARN jdbc.Utils: ***** JDBC param deprecation *****
14/10/07 16:22:24 WARN jdbc.Utils: The use of hive.server2.thrift.http.path is deprecated.
14/10/07 16:22:24 WARN jdbc.Utils: Please use httpPath like so: jdbc:hive2://<host>:<port>/dbName;httpPath=<http_path_value>
Connected to: Apache Hive (version 0.14.0-SNAPSHOT)
Driver: Hive JDBC (version 0.14.0-SNAPSHOT)
Transaction isolation: TRANSACTION_REPEATABLE_READ
{code}

",,larsfrancke,leftyl,thejas,vgumashta,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/14 23:31;vgumashta;HIVE-6972.1.patch;https://issues.apache.org/jira/secure/attachment/12673471/HIVE-6972.1.patch","07/Oct/14 23:50;vgumashta;HIVE-6972.2.patch;https://issues.apache.org/jira/secure/attachment/12673476/HIVE-6972.2.patch","08/Oct/14 20:02;vgumashta;HIVE-6972.3.patch;https://issues.apache.org/jira/secure/attachment/12673684/HIVE-6972.3.patch","09/Oct/14 07:48;vgumashta;HIVE-6972.4.patch;https://issues.apache.org/jira/secure/attachment/12673851/HIVE-6972.4.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388724,,,,Tue Dec 08 15:08:48 UTC 2015,,,,,,,,,,"0|i1uyl3:",388974,Check jira description.,,,,,,,,,,,,,,,,,,,,"24/Apr/14 19:54;thejas;As we have released with support in HiveConf, we should continue to support its use there. After this proposed changes is implemented, we can stop documenting the HiveConf usage.
;;;","07/Oct/14 23:25;vgumashta;[~vikram.dixit] [~thejas] Do you think we can have this for 14? ;;;","07/Oct/14 23:31;thejas;I think we should include it in 0.14. This is a safe backward compatible change, and it would be good to start supporting the right format.
;;;","07/Oct/14 23:59;vgumashta;Fixed a misplaced comment.;;;","08/Oct/14 00:02;thejas;+1
Thanks for fixing this before more users start using the deprecated format!
;;;","08/Oct/14 01:58;vikram.dixit;+1 for 0.14;;;","08/Oct/14 17:37;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12673481/HIVE-6972.3.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1168/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1168/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1168/

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN LPAREN KW_CASE"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_NOT KW_FALSE"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_DATE StringLiteral"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_NOT KW_TRUE"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_NOT KW_MAP"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_CASE KW_MAP"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN LPAREN KW_MAP"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_CASE KW_UNIONTYPE"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_CASE KW_STRUCT"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_NOT KW_IF"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_CASE KW_IF"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN LPAREN KW_IF"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:115:5: 
Decision can match input such as ""KW_CLUSTER KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:127:5: 
Decision can match input such as ""KW_PARTITION KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:138:5: 
Decision can match input such as ""KW_DISTRIBUTE KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:149:5: 
Decision can match input such as ""KW_SORT KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:166:7: 
Decision can match input such as ""STAR"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:179:5: 
Decision can match input such as ""KW_STRUCT"" using multiple alternatives: 4, 6

As a result, alternative(s) 6 were disabled for that input
warning(200): IdentifiersParser.g:179:5: 
Decision can match input such as ""KW_ARRAY"" using multiple alternatives: 2, 6

As a result, alternative(s) 6 were disabled for that input
warning(200): IdentifiersParser.g:179:5: 
Decision can match input such as ""KW_UNIONTYPE"" using multiple alternatives: 5, 6

As a result, alternative(s) 6 were disabled for that input
warning(200): IdentifiersParser.g:261:5: 
Decision can match input such as ""KW_TRUE"" using multiple alternatives: 3, 8

As a result, alternative(s) 8 were disabled for that input
warning(200): IdentifiersParser.g:261:5: 
Decision can match input such as ""KW_NULL"" using multiple alternatives: 1, 8

As a result, alternative(s) 8 were disabled for that input
warning(200): IdentifiersParser.g:261:5: 
Decision can match input such as ""KW_FALSE"" using multiple alternatives: 3, 8

As a result, alternative(s) 8 were disabled for that input
warning(200): IdentifiersParser.g:261:5: 
Decision can match input such as ""KW_DATE StringLiteral"" using multiple alternatives: 2, 3

As a result, alternative(s) 3 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""KW_BETWEEN KW_MAP LPAREN"" using multiple alternatives: 8, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_INSERT KW_INTO"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_UNION KW_ALL"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_SORT KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_CLUSTER KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_LATERAL KW_VIEW"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_MAP LPAREN"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_DISTRIBUTE KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_ORDER KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_GROUP KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_INSERT KW_OVERWRITE"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:518:5: 
Decision can match input such as ""{AMPERSAND..BITWISEXOR, DIV..DIVIDE, EQUAL..EQUAL_NS, GREATERTHAN..GREATERTHANOREQUALTO, KW_AND, KW_ARRAY, KW_BETWEEN..KW_BOOLEAN, KW_CASE, KW_DOUBLE, KW_FLOAT, KW_IF, KW_IN, KW_INT, KW_LIKE, KW_MAP, KW_NOT, KW_OR, KW_REGEXP, KW_RLIKE, KW_SMALLINT, KW_STRING..KW_STRUCT, KW_TINYINT, KW_UNIONTYPE, KW_WHEN, LESSTHAN..LESSTHANOREQUALTO, MINUS..NOTEQUAL, PLUS, STAR, TILDE}"" using multiple alternatives: 1, 3

As a result, alternative(s) 3 were disabled for that input
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-exec ---
Downloading: http://www.datanucleus.org/downloads/maven2/net/hydromatic/linq4j/0.4/linq4j-0.4.pom
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [12.308s]
[INFO] Hive Shims Common ................................. SUCCESS [7.475s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [3.777s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [5.297s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.232s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [7.497s]
[INFO] Hive Shims ........................................ SUCCESS [0.921s]
[INFO] Hive Common ....................................... SUCCESS [9.194s]
[INFO] Hive Serde ........................................ SUCCESS [15.308s]
[INFO] Hive Metastore .................................... SUCCESS [33.921s]
[INFO] Hive Ant Utilities ................................ SUCCESS [1.694s]
[INFO] Hive Query Language ............................... FAILURE [29.935s]
[INFO] Hive Service ...................................... SKIPPED
[INFO] Hive Accumulo Handler ............................. SKIPPED
[INFO] Hive JDBC ......................................... SKIPPED
[INFO] Hive Beeline ...................................... SKIPPED
[INFO] Hive CLI .......................................... SKIPPED
[INFO] Hive Contrib ...................................... SKIPPED
[INFO] Hive HBase Handler ................................ SKIPPED
[INFO] Hive HCatalog ..................................... SKIPPED
[INFO] Hive HCatalog Core ................................ SKIPPED
[INFO] Hive HCatalog Pig Adapter ......................... SKIPPED
[INFO] Hive HCatalog Server Extensions ................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client ................. SKIPPED
[INFO] Hive HCatalog Webhcat ............................. SKIPPED
[INFO] Hive HCatalog Streaming ........................... SKIPPED
[INFO] Hive HWI .......................................... SKIPPED
[INFO] Hive ODBC ......................................... SKIPPED
[INFO] Hive Shims Aggregator ............................. SKIPPED
[INFO] Hive TestUtils .................................... SKIPPED
[INFO] Hive Packaging .................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 2:12.261s
[INFO] Finished at: Wed Oct 08 13:36:55 EDT 2014
[INFO] Final Memory: 69M/413M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (default) on project hive-exec: Error resolving project artifact: Could not transfer artifact net.hydromatic:linq4j:pom:0.4 from/to datanucleus (http://www.datanucleus.org/downloads/maven2): Access denied to: http://www.datanucleus.org/downloads/maven2/net/hydromatic/linq4j/0.4/linq4j-0.4.pom, ReasonPhrase: Forbidden. for project net.hydromatic:linq4j:jar:0.4 -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-exec
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12673481;;;","09/Oct/14 07:14;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12673684/HIVE-6972.3.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 6531 tests executed
*Failed tests:*
{noformat}
org.apache.hive.jdbc.TestJdbcDriver2.testParseUrlHttpMode
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1184/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1184/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1184/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12673684;;;","09/Oct/14 07:48;vgumashta;Fixes test failure.;;;","11/Oct/14 05:39;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12673851/HIVE-6972.4.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4137 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_tez_smb_1
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1205/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1205/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1205/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12673851;;;","11/Oct/14 17:55;vgumashta;[~thejas] I don't think the test failure is related, but since all tests are not running, I'm not sure if this should be committed yet. 
Although I do see all the test from the following packages pass:
{code}
org.apache.hive.jdbc	
org.apache.hive.jdbc.authorization	
org.apache.hive.jdbc.miniHS2	
org.apache.hive.minikdc	
org.apache.hive.service	
org.apache.hive.service.auth		
org.apache.hive.service.cli		
org.apache.hive.service.cli.operation		
org.apache.hive.service.cli.session		
org.apache.hive.service.cli.thrift		
org.apache.hive.service.server
{code}

I doubt if the patch affects any other tests.;;;","11/Oct/14 18:37;thejas;Yes, those packages should have all tests that can be affected by this change. I think we can go ahead and commit.
;;;","11/Oct/14 20:16;vgumashta;Patch committed to 14 and trunk as all of the tests for jdbc/HiveServer2 seem to be running . Thanks for reviewing [~thejas]!;;;","11/Oct/14 20:20;vgumashta;Sorry missed out adding ""reviewed by"". ;;;","12/Oct/14 00:32;leftyl;What documentation does this need?  Suggestions:

* [HiveServer2 Clients -- Connection URL When Hive Server2 Is Running in HTTP Mode | https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-ConnectionURLWhenHiveServer2IsRunninginHTTPMode]
* [HiveServer2 Clients -- Connection URL When SSL Is Enabled in HiveServer2 | https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-ConnectionURLWhenSSLIsEnabledinHiveServer2]
* [Setting Up HiveServer2 -- Running in HTTP mode | https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2#SettingUpHiveServer2-RunninginHTTPmode]

Also, how about a release note?;;;","12/Oct/14 22:07;vgumashta;[~leftylev] Just added the release note in the description (wasn't able to format the release note text box properly).;;;","13/Oct/14 07:21;leftyl;Very thorough release note, thanks [~vgumashta].  Should the release note field point to the description, or will people just look there anyway?;;;","13/Oct/14 20:23;vgumashta;[~leftylev] Done. Thanks again!;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;","08/Dec/15 15:08;larsfrancke;I just documented this in the wiki on both pages [~leftylev] mentioned.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
list bucketing feature does not update the location map for unpartitioned tables,HIVE-6968,12710201,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,prasanth_j,prasanth_j,prasanth_j,24/Apr/14 00:41,13/Nov/14 19:40,14/Jul/23 06:14,29/Apr/14 01:18,0.11.0,0.12.0,0.13.0,0.14.0,,,,,,0.14.0,,,,,,0,,,"list bucketing feature maintains a map of skewed columns/values to location in metastore. This map is not getting updated for unpartitioned tables. For partitioned tables the location map gets updated properly. To reproduce the issue
{code}
hive>set hive.mapred.supports.subdirectories=true;
hive>set mapred.input.dir.recursive=true;

hive>create table t(col1 string, col2 string);
hive>load  data local inpath '/home/hadoop/a.txt' into table t; 
hive> select * from t;                                                                   
OK
1	a
2	b
3	c
4	a
5	b
6	a

hive>create tablet1(r1 string, r2 string) skewed by (r2) on (‘a’) stored as directories;
hive>insert into table t1 select * from t;
hive>desc extended t1;
OK
r1                  	string              	                    
r2                  	string              	                    
	 	 
Detailed Table Information	Table(tableName:t1, dbName:default, owner:pjayachandran, createTime:1398295903, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:r1, type:string, comment:null), FieldSchema(name:r2, type:string, comment:null)], location:file:/app/warehouse/t1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[r2], skewedColValues:[[a]], skewedColValueLocationMaps:{}), storedAsSubDirectories:true), partitionKeys:[], parameters:{numFiles=6, COLUMN_STATS_ACCURATE=true, transient_lastDdlTime=1398297887, numRows=6, totalSize=72, rawDataSize=18}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
Time taken: 0.119 seconds, Fetched: 4 row(s)
{code}

as seen from describe output *skewedColValueLocationMaps* is empty",,leftyl,prasanth_j,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/14 03:13;prasanth_j;HIVE-6968.1.patch;https://issues.apache.org/jira/secure/attachment/12641642/HIVE-6968.1.patch","29/Apr/14 01:03;prasanth_j;HIVE-6968.2.patch;https://issues.apache.org/jira/secure/attachment/12642379/HIVE-6968.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388523,,,,Thu Nov 13 19:40:34 UTC 2014,,,,,,,,,,"0|i1uxcn:",388774,,,,,,,,,,,,,,,,,,,,,"24/Apr/14 03:16;prasanth_j;Attaching RB link;;;","25/Apr/14 00:56;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641642/HIVE-6968.1.patch

{color:red}ERROR:{color} -1 due to 41 failed/errored test(s), 5419 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/32/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/32/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 41 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641642;;;","25/Apr/14 22:21;prasanth_j;test failures are not related.;;;","26/Apr/14 18:00;ashutoshc;+1;;;","29/Apr/14 01:03;prasanth_j;Refreshed patch against trunk and fix diff after HIVE-6979 changes.;;;","29/Apr/14 01:18;prasanth_j;Committed to trunk.;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive transaction manager fails when SQLServer is used as an RDBMS,HIVE-6967,12710172,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,gates,gates,gates,23/Apr/14 22:25,13/Nov/14 19:43,14/Jul/23 06:14,20/Jun/14 17:33,0.13.0,,,,,,,,,0.14.0,,Locking,,,,0,,,"When using SQLServer as an RDBMS for the metastore, any transaction or DbLockMgr operations fail with:
{code}
MetaException(message:Unable to select from transaction database com.microsoft.sqlserver.jdbc.SQLServerException: Line 1: FOR UPDATE clause allowed only for DECLARE CURSOR.
{code}
The issue is that SQLServer does not support the FOR UPDATE clause in SELECT.",,gates,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/May/14 22:19;gates;HIVE-6967.patch;https://issues.apache.org/jira/secure/attachment/12646620/HIVE-6967.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388494,,,,Thu Nov 13 19:43:51 UTC 2014,,,,,,,,,,"0|i1ux6n:",388747,,,,,,,,,,,,,,,,,,,,,"23/May/14 22:19;gates;A patch that changes the transaction manager to use serializable isolation instead of select for update.  Each operation is allowed to select whether it wants serializable (which ones that used to use select for update use) or read committed (for other operations).;;;","25/May/14 05:53;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646620/HIVE-6967.patch

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 5462 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/286/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/286/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-286/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646620;;;","19/Jun/14 20:40;gates;Failed tests pass when run locally, except for root_dir_external_table which fails against trunk, so I don't think any of these failures relate to this patch.;;;","20/Jun/14 16:07;ashutoshc;+1;;;","20/Jun/14 17:33;ashutoshc;Committed to trunk. Thanks, Alan!;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More fixes for TestCliDriver on Windows,HIVE-6966,12710129,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,23/Apr/14 19:51,13/Nov/14 19:42,14/Jul/23 06:14,01/May/14 01:01,,,,,,,,,,0.14.0,,Tests,Windows,,,0,,,This prevents the Test*CliDriver tests from compiling properly.,,jdere,shanyu,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/14 21:06;jdere;HIVE-6966.1.patch;https://issues.apache.org/jira/secure/attachment/12641575/HIVE-6966.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388451,,,,Thu Nov 13 19:42:56 UTC 2014,,,,,,,,,,"0|i1uwxr:",388706,,,,,,,,,,,,,,,,,,,,,"23/Apr/14 20:32;jdere;Have a few more Windows-specific fixes for TestCliDriver, will also add here;;;","23/Apr/14 21:06;jdere;Patch v1:
- escape hiveConfDir when generating Test*CliDriver
- Fix log4j file scheme on Windows (file:/ instead of file://)
- Add HADOOP_HOME/bin to PATH when running tests on Windows to fix issues with resolving native libs.;;;","23/Apr/14 21:54;shanyu;+1.
I verified that this patch make compile in itests folder work.;;;","24/Apr/14 21:07;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641575/HIVE-6966.1.patch

{color:red}ERROR:{color} -1 due to 41 failed/errored test(s), 5418 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_filters
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/29/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/29/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 41 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641575;;;","30/Apr/14 19:01;sushanth;Looks good to me. +1.;;;","30/Apr/14 19:33;sushanth;The test failures noted here seem to not be related to the patch. [~jdere], could you please verify/confirm? I can go ahead and commit it after that.;;;","30/Apr/14 20:35;jdere;These failures do not look related, they appear to be the list of failing tests on hadoop-2 at the time. Most of these should be resolved by now, we can re-run the tests if you want.;;;","01/May/14 01:01;sushanth;Committed to trunk. Thanks, Jason!;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transaction manager should use RDBMS time instead of machine time,HIVE-6965,12710089,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,gates,gates,gates,23/Apr/14 17:05,13/Nov/14 19:41,14/Jul/23 06:14,20/May/14 04:34,0.13.0,,,,,,,,,0.14.0,,Locking,,,,0,,,"Current TxnHandler and CompactionTxnHandler use System.currentTimeMillis() when they need to determine the time (such as heartbeating transactions).  In situations where there are multiple Thrift metastore services or users are using an embedded metastore this will lead to issues.  We should instead be using time from the RDBMS, which is guaranteed to be the same for all users.",,gates,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/May/14 16:59;gates;HIVE-6965.patch;https://issues.apache.org/jira/secure/attachment/12645587/HIVE-6965.patch","13/May/14 19:46;gates;HIVE-6965.patch;https://issues.apache.org/jira/secure/attachment/12644684/HIVE-6965.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388411,,,,Thu Nov 13 19:41:55 UTC 2014,,,,,,,,,,"0|i1uwov:",388666,,,,,,,,,,,,,,,,,,,,,"13/May/14 19:47;gates;This patch changes the code to ask the database for the time rather than calling currentTimeMillis().;;;","14/May/14 21:51;ashutoshc;+1;;;","19/May/14 16:58;gates;Recycling patch to get tests to run.;;;","19/May/14 20:56;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12645587/HIVE-6965.patch

{color:red}ERROR:{color} -1 due to 16 failed/errored test(s), 5526 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/237/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/237/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 16 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12645587;;;","20/May/14 04:34;gates;Failed tests fail for me on trunk and with the patch and are in completely unrelated code so I do not believe they are a result of this change.  Patch checked in.  Thanks Ashutosh for the review.;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop partitions treats partition columns as strings,HIVE-6961,12709913,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sershe,sershe,sershe,22/Apr/14 23:47,09/Jun/14 06:39,14/Jul/23 06:14,24/Apr/14 02:38,,,,,,,,,,0.13.1,0.14.0,,,,,0,,,Discovered just now while testing HIVE-6945,,jnp,sershe,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/14 23:48;sershe;HIVE-6961.patch;https://issues.apache.org/jira/secure/attachment/12641374/HIVE-6961.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388235,,,,Mon Jun 09 06:39:37 UTC 2014,,,,,,,,,,"0|i1uvlr:",388490,,,,,,,,,,,,,,,,,,,,,"22/Apr/14 23:49;sershe;[~jnp] can you take a look?;;;","23/Apr/14 00:10;jnp;+1, lgtm;;;","24/Apr/14 01:35;sershe;tests passed locally;;;","24/Apr/14 02:38;sershe;committed to trunk;;;","09/Jun/14 06:39;thejas;This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set Hive pom to use Hadoop-2.4,HIVE-6960,12709908,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,22/Apr/14 23:27,08/Jun/17 18:29,14/Jul/23 06:14,04/May/14 16:11,0.14.0,,,,,,,,,0.14.0,,Build Infrastructure,,,,0,,,"A number of the hadoop-2 unit test failures are due to HADOOP-10425, fixed in Hadoop 2.4.  Perhaps we should move onto that version.

- org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
- org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
- org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
- org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
- org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
- org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat",,jdere,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6741,HIVE-6900,HIVE-16860,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/May/14 01:22;jdere;HIVE-6960.1.patch;https://issues.apache.org/jira/secure/attachment/12642785/HIVE-6960.1.patch","26/Apr/14 00:46;jdere;HIVE-6960.1.patch;https://issues.apache.org/jira/secure/attachment/12642050/HIVE-6960.1.patch","02/May/14 20:34;jdere;HIVE-6960.2.patch;https://issues.apache.org/jira/secure/attachment/12643111/HIVE-6960.2.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388230,,,,Thu Nov 13 19:44:08 UTC 2014,,,,,,,,,,"0|i1uvkv:",388486,,,,,,,,,,,,,,,,,,,,,"26/Apr/14 01:36;ashutoshc;+1;;;","27/Apr/14 05:33;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642050/HIVE-6960.1.patch

{color:red}ERROR:{color} -1 due to 42 failed/errored test(s), 5418 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatCTAS
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_alter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_tblproperties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unset_table_view_property
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unset_table_property
org.apache.hive.minikdc.TestJdbcWithMiniKdc.testConnection
org.apache.hive.minikdc.TestJdbcWithMiniKdc.testNegativeTokenAuth
org.apache.hive.minikdc.TestJdbcWithMiniKdc.testProxyAuth
org.apache.hive.minikdc.TestJdbcWithMiniKdc.testTokenAuth
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/58/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/58/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 42 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642050;;;","27/Apr/14 06:20;ashutoshc;TestJdbcwithKDC looks genuine failure. [~jdere] can you take a look?;;;","29/Apr/14 00:57;jdere;Looks like getHadoopSaslProperties() is broken due to HADOOP-10221.  We will need to make a hadoop23 shim for HiveAuthFactory.  Making this change will require hadoop 2.4.0, unless we want to use Java reflection similar to Navis' patch for HIVE-9600.;;;","29/Apr/14 01:20;jdere;The replacement API provided in hadoop 2.4.0 does not have public scope. I've created HADOOP-10547.;;;","29/Apr/14 01:31;vgumashta;[~jdere] I created HIVE-6741 sometime back to fix this. You can actually close the hadoop jira as the plan is to remove the hadoop call (see discussion). I will upload a patch soon.;;;","01/May/14 01:22;jdere;re-upload patch and try tests again;;;","02/May/14 16:33;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642785/HIVE-6960.1.patch

{color:red}ERROR:{color} -1 due to 12 failed/errored test(s), 5428 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/103/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/103/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 12 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642785;;;","02/May/14 17:55;jdere;Will take a look at the q file failures, it's possible that these golden files were changed in HIVE-6979 to have stats values that reflected the bug in HADOOP-10425.;;;","02/May/14 20:34;jdere;patch v2 updates a number of diffs that were changed by HIVE-6979;;;","02/May/14 21:13;ashutoshc;+1;;;","03/May/14 09:51;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12643111/HIVE-6960.2.patch

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 5430 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/110/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/110/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12643111;;;","04/May/14 16:11;ashutoshc;Committed to trunk. Thanks, Jason!;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable Constant propagation optimizer for Hive Vectorization,HIVE-6959,12709897,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hsubramaniyan,hsubramaniyan,hsubramaniyan,22/Apr/14 22:27,13/Nov/14 19:43,14/Jul/23 06:14,11/Aug/14 02:58,0.14.0,,,,,,,,,0.14.0,,Vectorization,,,,0,,,"HIVE-5771 covers Constant propagation optimizer for Hive. Now that HIVE-5771 is committed, we should remove any vectorization related code which duplicates this feature. For example, a fn to be cleaned is VectorizarionContext::foldConstantsForUnaryExprs(). In addition to this change, constant propagation should kick in when vectorization is enabled. i.e. we need to lift the HIVE_VECTORIZATION_ENABLED restriction inside ConstantPropagate::transform().",,hsubramaniyan,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/14 18:08;hsubramaniyan;HIVE-6959.1.patch;https://issues.apache.org/jira/secure/attachment/12658203/HIVE-6959.1.patch","31/Jul/14 22:25;hsubramaniyan;HIVE-6959.2.patch;https://issues.apache.org/jira/secure/attachment/12659025/HIVE-6959.2.patch","06/Aug/14 03:12;hsubramaniyan;HIVE-6959.4.patch;https://issues.apache.org/jira/secure/attachment/12660035/HIVE-6959.4.patch","07/Aug/14 21:51;hsubramaniyan;HIVE-6959.5.patch;https://issues.apache.org/jira/secure/attachment/12660482/HIVE-6959.5.patch","08/Aug/14 19:15;hsubramaniyan;HIVE-6959.6.patch;https://issues.apache.org/jira/secure/attachment/12660699/HIVE-6959.6.patch",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388219,,,,Thu Nov 13 19:43:46 UTC 2014,,,,,,,,,,"0|i1uvif:",388475,,,,,,,,,,,,,,,,,,,,,"28/Jul/14 18:08;hsubramaniyan;Initial patch to check if  the changes break any existing unit tests.;;;","28/Jul/14 19:38;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12658203/HIVE-6959.1.patch

{color:red}ERROR:{color} -1 due to 14 failed/errored test(s), 5784 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_between_in
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_coalesce
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_expressions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_math_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_elt
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_div0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_short_regress
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_math_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_parquet
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testBetweenFilters
org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testInFiltersAndExprs
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/80/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/80/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-80/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 14 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12658203;;;","30/Jul/14 00:32;ashutoshc;Among failed test cases, many changes in .q.out looks correct where constants got folded. However, in some cases there is an additional change which suggests queries didnt vectorize for those cases. Seems like those needs investigation.;;;","30/Jul/14 01:06;hsubramaniyan;[~ashutoshc] You are right. I am looking at the issues and will upload a new patch soon.

Thanks
Hari;;;","31/Jul/14 22:25;hsubramaniyan;2nd upload.;;;","01/Aug/14 00:17;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12659025/HIVE-6959.2.patch

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 5859 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_cast_constant
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_coalesce
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_cast_constant
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataPrimitiveTypes
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/128/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/128/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-128/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12659025;;;","02/Aug/14 00:30;hsubramaniyan;Will upload for review if there is a clean run.;;;","06/Aug/14 21:26;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12660035/HIVE-6959.4.patch

{color:red}ERROR:{color} -1 due to 13 failed/errored test(s), 5881 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_coalesce
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_9
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_opt_vectorization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_cast_constant
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_14
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_15
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_9
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_short_regress
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.testCreateTableWithIndexAndPartitionsNonDefaultNameNode
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/193/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/193/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-193/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 13 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12660035;;;","07/Aug/14 16:40;ashutoshc;Seems like vectorization_14.q & vector_coalesce.q failed to vectorize and vector_cast_constant.q failed altogether.;;;","07/Aug/14 21:51;hsubramaniyan;[~ashutoshc] 1. vectorization_14.q has been fixed with the new patch
2. vector_coalesce happens because of HIVE-7650 and I have disabled constant propagation for the specific  test case temporarily.
3. vector_cast_constant.q exposed an existing hive vectorization issue and I have uploaded this change in the new patch,.

;;;","07/Aug/14 21:51;hsubramaniyan;Fixed the previous failures;;;","08/Aug/14 13:43;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12660482/HIVE-6959.5.patch

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 5885 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_opt_vectorization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_cast_constant
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_14
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_15
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_9
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_short_regress
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.testCreateTableWithIndexAndPartitionsNonDefaultNameNode
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataPrimitiveTypes
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/227/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/227/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-227/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12660482;;;","08/Aug/14 19:15;hsubramaniyan;updated the MiniTezCliDriver test results as well.;;;","09/Aug/14 06:33;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12660699/HIVE-6959.6.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5887 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_opt_vectorization
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.testCreateTableWithIndexAndPartitionsNonDefaultNameNode
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/241/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/241/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-241/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12660699;;;","10/Aug/14 20:27;ashutoshc;+1;;;","11/Aug/14 02:58;ashutoshc;Committed to trunk. Thanks, Hari!;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL authorization does not work with HS2 binary mode and Kerberos auth,HIVE-6957,12709881,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,thejas,thejas,thejas,22/Apr/14 21:24,09/Jun/14 06:39,14/Jul/23 06:14,28/Apr/14 22:09,0.13.0,,,,,,,,,0.13.1,0.14.0,Authorization,HiveServer2,,,0,,,"In HiveServer2, when Kerberos auth and binary transport modes are used, the user name that gets passed on to authorization is the long kerberos username.
The username that is used in grant/revoke statements tend to be the short usernames.
This also fails in authorizing statements that involve URI, as the authorization mode checks the file system permissions for given user. It does not recognize that the given long username actually owns the file or belongs to the group that owns the file.
",,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/14 17:33;thejas;HIVE-6957.04-branch.0.13.patch;https://issues.apache.org/jira/secure/attachment/12641964/HIVE-6957.04-branch.0.13.patch","22/Apr/14 21:26;thejas;HIVE-6957.1.patch;https://issues.apache.org/jira/secure/attachment/12641337/HIVE-6957.1.patch","24/Apr/14 22:15;thejas;HIVE-6957.2.patch;https://issues.apache.org/jira/secure/attachment/12641810/HIVE-6957.2.patch","24/Apr/14 23:19;thejas;HIVE-6957.3.patch;https://issues.apache.org/jira/secure/attachment/12641824/HIVE-6957.3.patch","25/Apr/14 00:34;thejas;HIVE-6957.4.patch;https://issues.apache.org/jira/secure/attachment/12641843/HIVE-6957.4.patch",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388203,,,,Mon Jun 09 06:39:37 UTC 2014,,,,,,,,,,"0|i1uvf3:",388460,,,,,,,,,,,,,,,,,,,,,"22/Apr/14 21:25;thejas;A workaround is to use the http transport mode for HS2.;;;","22/Apr/14 21:27;thejas;The long username is not of any significance within hive. We always use the short username for all purposes including the owner in metastore.
This patch changes the username that gets set for HS2 purposes, to the short username.
;;;","22/Apr/14 21:30;thejas;Error looks like this 
{code}
java.sql.SQLException: Error while compiling statement: FAILED: HiveAccessControlException Permission denied. Principal [name=user2@EXAMPLE.COM, type=USER] does not have following privileges on Object [type=TABLE_OR_VIEW, name=default.test_jdbc_sql_auth2] : [SELECT]
{code};;;","23/Apr/14 16:18;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641337/HIVE-6957.1.patch

{color:red}ERROR:{color} -1 due to 42 failed/errored test(s), 5418 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hive.minikdc.TestJdbcWithMiniKdc.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/14/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/14/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 42 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641337;;;","23/Apr/14 22:06;vgumashta;[~thejas] The patch & added tests look good. I've added some minor comments on rb. The documentation related comments are unrelated to this patch, so we can always do it later. Thanks!;;;","24/Apr/14 22:15;thejas;Addressing review comments.
;;;","24/Apr/14 23:21;thejas;3.patch - fixing TestSSL failures because of change in MiniHS2
;;;","25/Apr/14 00:26;vgumashta;+1 non-binding. Latest patch looks good - tests are super useful, I think I'll use this as base to add more kerberos related tests.;;;","25/Apr/14 00:34;thejas;HIVE-6957.4.patch - rename the abstract base test class to *Test.java
;;;","25/Apr/14 12:14;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641843/HIVE-6957.4.patch

{color:red}ERROR:{color} -1 due to 40 failed/errored test(s), 5420 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/35/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/35/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 40 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641843;;;","25/Apr/14 17:33;thejas;HIVE-6957.04-branch.0.13.patch - patch for 0.13 branch .
;;;","25/Apr/14 18:46;vgumashta;+1;;;","28/Apr/14 08:11;vgumashta;[~thejas] I don't have access to svn yet. This should be good to commit. ;;;","28/Apr/14 22:09;thejas;Patch committed to trunk. Thanks for the review Vaibhav!
;;;","09/Jun/14 06:39;thejas;This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate partitioning column for union when dynamic partition sort optimization is enabled,HIVE-6956,12709873,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,prasanth_j,prasanth_j,prasanth_j,22/Apr/14 21:14,13/Nov/14 19:43,14/Jul/23 06:14,24/Apr/14 06:36,0.13.0,0.14.0,,,,,,,,0.14.0,,,,,,0,optimization,,"HIVE-6455 optimization uses PreOrder graph walker resulting in double invocation of SortedDynPartitionOptimizer(). Because of this there are duplicate partitioning columns in RSOp for few test cases (union_remove_17.q and similar test cases..)..

HIVE-6455 optimization does not actually need a PreOrder graph walker. The fix is to revert it to DefaultGraphWalker.",,prasanth_j,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/14 22:20;prasanth_j;HIVE-6956.1.patch;https://issues.apache.org/jira/secure/attachment/12641360/HIVE-6956.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388195,,,,Thu Nov 13 19:43:38 UTC 2014,,,,,,,,,,"0|i1uvdb:",388452,,,,,,,,,,,,,,,,,,,,,"22/Apr/14 22:20;prasanth_j;The test diffs are valid since SEL->RS->EX->SEL->FS is same as SEL->RS->EX->FS.. The diffs have the redundant SEL in reducer removed after changing the graph walker.;;;","22/Apr/14 22:23;prasanth_j;Attaching RB link;;;","22/Apr/14 22:29;ashutoshc;+1;;;","23/Apr/14 20:07;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641360/HIVE-6956.1.patch

{color:red}ERROR:{color} -1 due to 43 failed/errored test(s), 5412 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_stats_counter
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_minimr_broken_pipe
org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/17/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/17/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 43 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641360;;;","23/Apr/14 20:37;prasanth_j;These are the usual test failures for hadoop-2. The failures are not related to this patch.;;;","24/Apr/14 06:36;ashutoshc;Committed to trunk.;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExprNodeColDesc isSame doesn't account for tabAlias: this affects trait Propagation in Joins,HIVE-6955,12709837,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,rhbutani,rhbutani,rhbutani,22/Apr/14 18:43,09/Jun/14 06:39,14/Jul/23 06:14,28/Apr/14 17:26,,,,,,,,,,0.13.1,0.14.0,,,,,0,,,"For tpcds Q15:
{code}
explain
select ca_zip, sum(cs_sales_price)
from catalog_sales, customer, customer_address, date_dim
where catalog_sales.cs_bill_customer_sk = customer.c_customer_sk
  and customer.c_current_addr_sk = customer_address.ca_address_sk
  and (substr(ca_zip,1,5) in ('85669', '86197','88274','83405','86475',
                              '85392', '85460', '80348', '81792')
       or ca_state in ('CA','WA','GA')
       or cs_sales_price > 500)
  and catalog_sales.cs_sold_date_sk = date_dim.d_date_sk
  and d_qoy = 2 and d_year = 2001
group by ca_zip
order by ca_zip
limit 100;
{code}

The Traits setup for the Operators are:
{code}
FIL[23]: bucketCols=[[]],numBuckets=-1
RS[11]: bucketCols=[[VALUE._col0]],numBuckets=-1
JOIN[12]: bucketCols=[[_col71], [_col71]],numBuckets=-1
FIL[13]: bucketCols=[[_col71], [_col71]],numBuckets=-1
SEL[14]: bucketCols=[[_col71], [_col71]],numBuckets=-1
GBY[15]: bucketCols=[[_col0]],numBuckets=-1
RS[16]: bucketCols=[[KEY._col0]],numBuckets=-1
GBY[17]: bucketCols=[[_col0]],numBuckets=-1
SEL[18]: bucketCols=[[_col0]],numBuckets=-1
LIM[21]: bucketCols=[[_col0]],numBuckets=-1
FS[22]: bucketCols=[[_col0]],numBuckets=-1
TS[3]: bucketCols=[[]],numBuckets=-1
RS[5]: bucketCols=[[VALUE._col0]],numBuckets=-1
JOIN[6]: bucketCols=[[_col3], [_col36]],numBuckets=-1
RS[7]: bucketCols=[[VALUE._col40]],numBuckets=-1
JOIN[9]: bucketCols=[[_col40], [_col0]],numBuckets=-1
RS[10]: bucketCols=[[VALUE._col0]],numBuckets=-1
TS[1]: bucketCols=[[]],numBuckets=-1
RS[8]: bucketCols=[[VALUE._col0]],numBuckets=-1
TS[0]: bucketCols=[[]],numBuckets=-1
RS[4]: bucketCols=[[VALUE._col3]],numBuckets=-1
{code}

This is incorrect:
Join[9] joins ca join (cs join cust). In this case both sides of join have a '_col0' column. The reverse mapping of trait propagation relies on ExprNodeColumnDesc.isSame; since this doesn't account for the tabAlias we end up with Join[9] being bucketed on cs_sold_date_sk; Join[12] has the same issue, only compounds the error.",,hagleitn,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/14 18:52;rhbutani;HIVE-6955.1.patch;https://issues.apache.org/jira/secure/attachment/12641301/HIVE-6955.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388159,,,,Mon Jun 09 06:39:38 UTC 2014,,,,,,,,,,"0|i1uv5j:",388416,,,,,,,,,,,,,,,,,,,,,"23/Apr/14 12:27;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641301/HIVE-6955.1.patch

{color:red}ERROR:{color} -1 due to 41 failed/errored test(s), 5417 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/11/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/11/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 41 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641301;;;","25/Apr/14 18:51;hagleitn;+1

There seem to be a bunch of unit test failures [~rhbutani] - are they related?;;;","25/Apr/14 18:53;rhbutani;Thanks for reviewing [~hagleitn].
No look like failures because of the move to hadoop-2. Validating by running failed tests locally.;;;","25/Apr/14 19:09;rhbutani;Ran tests locally on hadoop-1. Tests pass. These are related to hadoop-2 switch.
See similar failures in HIVE-6934;;;","28/Apr/14 17:26;rhbutani;thanks for the review Gunther;;;","09/Jun/14 06:39;thejas;This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive 0.13 HiveOutputFormat breaks backwards compatibility,HIVE-6952,12709726,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,ashutoshc,costin.leau,costin.leau,22/Apr/14 09:42,06/Nov/15 01:23,14/Jul/23 06:14,25/Apr/14 05:54,0.13.0,,,,,,,,,0.13.1,0.14.0,File Formats,Serializers/Deserializers,,,0,backward-incompatible,,"Hive 0.13 changed the signature of HiveOutputFormat (through commit r1527149) breaking backwards compatibility with previous releases; the return type of getHiveRecordWriter has been changed from RecordWriter to FSRecordWriter.

FSRecordWriter introduces one new method on top of RecordWriter however it does not extend the previous interface and it lives in a completely new package.
Thus code running fine on Hive 0.12 breaks on Hive 0.13. After the upgrade, code running on HIve 0.13, will break on anything lower than this.

This could have easily been avoided by extending the existing interface or introducing a new one that RecordWriter could have extended going forward. By changing the signature, the existing contract (and compatibility) has been voided.
",,costin.leau,cwsteinbach,prasanth_j,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-5324,,,,,,,,,"23/Apr/14 02:09;ashutoshc;HIVE-6952.patch;https://issues.apache.org/jira/secure/attachment/12641391/HIVE-6952.patch","25/Apr/14 21:54;ashutoshc;HIVE-6952_branch-13.patch;https://issues.apache.org/jira/secure/attachment/12642026/HIVE-6952_branch-13.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388048,,,,Fri Nov 06 01:23:45 UTC 2015,,,,,,,,,,"0|i1uuh3:",388306,,,,,,,,,,,,,,,,,,,,,"22/Apr/14 09:46;costin.leau;Actually taking a closer look indicates that FSRecordWriter and RecordWriter are identical - they both implement the same two methods, with the _exact_ same signature which makes the breaking change even more puzzling.;;;","23/Apr/14 02:09;ashutoshc;[~costin] Sorry for inconvenience. I have attached a patch. Can you try it out and see if it fixes the issue for you. If it does, we can aim to have it in 0.13.1 ;;;","23/Apr/14 02:10;ashutoshc;https://reviews.apache.org/r/20598/;;;","23/Apr/14 03:24;prasanth_j;LGTM. +1 (non-binding);;;","23/Apr/14 09:52;costin.leau;Can't comment on the entire commit but the HiveOutputFormat seems to be fixed now (it's the same as before). Do you have any ETA for 0.13.1? Cause as it stands right now 0.13 is unusable and the more 0.13.1 is postpone, the more likely the breakage will be propagated into the various Hadoop distros out there.

Thanks,;;;","24/Apr/14 07:24;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641391/HIVE-6952.patch

{color:red}ERROR:{color} -1 due to 42 failed/errored test(s), 5417 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/22/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/22/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 42 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641391;;;","25/Apr/14 05:54;ashutoshc;Committed on trunk. Thanks [~prasanth_j] for review now your +1 is binding : )
[~costin] For now I have committed this on trunk. If Hive community decides to do 0.13.1 release, I will request for backport of this patch.;;;","25/Apr/14 21:54;ashutoshc;patch for branch 0.13;;;","09/Jun/14 06:39;thejas;This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
;;;","06/Nov/15 01:23;cwsteinbach;Linking HIVE-5324 which added the backward incompatible changes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parsing Error in GROUPING SETS,HIVE-6950,12709686,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,pxiong,ragarwal,ragarwal,22/Apr/14 06:09,18/May/15 19:50,14/Jul/23 06:14,14/Apr/15 17:02,,,,,,,,,,1.2.0,,,,,,0,,,"The following query:
{code}
SELECT tab1.a,
       tab1.b,
       SUM(tab1.c)
FROM tab1
GROUP BY tab1.a,
         tab1.b
GROUPING SETS ((tab1.a, tab1.b))
{code}
results in the following error:
{code}
ParseException line 7:22 missing ) at ',' near '<EOF>'
line 7:31 extraneous input ')' expecting EOF near '<EOF>'
{code}

Changing the query to:
{code}
SELECT tab1.a,
       tab1.b,
       SUM(tab1.c)
FROM tab1
GROUP BY tab1.a,
         tab1.b
GROUPING SETS ((a, tab1.b))
{code}
makes it work.",,goun,Jason Rosendale,leftyl,qwertymaniac,ragarwal,rhbutani,sushanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7178,,,,,,,,,,HIVE-6617,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,388008,,,,Mon May 18 19:50:30 UTC 2015,,,,,,,,,,"0|i1uu8f:",388266,addressed in HIVE-6617,,,,,,,,,,,,,,,,,,,,"22/Apr/14 20:22;rhbutani;Yes there s an ambiguity in the grammar. 
The dfa for the groupingSetExpression chooses to interpret the '((tab1.a, tab1.b))' as 
'((tab1.a), (tab1.b))' and so it complains that the ')' is missing.

In the case of '((a, tab1.b))' the dfa chooses the right path.

The fix is to add Syntactic predicate for this rule.;;;","22/Apr/14 22:13;leftyl;Should this be documented in the wiki?

* [GROUPING SETS clause |https://cwiki.apache.org/confluence/display/Hive/Enhanced+Aggregation%2C+Cube%2C+Grouping+and+Rollup#EnhancedAggregation,Cube,GroupingandRollup-GROUPINGSETSclause];;;","22/Apr/14 23:05;rhbutani;My 2 cents would be: no. This is a bug. Should get fixed. 
Documenting every bug and then updating when we fix, would be asking too much...
;;;","24/Apr/14 07:57;leftyl;That's music to my ears.  Thanks.;;;","27/May/14 22:48;Jason Rosendale;I don't know if my issue is identical to this one or just very closely related, but I can recreate the same error with the following query:
{code}
select a, left(b,5), count(1) from temp_table
group by a, left(b,5)
grouping sets 
((left(b,5),a))
{code}
The error does not occur if I just switch the order of the two elements in the grouping set:
{code}
select a, left(b,5), count(1) from temp_table
group by a, left(b,5)
grouping sets 
((a,left(b,5))) 
{code}
The error occurs whenever the LEFT function is not the very last element in its grouping set. The error also occurs when I replace LEFT with other functions.;;;","13/Oct/14 23:46;qwertymaniac;Some more failing examples are present on the HIVE-7178 JIRA that was marked dupe of this.;;;","18/May/15 19:50;sushanth;This issue has been fixed and released as part of the 1.2.0 release. If you find an issue which seems to be related to this one, please create a new jira and link this one with new jira.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More fixes for tests on hadoop-2 ,HIVE-6947,12709657,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,22/Apr/14 00:44,13/Nov/14 19:43,14/Jul/23 06:14,23/Apr/14 01:24,,,,,,,,,,0.14.0,,Tests,,,,0,,,Few more fixes for test cases on hadoop-2,,jdere,prasanth_j,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/14 21:36;prasanth_j;HIVE-6947.1.patch;https://issues.apache.org/jira/secure/attachment/12641341/HIVE-6947.1.patch","22/Apr/14 00:48;ashutoshc;HIVE-6947.patch;https://issues.apache.org/jira/secure/attachment/12641157/HIVE-6947.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387979,,,,Thu Nov 13 19:43:25 UTC 2014,,,,,,,,,,"0|i1uu2f:",388238,,,,,,,,,,,,,,,,,,,,,"22/Apr/14 00:48;ashutoshc;Doesnt include file size change diffs.;;;","22/Apr/14 10:55;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641157/HIVE-6947.patch

{color:red}ERROR:{color} -1 due to 53 failed/errored test(s), 5483 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_count
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_limit_pushdown
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mrr
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan3
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/3/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/3/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 53 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641157;;;","22/Apr/14 19:05;jdere;Just a bit of explanation on auto_sortmerge_join_16: the query results had been changed by HIVE-6455, looks like a subsequent fix has restored the result set back to its original results.

+1;;;","22/Apr/14 21:36;prasanth_j;Remove union_remove_17.q as its not an expected output. The duplication of partition columns in RSOp will be fixed in HIVE-6956.;;;","23/Apr/14 01:24;ashutoshc;Committed to trunk. Thanks, Jason & Prashant!;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make it easier to run WebHCat e2e tests,HIVE-6946,12709643,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,21/Apr/14 23:31,13/Nov/14 19:41,14/Jul/23 06:14,02/May/14 20:05,0.14.0,,,,,,,,,0.14.0,,WebHCat,,,,0,,,"Right now hcatalog/src/test/e2e/templeton/README.txt explains the steps to set up WebHCat e2e tests but it's cumbersome and error prone.  Need to make some improvements here.

The high level goal here is to simplify code-compile-test loop for WebHCat.  

NO PRECOMMIT TESTS",,ekoifman,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/14 17:51;ekoifman;HIVE-6946.2.patch;https://issues.apache.org/jira/secure/attachment/12642291/HIVE-6946.2.patch","02/May/14 01:44;ekoifman;HIVE-6946.3.patch;https://issues.apache.org/jira/secure/attachment/12642983/HIVE-6946.3.patch","22/Apr/14 01:42;ekoifman;HIVE-6946.patch;https://issues.apache.org/jira/secure/attachment/12641165/HIVE-6946.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387965,,,,Thu Nov 13 19:41:19 UTC 2014,,,,,,,,,,"0|i1utzb:",388224,,,,,,,,,,,,,,,,,,,,,"22/Apr/14 19:49;ekoifman;hcatalog/src/test/e2e/templeton/deployers/README.txt in the attached patch explains in detail what's added in the patch;;;","22/Apr/14 21:15;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641165/HIVE-6946.patch

{color:red}ERROR:{color} -1 due to 43 failed/errored test(s), 5417 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan3
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/6/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/6/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 43 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641165;;;","28/Apr/14 18:00;ekoifman;Attached patch contains a few customizable scripts and template config files.

After setting up a Hadoop cluster, defining a few variables in env.sh and ensuring 3rd party jars (such as proper version of Pig) is available, ""restart_hive_redeploy_artifacts.sh"" can be used to redeploy all necessary artifacts to the cluster and restart Metastore and WebHcat services so that tests can be run in a clean environment.

e2e/templeton/deployers/README.txt has more details.
;;;","02/May/14 01:18;thejas;Looks great! Thanks for making webhcat e2e tests easier to run!
One minor comment, i think the commented out derby config parameters can be removed from the hive-site.mssql.xml .
;;;","02/May/14 01:47;ekoifman;done;;;","02/May/14 02:06;thejas;+1;;;","02/May/14 20:05;thejas;Patch committed to trunk.
Thanks for the contribution Eugene!
;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
issues with dropping partitions on Oracle,HIVE-6945,12709636,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sershe,sershe,sershe,21/Apr/14 22:06,09/Jun/14 06:39,14/Jul/23 06:14,25/Apr/14 21:00,0.13.0,,,,,,,,,0.13.1,0.14.0,,,,,0,,,"1) Direct SQL is broken on Oracle due to the usage of NUMBER type which is translated by DN into decimal rather than long. This appears to be specific to some cases because it seemed to have worked before (different version of Oracle? JDBC? DN? Maybe depends on whether db was auto-created).
2) When partition dropping code falls back to JDO, it creates objects to return, then drops partitions. It appears that dropping makes DN objects invalid. We create metastore partition objects out of DN objects before drop, however the list of partition column values is re-used, rather than copied, into these. DN appears to clear this list during drop, so the returned object becomes invalid and the exception is thrown.",,sershe,sushanth,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/May/14 12:54;sushanth;HIVE-6945-0.13.1.patch;https://issues.apache.org/jira/secure/attachment/12643950/HIVE-6945-0.13.1.patch","22/Apr/14 21:57;sershe;HIVE-6945.01.patch;https://issues.apache.org/jira/secure/attachment/12641348/HIVE-6945.01.patch","23/Apr/14 02:37;sershe;HIVE-6945.02.patch;https://issues.apache.org/jira/secure/attachment/12641396/HIVE-6945.02.patch","22/Apr/14 18:45;sershe;HIVE-6945.patch;https://issues.apache.org/jira/secure/attachment/12641299/HIVE-6945.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387958,,,,Mon Jun 09 06:39:35 UTC 2014,,,,,,,,,,"0|i1utxr:",388217,,,,,,,,,,,,,,,,,,,,,"21/Apr/14 23:14;xuefuz;Could we have some description about what issues are in focus here? The title alone doesn't seem providing any essential information that help the readers.;;;","22/Apr/14 17:45;sershe;Updated;;;","22/Apr/14 18:56;sershe;https://reviews.apache.org/r/20565/;;;","22/Apr/14 21:41;sershe;some missed places on sql path;;;","23/Apr/14 08:40;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641396/HIVE-6945.02.patch

{color:red}ERROR:{color} -1 due to 48 failed/errored test(s), 5417 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatCTAS
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_alter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_tblproperties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unset_table_view_property
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_stats_counter_partitioned
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unset_table_property
org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.testRevokeTimedOutWorkers
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/10/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/10/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 48 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641396;;;","24/Apr/14 02:26;sershe;Most of these tests pass on my machine (others are running) and the output looks bogus (has wrong change list, and no meaningful test results). I think this may be precommit test glitch.;;;","24/Apr/14 06:38;ashutoshc;+1;;;","25/Apr/14 21:00;sershe;committed to trunk;;;","08/May/14 02:26;sushanth;FYI, on testing 0.13.0 RC0, this patch seemed to be the one git bisect identifies as causing a couple of test failures.

The test failures are as follows:

org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatCTAS
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_alter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_tblproperties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unset_table_view_property
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unset_table_property

The logs for the git bisect run for this bug are available over at http://people.apache.org/~khorgath/releases/0.13.1_RC0/test_failures/HIVE-6945.bisect/

The .sh files there were the scripts used to test hive for the bugs in question, and the file prior.patch available there is the patch that you will need to apply on top of 0.13.0 if you want to arrive at the last clean state before this patch was introduced and the issue was observed.
;;;","08/May/14 12:54;sushanth;Attaching updated patch for 0.13.1 that causes all tests to pass. (It was to do with reordering of TBLPROPERTIES contents in the .out files);;;","09/Jun/14 06:39;thejas;This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WebHCat e2e tests broken by HIVE-6432,HIVE-6944,12709628,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,21/Apr/14 21:41,13/Nov/14 19:44,14/Jul/23 06:14,23/Apr/14 15:57,0.14.0,,,,,,,,,0.14.0,,WebHCat,,,,0,,,"HIVE-6432 removed templeton/v/queue REST endpoint and broke webhcat e2e tests

NO PRECOMMIT TESTS",,ekoifman,sushanth,szehon,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6432,,,,,,,,,"21/Apr/14 21:44;ekoifman;HIVE-6944.patch;https://issues.apache.org/jira/secure/attachment/12641125/HIVE-6944.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387950,,,,Thu Nov 13 19:44:09 UTC 2014,,,,,,,,,,"0|i1utvz:",388209,,,,,,,,,,,,,,,,,,,,,"21/Apr/14 23:27;sushanth;+1 , will commit after the 24h period. :);;;","22/Apr/14 04:29;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641125/HIVE-6944.patch

{color:red}ERROR:{color} -1 due to 48 failed/errored test(s), 5417 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_stats_counter
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testListPartitions
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testNameMethods
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testPartition
org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan3
org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/1/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/1/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 48 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641125;;;","22/Apr/14 04:55;ekoifman;in spite of ""NO PRECOMMIT TESTS"" it still ran the tests
in any case, this is WebHCat only change so these test failures are not related;;;","22/Apr/14 06:46;szehon;Sorry about that, looks like a mistake on the new Jenkins, I just fixed the flag 'check no precommit tests=true' on the default job configuration, hopefully new jobs in the queue will now check for ""NO PRECOMMIT TESTS"" string (though jobs already queued will still have this issue).  Thanks for putting it.;;;","23/Apr/14 15:57;sushanth;Committed to trunk. Thanks, Eugene!;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestExecDriver.testMapRedPlan3 fails on hadoop-2,HIVE-6939,12709599,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,21/Apr/14 18:55,13/Nov/14 19:43,14/Jul/23 06:14,22/Apr/14 17:59,,,,,,,,,,0.14.0,,Tests,,,,0,,,"Passes on hadoop-1, but fails on hadoop-2.",,jdere,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/14 20:25;jdere;HIVE-6939.1.patch;https://issues.apache.org/jira/secure/attachment/12641114/HIVE-6939.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387921,,,,Thu Nov 13 19:43:23 UTC 2014,,,,,,,,,,"0|i1utpr:",388181,,,,,,,,,,,,,,,,,,,,,"21/Apr/14 20:23;jdere;It appears that on hadoop-2 there are multiple output files for this test, whereas on hadoop-1 there is just a single file.  Also the test looks like it assumes there is just a single file to compare to.
This test for some reason is specifying 5 reducers.  I've been told that hadoop-1 Hive did not obey the number of reducers, while hadoop-2 does.  This could explain why this test works for hadoop-1 since if it only ever used 1 reducer and generated a single output file.

Changing this test to use just a single reducer allows the test to pass with hadoop-2.  Does anyone know the history of this test and why it was set to use 5 reducers?;;;","21/Apr/14 20:25;jdere;Patch changes test to use single reducer so that there is just a single output file.;;;","22/Apr/14 01:14;ashutoshc;Description of test says "" test reduce with multiple tagged inputs."" so I dont think it has any specific intention for # of reducers = 5. So, # of reducers = 1, sounds good to me. +1;;;","22/Apr/14 17:59;ashutoshc;Committed to trunk.;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix test reporting url's after jenkins move from bigtop,HIVE-6937,12709430,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,szehon,szehon,szehon,20/Apr/14 01:00,13/Nov/14 19:41,14/Jul/23 06:14,17/May/14 21:56,,,,,,,,,,0.14.0,,,,,,0,,,"This move co-located HivePtest webserver and Jenkins server.  Due to the conflicts, I had to remap some URL's, thus breaking the URL of getting logs and test-reports.

The Hive Ptest2 framework makes some assumption about the relative location of logs and REST endpoint URL's, that are no longer true, namely that they are located at endpoint:/logs and endpoint:/hive-ptest/api. This needs to be fixed.  Now, the logs are at host/logs, and HivePtest webserver REST endpoints are at: endpoint/hive-ptest/api.

NO PRECOMMIT TESTS",,brocknoland,szehon,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7080,,,,,,,,,,,,,,,,,,,,,,,,"16/May/14 20:54;szehon;HIVE-6937.patch;https://issues.apache.org/jira/secure/attachment/12645326/HIVE-6937.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387752,,,,Thu Nov 13 19:41:10 UTC 2014,,,,,,,,,,"0|i1usof:",388013,,,,,,,,,,,,,,,,,,,,,"16/May/14 05:26;brocknoland;I also think it'd be great to link directly to the logs in the message we post jira.;;;","16/May/14 05:56;szehon;Yep , I can look into making that enhancement as well.;;;","16/May/14 20:54;szehon;Attaching a fix.  This is a client-side fix only, I'll probably do the enhancement in a separate JIRA as its a server-side fix and probably better to apply one at a time.;;;","16/May/14 21:23;ashutoshc;+1;;;","16/May/14 21:27;szehon;If I modify the jenkins call now without this change, PTestClient will throw a validation error, so one note, when commited, can you please also add this to the jenkins job?

{noformat}
--logsEndpoint http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/ 
{noformat};;;","16/May/14 21:28;szehon;Or let me know and I can modify it.;;;","16/May/14 21:33;brocknoland;[~szehon] isn't this just modifying the jenkins job on the jenkins instance you created?

http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/configure;;;","16/May/14 21:35;brocknoland;Oh nevermind, I see what you are saying. Yes when we commit we can modify the job. ;;;","17/May/14 21:56;brocknoland;Thank you Szehon for the contribution! I have committed this to trunk.;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide table properties to InputFormats,HIVE-6936,12709374,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,omalley,omalley,omalley,18/Apr/14 23:36,13/Nov/14 19:43,14/Jul/23 06:14,19/Sep/14 18:46,,,,,,,,,,0.14.0,,File Formats,,,,1,,,"Some advanced file formats need the table properties made available to them. Additionally, it would be convenient to provide a unique id for fetch operators and the complete list of directories.",,dweeks-netflix,omalley,sushanth,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/14 21:53;omalley;HIVE-6936.patch;https://issues.apache.org/jira/secure/attachment/12669527/HIVE-6936.patch","16/Sep/14 21:36;omalley;HIVE-6936.patch;https://issues.apache.org/jira/secure/attachment/12669225/HIVE-6936.patch","27/May/14 15:09;omalley;HIVE-6936.patch;https://issues.apache.org/jira/secure/attachment/12646920/HIVE-6936.patch","24/May/14 16:35;omalley;HIVE-6936.patch;https://issues.apache.org/jira/secure/attachment/12646677/HIVE-6936.patch","23/May/14 15:30;omalley;HIVE-6936.patch;https://issues.apache.org/jira/secure/attachment/12646531/HIVE-6936.patch","22/May/14 22:01;omalley;HIVE-6936.patch;https://issues.apache.org/jira/secure/attachment/12646394/HIVE-6936.patch","22/May/14 21:32;omalley;HIVE-6936.patch;https://issues.apache.org/jira/secure/attachment/12646390/HIVE-6936.patch","22/May/14 17:39;omalley;HIVE-6936.patch;https://issues.apache.org/jira/secure/attachment/12646338/HIVE-6936.patch","22/Apr/14 21:45;omalley;HIVE-6936.patch;https://issues.apache.org/jira/secure/attachment/12641344/HIVE-6936.patch","18/Apr/14 23:39;omalley;HIVE-6936.patch;https://issues.apache.org/jira/secure/attachment/12640917/HIVE-6936.patch",,,,,,10.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387696,,,,Thu Nov 13 19:43:07 UTC 2014,,,,,,,,,,"0|i1usbz:",387957,,,,,,,,,,,,,,,,,,,,,"18/Apr/14 23:39;omalley;This patch added the table properties to the JobConf that is passed to the InputFormat. The JobConf is just passed around in process. It also adds a uuid to fetch operators and provides the list of directories.;;;","21/Apr/14 15:22;omalley;Sorry, I closed the wrong bug!;;;","22/Apr/14 21:45;omalley;I've added a test case where I use a custom input format and run ""select *"" to ensure that the properties are correctly passed down to the input format.;;;","24/Apr/14 16:28;ashutoshc;[~owen.omalley] Can you create RB request for this? Some high level comments:
* It will be useful to list use-case for table properties in IF, UUID in FetchOp and dir lists for FetchOp.
* I am in double mind about adding table prefix, although its useful from namespacing point of view, but than users also need to deal with this extra prefix while accessing properties in their IF.
* [~sushanth] I believe we also play same game for HBaseIF & HCatIF (of supplying table properties to them). Can you take a look at the patch and see if we are doing this in a common & consistent way? 
* Also, consider to use one of test data files in data/files/ instead of adding a new one.;;;","24/Apr/14 21:30;sushanth;Looking into it. Will provide a quick description of how it's handled in HCat:

In HCat, we pass along tableProperties using HiveStorageHandler.configureInputJobProperties and HiveStorageHandler.configureOutputJobProperties. HBaseSH sees only this.

To standardize usage across all IF/OF cases, we have a storage handler called FosterStorageHandler that acts as a wrapper around the generic IF/OF cases that do not have storage handlers, so that the rest of the HCat code can still stick to using StorageHandlers as-is and not worry about the underlying system.

To make FosterStorageHandler work with orc, we had to change orc so it accepted table property overrides from jobconf as well (since we have to conform to mapreduce interfaces only, and that has no knowledge of tables), and had FosterStorageHandler set up jobconf appropriately with any overrides from table properties.;;;","22/May/14 17:39;omalley;I've addressed Ashutosh's suggestions:

* Reused current data files for the test.
* Removed the UUID.;;;","22/May/14 17:42;omalley;I've also created a review board entry:

https://reviews.apache.org/r/21818/;;;","22/May/14 18:05;xuefuz;{quote}
I am in double mind about adding table prefix, although its useful from namespacing point of view, but than users also need to deal with this extra prefix while accessing properties in their IF.
{quote}

I sympathize with [~ashutoshc] as to the prefix approach. Even we do, the prefix currently chosen (table.) seems so commonly used that it may be in fight with user's existing properties, causing unexpected result.;;;","22/May/14 21:32;omalley;Added tab quoting code.;;;","22/May/14 21:46;omalley;My concern with removing the ""table."" prefix is that it becomes possible to clobber security settings. For example, creating a table with the ""hive.security.authorization.manager"" table property shouldn't allow the query to override the setting.

For a compromise, how about removing the table prefix, but we check to see if that key has a value in the configuration and won't override such a value. Does that make sense?;;;","22/May/14 21:50;ashutoshc;Sounds good to me.;;;","22/May/14 22:01;omalley;Removed the table prefix and added the check to ensure the property wasn't already defined in the configuration.;;;","22/May/14 22:06;ashutoshc;+1;;;","23/May/14 15:30;omalley;Resubmit for jenkins.;;;","23/May/14 17:26;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646394/HIVE-6936.patch

{color:red}ERROR:{color} -1 due to 703 failed/errored test(s), 5458 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_add_part_multiple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_char1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_numbuckets_partitioned_table2_h23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_numbuckets_partitioned_table_h23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_coltype
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_rename_partition_authorization
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_table_serde2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_varchar1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_groupby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_select
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_archive_excludeHadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_archive_multi
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_index
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_reordering_values
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ba_table1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ba_table2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ba_table3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ba_table_udfs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ba_table_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binary_table_bincolserde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binary_table_colserde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_groupby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_case_sensitivity
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cast1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_nested_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_udf1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_column_access_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_columnarserde_create_shortcut
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_combine1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_combine3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_concatenate_inherit_table_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cp_mj_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_escape
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_genericudf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_insert_outputformat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_like
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_like_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_merge_compressed
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas_char
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas_varchar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cte_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_custom_input_output_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_database_drop
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dbtxnmgr_query1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dbtxnmgr_query2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dbtxnmgr_query3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ddltime
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_disallow_incompatible_type_change_off
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_distinct_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_database_removes_partition_dirs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_table_removes_partition_dirs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynpart_sort_opt_vectorization
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_enforce_order
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fileformat_mix
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fileformat_sequencefile
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fileformat_text
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1_map_nomap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_noskew_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3_map_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3_noskew_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby5_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7_map_multi_single_reducer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7_noskew_multi_single_reducer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby8_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby8_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby8_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_complex_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_complex_types_multi_single_reducer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_cube1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_distinct_samekey
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_sets2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_map_ppr_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_multi_insert_common_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_multi_single_reducer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_multi_single_reducer2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_multi_single_reducer3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_position
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_ppr_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_rollup1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_test_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auth
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_empty
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_file_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables_compact
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_multiple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_self_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_unused
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_update
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_compression
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_binary_search
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compression
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_creation
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_stale
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_stale_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_convert_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_grouping_operators
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_multi_insert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_innerjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input11_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input14_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input1_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input35
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input36
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input38
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input39
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input3_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input44
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input49
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_columnarserde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_dynamicserde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_lazyserde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_testsequencefile
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_testxpath
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_testxpath2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert1_overwrite_partitions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert2_overwrite_partitions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_compressed
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insertexternal1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join27
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join29
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32_lessSize
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join35
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join36
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join37
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join38
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join39
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view_cp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lb_fs_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lineage1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_binary_data
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_overwrite
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_hook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_memcheck
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadata_only_queries
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadata_only_queries_with_filters
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mi
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_gby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_gby3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_lateral_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_move_tasks_share_dependencies
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_newline
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nomore_ambiguous_table_col
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonreserved_keywords_insert_into1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_notable_alias1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_notable_alias2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_notable_alias3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_null_column
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_analyze
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_create
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_diff_part_cols
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_empty_files
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_min_max
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ppd_char
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ppd_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ppd_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ppd_varchar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_predicate_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_vectorization_ppd
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parallel
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_create
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_ctas
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_date2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_schema1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_serde_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_varchar1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_varchar2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_vs_table_metadata
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pcr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_constant_expr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_push_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quote1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_alter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_basic
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_bigdata
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_columnar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_lazydecompress
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_merge1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_merge2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_merge3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_merge4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_null_value
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_toleratecorruptions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rename_external_partition_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rename_partition_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rename_table_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample_islocalmode_hook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_unquote_and
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_unquote_not
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_unquote_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_indexes_edge_cases
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_indexes_syntax
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_aggregator_error_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_empty_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_invalidation
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_noscan_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_noscan_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_only_null
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_publisher_error_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_statsfs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_multiinsert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_table_access_keys_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_tablename_with_select
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_lazy
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_touch
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_merge
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_bitmap_and
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_bitmap_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_concat_insert1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_concat_insert2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_concat_ws
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_printf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_translate
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_json_tuple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_parse_url_tuple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union29
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_lateralview
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_top_level
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_updateAccessTime
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_nested_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_udf1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_non_string_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_part_project
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_bucketmapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_context
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_date_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_distinct_gby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_parquet
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_rcfile_columnar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_timestamp_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_virtual_column
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_expressions
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_fileformat_base64
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes2
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes3
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes4
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes5
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_external_table_ppd
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_binary_map_queries
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_binary_map_queries_prefix
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_binary_storage_queries
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_custom_key
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_custom_key2
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_joins
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_ppd_key_range
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_pushdown
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_queries
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_scan_params
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_single_sourced_multi_insert
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats2
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats3
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats_empty_partition
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_ppd_key_ranges
org.apache.hadoop.hive.cli.TestHBaseMinimrCliDriver.testCliDriver_hbase_bulk
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket4
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket5
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers2
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_disable_merge_for_bucketing
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_groupby2
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_bucketed_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_dyn_part
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_map_operators
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_merge
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_num_buckets
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_reducers_power_two
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_join1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_list_bucket_dml_10
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_parallel_orderby
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_quotedid_smb
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_reduce_deduplicate
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_remote_script
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_smb_mapjoin_8
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_stats_counter_partitioned
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_truncate_column_buckets
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_coltype_2columns
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_coltype_invalidcolname
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_coltype_invalidtype
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_insert1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_insert2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_insert3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_insert4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi6
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi7
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_partspec1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_partspec2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_partspec3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_partspec4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_partspec5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_part
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_column_rename1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_column_rename2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_column_rename4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_disallow_incompatible_type_change_on1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_disallow_incompatible_type_change_on2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_fileformat_void_input
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_index_compact_entry_limit
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_index_compact_size_limit
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into6
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg_query_tbl_in_locked_db
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg_try_lock_db_in_use
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_merge_negative_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_merge_negative_3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_smb_bucketmapjoin
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_sortmerge_mapjoin_mismatch_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_aggregator_error_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_aggregator_error_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partscan_norcfile
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_publisher_error_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_publisher_error_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_bucketed_column
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_column_indexed_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_column_list_bucketing
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_column_seqfile
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_nonexistant_column
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_partition_column
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_partition_column2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_union22
org.apache.hadoop.hive.ql.TestMTQueries.testMTQueries1
org.apache.hadoop.hive.ql.security.TestFolderPermissions.testAlterPartitionPerms
org.apache.hadoop.hive.ql.security.TestFolderPermissions.testDynamicPartitions
org.apache.hadoop.hive.ql.security.TestFolderPermissions.testExternalTable
org.apache.hadoop.hive.ql.security.TestFolderPermissions.testInsertOverwrite
org.apache.hadoop.hive.ql.security.TestFolderPermissions.testStaticPartitionPerms
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.jdbc.TestJdbcDriver2.testNullResultSet
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/272/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/272/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-272/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 703 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646394;;;","24/May/14 16:35;omalley;I had to add java quoting to the values of the table properties for the unit tests.;;;","25/May/14 01:56;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646677/HIVE-6936.patch

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 5539 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_multi_single_reducer3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/284/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/284/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-284/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646677;;;","27/May/14 15:09;omalley;Reattaching for jenkins.;;;","28/May/14 09:44;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646920/HIVE-6936.patch

{color:red}ERROR:{color} -1 due to 13 failed/errored test(s), 5543 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/317/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/317/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-317/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 13 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646920;;;","28/May/14 17:17;ashutoshc;Failures look unrelated +1 for latest patch;;;","16/Sep/14 18:20;omalley;Resubmitting patch to jenkins.;;;","16/Sep/14 21:36;omalley;Fixes problem passing null values down to Configuration.;;;","17/Sep/14 11:41;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12669225/HIVE-6936.patch

{color:green}SUCCESS:{color} +1 6280 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/838/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/838/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-838/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12669225;;;","17/Sep/14 21:53;omalley;This patch is the same as the previous, except that it includes commons-lang3 in the exec jar.;;;","18/Sep/14 15:37;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12669527/HIVE-6936.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6281 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataPrimitiveTypes
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/860/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/860/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-860/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12669527;;;","18/Sep/14 21:04;ashutoshc;+1;;;","19/Sep/14 18:46;omalley;I messed up and committed both HIVE-7812 and HIVE-6936 (with one change) in r1626292. The last part of HIVE-6936 is r1626294.;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PartitionPruner doesn't handle top level constant expression correctly,HIVE-6934,12709318,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,hsubramaniyan,rhbutani,rhbutani,18/Apr/14 19:03,13/Nov/14 19:41,14/Jul/23 06:14,21/Oct/14 18:30,,,,,,,,,,0.14.0,,Logical Optimizer,Query Processor,,,0,,,"You hit this error indirectly, because how we handle invalid constant comparisons. Consider:
{code}
create table x(key int, value string) partitioned by (dt int, ts string);

-- both these queries hit this issue
select * from x where key = 'abc';
select * from x where dt = 'abc';

-- the issue is the comparison get converted to the constant false
-- and the PartitionPruner doesn't handle top level constant exprs corrcetly
{code}

Thanks to [~hsubramaniyan] for uncovering this as part of adding tests for HIVE-5376",,erwaman,hsubramaniyan,rhbutani,sershe,thejas,vikram.dixit,wanchang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-8490,,,,,,,,,,,HIVE-7417,,,HIVE-5376,,,,,,,,,,,,,,,,,,,,,"17/Aug/14 08:15;hsubramaniyan;HIVE-6934.4.patch;https://issues.apache.org/jira/secure/attachment/12662357/HIVE-6934.4.patch","17/Aug/14 19:04;hsubramaniyan;HIVE-6934.5.patch;https://issues.apache.org/jira/secure/attachment/12662382/HIVE-6934.5.patch","18/Aug/14 09:50;hsubramaniyan;HIVE-6934.6.patch;https://issues.apache.org/jira/secure/attachment/12662466/HIVE-6934.6.patch","13/Oct/14 20:16;hsubramaniyan;HIVE-6934.7.patch;https://issues.apache.org/jira/secure/attachment/12674582/HIVE-6934.7.patch","14/Oct/14 02:48;hsubramaniyan;HIVE-6934.8.patch;https://issues.apache.org/jira/secure/attachment/12674667/HIVE-6934.8.patch","17/Oct/14 22:36;hsubramaniyan;HIVE-6934.9.patch;https://issues.apache.org/jira/secure/attachment/12675601/HIVE-6934.9.patch","20/Oct/14 18:36;hsubramaniyan;HIVE-6934.91.patch;https://issues.apache.org/jira/secure/attachment/12675898/HIVE-6934.91.patch","20/Oct/14 23:50;hsubramaniyan;HIVE-6934.92.patch;https://issues.apache.org/jira/secure/attachment/12675979/HIVE-6934.92.patch",,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387640,,,,Thu Nov 13 19:41:40 UTC 2014,,,,,,,,,,"0|i1urz3:",387902,,,,,,,,,,,,,,,,,,,,,"22/Apr/14 22:23;hsubramaniyan;encountered another hive issue as part of this fix. 
explain select * from table where 1;
The above query passes where as ' select * from table where 1;' fails when trying to convert 1 to boolean. We should be able to catch this exception semantically.
;;;","23/Apr/14 23:52;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641359/HIVE-6934.1.patch

{color:red}ERROR:{color} -1 due to 44 failed/errored test(s), 5418 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/18/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/18/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 44 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641359;;;","24/Apr/14 01:11;sershe;Can you post RB? Also is it possible to compare actual boolean rather than string?;;;","28/Apr/14 21:04;hsubramaniyan;https://reviews.apache.org/r/20795;;;","29/Apr/14 06:20;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642318/HIVE-6934.2.patch

{color:red}ERROR:{color} -1 due to 78 failed/errored test(s), 5424 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_archive_excludeHadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_archive_multi
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_or_replace_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_view_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_unused
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_stale_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input42
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_louter_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonmr_fetch
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_create
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_split_elimination
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pcr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_udf_case
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_allchildsarenull
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_push_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_router_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_view_cast
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_union22
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input_part1
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/71/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/71/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 78 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642318;;;","29/Apr/14 21:38;sershe;patch looks good; what about test failures;;;","24/May/14 23:06;ashutoshc;Failed tests need to be looked at;;;","28/Jul/14 18:54;hsubramaniyan;Attaching the fix after rebasing with the latest trunk.;;;","28/Jul/14 22:22;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12658220/HIVE-6934.3.patch

{color:red}ERROR:{color} -1 due to 126 failed/errored test(s), 5770 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_archive_excludeHadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_archive_multi
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin_negative
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_or_replace_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_view_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_explain_dependency
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_explain_logical
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_hook_context_cs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_unused
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_stale_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input42
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32_lessSize
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_louter_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_hook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_subquery
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonmr_fetch
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_create
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_split_elimination
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pcr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_udf_case
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_vc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_allchildsarenull
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_push_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_regex_col
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_router_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_view_cast
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_schema_evolution
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin7
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_bucket_mapjoin_mismatch1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_bucket_mapjoin_wrong_table_metadata_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_join28
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_join32
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_join_nonexistent_part
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_union22
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input_part1
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/81/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/81/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-81/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 126 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12658220;;;","16/Aug/14 08:14;ashutoshc;[~hsubramaniyan] Are you still working on this? This looks useful to fix this. I believe as a side effect of this HIVE-7417 may also get fixed.;;;","16/Aug/14 17:56;hsubramaniyan;[~ashutoshc] I will upload the patch for this soon.

Thanks
Hari;;;","17/Aug/14 08:12;hsubramaniyan;[~ashutoshc] HIVE-6934.4.patch should solve all the related issues.;;;","17/Aug/14 08:19;hsubramaniyan;https://reviews.apache.org/r/24785/;;;","17/Aug/14 09:56;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12662357/HIVE-6934.4.patch

{color:red}ERROR:{color} -1 due to 129 failed/errored test(s), 5818 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_archive_excludeHadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_archive_multi
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin_negative
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_or_replace_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_view_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_explain_dependency
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_explain_logical
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_hook_context_cs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_unused
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_stale_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input42
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32_lessSize
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_louter_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_hook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_subquery
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonmr_fetch
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_create
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_split_elimination
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_boolexpr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pcr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_udf_case
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_vc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_allchildsarenull
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_push_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_regex_col
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_router_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_view_cast
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_schema_evolution
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin7
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_bucket_mapjoin_mismatch1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_bucket_mapjoin_wrong_table_metadata_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_join28
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_join32
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_join_nonexistent_part
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_union22
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input_part1
org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.testInputSomeColumnsUsed
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/368/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/368/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-368/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 129 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12662357;;;","17/Aug/14 20:43;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12662382/HIVE-6934.5.patch

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 5818 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_boolexpr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pcr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_push_or
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/369/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/369/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-369/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12662382;;;","18/Aug/14 15:51;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12662466/HIVE-6934.6.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5820 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_boolexpr
org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Json
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/380/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/380/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-380/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12662466;;;","22/Aug/14 19:44;ashutoshc;Can you also add following tests in your patch:
{code}
explain select count(1) from srcpart where true;
explain select count(1) from srcpart where false;
explain select count(1) from srcpart where true and hr='11';
explain select count(1) from srcpart where true or hr='11';
explain select count(1) from srcpart where false or hr='11';
explain select count(1) from srcpart where false and hr='11';
{code};;;","09/Oct/14 15:47;ashutoshc;[~hsubramaniyan] Hit this again on HIVE-8358 will you have time to rebase your patch on trunk and include test cases I outlined above.;;;","13/Oct/14 20:16;hsubramaniyan;Rebasing the patch. ccing [~ashutoshc] for reviewing the change.;;;","14/Oct/14 00:02;ashutoshc;[~hsubramaniyan] Can you also add tests as per my previous comment?;;;","14/Oct/14 02:06;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12674582/HIVE-6934.7.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 6553 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parallel
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1252/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1252/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1252/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12674582;;;","14/Oct/14 02:48;hsubramaniyan;added additional tests as per [~ashutoshc]'s comment;;;","14/Oct/14 18:11;ashutoshc;I think there is an issue here. Consider, query like : 
{code}
 select count(1) from srcpart where false;
{code}

After patch this will retrieve every partition, whereas it should retrieve none. That is because after change compactExpr() will return null for this expression and then prune() method will retrieve all partitions. [~hsubramaniyan] Can you confirm?;;;","15/Oct/14 00:16;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12674667/HIVE-6934.8.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 6556 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parallel
org.apache.hive.beeline.TestSchemaTool.testSchemaInit
org.apache.hive.beeline.TestSchemaTool.testSchemaUpgrade
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1266/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1266/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1266/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12674667;;;","15/Oct/14 20:26;ashutoshc;Canceling patch, as it needs more work.;;;","17/Oct/14 22:46;ashutoshc;[~hsubramaniyan] Can you update RB entry?;;;","18/Oct/14 05:24;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12675601/HIVE-6934.9.patch

{color:red}ERROR:{color} -1 due to 201 failed/errored test(s), 6566 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_archive_excludeHadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_archive_multi
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketizedhiveinputformat_auto
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin_negative
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin_negative2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_correctness
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_or_replace_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_view_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_delete_where_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_explain_dependency
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_explain_logical
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_hook_context_cs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables_compact
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_unused
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_in_db
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_stale_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input42
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32_lessSize
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_empty
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_merge_multi_expressions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_leftsemijoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_louter_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_hook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_subquery
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonmr_fetch
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_create
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge_incompat2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_split_elimination
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pcr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_udf_case
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_vc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_allchildsarenull
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_pushdown3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_push_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_regex_col
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_router_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_update_where_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_non_string_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_view_cast
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_12
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_4
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_8
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_correctness
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_delete_where_partitioned
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynamic_partition_pruning
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynamic_partition_pruning_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_filter_join_breaktask2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_leftsemijoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mapjoin_mapjoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_metadataonly1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_optimize_nullscan
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_orc_merge7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_orc_merge_incompat2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_bmj_schema_evolution
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_schema_evolution
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_smb_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_smb_main
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union_group_by
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_update_where_partitioned
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_non_string_partition
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_part
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_dynamic_partition_pruning
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin7
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_file_with_header_footer
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_bucket_mapjoin_mismatch1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_bucket_mapjoin_wrong_table_metadata_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_join28
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_join32
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_join_nonexistent_part
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_union22
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input_part1
org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.testDeleteAllWherePartitioned
org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.testDeleteOnePartitionWhere
org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.testUpdateAllPartitionedWhere
org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.testUpdateOnePartitionWhere
org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.testInputSomeColumnsUsed
org.apache.hive.jdbc.TestJdbcDriver2.testBuiltInUDFCol
org.apache.hive.jdbc.TestJdbcDriver2.testExprCol
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1329/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1329/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1329/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 201 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12675601
 - PreCommit-HIVE-TRUNK-Build;;;","20/Oct/14 18:13;ashutoshc;Test failures need to be looked at.;;;","20/Oct/14 23:01;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12675898/HIVE-6934.91.patch

{color:red}ERROR:{color} -1 due to 82 failed/errored test(s), 6568 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketizedhiveinputformat_auto
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin_negative2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_correctness
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_delete_where_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables_compact
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_merge_multi_expressions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_leftsemijoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_louter_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonmr_fetch
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_create
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge_incompat2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pcr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_pushdown3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_regex_col
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_router_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_update_where_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_non_string_partition
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_12
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_4
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_8
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_correctness
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_delete_where_partitioned
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynamic_partition_pruning
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_leftsemijoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mapjoin_mapjoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_optimize_nullscan
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_orc_merge7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_orc_merge_incompat2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_bmj_schema_evolution
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_schema_evolution
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_smb_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_smb_main
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_update_where_partitioned
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_non_string_partition
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_part
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_dynamic_partition_pruning
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_file_with_header_footer
org.apache.hive.jdbc.TestJdbcDriver2.testBuiltInUDFCol
org.apache.hive.jdbc.TestJdbcDriver2.testExprCol
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1359/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1359/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1359/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 82 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12675898 - PreCommit-HIVE-TRUNK-Build;;;","21/Oct/14 03:39;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12675979/HIVE-6934.92.patch

{color:green}SUCCESS:{color} +1 6568 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1363/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1363/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1363/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12675979 - PreCommit-HIVE-TRUNK-Build;;;","21/Oct/14 04:39;hsubramaniyan;[~ashutoshc] Can you review the latest patch.

Thanks
Hari;;;","21/Oct/14 16:33;ashutoshc;+1;;;","21/Oct/14 16:48;ashutoshc;[~vikram.dixit] it will be good to have this bug fix in 0.14 as well;;;","21/Oct/14 18:30;ashutoshc;Committed to trunk.;;;","21/Oct/14 22:34;vikram.dixit;+1 for 0.14;;;","22/Oct/14 17:58;ashutoshc;committed to 0.14;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,
hive README needs update,HIVE-6932,12709202,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,thejas,thejas,thejas,18/Apr/14 02:06,01/Oct/19 22:07,14/Jul/23 06:14,30/Apr/14 22:41,0.13.0,,,,,,,,,0.14.0,,,,,,0,,,"It needs to be updated to include Tez as a runtime. Also, it talks about average latency being in minutes, which is very misleading.
NO PRECOMMIT TESTS
",,ashutoshc,gates,leftyl,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7003,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/14 01:08;thejas;HIVE-6932.1.patch;https://issues.apache.org/jira/secure/attachment/12641158/HIVE-6932.1.patch","22/Apr/14 16:59;thejas;HIVE-6932.2.patch;https://issues.apache.org/jira/secure/attachment/12641280/HIVE-6932.2.patch","28/Apr/14 18:27;thejas;HIVE-6932.3.patch;https://issues.apache.org/jira/secure/attachment/12642300/HIVE-6932.3.patch","28/Apr/14 20:49;thejas;HIVE-6932.4.patch;https://issues.apache.org/jira/secure/attachment/12642316/HIVE-6932.4.patch","28/Apr/14 21:44;thejas;HIVE-6932.5.patch;https://issues.apache.org/jira/secure/attachment/12642338/HIVE-6932.5.patch",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387525,,,,Thu Nov 13 19:41:25 UTC 2014,,,,,,,,,,"0|i1ur9j:",387787,,,,,,,,,,,,,,,,,,,,,"18/Apr/14 02:08;thejas;Also needing update is the requirements section. We should include Java 1.7.

;;;","18/Apr/14 02:09;thejas;Also add Microsoft SQL Server in databases supported (for 0.14) release.
;;;","22/Apr/14 07:38;leftyl;Nano-nit:  an extra space before ""your own upgrade script.""

{noformat}
+- We have provided upgrade scripts for MySQL, PostgreSQL, Oracle,
+  Microsoft SQL Server, and Derby databases. If you are using a
+  different database for your MetaStore you will need to provide
+   your own upgrade script.
{noformat};;;","22/Apr/14 07:51;leftyl;Nano-nit #2:  need another space before ""frameworks.""

{noformat}
+* Query execution via Apache Hadoop MapReduce and using Apache Tez
+ frameworks.
{noformat}

Also a question:  Is ""QL"" the proper name for the language or ""HiveQL""?  The README says HiveQL twice in the Getting Started section.  The wiki is inconsistent with QL, HiveQL, and Hive QL.;;;","22/Apr/14 16:59;thejas;[~leftylev] Updated patch addressing the comments. Also changed the wording regarding query execution.
;;;","22/Apr/14 17:26;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641158/HIVE-6932.1.patch

{color:red}ERROR:{color} -1 due to 43 failed/errored test(s), 5416 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan3
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/5/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/5/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 43 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641158;;;","22/Apr/14 19:51;leftyl;Good, I'd wondered about that phrase but moved on to minutiae.;;;","23/Apr/14 01:28;thejas;bq. Also a question: Is ""QL"" the proper name for the language or ""HiveQL""? The README says HiveQL twice in the Getting Started section. The wiki is inconsistent with QL, HiveQL, and Hive QL.
I am not sure. I actually prefer ""SQL"" or ""Hive SQL"" , but not everyone was happy with that. Maybe we should have a discussion on mailing list to see if we have an agreement now.
;;;","23/Apr/14 01:33;ashutoshc;+1 LGTM;;;","23/Apr/14 02:14;leftyl;bq. I actually prefer ""SQL"" or ""Hive SQL""

A general discussion would be good, but let's not hold up this patch for it.

bq. Also, it talks about average latency being in minutes, which is very misleading.

Did you mean to revise the latency part too?;;;","23/Apr/14 17:46;thejas;bq. Did you mean to revise the latency part too?
Yes, I did mean to update that, but forgot about it. Let me work on that.
Thanks for pointing that out!
;;;","28/Apr/14 18:29;thejas;HIVE-6932.3.patch - Update the section about latency, and also updated the description of the language.
;;;","28/Apr/14 20:21;ashutoshc;* Space between  userswith
*  ""MapReduce framework will experience long scheduling latencies"" better to say  ""MapReduce framework *may* experience long scheduling latencies""
* There is also a bit about row level insert / update. You may want to re-word that in light of recent work released in 0.13;;;","28/Apr/14 21:05;thejas;HIVE-6932.4.patch - Addressing first two comments from Ashutosh.
Not sure how I should reword the statement about 'row level insert/update' as we still don't allow that through SQL. 
[~owen.omalley] [~gates] Do you have any opinion on how this paragraph in README should be worded ? Any change to be made at this point in time ?

{code}
Hive is not designed for online transaction processing and does not
support real-time queries or row level insert/updates. It is best used
for batch jobs over large sets of immutable data (like web logs). What
Hive values most are scalability (scale out with more machines added
dynamically to the Hadoop cluster), extensibility (with MapReduce
framework and UDF/UDAF/UDTF), fault-tolerance, and loose-coupling with
its input formats.
{code};;;","28/Apr/14 21:18;gates;I'd leave the row level insert/update for now, but file a new JIRA and attach it to HIVE-5317 to change this when insert/update/delete are added.  

I'd change the part about not supporting real-time queries to not supporting sub-second queries, as with 0.13 we are now in the few seconds range for many queries, and it's not clear what real-time means to people.;;;","28/Apr/14 21:44;thejas;HIVE-6932.5.patch - Addressing Alan's comment.
Saying hive does not support sub-second queries is also not accurate. Queries that don't spawn tasks on the cluster can finish in less than a second. For example queries that can be answered using metadata finish in less than second. I think, even 'select * from tab limit 10;' queries also finish in less than a second.
So I have removed the potentially misleading 'real-time' query part from the paragraph. When we say OLTP workloads are not supported, this is sort of implied.
;;;","29/Apr/14 21:56;ashutoshc;+1;;;","30/Apr/14 22:41;thejas;Patch committed to trunk.
Thanks for the reviews!
;;;","01/May/14 22:41;leftyl;Uh oh, I should have reviewed patch 5 before you committed.  But the typo won't confuse anyone:  ""Users are free to swtich back...."";;;","01/May/14 23:03;thejas;HIVE-7003 to fix the typo.
;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Windows unit test fixes,HIVE-6931,12709195,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,18/Apr/14 01:23,13/Nov/14 19:44,14/Jul/23 06:14,01/May/14 01:08,,,,,,,,,,0.14.0,,Tests,Windows,,,0,,,A few misc fixes for some of the unit tests on Windows.,,jdere,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/14 17:29;jdere;HIVE-6931.1.patch;https://issues.apache.org/jira/secure/attachment/12641760/HIVE-6931.1.patch","18/Apr/14 01:29;jdere;HIVE-6931.1.patch;https://issues.apache.org/jira/secure/attachment/12640757/HIVE-6931.1.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387518,,,,Thu Nov 13 19:44:29 UTC 2014,,,,,,,,,,"0|i1ur7z:",387780,,,,,,,,,,,,,,,,,,,,,"18/Apr/14 01:29;jdere;Patch v1:
- Remove setAuxJars() call which was breaking Minimr tests
- Refactor common code between QTestUtil/WindowsPathUtil
- TestExecDriver should initialize tmpdir after converting Windows paths
- Fix a couple of q file tests;;;","18/Apr/14 01:33;jdere;RB at https://reviews.apache.org/r/20472/;;;","24/Apr/14 17:29;jdere;re-upload patch to kick of precommit tests;;;","25/Apr/14 04:41;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641760/HIVE-6931.1.patch

{color:red}ERROR:{color} -1 due to 40 failed/errored test(s), 5418 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/33/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/33/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 40 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641760;;;","30/Apr/14 19:09;sushanth;Looks good to me. About the only nitpick I have is that it feels wrong to call WindowsPathUtil.getHdfsUriString (after moving the function to WindowsPathUtil) from QTestUtil from a section of code that's not specific to windows. From the actual code, it's perfectly fine, since the function only modifies the uri if it detects it's in windows, but it reads as if we get the windows version of hdfs uri from code. That said, my issue is not really with this change as much as code readability in general, and this is helping remove redundant code and packaging it in the relevant place, it's merely that it reads funnily. Eventually, I would like us to move WindowsPathUtil into PathUtil or somesuch.

+1.;;;","30/Apr/14 19:33;sushanth;Also, the test failures noted here seem to not be related to the patch. [~jdere], could you please verify/confirm? I can go ahead and commit it after that.;;;","30/Apr/14 20:49;jdere;These do not look related to the patch, looks like the existing set of failing tests on hadoop-2;;;","01/May/14 01:08;sushanth;Committed to trunk. Thanks, Jason!;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Beeline should not chop off ""describe extended"" results by default",HIVE-6928,12709131,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,Ferd,szehon,szehon,17/Apr/14 20:00,13/Nov/14 19:44,14/Jul/23 06:14,17/Jul/14 18:56,,,,,,,,,,0.14.0,,CLI,,,,0,,,"By default, beeline truncates long results based on the console width like:
{code}
+-----------------------------+----------------------------------------------------------------------------------------------------------------------+
|          col_name           |                                                                                                                      |
+-----------------------------+----------------------------------------------------------------------------------------------------------------------+
| pat_id                      | string                                                                                                               |
| score                       | float                                                                                                                |
| acutes                      | float                                                                                                                |
|                             |                                                                                                                      |
| Detailed Table Information  | Table(tableName:refills, dbName:default, owner:hdadmin, createTime:1393882396, lastAccessTime:0, retention:0, sd:Sto |
+-----------------------------+----------------------------------------------------------------------------------------------------------------------+
5 rows selected (0.4 seconds)
{code}
This can be changed by !outputformat, but the default should behave better to give a better experience to the first-time beeline user.


",,chinnalalam,dongc,Ferd,glenn.strycker@gmail.com,leftyl,szehon,thejas,xu,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7941,,,HIVE-7224,,,,,,,,,,,,,,,,,,,,,"25/May/14 11:34;chinnalalam;HIVE-6928.1.patch;https://issues.apache.org/jira/secure/attachment/12646709/HIVE-6928.1.patch","12/Jun/14 18:19;chinnalalam;HIVE-6928.2.patch;https://issues.apache.org/jira/secure/attachment/12650101/HIVE-6928.2.patch","16/Jul/14 22:50;xuefuz;HIVE-6928.3 .patch;https://issues.apache.org/jira/secure/attachment/12656168/HIVE-6928.3+.patch","16/Jul/14 03:04;xuefuz;HIVE-6928.3 .patch;https://issues.apache.org/jira/secure/attachment/12655968/HIVE-6928.3+.patch","10/Jul/14 05:28;xu;HIVE-6928.3 .patch;https://issues.apache.org/jira/secure/attachment/12654943/HIVE-6928.3+.patch","17/Jul/14 00:22;xu;HIVE-6928.3.patch;https://issues.apache.org/jira/secure/attachment/12656188/HIVE-6928.3.patch","30/Apr/14 11:26;chinnalalam;HIVE-6928.patch;https://issues.apache.org/jira/secure/attachment/12642628/HIVE-6928.patch",,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387454,,,,Thu Nov 13 19:44:37 UTC 2014,,,,,,,,,,"0|i1uqtr:",387716,,,,,,,,,,,,,,,,,,,,,"30/Apr/14 11:28;chinnalalam;In this example the length of the row is more than 1000 characters. These kind of outputs showing in table format wont be look good. When ever the row length is bigger than width present that in vertical format (decide this in run time).;;;","01/May/14 00:53;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642628/HIVE-6928.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 5426 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/89/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/89/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642628;;;","21/May/14 19:25;szehon;Thanks [~chinnalalam] for the fix.  

One minor nit here, as there is no review-board, do you think it would be better to call BufferedRows constructor width argument to be 'consoleWidth' for clarity, to disambiguate from row width.

I was also wondering do we need an upper-bound as now it will show entire result length even if its huge, but Hive CLI did not have that either.;;;","25/May/14 11:35;chinnalalam;Hi Szehon Ho, Thanks for reviewing the patch. Reworked the patch.;;;","26/May/14 16:01;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646709/HIVE-6928.1.patch

{color:red}ERROR:{color} -1 due to 11 failed/errored test(s), 5538 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/299/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/299/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-299/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 11 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646709;;;","27/May/14 20:40;szehon;Thanks Chinna, +1 (non-binding).;;;","10/Jun/14 23:38;szehon;[~brocknoland] , [~xuefuz], any of you guys have a cycle to look and see if this can get in?  Thanks.;;;","11/Jun/14 19:17;xuefuz;Could we have a review board entry which makes the review easier?;;;","12/Jun/14 14:15;chinnalalam;Created review board entry.

https://reviews.apache.org/r/22513/;;;","12/Jun/14 14:21;xuefuz;+1, patch looks good. Minor comment on RB.;;;","12/Jun/14 18:21;chinnalalam;Reworked the patch. Thanks for the review Xuefu Zhang.
;;;","13/Jun/14 11:58;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12650101/HIVE-6928.2.patch

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 5610 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_stats_counter_partitioned
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.testPropertiesParsing
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/454/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/454/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-454/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12650101;;;","13/Jun/14 13:42;xuefuz;[~chinnalalam] could you please update RB with your latest patch? Thanks.;;;","14/Jun/14 03:23;thejas;I have a few concerns about this patch.
This will change the output format on the fly. It is possible that for a query that was returning results in table format would suddenly change to vertical format when some records in the input changes. This sort of surprise is not good. I agree that truncating columns is also not good.
I think we should instead improve the table format to wrap around the lines instead of truncating them. Or look at how other databases command lines tend to format their results.

There is also a change proposed in HIVE-7224 that changes default to incremental output to avoid OOM. This patch will result in disabling table format output with that option.
;;;","14/Jun/14 08:14;szehon;The concern does make sense.  I only have mysql now, and saw they are wrapping long lines by default.

I dont have that much DB-CLI exp, but Beeline is the only one I've seen that by default sets each Row's max-width to be the console width at startup time.  Then it keeps truncating to that size even if you resize the window, it's not a great experience.  Changing the default (table format) to wrap long lines like mysql sounds better to better.  Thoughts?;;;","14/Jun/14 14:02;xuefuz;Cancel the patch until the above concerns are addressed.;;;","10/Jul/14 05:28;xu;A patch which change the default table format wrapping long lines like mysql is attached. In this patch, an option (--truncateTable) is added which is false by default. It means table output format will wrap the long lines when it is set to false. If user wants to truncate long lines,  add option --truncate in beeline command. Besides that, we can file a bug to add ellipsis at the end of truncated item in order to highlighting it is truncated.;;;","15/Jul/14 18:12;xuefuz;The latest patch looks good to me. Can we have a RB for this? I submit the patch to allow tests to run.;;;","16/Jul/14 02:17;xu;review board entry was created:
https://reviews.apache.org/r/23541;;;","16/Jul/14 03:04;xuefuz;Reattach the same patch to trigger the test run.;;;","16/Jul/14 22:49;xuefuz;I have started wondering why test wasn't triggered. Reattach and try again.;;;","17/Jul/14 00:22;xu;reattach file since there is a blank in the file name in order to trigger the hive qa;;;","17/Jul/14 06:03;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12656188/HIVE-6928.3.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5740 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_temp_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchAbortAndCommit
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/825/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/825/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-825/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12656188;;;","17/Jul/14 18:48;xuefuz;The test failures don't seem related to the patch. I have seen them in other test runs also. Will commit the patch shortly.;;;","17/Jul/14 18:56;xuefuz;Patch committed to trunk. Thanks to Chinna and ferdinand for the contribution.;;;","17/Jul/14 22:52;leftyl;The --truncateTable option needs to be documented in the Beeline section of HiveServer2 Clients (with a version note and link to this jira).

* [HiveServer2 Clients -- Beeline Command Options | https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-BeelineCommandOptions];;;","17/Jul/14 22:58;xuefuz;Thanks, Lefty.;;;","02/Sep/14 20:35;thejas;The patch does not have a test that verifies linewrapping actually works. It would be useful to have a test for it. Created HIVE-7941 to track that.
[~Ferd] Would you like to take a shot at it ?
;;;","12/Nov/14 02:03;szehon;Added information about truncateTable Beeline option in:
[https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-BeelineCommandOptions|https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-BeelineCommandOptions];;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for MSSQL in schematool,HIVE-6927,12709128,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,deepesh,deepesh,deepesh,17/Apr/14 19:47,13/Nov/14 19:41,14/Jul/23 06:14,22/Apr/14 16:20,0.13.0,,,,,,,,,0.14.0,,Metastore,,,,0,TODOC14,,Schematool is the preferred way of initializing schema for Hive. Since HIVE-6862 provided the script for MSSQL it would be nice to add the support for it in schematool.,,deepesh,gates,leftyl,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/14 19:49;deepesh;HIVE-6927.patch;https://issues.apache.org/jira/secure/attachment/12640686/HIVE-6927.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387451,,,,Thu Nov 13 19:41:17 UTC 2014,,,,,,,,,,"0|i1uqt3:",387713,,,,,,,,,,,,,,,,,,,,,"17/Apr/14 19:49;deepesh;Attaching the patch for review.;;;","21/Apr/14 17:25;ashutoshc;+1;;;","22/Apr/14 13:34;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12640686/HIVE-6927.patch

{color:red}ERROR:{color} -1 due to 58 failed/errored test(s), 5483 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_count
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_limit_pushdown
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mrr
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testNameMethods
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testPartition
org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan3
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/4/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/4/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 58 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12640686;;;","22/Apr/14 16:20;ashutoshc;Failures are hadoop-2 related. Committed to trunk. Thanks, Deepesh!;;;","22/Apr/14 23:17;leftyl;This doesn't seem to need any documentation in the wiki ... or does it?

* [Hive Metastore Administration |https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin]
* [Hive Schema Tool |https://cwiki.apache.org/confluence/display/Hive/Hive+Schema+Tool];;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in collect_set() UDAF,HIVE-6922,12708985,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sunrui,sunrui,sunrui,17/Apr/14 04:03,13/Nov/14 19:44,14/Jul/23 06:14,24/Apr/14 01:13,,,,,,,,,,0.14.0,,UDF,,,,0,,,"Steps to reproduce the bug:
{noformat}
create table temp(key int, value string);
-- leave the table empty
select collect_set(key) from temp where key=0;

Error: java.lang.RuntimeException: Hive Runtime Error while closing operators: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:326)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:471)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1477)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.closeOp(GroupByOperator.java:1141)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:577)
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:318)
	... 7 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator.merge(GenericUDAFMkCollectionEvaluator.java:140)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:186)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.closeOp(GroupByOperator.java:1132)
	... 9 more
{noformat}

The root cause is that in GenericUDAFMkCollectionEvaluator.merge() partialResult could be null but is not validated before it is used.
{code}
    List<Object> partialResult = (ArrayList<Object>) internalMergeOI.getList(partial);
    for(Object i : partialResult) {
      putIntoCollection(i, myagg);
    }
{code}
",,jdere,sunrui,szehon,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/14 04:10;sunrui;HIVE-6922.patch;https://issues.apache.org/jira/secure/attachment/12640588/HIVE-6922.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387308,,,,Thu Nov 13 19:44:10 UTC 2014,,,,,,,,,,"0|i1upxj:",387571,,,,,,,,,,,,,,,,,,,,,"17/Apr/14 21:29;jdere;Would you be able to add a testcase for this bug?;;;","18/Apr/14 01:56;sunrui;[~jdere] I thought the bug was trivial and due to a casual missing of null pointer check, so a testcase for it would be trivial. However, if you still prefer a testcase, I can add it.;;;","18/Apr/14 02:09;xuefuz;Yes, adding the null check is trivial, but I guess it's more important to know why the variable might be null. Otherwise, null check might just hide other bug.;;;","18/Apr/14 02:49;sunrui;[~xuefuz] The reason for the variable being null is that the table is empty and thus no input data. 
{code}
  /**
   * Merge with partial aggregation result. NOTE: null might be passed in case
   * there is no input data.
   * 
   * @param partial
   *          The partial aggregation result.
   */
  public abstract void merge(AggregationBuffer agg, Object partial) throws HiveException;
{code}
""Null might be passed in case there is no input data"" in the description for merge() in GenericUDAFEvaluator.
I found existing examples of checking if partial is null. GenericUDAFComputeStats as an example:
{code}
    @Override
    public void merge(AggregationBuffer agg, Object partial) throws HiveException {
      if (partial != null) {
        ...
      }
    }
{code};;;","18/Apr/14 02:59;xuefuz;Sounds good then. Thanks for the explanation.;;;","18/Apr/14 08:01;jdere;+1;;;","19/Apr/14 03:16;sunrui;Seems no auto-generated test report for this patch. Should I submit a review entry?;;;","19/Apr/14 08:58;szehon;Auto-trigger of test isnt working now, due to jenkins trouble.  I've moved the host and am submitting this patch manually on the new host, it should give the report soon I hope.;;;","19/Apr/14 10:31;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12640588/HIVE-6922.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5406 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/precommit-hive/7/testReport
Console output: http://bigtop01.cloudera.org:8080/job/precommit-hive/7/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12640588;;;","24/Apr/14 01:13;jdere;Committed to trunk, thanks Sun Rui!;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
index creation fails with sql std auth turned on ,HIVE-6921,12708909,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,16/Apr/14 20:58,09/Jun/14 06:39,14/Jul/23 06:14,24/Apr/14 06:29,0.13.0,,,,,,,,,0.13.1,0.14.0,Authorization,Indexing,Security,,0,,,,,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/14 21:03;ashutoshc;HIVE-6921.patch;https://issues.apache.org/jira/secure/attachment/12640536/HIVE-6921.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387232,,,,Mon Jun 09 06:39:38 UTC 2014,,,,,,,,,,"0|i1upgv:",387495,,,,,,,,,,,,,,,,,,,,,"16/Apr/14 21:10;ashutoshc;https://reviews.apache.org/r/20426/;;;","17/Apr/14 01:54;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12640536/HIVE-6921.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5406 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_map_operators
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/precommit-hive/15/testReport
Console output: http://bigtop01.cloudera.org:8080/job/precommit-hive/15/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12640536;;;","23/Apr/14 02:11;thejas;+1;;;","24/Apr/14 06:29;ashutoshc;Committed to trunk.;;;","09/Jun/14 06:39;thejas;This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hive sql std auth select query fails on partitioned tables,HIVE-6919,12708669,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,thejas,thejas,thejas,16/Apr/14 04:15,09/Jun/14 06:39,14/Jul/23 06:14,17/Apr/14 19:55,0.13.0,,,,,,,,,0.13.1,0.14.0,Authorization,,,,0,,,"{code}
analyze table studentparttab30k partition (ds) compute statistics;
Error: Error while compiling statement: FAILED: HiveAccessControlException Permission denied. Principal [name=hadoopqa, type=USER] does not have following privileges on Object [type=PARTITION, name=null] : [SELECT] (state=42000,code=40000)
{code}

Sql std auth is supposed to ignore partition level objects for privilege checks, but that is not working as intended.
",,cdrome,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-5837,,,,,,,,,,,,,,,,,,,,,"16/Apr/14 04:16;thejas;HIVE-6919.1.patch;https://issues.apache.org/jira/secure/attachment/12640382/HIVE-6919.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386992,,,,Mon Jun 09 06:39:39 UTC 2014,,,,,,,,,,"0|i1unzj:",387255,,,,,,,,,,,,,,,,,,,,,"16/Apr/14 04:16;thejas;Attaching trivial fix.
;;;","16/Apr/14 05:10;ashutoshc;+1;;;","16/Apr/14 14:40;thejas;Ran tests locally and they passed.
;;;","17/Apr/14 19:55;ashutoshc;Committed to trunk. Thanks, Thejas!;;;","09/Jun/14 06:39;thejas;This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Release Notes for Hive 0.13 RC2,HIVE-6917,12708615,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,Fixed,,rhbutani,rhbutani,15/Apr/14 20:47,15/Apr/14 20:53,14/Jul/23 06:14,15/Apr/14 20:53,,,,,,,,,,0.13.0,,,,,,0,,,,,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/14 20:51;rhbutani;HIVE-6917.1.patch;https://issues.apache.org/jira/secure/attachment/12640331/HIVE-6917.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386938,,,,Tue Apr 15 20:53:47 UTC 2014,,,,,,,,,,"0|i1unnr:",387202,,,,,,,,,,,,,,,,,,,,,"15/Apr/14 20:53;rhbutani;Committed to trunk and 0.13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Export/import inherit permissions from parent directory,HIVE-6916,12708610,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,szehon,szehon,szehon,15/Apr/14 20:23,13/Nov/14 19:43,14/Jul/23 06:14,21/Apr/14 18:07,,,,,,,,,,0.14.0,,Security,,,,0,,,"Export table into an external location and importing into hive, should set the table to have the permission of the parent directory, if the flag hive.warehouse.subdir.inherit.perms is set.",,szehon,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6892,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/14 19:51;szehon;HIVE-6916.2.patch;https://issues.apache.org/jira/secure/attachment/12640875/HIVE-6916.2.patch","17/Apr/14 00:23;szehon;HIVE-6916.patch;https://issues.apache.org/jira/secure/attachment/12640558/HIVE-6916.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386933,,,,Thu Nov 13 19:43:19 UTC 2014,,,,,,,,,,"0|i1unmn:",387197,,,,,,,,,,,,,,,,,,,,,"17/Apr/14 03:46;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12640558/HIVE-6916.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5406 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/precommit-hive/16/testReport
Console output: http://bigtop01.cloudera.org:8080/job/precommit-hive/16/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12640558;;;","18/Apr/14 00:25;szehon;[~xuefuz] can you please help review this?;;;","18/Apr/14 02:58;xuefuz;I put a couple of minor comments on RB.

One thing to clarify though, the purpose of this JIRA is to make imported data file inherit the target directory's permission, regardless where we got the original data file, either exported or hand-made. Thus, it has nothing to with export. Is my assumption correct?;;;","18/Apr/14 03:04;szehon;Thanks for the review.  I can address the first comment.  For the second comment, the FileUtil is actually in HDFS, and there is a separate one in Hive.

It is actually going to be for both export or import, as the use-case is user also want inheritance on the folder/files after hive exports to another location.;;;","18/Apr/14 05:14;xuefuz;Got it. Thanks for the explanation.

I left additional comments on RB for your consideration.;;;","18/Apr/14 19:51;szehon;Thanks for the review.  Sorry forgot to upload the new patch.;;;","18/Apr/14 21:15;xuefuz;+1;;;","18/Apr/14 21:48;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12640875/HIVE-6916.2.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5407 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_groupby2
org.apache.hadoop.hive.metastore.TestRetryingHMSHandler.testRetryingHMSHandler
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/precommit-hive/20/testReport
Console output: http://bigtop01.cloudera.org:8080/job/precommit-hive/20/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12640875;;;","18/Apr/14 22:12;szehon;Failures not related to the patch.  Interestingly, testRetryingHMSHandler reproduced the 'out of sequence' thrift issue, looks like concurrency bug.;;;","21/Apr/14 18:07;xuefuz;Patch committed to trunk. Thanks to Szehon for the patch.;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive Hbase queries fail on secure Tez cluster,HIVE-6915,12708608,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sseth,deepesh,deepesh,15/Apr/14 20:20,13/Nov/14 19:43,14/Jul/23 06:14,24/Oct/14 22:01,0.13.0,,,,,,,,,0.14.0,,Tez,,,,1,,,"Hive queries reading and writing to HBase are currently failing with the following exception in a secure Tez cluster:
{noformat}
2014-04-14 13:47:05,644 FATAL [InputInitializer [Map 1] #0] org.apache.hadoop.ipc.RpcClient: SASL authentication failed. The most likely cause is missing or invalid credentials. Consider 'kinit'.
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
	at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)
	at org.apache.hadoop.hbase.security.HBaseSaslRpcClient.saslConnect(HBaseSaslRpcClient.java:152)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupSaslConnection(RpcClient.java:792)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.access$800(RpcClient.java:349)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection$2.run(RpcClient.java:918)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection$2.run(RpcClient.java:915)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:915)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1065)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1032)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1474)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1684)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1737)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.execService(ClientProtos.java:29288)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.execService(ProtobufUtil.java:1562)
	at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:87)
	at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:84)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:121)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:97)
	at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel.callExecService(RegionCoprocessorRpcChannel.java:90)
	at org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel.callBlockingMethod(CoprocessorRpcChannel.java:67)
	at org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$AuthenticationService$BlockingStub.getAuthenticationToken(AuthenticationProtos.java:4512)
	at org.apache.hadoop.hbase.security.token.TokenUtil.obtainToken(TokenUtil.java:60)
	at org.apache.hadoop.hbase.security.token.TokenUtil$3.run(TokenUtil.java:174)
	at org.apache.hadoop.hbase.security.token.TokenUtil$3.run(TokenUtil.java:172)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)
	at org.apache.hadoop.hbase.security.token.TokenUtil.obtainTokenForJob(TokenUtil.java:171)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.hbase.util.Methods.call(Methods.java:39)
	at org.apache.hadoop.hbase.security.User$SecureHadoopUser.obtainAuthTokenForJob(User.java:334)
	at org.apache.hadoop.hbase.mapred.TableMapReduceUtil.initCredentials(TableMapReduceUtil.java:201)
	at org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getSplits(HiveHBaseTableInputFormat.java:415)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:291)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:372)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getSplits(TezGroupedSplitsInputFormat.java:68)
	at org.apache.tez.mapreduce.hadoop.MRHelpers.generateOldSplits(MRHelpers.java:263)
	at org.apache.tez.mapreduce.common.MRInputAMSplitGenerator.initialize(MRInputAMSplitGenerator.java:139)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:154)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:146)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:146)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:114)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)
	at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)
	at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:121)
	at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)
	at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:223)
	at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)
	at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)
	at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:193)
	... 55 more
{noformat}
kinit was performed by the client. The error appears in the Tez application logs. The queries work fine when i change the hive.execution.engine to MR.",Kerberos secure Tez cluster,brett_s_r,ccondit,deepesh,hagleitn,jxiang,ngangam,sershe,sseth,sushanth,szehon,thejas,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-8782,,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/14 20:22;sershe;HIVE-6915.03.patch;https://issues.apache.org/jira/secure/attachment/12676994/HIVE-6915.03.patch","15/Apr/14 22:12;sseth;HIVE-6915.1.patch;https://issues.apache.org/jira/secure/attachment/12640343/HIVE-6915.1.patch","02/May/14 23:45;ccondit;HIVE-6915.2.patch;https://issues.apache.org/jira/secure/attachment/12643161/HIVE-6915.2.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386931,,,,Thu Nov 13 19:43:37 UTC 2014,,,,,,,,,,"0|i1unm7:",387195,,,,,,,,,,,,,,,,,,,,,"15/Apr/14 22:12;sseth;Changes in the patch
- Obtain delegation tokens in the HBaseStorageHandler configureJobConf method.
- Ensure that these tokens are sent to Tez while constructing the DAG.

MR works since it does InputsSplits client side and HiveHbaseTableInput|OutpoutFormat take care of getting relevant tokens via getSplits / checkOutputSpecs;;;","15/Apr/14 22:21;hagleitn;+1 LGTM. Will commit assuming tests pass.;;;","02/May/14 22:08;ccondit;Patch applied here, get a different error now:
{noformat}
Vertex failed, vertexName=Map 1, vertexId=vertex_1392942637536_9375_1_00, diagnostics=[Vertex Input: hosting_scheduled_jobs initializer failed., org.apache.hadoop.hbase.security.AccessDeniedException: Token generation only allowed for Kerberos authenticated clients
	at org.apache.hadoop.hbase.security.token.TokenProvider.getAuthenticationToken(TokenProvider.java:122)
	at org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$AuthenticationService$1.getAuthenticationToken(AuthenticationProtos.java:4267)
	at org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$AuthenticationService.callMethod(AuthenticationProtos.java:4387)
	at org.apache.hadoop.hbase.regionserver.HRegion.execService(HRegion.java:5088)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.execService(HRegionServer.java:3197)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:26933)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2146)
	at org.apache.hadoop.hbase.ipc.RpcServer$Handler.run(RpcServer.java:1851)
]
14/05/02 15:06:21 ERROR tez.TezJobMonitor: Vertex failed, vertexName=Map 1, vertexId=vertex_1392942637536_9375_1_00, diagnostics=[Vertex Input: hosting_scheduled_jobs initializer failed., org.apache.hadoop.hbase.security.AccessDeniedException: Token generation only allowed for Kerberos authenticated clients
	at org.apache.hadoop.hbase.security.token.TokenProvider.getAuthenticationToken(TokenProvider.java:122)
	at org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$AuthenticationService$1.getAuthenticationToken(AuthenticationProtos.java:4267)
	at org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$AuthenticationService.callMethod(AuthenticationProtos.java:4387)
	at org.apache.hadoop.hbase.regionserver.HRegion.execService(HRegion.java:5088)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.execService(HRegionServer.java:3197)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:26933)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2146)
	at org.apache.hadoop.hbase.ipc.RpcServer$Handler.run(RpcServer.java:1851)
]
DAG failed due to vertex failure. failedVertices:1 killedVertices:0
14/05/02 15:06:21 ERROR tez.TezJobMonitor: DAG failed due to vertex failure. failedVertices:1 killedVertices:0
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask
14/05/02 15:06:21 ERROR ql.Driver: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask
{noformat};;;","02/May/14 23:09;ccondit;Better stack trace:
{noformat}
org.apache.hadoop.hbase.security.AccessDeniedException: org.apache.hadoop.hbase.security.AccessDeniedException: Token generation only allowed for Kerberos authenticated clients
	at org.apache.hadoop.hbase.security.token.TokenProvider.getAuthenticationToken(TokenProvider.java:122)
	at org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$AuthenticationService$1.getAuthenticationToken(AuthenticationProtos.java:4267)
	at org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$AuthenticationService.callMethod(AuthenticationProtos.java:4387)
	at org.apache.hadoop.hbase.regionserver.HRegion.execService(HRegion.java:5088)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.execService(HRegionServer.java:3197)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:26933)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2146)
	at org.apache.hadoop.hbase.ipc.RpcServer$Handler.run(RpcServer.java:1851)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.getRemoteException(ProtobufUtil.java:235)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.execService(ProtobufUtil.java:1348)
	at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:87)
	at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:84)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:116)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:94)
	at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel.callExecService(RegionCoprocessorRpcChannel.java:90)
	at org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel.callBlockingMethod(CoprocessorRpcChannel.java:67)
	at org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$AuthenticationService$BlockingStub.getAuthenticationToken(AuthenticationProtos.java:4512)
	at org.apache.hadoop.hbase.security.token.TokenUtil.obtainToken(TokenUtil.java:60)
	at org.apache.hadoop.hbase.security.token.TokenUtil$3.run(TokenUtil.java:174)
	at org.apache.hadoop.hbase.security.token.TokenUtil$3.run(TokenUtil.java:172)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
	at org.apache.hadoop.hbase.security.token.TokenUtil.obtainTokenForJob(TokenUtil.java:171)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.util.Methods.call(Methods.java:39)
	at org.apache.hadoop.hbase.security.User$SecureHadoopUser.obtainAuthTokenForJob(User.java:314)
	at org.apache.hadoop.hbase.mapred.TableMapReduceUtil.initCredentials(TableMapReduceUtil.java:181)
	at org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getSplits(HiveHBaseTableInputFormat.java:416)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:291)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:372)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getSplits(TezGroupedSplitsInputFormat.java:68)
	at org.apache.tez.mapreduce.hadoop.MRHelpers.generateOldSplits(MRHelpers.java:263)
	at org.apache.tez.mapreduce.common.MRInputAMSplitGenerator.initialize(MRInputAMSplitGenerator.java:139)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:146)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:139)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:139)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:110)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{noformat}

As a quick test, I patched HiveHBaseTableInputFormat.getSplits() to ignore exceptions from TableMapReduceUtil.initCredentials and my query completed successfully. Is there a way to detect if this call is required? Something in the jobConf?;;;","02/May/14 23:45;ccondit;New patch version. This version calls TableMapReduceUtil.initCredentials() only in the case where the current user is logged in via Kerberos.;;;","03/May/14 23:30;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12643161/HIVE-6915.2.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 5430 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/116/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/116/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12643161;;;","06/Jul/14 00:47;sushanth;[~hagleitn],[~sseth], could you please help verify/review Craig's patch?;;;","08/Oct/14 16:54;ngangam;I am using the fix from the patch with Hive 0.13 and I continue to see the issue with Tez while it works fine with MR.
I am running a simple ""SELECT count(*) from hbasetable;""

{code}
2014-10-08 08:12:39,910 FATAL [InputInitializer [Map 1] #0] org.apache.hadoop.ipc.RpcClient: SASL authentication failed. The most likely cause is missing or invalid credentials. Consider 'kinit'.
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
	at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)
	at org.apache.hadoop.hbase.security.HBaseSaslRpcClient.saslConnect(HBaseSaslRpcClient.java:179)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupSaslConnection(RpcClient.java:770)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.access$600(RpcClient.java:357)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection$2.run(RpcClient.java:891)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection$2.run(RpcClient.java:888)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:888)
	at org.apache.hadoop.hbase.ipc.RpcClient.getConnection(RpcClient.java:1543)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1442)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1661)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1719)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.scan(ClientProtos.java:29990)
	at org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:308)
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:164)
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:59)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:114)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:90)
	at org.apache.hadoop.hbase.client.ClientScanner.nextScanner(ClientScanner.java:283)
	at org.apache.hadoop.hbase.client.ClientScanner.initializeScannerInConstruction(ClientScanner.java:188)
	at org.apache.hadoop.hbase.client.ClientScanner.<init>(ClientScanner.java:183)
	at org.apache.hadoop.hbase.client.ClientScanner.<init>(ClientScanner.java:110)
	at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:738)
	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:178)
	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:82)
	at org.apache.hadoop.hbase.client.MetaScanner.allTableRegions(MetaScanner.java:282)
	at org.apache.hadoop.hbase.client.HTable.getRegionLocations(HTable.java:616)
	at org.apache.hadoop.hbase.util.RegionSizeCalculator.<init>(RegionSizeCalculator.java:79)
	at org.apache.hadoop.hbase.util.RegionSizeCalculator.<init>(RegionSizeCalculator.java:64)
	at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(TableInputFormatBase.java:160)
	at org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getSplits(HiveHBaseTableInputFormat.java:482)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:291)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:372)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getSplits(TezGroupedSplitsInputFormat.java:68)
	at org.apache.tez.mapreduce.hadoop.MRHelpers.generateOldSplits(MRHelpers.java:263)
	at org.apache.tez.mapreduce.common.MRInputAMSplitGenerator.initialize(MRInputAMSplitGenerator.java:139)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:154)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:146)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:146)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:114)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)
	at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)
	at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:121)
	at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)
	at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:223)
	at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)
	at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)
	at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:193)
{code}

Notice ;;;","24/Oct/14 20:22;sershe;rebased patch. I think it was previously tested in our test env, and worked then. We'll try again before committing.;;;","24/Oct/14 21:14;sershe;Seems to work at least in some cases where queries previously got stuck. [~ashutoshc] can you review?;;;","24/Oct/14 21:23;hagleitn;Still looks good to me. +1;;;","24/Oct/14 21:54;sershe;[~vikram.dixit] ok for 14?;;;","24/Oct/14 22:01;sershe;committed to trunk meanwhile;;;","24/Oct/14 22:27;vikram.dixit;+1 for 0.14;;;","24/Oct/14 23:00;sershe;committed to 14;;;","31/Oct/14 22:52;szehon;This breaks hadoop-1 compilation , I suppose the hadoop-1 version Credentials class doesn't have ""mergeAll"" method.  Not sure is there any way we can do this in shim?  Thanks.;;;","07/Nov/14 18:35;jxiang;[~szehon], I filed HIVE-8782 for the hadoop-1 compilation issue.;;;","07/Nov/14 18:57;szehon;Thanks a lot Jimmy, really appreciate someone taking a look.;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
parquet-hive cannot write nested map (map value is map),HIVE-6914,12708602,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,rdblue,tongjie,tongjie,15/Apr/14 20:06,12/Feb/15 23:41,14/Jul/23 06:14,25/Nov/14 18:48,0.12.0,0.13.0,,,,,,,,1.1.0,,File Formats,,,,1,parquet,serialization,"// table schema (identical for both plain text version and parquet version)
desc hive> desc text_mmap;
m map>

// sample nested map entry
{""level1"":{""level2_key1"":""value1"",""level2_key2"":""value2""}}

The following query will fail, 
insert overwrite table parquet_mmap select * from text_mmap;

Caused by: parquet.io.ParquetEncodingException: This should be an ArrayWritable or MapWritable: org.apache.hadoop.hive.ql.io.parquet.writable.BinaryWritable@f2f8106
at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writeData(DataWritableWriter.java:85)
at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writeArray(DataWritableWriter.java:118)
at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writeData(DataWritableWriter.java:80)
at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writeData(DataWritableWriter.java:82)
at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.write(DataWritableWriter.java:55)
at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.write(DataWritableWriteSupport.java:59)
at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.write(DataWritableWriteSupport.java:31)
at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:115)
at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(ParquetRecordWriterWrapper.java:77)
at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(ParquetRecordWriterWrapper.java:90)
at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:622)
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:793)
at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:793)
at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:793)
at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:540)
... 9 more",,brocknoland,langea,mdominguez@cloudera.com,mickaellcr,rdblue,spena,tongjie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-8359,,,,,HIVE-8120,,,,,,,,,,,,,,,,,,,HIVE-8909,,"22/Nov/14 00:33;rdblue;HIVE-6914.1.patch;https://issues.apache.org/jira/secure/attachment/12683004/HIVE-6914.1.patch","08/Oct/14 11:15;mickaellcr;HIVE-6914.1.patch;https://issues.apache.org/jira/secure/attachment/12673580/HIVE-6914.1.patch","13/Nov/14 10:38;mickaellcr;HIVE-6914.2.patch;https://issues.apache.org/jira/secure/attachment/12681303/HIVE-6914.2.patch","22/Nov/14 00:52;rdblue;HIVE-6914.3.patch;https://issues.apache.org/jira/secure/attachment/12683010/HIVE-6914.3.patch","24/Nov/14 17:43;spena;HIVE-6914.4.patch;https://issues.apache.org/jira/secure/attachment/12683366/HIVE-6914.4.patch","22/Nov/14 00:33;rdblue;NestedMap.parquet;https://issues.apache.org/jira/secure/attachment/12683005/NestedMap.parquet",,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386925,,,,Wed Nov 26 10:23:01 UTC 2014,,,,,,,,,,"0|i1unl3:",387189,,,,,,,,,,,,,,,,,,,,,"02/Jul/14 19:27;langea;I get the same error, no matter what file format I use for the source table. I've tried Textfile, Sequencefile, RCFile, and ORC.;;;","02/Jul/14 19:38;langea;Could it be that [HIVE-7073] is the origin for this bug?;;;","08/Oct/14 08:54;mickaellcr;[~langea], no the culprit is inside the method writeArray.
The wrong data was written.

Patch in progress;;;","08/Oct/14 11:15;mickaellcr;Patch to allow parquet-hive to write complex nested type as array of map, map of array as value, array of struct, ...;;;","08/Oct/14 11:16;mickaellcr;[HIVE-6914] parquet-hive cannot write nested map (map value is map)

     * Change the way ArrayWritable was filled
     * Now parquet-hive can write complex nested type (array of map, map of array, struct ...)
     * Adding some qtests;;;","08/Oct/14 18:05;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12673580/HIVE-6914.1.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1177/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1177/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1177/

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN LPAREN KW_CASE"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_NOT KW_FALSE"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_DATE StringLiteral"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_NOT KW_TRUE"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_NOT KW_MAP"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_CASE KW_MAP"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN LPAREN KW_MAP"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_CASE KW_UNIONTYPE"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_CASE KW_STRUCT"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_NOT KW_IF"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_CASE KW_IF"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN LPAREN KW_IF"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:115:5: 
Decision can match input such as ""KW_CLUSTER KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:127:5: 
Decision can match input such as ""KW_PARTITION KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:138:5: 
Decision can match input such as ""KW_DISTRIBUTE KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:149:5: 
Decision can match input such as ""KW_SORT KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:166:7: 
Decision can match input such as ""STAR"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:179:5: 
Decision can match input such as ""KW_STRUCT"" using multiple alternatives: 4, 6

As a result, alternative(s) 6 were disabled for that input
warning(200): IdentifiersParser.g:179:5: 
Decision can match input such as ""KW_ARRAY"" using multiple alternatives: 2, 6

As a result, alternative(s) 6 were disabled for that input
warning(200): IdentifiersParser.g:179:5: 
Decision can match input such as ""KW_UNIONTYPE"" using multiple alternatives: 5, 6

As a result, alternative(s) 6 were disabled for that input
warning(200): IdentifiersParser.g:261:5: 
Decision can match input such as ""KW_TRUE"" using multiple alternatives: 3, 8

As a result, alternative(s) 8 were disabled for that input
warning(200): IdentifiersParser.g:261:5: 
Decision can match input such as ""KW_NULL"" using multiple alternatives: 1, 8

As a result, alternative(s) 8 were disabled for that input
warning(200): IdentifiersParser.g:261:5: 
Decision can match input such as ""KW_FALSE"" using multiple alternatives: 3, 8

As a result, alternative(s) 8 were disabled for that input
warning(200): IdentifiersParser.g:261:5: 
Decision can match input such as ""KW_DATE StringLiteral"" using multiple alternatives: 2, 3

As a result, alternative(s) 3 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""KW_BETWEEN KW_MAP LPAREN"" using multiple alternatives: 8, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_INSERT KW_INTO"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_UNION KW_ALL"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_SORT KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_CLUSTER KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_LATERAL KW_VIEW"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_MAP LPAREN"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_DISTRIBUTE KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_ORDER KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_GROUP KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_INSERT KW_OVERWRITE"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:518:5: 
Decision can match input such as ""{AMPERSAND..BITWISEXOR, DIV..DIVIDE, EQUAL..EQUAL_NS, GREATERTHAN..GREATERTHANOREQUALTO, KW_AND, KW_ARRAY, KW_BETWEEN..KW_BOOLEAN, KW_CASE, KW_DOUBLE, KW_FLOAT, KW_IF, KW_IN, KW_INT, KW_LIKE, KW_MAP, KW_NOT, KW_OR, KW_REGEXP, KW_RLIKE, KW_SMALLINT, KW_STRING..KW_STRUCT, KW_TINYINT, KW_UNIONTYPE, KW_WHEN, LESSTHAN..LESSTHANOREQUALTO, MINUS..NOTEQUAL, PLUS, STAR, TILDE}"" using multiple alternatives: 1, 3

As a result, alternative(s) 3 were disabled for that input
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-exec ---
Downloading: http://www.datanucleus.org/downloads/maven2/net/hydromatic/linq4j/0.4/linq4j-0.4.pom
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [11.597s]
[INFO] Hive Shims Common ................................. SUCCESS [7.054s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [3.858s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.831s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.317s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [6.509s]
[INFO] Hive Shims ........................................ SUCCESS [1.296s]
[INFO] Hive Common ....................................... SUCCESS [8.962s]
[INFO] Hive Serde ........................................ SUCCESS [14.361s]
[INFO] Hive Metastore .................................... SUCCESS [36.803s]
[INFO] Hive Ant Utilities ................................ SUCCESS [1.657s]
[INFO] Hive Query Language ............................... FAILURE [26.972s]
[INFO] Hive Service ...................................... SKIPPED
[INFO] Hive Accumulo Handler ............................. SKIPPED
[INFO] Hive JDBC ......................................... SKIPPED
[INFO] Hive Beeline ...................................... SKIPPED
[INFO] Hive CLI .......................................... SKIPPED
[INFO] Hive Contrib ...................................... SKIPPED
[INFO] Hive HBase Handler ................................ SKIPPED
[INFO] Hive HCatalog ..................................... SKIPPED
[INFO] Hive HCatalog Core ................................ SKIPPED
[INFO] Hive HCatalog Pig Adapter ......................... SKIPPED
[INFO] Hive HCatalog Server Extensions ................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client ................. SKIPPED
[INFO] Hive HCatalog Webhcat ............................. SKIPPED
[INFO] Hive HCatalog Streaming ........................... SKIPPED
[INFO] Hive HWI .......................................... SKIPPED
[INFO] Hive ODBC ......................................... SKIPPED
[INFO] Hive Shims Aggregator ............................. SKIPPED
[INFO] Hive TestUtils .................................... SKIPPED
[INFO] Hive Packaging .................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 2:08.890s
[INFO] Finished at: Wed Oct 08 14:05:44 EDT 2014
[INFO] Final Memory: 78M/414M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (default) on project hive-exec: Error resolving project artifact: Could not transfer artifact net.hydromatic:linq4j:pom:0.4 from/to datanucleus (http://www.datanucleus.org/downloads/maven2): Access denied to: http://www.datanucleus.org/downloads/maven2/net/hydromatic/linq4j/0.4/linq4j-0.4.pom, ReasonPhrase: Forbidden. for project net.hydromatic:linq4j:jar:0.4 -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-exec
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12673580;;;","09/Oct/14 01:43;brocknoland;Linking to HIVE-8120.

FYI [~spena] who is looking into some parquet hive schema stuff as well.;;;","13/Nov/14 10:38;mickaellcr;Patch updated

Better way to handle this case.

[HIVE-6914] parquet-hive cannot write nested map (map value is map)

     * Change the way ArrayWritable was filled
     * Now parquet-hive can write complex nested type (array of map, map of array, struct ...)
     * Adding some qtests

;;;","13/Nov/14 17:46;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12681303/HIVE-6914.2.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6687 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_complex_type_nested
org.apache.hadoop.hive.ql.io.parquet.TestParquetSerDe.testParquetHiveSerDe
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1773/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1773/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1773/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12681303 - PreCommit-HIVE-TRUNK-Build;;;","19/Nov/14 17:02;mickaellcr;[~spena], [~brocknoland], [~rdblue]

I will redo this patch using the path available for HIVE-8359. I think there is a link with this one too HIVE-8909. With the previous patch I can read and write parquet complex nested types.
So maybe it will be better to add my qtests to HIVE-8909 and fix the writing bug ?

What do you think ?
;;;","19/Nov/14 18:08;spena;Hi [~mickaellcr],

It sounds good if you use the patch from HIVE-8359 for this bug. Regarding adding the qtests to HIVE-8909, I think that ticket is meant to fix the reading part of different nested types formats generated by Thrift and Avro tools (it does not touch the writing part); so I think it should be good to have these writing tests separated from the reading tests.

;;;","21/Nov/14 23:42;rdblue;HIVE-8909 tests that parquet-hive can now read this case in its map tests, but it doesn't test the write side.;;;","22/Nov/14 00:33;rdblue;Attaching a qtest and data file that verifies [~spena]'s fix for HIVE-8359 fixes this bug. The qtest relies on HIVE-8909 to read the data.;;;","22/Nov/14 00:52;rdblue;Updating qtest patch to drop all created tables.;;;","22/Nov/14 01:11;brocknoland;+1 pending tests;;;","24/Nov/14 03:48;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12683010/HIVE-6914.3.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 6682 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_map_of_maps
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_optimize_nullscan
org.apache.hive.hcatalog.streaming.TestStreaming.testEndpointConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1885/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1885/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1885/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12683010 - PreCommit-HIVE-TRUNK-Build;;;","24/Nov/14 04:13;brocknoland;I committed the {{NestedMap.parquet}} file but it looks like the {{.q.out}} file needs to be updated due to an additional drop table.;;;","24/Nov/14 17:43;spena;Here's the file with the .q.out updated;;;","25/Nov/14 11:13;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12683366/HIVE-6914.4.patch

{color:green}SUCCESS:{color} +1 6682 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1898/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1898/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1898/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12683366 - PreCommit-HIVE-TRUNK-Build;;;","25/Nov/14 17:45;rdblue;Thanks for fixing the test [~spena]! Looks like it is passing and ready to go.;;;","25/Nov/14 18:48;brocknoland;Thank you much guys! I have committed this to trunk!;;;","26/Nov/14 10:23;mickaellcr;Thx !!!!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive unable to find the hashtable file during complex multi-staged map join,HIVE-6913,12708537,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,brocknoland,brocknoland,brocknoland,15/Apr/14 15:38,13/Nov/14 19:43,14/Jul/23 06:14,18/Apr/14 02:20,,,,,,,,,,0.14.0,,,,,,0,,,"If a query has multiple mapjoins and one of the tables to be mapjoined is empty, the query can result in a ""no such file or directory"" when looking for the hashtable.

This is because when we generate a dummy hash table, we do not close the TableScan (TS) operator for that table. Additionally, HashTableSinkOperator (HTSO) outputs it's hash tables in the closeOp method. However, when close is called on HTSO we check to ensure that all parents are closed: https://github.com/apache/hive/blob/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java#L333

which is not true on this case, because the TS operator for the empty table was never closed.",,brocknoland,ctang,sershe,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/14 17:23;brocknoland;HIVE-6913.patch;https://issues.apache.org/jira/secure/attachment/12640498/HIVE-6913.patch","15/Apr/14 15:40;brocknoland;HIVE-6913.patch;https://issues.apache.org/jira/secure/attachment/12640286/HIVE-6913.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386860,,,,Thu Nov 13 19:43:43 UTC 2014,,,,,,,,,,"0|i1un6n:",387124,,,,,,,,,,,,,,,,,,,,,"15/Apr/14 16:22;xuefuz;Nice catch, Brock!

The code line you referred to seems off the description that you gave. In addition, s it hard to construct a test case?;;;","16/Apr/14 01:30;brocknoland;Thank you for pointing this out! The line of code is: https://github.com/apache/hive/blob/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java#L567

I tried for an entire day to get a test for this but could not reproduce outside of the production environment. I did however, test the fix in the production environment and it worked well.;;;","16/Apr/14 11:02;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12640286/HIVE-6913.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5401 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_map_operators
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/precommit-hive/6/testReport
Console output: http://bigtop01.cloudera.org:8080/job/precommit-hive/6/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12640286;;;","16/Apr/14 17:23;brocknoland;All three tests passed locally. Re-uploading for a another run.;;;","16/Apr/14 21:56;xuefuz;+1;;;","16/Apr/14 22:06;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12640498/HIVE-6913.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5401 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/precommit-hive/13/testReport
Console output: http://bigtop01.cloudera.org:8080/job/precommit-hive/13/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12640498;;;","17/Apr/14 15:32;ashutoshc;+1;;;","18/Apr/14 02:20;xuefuz;Patch committed to trunk. Thanks Brock for the fix.;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid column access info for partitioned table,HIVE-6910,12708450,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,navis,navis,navis,15/Apr/14 05:38,13/Nov/14 19:41,14/Jul/23 06:14,27/May/14 01:35,0.11.0,0.12.0,0.13.0,,,,,,,0.14.0,,Query Processor,,,,2,,,"From http://www.mail-archive.com/user@hive.apache.org/msg11324.html

neededColumnIDs in TS is only for non-partition columns. But ColumnAccessAnalyzer is calculating it on all columns.",,adeelmahmood,navis,szehon,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/14 05:38;navis;HIVE-6910.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12640206/HIVE-6910.1.patch.txt","16/Apr/14 01:05;navis;HIVE-6910.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12640374/HIVE-6910.2.patch.txt","07/May/14 04:05;navis;HIVE-6910.3.patch.txt;https://issues.apache.org/jira/secure/attachment/12643681/HIVE-6910.3.patch.txt","08/May/14 02:33;navis;HIVE-6910.4.patch.txt;https://issues.apache.org/jira/secure/attachment/12643891/HIVE-6910.4.patch.txt","19/May/14 07:04;navis;HIVE-6910.5.patch.txt;https://issues.apache.org/jira/secure/attachment/12645517/HIVE-6910.5.patch.txt","24/May/14 05:24;navis;HIVE-6910.6.patch.txt;https://issues.apache.org/jira/secure/attachment/12646661/HIVE-6910.6.patch.txt","26/May/14 01:42;navis;HIVE-6910.7.patch.txt;https://issues.apache.org/jira/secure/attachment/12646726/HIVE-6910.7.patch.txt",,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386773,,,,Thu Nov 13 19:41:54 UTC 2014,,,,,,,,,,"0|i1umnb:",387037,,,,,,,,,,,,,,,,,,,,,"15/Apr/14 17:10;ashutoshc;+1;;;","15/Apr/14 21:41;adeelmahmood;does the patch works if the query includes not all of the partition columns because it seems like you are adding all partition columns to columnAccessInfo instead of just the needed ones based on the query

select non-part-col1, part-col-2 from table

should return only these two columns and not all other partition columns as well. Just wanted to make sure that is being taken into consideration.;;;","16/Apr/14 00:07;ashutoshc;[~adeelmahmood] observation is correct. Patch is adding all partition columns unconditionally, instead of adding only the ones referred in query.;;;","16/Apr/14 00:14;navis;Current neededColumnIDs does not contain any partition columns, because those are provided unconditionally by MapOp. Should we discern partition columns referred in query? Then it needs some more works.;;;","16/Apr/14 00:24;ashutoshc;I think we need to do that if we want to fix it as requested by user (which to me seems correct desired behavior).;;;","16/Apr/14 14:42;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12640374/HIVE-6910.2.patch.txt

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5401 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority2
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/precommit-hive/8/testReport
Console output: http://bigtop01.cloudera.org:8080/job/precommit-hive/8/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12640374;;;","16/Apr/14 21:48;ashutoshc;Seems like patch is only adding partition columns from where clause not from select list. See my comments on RB;;;","07/May/14 16:47;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12643681/HIVE-6910.3.patch.txt

{color:red}ERROR:{color} -1 due to 46 failed/errored test(s), 5428 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.ql.parse.TestParse.testParse_case_sensitivity
org.apache.hadoop.hive.ql.parse.TestParse.testParse_cast1
org.apache.hadoop.hive.ql.parse.TestParse.testParse_groupby1
org.apache.hadoop.hive.ql.parse.TestParse.testParse_groupby2
org.apache.hadoop.hive.ql.parse.TestParse.testParse_groupby3
org.apache.hadoop.hive.ql.parse.TestParse.testParse_groupby4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_groupby5
org.apache.hadoop.hive.ql.parse.TestParse.testParse_groupby6
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input1
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input2
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input20
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input3
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input5
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input6
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input7
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input8
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input9
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input_part1
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input_testsequencefile
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input_testxpath
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input_testxpath2
org.apache.hadoop.hive.ql.parse.TestParse.testParse_join1
org.apache.hadoop.hive.ql.parse.TestParse.testParse_join2
org.apache.hadoop.hive.ql.parse.TestParse.testParse_join3
org.apache.hadoop.hive.ql.parse.TestParse.testParse_join4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_join5
org.apache.hadoop.hive.ql.parse.TestParse.testParse_join6
org.apache.hadoop.hive.ql.parse.TestParse.testParse_join7
org.apache.hadoop.hive.ql.parse.TestParse.testParse_join8
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample1
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample2
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample3
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample5
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample6
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample7
org.apache.hadoop.hive.ql.parse.TestParse.testParse_subq
org.apache.hadoop.hive.ql.parse.TestParse.testParse_udf1
org.apache.hadoop.hive.ql.parse.TestParse.testParse_udf4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_udf6
org.apache.hadoop.hive.ql.parse.TestParse.testParse_udf_case
org.apache.hadoop.hive.ql.parse.TestParse.testParse_udf_when
org.apache.hadoop.hive.ql.parse.TestParse.testParse_union
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/138/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/138/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 46 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12643681;;;","09/May/14 23:12;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12643891/HIVE-6910.4.patch.txt

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5433 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/155/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/155/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12643891;;;","12/May/14 19:55;ashutoshc;Patch looks good. But looks like there are few changes which may not be essential for the patch. Left comments on RB.;;;","19/May/14 07:26;ashutoshc;+1 Lets do clean up of TableScan Operator in a follow-up;;;","19/May/14 18:48;szehon;Posting result manually:

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12645517/HIVE-6910.5.patch.txt

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/236/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/236/console

{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]
+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-236/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnAssignFactory.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/io/parquet/VectorizedParquetInputFormat.java'
++ egrep -v '^X|^Performing status on external'
++ awk '{print $2}'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-minikdc/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/core/target hcatalog/streaming/target hcatalog/server-extensions/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target hwi/target common/target common/src/gen service/target contrib/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target
+ svn update
U    ql/src/test/queries/clientpositive/vector_decimal_math_funcs.q
U    ql/src/test/queries/clientpositive/udf_java_method.q
U    ql/src/test/queries/clientpositive/udf_reflect.q
U    ql/src/test/results/clientpositive/tez/script_pipe.q.out
U    ql/src/test/results/clientpositive/tez/transform_ppr2.q.out
U    ql/src/test/results/clientpositive/tez/transform1.q.out
U    ql/src/test/results/clientpositive/tez/transform_ppr1.q.out
U    ql/src/test/results/clientpositive/udf_java_method.q.out
U    ql/src/test/results/clientpositive/vector_decimal_math_funcs.q.out
U    ql/src/test/results/clientpositive/udf_reflect.q.out
U    ql/src/test/results/clientpositive/auto_sortmerge_join_11.q.out
U    ql/src/test/results/compiler/plan/input4.q.xml
U    ql/src/test/results/compiler/plan/input5.q.xml
U    ql/src/test/results/compiler/plan/input20.q.xml
U    testutils/ptest2/src/main/java/org/apache/hive/ptest/execution/conf/TestConfiguration.java
U    testutils/ptest2/src/main/java/org/apache/hive/ptest/execution/JIRAService.java

Fetching external item into 'hcatalog/src/test/e2e/harness'
Updated external to revision 1595985.

Updated to revision 1595985.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat};;;","20/May/14 16:44;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12645517/HIVE-6910.5.patch.txt

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/244/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/244/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logsPreCommit-HIVE-Build-244/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]
+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-maven-3.0.5/bin:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-maven-3.0.5/bin:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-244/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/tez/CustomPartitionVertex.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java'
++ awk '{print $2}'
++ egrep -v '^X|^Performing status on external'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-minikdc/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/core/target hcatalog/streaming/target hcatalog/server-extensions/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target hwi/target common/target common/src/gen contrib/target service/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SplitGrouper.java ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HiveSplitGenerator.java
+ svn update
U    serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java
U    serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java
U    ql/src/java/org/apache/hadoop/hive/ql/exec/SkewJoinHandler.java
U    ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
U    ql/src/java/org/apache/hadoop/hive/ql/io/SymlinkTextInputFormat.java

Fetching external item into 'hcatalog/src/test/e2e/harness'
Updated external to revision 1596312.

Updated to revision 1596312.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12645517;;;","23/May/14 06:06;ashutoshc;Patch needs to be rebased.;;;","24/May/14 06:29;ashutoshc;+1;;;","25/May/14 10:54;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646661/HIVE-6910.6.patch.txt

{color:red}ERROR:{color} -1 due to 1384 failed/errored test(s), 5537 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_add_part_multiple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_allcolref_in_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_char1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_char2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_concatenate_indexed_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_merge
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_merge_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_merge_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_numbuckets_partitioned_table2_h23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_numbuckets_partitioned_table_h23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_coltype
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_rename_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_rename_partition_authorization
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_table_serde2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_varchar1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_varchar2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_view_rename
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_analyze_table_null_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_groupby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_select
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ansi_sql_arithmetic
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_archive_excludeHadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_archive_multi
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_index
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_insert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_parts
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_view_sqlstd
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join18_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join27
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join29
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_filters
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_nulls
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_reordering_values
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_without_localtask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_autogen_colalias
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_compression_enabled
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_evolved_schemas
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_joins
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_nullable_fields
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_sanity_test
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ba_table1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ba_table2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ba_table3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ba_table_udfs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ba_table_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binary_output_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binary_table_bincolserde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binary_table_colserde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binarysortable_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_groupby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_if_with_path_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketizedhiveinputformat_auto
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_case_sensitivity
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cast1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_cast
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_comparison
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_nested_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_udf1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_union1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_varchar_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cluster
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_column_access_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_columnarserde_create_shortcut
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_columnstats_partlvl
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_columnstats_tbllvl
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_combine1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_combine2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_combine3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_compile_processor
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_compute_stats_binary
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_compute_stats_boolean
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_compute_stats_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_compute_stats_double
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_compute_stats_empty_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_compute_stats_long
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_compute_stats_string
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_concatenate_inherit_table_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_count
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cp_mj_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_big_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_default_prop
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_escape
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_func1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_genericudaf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_genericudf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_insert_outputformat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_like
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_like_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_merge_compressed
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_or_replace_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_struct_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_udaf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_view_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_view_translate
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cross_product_check_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cross_product_check_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas_char
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas_colname
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas_uses_database_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas_varchar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cte_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cte_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_custom_input_output_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_database_drop
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dbtxnmgr_ddl1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dbtxnmgr_query1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dbtxnmgr_query2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dbtxnmgr_query3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dbtxnmgr_query4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ddltime
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_precision
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_delimiter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_diff_part_input_formats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_disallow_incompatible_type_change_off
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_distinct_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_database_removes_partition_dirs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_table_removes_partition_dirs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynamic_partition_skip_default
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynpart_sort_opt_vectorization
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_enforce_order
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_explode_null
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fetch_aggregation
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fileformat_mix
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fileformat_sequencefile
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fileformat_text
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_global_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1_map_nomap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_noskew_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3_map_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3_noskew_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby4_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby4_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby4_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby5_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby5_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby5_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby6_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby6_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby6_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7_map_multi_single_reducer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7_noskew_multi_single_reducer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby8_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby8_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby8_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_bigdata
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_complex_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_complex_types_multi_single_reducer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_cube1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_distinct_samekey
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_id1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_id2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_sets1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_sets2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_sets3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_sets4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_sets5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_map_ppr_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_multi_insert_common_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_multi_single_reducer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_multi_single_reducer2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_multi_single_reducer3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_neg_float
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_position
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_ppr_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_resolution
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_rollup1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_test_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_hook_context_cs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_hook_order
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_implicit_cast1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auth
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_empty
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_file_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables_compact
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_multiple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_self_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_unused
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_update
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_compression
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_binary_search
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compression
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_creation
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_stale
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_stale_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_convert_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_grouping_operators
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_multi_insert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_const_type
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_innerjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input11_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input14_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input1_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input2_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input35
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input36
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input37
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input38
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input39
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input3_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input40
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input41
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input42
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input43
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input44
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input45
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input46
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input49
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input4_cb_delim
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input4_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_columnarserde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_dynamicserde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_lazyserde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_testsequencefile
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_testxpath
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_testxpath2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_testxpath3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_testxpath4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_inputddl5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_inputddl7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert1_overwrite_partitions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert2_overwrite_partitions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_compressed
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_overwrite_local_directory_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insertexternal1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join18_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join27
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join29
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32_lessSize
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join35
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join36
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join37
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join38
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join39
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join40
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join41
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_1to1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_array
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_casesensitive
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_empty
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_filters
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_filters_overlap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_hive_626
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_literals
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_nulls
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_nullsafe
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_reorder
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_reorder2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_reorder3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_reorder4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_star
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_thrift
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_vc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view_cp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view_noalias
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view_ppd
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lb_fs_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_leadlag
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_leadlag_queries
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_leftsemijoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_limit_partition_metadataonly
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_limit_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lineage1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_binary_data
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_fs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_fs_overwrite
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_overwrite
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_loadpart1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_louter_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_addjar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_filter_on_outerjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_hook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_memcheck
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_subquery
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_subquery2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadata_only_queries
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadata_only_queries_with_filters
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadataonly1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mi
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_gby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_gby2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_gby3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_lateral_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_move_tasks_share_dependencies
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_join_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nested_complex
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nestedvirtual
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_newline
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_no_hooks
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_noalias_subq1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nomore_ambiguous_table_col
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonblock_op_deduplicate
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonmr_fetch
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonreserved_keywords_input37
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonreserved_keywords_insert_into1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_notable_alias1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_notable_alias2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_notable_alias3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_null_cast
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_null_column
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatCTAS
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatdir
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup4_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullinput
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullinput2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullscript
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_analyze
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_create
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_dictionary_threshold
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_diff_part_cols
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_diff_part_cols2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_empty_files
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_empty_strings
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ends_with_nulls
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_min_max
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ppd_char
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ppd_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ppd_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ppd_varchar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_predicate_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_split_elimination
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_vectorization_ppd
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_order
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_order2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_order_within_subquery
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_outer_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_overridden_confs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parallel
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parenthesis_star_by
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_create
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_ctas
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partInit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partcols1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_date2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_decode_name
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_schema1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_serde_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_special_char
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_type_check
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_varchar1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_varchar2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_vs_table_metadata
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pcr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_clusterby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_constant_expr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_constant_where
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_field_garbage
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_gby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_gby2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_multi_insert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_outer_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_outer_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_outer_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_outer_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_transform
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_udf_case
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_udtf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_vc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_allchildsarenull
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_pushdown2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_pushdown3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_print_header
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_progress_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_protectmode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf_general_queries
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf_matchpath
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf_rcfile
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf_register_tblfn
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf_seqfile
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf_streaming
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_push_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_query_properties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_query_result_fileformat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_query_with_semi
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quote1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_alter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_basic
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_bigdata
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_columnar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_default_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_lazydecompress
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_merge1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_merge2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_merge3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_merge4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_null_value
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_toleratecorruptions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_recursive_dir
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_reduce_deduplicate_exclude_gby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_reduce_deduplicate_extended
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_reducesink_dedup
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_regex_col
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_regexp_extract
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rename_external_partition_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rename_partition_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rename_table_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_router_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample_islocalmode_hook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_script_env_var1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_script_env_var2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_script_pipe
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_as_omitted
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_transform_hint
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_unquote_and
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_unquote_not
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_unquote_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_semicolon
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_semijoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_serde_regex
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_set_processor_namespaces
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_indexes_edge_cases
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_indexes_syntax
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_split
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_aggregator_error_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_empty_dyn_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_empty_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_invalidation
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_noscan_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_noscan_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_only_null
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_publisher_error_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_statsfs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_str_to_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subq
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subq2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_alias
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_exists
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_exists_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_in
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_in_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_multiinsert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_notexists
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_notexists_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_notin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_notin_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_views
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_table_access_keys_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_tablename_with_select
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_test_boolean_whereclause
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_lazy
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_null
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_touch
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_merge
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_type_widening
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_collect_set
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_context_ngrams
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_corr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_covar_pop
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_covar_samp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_histogram_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_ngrams
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_number_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_percentile
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_percentile_approx_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_sum_list
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_10_trims
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_bitmap_and
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_bitmap_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_case_column_pruning
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_compare_java_string
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_concat_insert1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_concat_insert2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_concat_ws
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_context_aware
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_count
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_field
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_get_json_object
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_in_file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_inline
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_length
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_lower
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_max
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_min
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_minute
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_parse_url
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_percentile
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_printf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_reflect2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_reverse
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_round_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_sentences
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_substr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_translate
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_json_tuple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_parse_url_tuple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_posexplode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_stack
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union27
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union29
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_lateralview
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_null
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_script
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_top_level
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_uniquejoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_updateAccessTime
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_nested_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_udf1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_union1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_between_in
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_coalesce
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_aggregate
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_cast
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_expressions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_math_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_left_outer_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_non_string_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_decimal_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_div0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_nested_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_not
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_part_project
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_short_regress
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_bucketmapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_case
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_casts
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_context
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_date_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_distinct_gby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_math_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_nested_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_parquet
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_rcfile_columnar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_shufflejoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_string_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_timestamp_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_view_cast
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_view_inputs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_virtual_column
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_adjust_rowcontainer_sz
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_columnPruning
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_expressions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_multipartitioning
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_navfn
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_ntile
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_rank
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_udaf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_udaf2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_windowspec
org.apache.hadoop.hive.cli.TestCompareCliDriver.testCompareCliDriver_vectorized_math_funcs
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_dboutput
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_fileformat_base64
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_java_mr_example
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_lateral_view_explode2
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_regex
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes2
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes3
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes4
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes5
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes_null
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udaf_example_avg
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udaf_example_group_concat
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udaf_example_max
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udaf_example_max_n
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udaf_example_min
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udaf_example_min_n
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udf_example_add
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udf_example_arraymapstruct
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udf_example_format
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udf_row_sequence
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udtf_explode2
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udtf_output_on_close
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_external_table_ppd
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_binary_external_table_queries
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_binary_map_queries
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_binary_map_queries_prefix
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_binary_storage_queries
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_custom_key
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_custom_key2
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_joins
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_ppd_key_range
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_pushdown
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_queries
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_scan_params
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_single_sourced_multi_insert
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats2
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats3
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats_empty_partition
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_ppd_key_ranges
org.apache.hadoop.hive.cli.TestHBaseMinimrCliDriver.testCliDriver_hbase_bulk
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_join0
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_join1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket4
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_count
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_create_merge_compressed
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cross_product_check_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cross_product_check_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_ctas
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_custom_input_output_format
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_disable_merge_for_bucketing
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_opt_vectorization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_enforce_order
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_filter_join_breaktask2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_groupby1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_groupby2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_groupby3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_having
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert_into1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert_into2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_join0
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_join1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_leftsemijoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_limit_pushdown
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mapjoin_decimal
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mapjoin_mapjoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mapreduce1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mapreduce2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_merge1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_merge2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_metadata_only_queries
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mrr
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_orc_analyze
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_ptf
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_sample1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_script_env_var1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_script_env_var2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_script_pipe
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_stats_counter
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_stats_counter_partitioned
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_stats_noscan_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_subquery_exists
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_subquery_in
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_fsstat
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_insert_overwrite_local_directory_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_join_tests
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_joins_explain
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_schema_evolution
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_transform1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_transform2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union4
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union6
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union8
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union9
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_15
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket4
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket5
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers2
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin7
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_disable_merge_for_bucketing
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_external_table_with_space_in_location_path
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_file_with_header_footer
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_groupby2
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_bucketed_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_dyn_part
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_map_operators
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_merge
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_num_buckets
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_reducers_power_two
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_input16_cc
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_join1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_leftsemijoin_mr
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_list_bucket_dml_10
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_parallel_orderby
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_quotedid_smb
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_reduce_deduplicate
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_remote_script
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority2
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_smb_mapjoin_8
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_stats_counter
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_stats_counter_partitioned
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_truncate_column_buckets
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_concatenate_indexed_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_coltype_2columns
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_coltype_invalidcolname
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_coltype_invalidtype
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_rename_partition_failure
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_rename_partition_failure2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_rename_partition_failure3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_insert1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_insert2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_insert3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_insert4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi6
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_multi7
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_partspec1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_partspec2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_partspec3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_partspec4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_partspec5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_part
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_cachingprintstream
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_column_rename1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_column_rename2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_column_rename4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_disallow_incompatible_type_change_on1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_disallow_incompatible_type_change_on2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dyn_part_max
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dyn_part_max_per_node
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_fileformat_void_input
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_index_compact_entry_limit
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_index_compact_size_limit
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into6
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_limit_partition_stats
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_local_mapred_error_cache
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg_query_tbl_in_locked_db
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_lockneg_try_lock_db_in_use
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_merge_negative_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_merge_negative_3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_protectmode_part1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_protectmode_tbl3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_smb_bucketmapjoin
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_sortmerge_mapjoin_mismatch_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_aggregator_error_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_aggregator_error_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partscan_norcfile
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_publisher_error_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_publisher_error_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_bucketed_column
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_column_indexed_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_column_list_bucketing
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_column_seqfile
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_nonexistant_column
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_partition_column
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_partition_column2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_assert_true
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_union22
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_turnoff
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_minimr_broken_pipe
org.apache.hadoop.hive.hwi.TestHWISessionManager.testHiveDriver
org.apache.hadoop.hive.jdbc.TestJdbcDriver.testDataTypes
org.apache.hadoop.hive.jdbc.TestJdbcDriver.testNullType
org.apache.hadoop.hive.jdbc.TestJdbcDriver.testPrepareStatement
org.apache.hadoop.hive.jdbc.TestJdbcDriver.testResultSetMetaData
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hadoop.hive.ql.TestMTQueries.testMTQueries1
org.apache.hadoop.hive.ql.history.TestHiveHistory.testSimpleQuery
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input20
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input5
org.apache.hadoop.hive.ql.security.TestFolderPermissions.testAlterPartitionPerms
org.apache.hadoop.hive.ql.security.TestFolderPermissions.testDynamicPartitions
org.apache.hadoop.hive.ql.security.TestFolderPermissions.testExternalTable
org.apache.hadoop.hive.ql.security.TestFolderPermissions.testInsertOverwrite
org.apache.hadoop.hive.ql.security.TestFolderPermissions.testStaticPartitionPerms
org.apache.hadoop.hive.service.TestHiveServer.testDynamicSerde
org.apache.hive.beeline.TestBeeLineWithArgs.testNullDefault
org.apache.hive.beeline.TestBeeLineWithArgs.testNullEmpty
org.apache.hive.beeline.TestBeeLineWithArgs.testNullEmptyCmdArg
org.apache.hive.beeline.TestBeeLineWithArgs.testNullNonEmpty
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestHCatLoaderStorer.testReadWrite
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.jdbc.TestJdbcDriver2.testBuiltInUDFCol
org.apache.hive.jdbc.TestJdbcDriver2.testDataTypes
org.apache.hive.jdbc.TestJdbcDriver2.testDataTypes2
org.apache.hive.jdbc.TestJdbcDriver2.testDuplicateColumnNameOrder
org.apache.hive.jdbc.TestJdbcDriver2.testExprCol
org.apache.hive.jdbc.TestJdbcDriver2.testFetchFirstQuery
org.apache.hive.jdbc.TestJdbcDriver2.testNonAsciiReturnValues
org.apache.hive.jdbc.TestJdbcDriver2.testNullResultSet
org.apache.hive.jdbc.TestJdbcDriver2.testNullType
org.apache.hive.jdbc.TestJdbcDriver2.testPrepareStatement
org.apache.hive.jdbc.TestJdbcDriver2.testResultSetMetaData
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testConnectionSchemaAPIs
org.apache.hive.jdbc.TestJdbcWithMiniMr.testMrQuery
org.apache.hive.jdbc.TestJdbcWithMiniMr.testPermFunc
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.testExecuteStatement
org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.testExecuteStatementAsync
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testExecuteStatement
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testExecuteStatementAsync
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testExecuteStatement
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testExecuteStatementAsync
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/290/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/290/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-290/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1384 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646661;;;","25/May/14 17:26;ashutoshc;Failures need to be looked at;;;","26/May/14 01:42;navis;Removed transient marks for neededColumns and neededColumnIDs.;;;","26/May/14 22:43;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646726/HIVE-6910.7.patch.txt

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 5463 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input20
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input5
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/301/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/301/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-301/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646726;;;","27/May/14 01:33;navis;Cannot reproduce fails of testParse_input*. testNegativeCliDriver_authorization_ctas seemed need update.;;;","27/May/14 01:35;ashutoshc;Committed to trunk. Thanks, Navis!;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Release Note for Hive 0.13 RC1,HIVE-6909,12708425,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,Fixed,rhbutani,rhbutani,rhbutani,15/Apr/14 00:55,15/Apr/14 01:02,14/Jul/23 06:14,15/Apr/14 01:02,,,,,,,,,,,,,,,,0,,,,,hagleitn,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/14 00:56;rhbutani;HIVE-6909.1.patch;https://issues.apache.org/jira/secure/attachment/12640185/HIVE-6909.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386748,,,,Tue Apr 15 01:02:10 UTC 2014,,,,,,,,,,"0|i1umhr:",387012,,,,,,,,,,,,,,,,,,,,,"15/Apr/14 00:59;hagleitn;+1;;;","15/Apr/14 01:02;rhbutani;Committed to trunk and 0.13
thanks Gunther;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestThriftBinaryCLIService.testExecuteStatementAsync has intermittent failures,HIVE-6908,12708402,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,szehon,szehon,szehon,14/Apr/14 23:19,13/Nov/14 19:44,14/Jul/23 06:14,13/May/14 16:31,0.13.0,,,,,,,,,0.14.0,,Tests,,,,0,,,"This has failed sometimes in the pre-commit tests.

ThriftCLIServiceTest.testExecuteStatementAsync runs two statements.  They are given 100 second timeout total, not sure if its by intention.  As the first is a select query, it will take a majority of the time.  The second statement (create table) should be quicker, but it fails sometimes because timeout is already mostly used up.

The timeout should probably be reset after the first statement.  If the operation finishes before the timeout, it wont have any effect as it'll break out.",,szehon,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6747,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/14 23:20;szehon;HIVE-6908.patch;https://issues.apache.org/jira/secure/attachment/12640168/HIVE-6908.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386725,,,,Thu Nov 13 19:44:19 UTC 2014,,,,,,,,,,"0|i1umcn:",386989,,,,,,,,,,,,,,,,,,,,,"14/Apr/14 23:20;szehon;Attaching a fix.;;;","14/Apr/14 23:55;ashutoshc;Doesn't this mean test is poorly written if it is relying on timing of operations ? May be test can be rewritten in a different way so it doesnt rely on timeouts.;;;","15/Apr/14 00:18;szehon;We can eliminate the polling timeout, but I guess the author was thinking that timeout is better than hang for unit-test purpose, if the status is not updated successfully.  Or did you have some other thoughts?;;;","16/Apr/14 18:24;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12640168/HIVE-6908.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5401 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/precommit-hive/10/testReport
Console output: http://bigtop01.cloudera.org:8080/job/precommit-hive/10/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12640168;;;","17/Apr/14 16:22;ashutoshc;I am not sure what original author of test had in mind for this. Perhaps [~vgumashta] may know more.;;;","12/May/14 21:08;ashutoshc;+1;;;","13/May/14 16:31;ashutoshc;Committed to trunk. Thanks, Szehon!;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2 - wrong user gets used for metastore operation with embedded metastore,HIVE-6907,12708376,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,thejas,thejas,thejas,14/Apr/14 21:23,16/Apr/14 07:32,14/Jul/23 06:14,15/Apr/14 00:43,0.13.0,,,,,,,,,0.13.0,,HiveServer2,,,,0,,,"When queries are being run concurrently against HS2, sometimes the wrong user ends performing the metastore action and you get an error like - 
{code}
..INFO|java.sql.SQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.security.AccessControlException: action WRITE not permitted on path hdfs://example.net:8020/apps/hive/warehouse/tbl_4eeulg9zp4 for user hrt_qa)
{code}",,jnp,navis,rhbutani,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/14 21:30;thejas;HIVE-6907.1.patch;https://issues.apache.org/jira/secure/attachment/12640147/HIVE-6907.1.patch","14/Apr/14 21:40;thejas;HIVE-6907.2.patch;https://issues.apache.org/jira/secure/attachment/12640151/HIVE-6907.2.patch","14/Apr/14 22:37;thejas;HIVE-6907.3.patch;https://issues.apache.org/jira/secure/attachment/12640157/HIVE-6907.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386699,,,,Wed Apr 16 07:29:08 UTC 2014,,,,,,,,,,"0|i1um6v:",386963,,,,,,,,,,,,,,,,,,,,,"14/Apr/14 21:30;thejas;HIVE-6907.1.patch - Doing an explicit doAs() asynchronous thread with the user in the thrift thread solves the problem.
;;;","14/Apr/14 21:40;thejas;HIVE-6907.2.patch - minor error message improvement
;;;","14/Apr/14 21:43;ashutoshc;Dupe of HIVE-6245 ?;;;","14/Apr/14 21:58;thejas;bq. Dupe of HIVE-6245 ?
HIVE-6245 was talking about issue when remote metastore is used. There have been changes that should fix the issue for remote metastore as I commented there just now.
This fixes the case when embedded metastore is used in hive server2.
;;;","14/Apr/14 22:04;thejas;cc [~vaibhavgumashta] [~prasadm] [~jnp] [~ashutoshc] [~xuefuz] [~rhbutani]
Since we are going to need another 0.13 rc because pom.xml was not included, I would like to get this in into the new rc. This is a serious concurrency issue, I think we should include in 0.13 .
Also, I am hoping we can avoid the 24 hr wait before commit for this one, otherwise we will end up delaying the RC further.
Can one or more people review it ? (Or at least approve including it without 24 hr commit wait period).
;;;","14/Apr/14 22:24;vgumashta;+1 (non-binding). 

I've tested the fix with following 7 combinations:
*Binary mode*
1. Secure MR: 291 passed in 2061.69 seconds
2. Unsecure MR: 334 passed in 2011.29 seconds
3. Secure Tez:	 2 failed, 378 passed in 2016.36 seconds (All create index failures - known issue)
4. Unsecure Tez: 4 failed, 435 passed in 1997.65 seconds (All create index failures - known issue)

*Http mode*
5. Secure MR: 287 passed in 2161.14 seconds
6. Secure Tez:	 1 failed, 369 passed in 2142.57 seconds(All create index failures - known issue)
7. Unsecure Tez: 3 failed, 429 passed in 2137.11 seconds (All create index failures - known issue)

My setup: Ran tests for 30 mins, with 20 test threads (firing different queries in parallel; [~deepesh], [~taksaito] thanks for the test code) and HS2 running only 10 async threads.;;;","14/Apr/14 22:26;vgumashta;[~thejas] Thanks for fixing the issue.

Here is the root cause: ideally when a foreground thread creates a background thread, the thread's Access Control Context is inherited (http://docs.oracle.com/javase/7/docs/technotes/guides/security/spec/security-spec.doc4.html). However, in our case, we're using a thread pool; so threads are not necessarily created by the foreground thread. Therefore, an explicit doAs is necessary when switching to background thread.;;;","14/Apr/14 22:32;jnp;+1. looks good to me.

A minor comment.
bq. runInternal(getConfigForOperation());
  If opConfig is final, the call to get the config may not be needed.;;;","14/Apr/14 22:37;thejas;HIVE-6907.3.patch - addressing review comment
;;;","14/Apr/14 22:42;ashutoshc;+1 Root cause identified by [~vgumashta] seems like plausible explanation. ;;;","14/Apr/14 22:43;rhbutani;+1 for 0.13;;;","15/Apr/14 00:12;thejas;I have run the tests and results look good.
[~ashutoshc] Can you commit this patch ? I have trouble doing that after the password reset.
;;;","15/Apr/14 00:43;rhbutani;Committed to trunk and 0.13
Thanks Thejas, Vaibhav, Jitendra, Ashutosh.;;;","15/Apr/14 01:10;navis;[~thejas] Doesn't this break use case for doAs disabled?;;;","15/Apr/14 02:51;thejas;[~navis] If doAs is disabled, then the HS2 user would end up doing a doAs as itself.  I need to check if hadoop allows that without proxy user configuration. Let me check. Thats a good question. Do let me know if you know about that. 
cc [~jnp];;;","15/Apr/14 08:34;navis;I remember in that case (no proxy user), unix ugi who started CLI or hiveserver would be used. 

Below is some part of patch fixing similar issue in our product version (HiveMetaStoreClient).
{noformat}
+            SessionBase session = SessionBase.get();
             // Call set_ugi, only in unsecure mode.
             try {
-              UserGroupInformation ugi = shim.getUGIForConf(conf);
+              UserGroupInformation ugi;
+              if (session != null) {
+                ugi = shim.createRemoteUser(session.getUserName(), session.getGroupNames());
+              } else {
+                ugi = shim.getUGIForConf(conf);
+              }
{noformat};;;","15/Apr/14 10:26;thejas;I ran some tests to verify it works with doAs=false and no proxy user setting for the hive server2 process user. (Also added some logging to verify that this code path is used - which was useful as it reminded me that this code path doesn't get used with hive 0.12 jdbc driver!)

I have verified that this patch works with unsecure mode, but I haven't yet verified the behavior with kerberos secured cluster.

{code}
beeline> !connect jdbc:hive2://localhost:10000/ user fakepwd org.apache.hive.jdbc.HiveDriver
Connecting to jdbc:hive2://localhost:10000/
Connected to: Apache Hive (version 0.13.0)
Driver: Hive JDBC (version 0.13.0)
Transaction isolation: TRANSACTION_REPEATABLE_READ

0: jdbc:hive2://localhost:10000/> set hive.server2.enable.doAs;                                                      
+---------------------------------+
|               set               |
+---------------------------------+
| hive.server2.enable.doAs=false  |
+---------------------------------+
1 row selected (0.007 seconds)
0: jdbc:hive2://localhost:10000/> set hadoop.proxyuser.hive.groups;                                                  
+--------------------------------------------+
|                    set                     |
+--------------------------------------------+
| hadoop.proxyuser.hive.groups is undefined  |
+--------------------------------------------+
1 row selected (0.009 seconds)
0: jdbc:hive2://localhost:10000/> set hadoop.proxyuser.hive.hosts;    
+-------------------------------------------+
|                    set                    |
+-------------------------------------------+
| hadoop.proxyuser.hive.hosts is undefined  |
+-------------------------------------------+
1 row selected (0.007 seconds)
0: jdbc:hive2://localhost:10000/> set hadoop.proxyuser.oozie.hosts;
+---------------------------------+
|               set               |
+---------------------------------+
| hadoop.proxyuser.oozie.hosts=*  |
+---------------------------------+
1 row selected (0.007 seconds)
0: jdbc:hive2://localhost:10000/> select count(*) from sample_07 ;                                                   
+------+
| _c0  |
+------+
| 823  |
+------+

{code};;;","15/Apr/14 18:24;thejas;[~vaibhavgumashta] Also verified with a secure cluster that this patch works with doAs disabled and no proxy entries for hive server2 user in core-site.xml . Thanks Vaibhav!
Also talked to [~jnp] (who has done lot of security work in core hadoop), this doAs as same user does not require any proxy user entries.
So we have both experimental and theoretical validation! :)
;;;","16/Apr/14 01:30;navis;Sorry, I've been strayed a little (security always makes me). The code above I've shown in above comment is not for fixing this. The problem is that when we uses authorization with AuthorizationPreEventListener in metastore with doAs disabled. With this configuration, metastore would always try authorize privileges of HS2 user. ;;;","16/Apr/14 02:58;thejas;When doAs is disabled, aren't the metastore actions already taking place as HS2 user (without this change)? With doAs disabled, HiveSessionProxy is not used in SessionManager, so metastore actions would be taking place as HS2 user. Isn't that the behavior you would expect from doAs being disabled (ie actions take place as HS2 user)?
;;;","16/Apr/14 07:29;navis;Yes, it's related to HIVE-6478, not to this issue. Some authorizations is taken place in metastore and, in stand-alone mode, we cannot hand-over username in SessionState to metastore because SessionState is in hive-exec which is not accessible from hive-metastore. Also in remote metastore, current set_ugi hands over HS2 user instead of user in session state.

Should we fix this, too?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix assembly/src.xml so that sr tar ball contains top level pom.xml,HIVE-6906,12708359,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,rhbutani,rhbutani,rhbutani,14/Apr/14 20:04,15/Apr/14 00:38,14/Jul/23 06:14,15/Apr/14 00:38,,,,,,,,,,0.13.0,,,,,,0,,,"This affects the tarballs we package with Hive 0.13.
Current src tarballs being generated are not buildable.",,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/14 20:08;rhbutani;HIVE-6906.1.patch;https://issues.apache.org/jira/secure/attachment/12640131/HIVE-6906.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386682,,,,Tue Apr 15 00:38:06 UTC 2014,,,,,,,,,,"0|i1um33:",386946,,,,,,,,,,,,,,,,,,,,,"14/Apr/14 23:25;ashutoshc;+1;;;","15/Apr/14 00:38;rhbutani;Committed to trunk and 0.13
Thanks Gunther, Ashutosh.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Explain plan doesn't show operator tree for the fetch operator,HIVE-6901,12708071,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,xuefuz,xuefuz,xuefuz,12/Apr/14 02:30,13/Nov/14 19:43,14/Jul/23 06:14,13/May/14 16:36,0.12.0,,,,,,,,,0.14.0,,Query Processor,,,,0,,,"Explaining a simple select query that involves a MR phase doesn't show processor tree for the fetch operator.
{code}
hive> explain select d from test;
OK
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
...
  Stage: Stage-0
    Fetch Operator
      limit: -1
{code}

It would be nice if the operator tree is shown even if there is only one node.

Please note that in local execution, the operator tree is complete:
{code}
hive> explain select * from test;
OK
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: test
          Statistics: Num rows: 8 Data size: 34 Basic stats: COMPLETE Column stats: NONE
          Select Operator
            expressions: d (type: int)
            outputColumnNames: _col0
            Statistics: Num rows: 8 Data size: 34 Basic stats: COMPLETE Column stats: NONE
            ListSink
{code}",,szehon,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-3925,,,,,,,,,,,,,,,,,,,,,,,,"02/May/14 19:16;xuefuz;HIVE-6109.10.patch;https://issues.apache.org/jira/secure/attachment/12643097/HIVE-6109.10.patch","14/Apr/14 15:48;xuefuz;HIVE-6901.1.patch;https://issues.apache.org/jira/secure/attachment/12640079/HIVE-6901.1.patch","21/Apr/14 20:25;szehon;HIVE-6901.2.patch;https://issues.apache.org/jira/secure/attachment/12641113/HIVE-6901.2.patch","25/Apr/14 18:35;xuefuz;HIVE-6901.3.patch;https://issues.apache.org/jira/secure/attachment/12641980/HIVE-6901.3.patch","26/Apr/14 04:45;xuefuz;HIVE-6901.4.patch;https://issues.apache.org/jira/secure/attachment/12642064/HIVE-6901.4.patch","27/Apr/14 14:15;xuefuz;HIVE-6901.5.patch;https://issues.apache.org/jira/secure/attachment/12642131/HIVE-6901.5.patch","27/Apr/14 23:08;xuefuz;HIVE-6901.6.patch;https://issues.apache.org/jira/secure/attachment/12642168/HIVE-6901.6.patch","29/Apr/14 19:02;xuefuz;HIVE-6901.7.patch;https://issues.apache.org/jira/secure/attachment/12642510/HIVE-6901.7.patch","30/Apr/14 05:38;xuefuz;HIVE-6901.8.patch;https://issues.apache.org/jira/secure/attachment/12642600/HIVE-6901.8.patch","30/Apr/14 21:53;xuefuz;HIVE-6901.9.patch;https://issues.apache.org/jira/secure/attachment/12642733/HIVE-6901.9.patch","12/Apr/14 02:42;xuefuz;HIVE-6901.patch;https://issues.apache.org/jira/secure/attachment/12639911/HIVE-6901.patch",,,,,11.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386394,,,,Thu Nov 13 19:43:25 UTC 2014,,,,,,,,,,"0|i1ukbb:",386659,,,,,,,,,,,,,,,,,,,,,"12/Apr/14 02:42;xuefuz;With the patch attached, the following will be shown instead:
{code}
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink
{code}

Expecting test output diffs.;;;","12/Apr/14 12:50;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639911/HIVE-6901.patch

{color:red}ERROR:{color} -1 due to 476 failed/errored test(s), 5615 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alias_casted_column
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_allcolref_in_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ambiguous_col
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_groupby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_select
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ansi_sql_arithmetic
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join18_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join27
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join29
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_reordering_values
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_without_localtask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binarysortable_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_groupby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin_negative3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cluster
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_column_access_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_combine2_hadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_count
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_genericudaf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cross_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cross_product_check_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cross_product_check_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cte_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_precision
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_distinct_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynamic_partition_skip_default
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape_clusterby1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape_distributeby1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape_orderby1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape_sortby1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_explain_rearrange
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_cube1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_distinct_samekey
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_sets2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_sets3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_sets4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_sets5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_position
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_ppd
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_resolution
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_rollup1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_implicit_cast1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_empty
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_file_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables_compact
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_multiple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_self_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_unused
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_update
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_compression
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compression
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_stale
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_stale_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_const_type
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_innerjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input2_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input39_hadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input42
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input4_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_testxpath3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_testxpath4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_inputddl6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join18_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join38
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join40
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join41
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_alt_syntax
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_unqual1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_unqual2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_unqual3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_unqual4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_filters_overlap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_hive_626
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_merging
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_nullsafe
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_reorder
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_reorder2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_reorder3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_reorder4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_star
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_thrift
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_vc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_keyword_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view_cp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view_noalias
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view_ppd
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_limit_partition_metadataonly
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_limit_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_limit_pushdown_negative
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_louter_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_filter_on_outerjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_memcheck
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_subquery
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_subquery2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mergejoins
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mergejoins_mixed
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadata_only_queries
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadata_only_queries_with_filters
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadataonly1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_join_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_no_hooks
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_noalias_subq1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonblock_op_deduplicate
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonmr_fetch
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonmr_fetch_threshold
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_null_cast
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup4_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullscript
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_optional_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_predicate_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_order
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_order2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_outer_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pcr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_plan_json
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_clusterby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_constant_where
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_gby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_gby2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_gby_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_outer_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_outer_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_outer_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_outer_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_outer_join5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_random
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_repeated_alias
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_transform
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_udf_case
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_udf_col
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_udtf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_vc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_allchildsarenull
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_pushdown3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_push_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_query_result_fileformat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quote1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_basic
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_reduce_deduplicate_exclude_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_reduce_deduplicate_extended
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_regex_col
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_regexp_extract
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_router_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_script_pipe
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_as_omitted
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_transform_hint
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_unquote_and
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_unquote_not
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_unquote_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_semijoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_serde_user_properties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_set_processor_namespaces
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_set_variable_sub
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_columns
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_indexes_edge_cases
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_indexes_syntax
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_tables
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_tablestatus
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_showparts
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_only_null
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_str_to_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subq2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subq_where_serialization
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_alias
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_exists
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_exists_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_in
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_in_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_notexists
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_notexists_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_notin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_notin_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_unqualcolumnrefs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_type_widening
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_number_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_case_column_pruning
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_count
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_in_file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_inline
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_lower
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_minute
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_parse_url
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_reflect2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_to_unix_timestamp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_json_tuple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_parse_url_tuple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_stack
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_top_level
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_between_in
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_coalesce
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_aggregate
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_cast
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_expressions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_math_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_left_outer_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_non_string_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_decimal_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_div0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_part_project
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_short_regress
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_bucketmapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_case
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_casts
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_context
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_date_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_distinct_gby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_math_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_nested_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_parquet
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_shufflejoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_string_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_timestamp_funcs
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_dboutput
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udaf_example_avg
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udaf_example_group_concat
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udaf_example_max
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udaf_example_max_n
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udaf_example_min
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udaf_example_min_n
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udf_example_add
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udf_example_arraymapstruct
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udf_example_format
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_udf_row_sequence
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_external_table_ppd
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_ppd_key_range
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_pushdown
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_queries
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_ppd_key_ranges
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket4
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin7
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_disable_merge_for_bucketing
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_bucket_mapjoin_mismatch1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_limit_partition_stats
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_script_error
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_sortmerge_mapjoin_mismatch_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_assert_true
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_udf_assert_true2
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
org.apache.hadoop.hive.ql.TestMTQueries.testMTQueries1
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2233/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2233/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 476 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639911;;;","14/Apr/14 15:48;xuefuz;Patch #1 regenerated some test outputs.;;;","14/Apr/14 16:40;ashutoshc;Dupe of HIVE-3925 ?;;;","14/Apr/14 16:49;xuefuz;Actually HIVE-3925, while closely related to this, is concerned with stage dependency. This JIRA is concerned with the plan manifesting for the fetch stage. Thus, they seem orthogonal to each other.

It's nice to have both though.;;;","16/Apr/14 20:15;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12640079/HIVE-6901.1.patch

{color:red}ERROR:{color} -1 due to 65 failed/errored test(s), 5401 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binarysortable_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_like_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input42
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_filters_overlap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_louter_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadataonly1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatCTAS
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_outer_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pcr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_vc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_allchildsarenull
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_push_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_regexp_extract
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_router_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_serde_user_properties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_alter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unset_table_view_property
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket4
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin7
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_disable_merge_for_bucketing
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_part
org.apache.hive.jdbc.TestJdbcDriver2.testNewConnectionConfiguration
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/precommit-hive/12/testReport
Console output: http://bigtop01.cloudera.org:8080/job/precommit-hive/12/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 65 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12640079;;;","19/Apr/14 00:00;xuefuz;Patch#1 seems having some issue in the generated test output file. Attach patch#2 as another attempt.;;;","19/Apr/14 03:47;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12640920/HIVE-6901.2.patch

{color:red}ERROR:{color} -1 due to 64 failed/errored test(s), 5406 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binarysortable_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input42
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_filters_overlap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_louter_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadataonly1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatCTAS
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_outer_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pcr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_vc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_allchildsarenull
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_push_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_regexp_extract
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_router_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_serde_user_properties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_alter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unset_table_view_property
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket4
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin7
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_disable_merge_for_bucketing
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_part
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/precommit-hive/24/testReport
Console output: http://bigtop01.cloudera.org:8080/job/precommit-hive/24/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 64 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12640920;;;","21/Apr/14 19:47;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641106/HIVE-6901.2.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/precommit-hive/17/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/precommit-hive/17/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Tests exited with: NullPointerException: driver
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641106;;;","21/Apr/14 20:24;szehon;Sorry about that, there was a problem with the hadoop-2 test-property file on the build machine, I'll re-submit this.;;;","22/Apr/14 00:31;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641113/HIVE-6901.2.patch

{color:red}ERROR:{color} -1 due to 122 failed/errored test(s), 5416 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binarysortable_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_combine2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input39
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input42
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_filters_overlap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_louter_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadataonly1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatCTAS
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_outer_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pcr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_vc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_allchildsarenull
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_push_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_regexp_extract
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_router_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_serde_user_properties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_alter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unset_table_view_property
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket4
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin7
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_disable_merge_for_bucketing
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_part
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan3
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/precommit-hive/18/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/precommit-hive/18/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 122 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641113;;;","24/Apr/14 03:39;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641390/HIVE-6901.2.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/21/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/21/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-21/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'common/src/java/org/apache/hadoop/hive/conf/HiveConf.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java'
++ egrep -v '^X|^Performing status on external'
++ awk '{print $2}'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-minikdc/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/core/target hcatalog/streaming/target hcatalog/server-extensions/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target hwi/target common/target common/src/gen contrib/target service/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target
+ svn update
U    ql/src/test/results/clientpositive/drop_partitions_filter2.q.out
U    ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFUtils.java
U    ql/src/test/queries/clientpositive/drop_partitions_filter2.q
U    ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFMkCollectionEvaluator.java
U    ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java
U    ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java

Fetching external item into 'hcatalog/src/test/e2e/harness'
Updated external to revision 1589561.

Updated to revision 1589561.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641390;;;","25/Apr/14 18:22;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641974/HIVE-6901.2.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/42/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/42/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-42/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
++ egrep -v '^X|^Performing status on external'
++ awk '{print $2}'
++ svn status --no-ignore
+ rm -rf
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1590092.

At revision 1590092.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641974;;;","26/Apr/14 02:01;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641980/HIVE-6901.3.patch

{color:red}ERROR:{color} -1 due to 123 failed/errored test(s), 5418 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_coltype
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binarysortable_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_combine2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input39
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input42
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_filters_overlap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_louter_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadataonly1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatCTAS
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_outer_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pcr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_vc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_allchildsarenull
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_push_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_regexp_extract
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_router_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_serde_user_properties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_alter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_tblproperties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unset_table_view_property
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket4
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin7
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_disable_merge_for_bucketing
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_part
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unset_table_property
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/45/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/45/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 123 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641980;;;","27/Apr/14 01:47;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642064/HIVE-6901.4.patch

{color:red}ERROR:{color} -1 due to 103 failed/errored test(s), 5484 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_coltype
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_combine2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input39
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_oneskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatCTAS
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_alter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_tblproperties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unset_table_view_property
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_join0
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket4
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_count
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cross_join
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cross_product_check_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cross_product_check_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_disable_merge_for_bucketing
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_having
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_join0
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_limit_pushdown
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mapjoin_decimal
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mapjoin_mapjoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_metadata_only_queries
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mrr
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_subquery_exists
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_subquery_in
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_join_tests
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_joins_explain
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union8
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_stats_counter
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_part
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unset_table_property
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/57/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/57/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 103 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642064;;;","27/Apr/14 17:27;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642131/HIVE-6901.5.patch

{color:red}ERROR:{color} -1 due to 55 failed/errored test(s), 5484 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatCTAS
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_alter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_tblproperties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unset_table_view_property
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_count
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_limit_pushdown
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unset_table_property
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/60/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/60/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 55 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642131;;;","28/Apr/14 03:37;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642168/HIVE-6901.6.patch

{color:red}ERROR:{color} -1 due to 46 failed/errored test(s), 5419 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatCTAS
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_alter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_tblproperties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unset_table_view_property
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unset_table_property
org.apache.hadoop.hive.metastore.TestMetaStoreAuthorization.testMetaStoreAuthorization
org.apache.hadoop.hive.metastore.TestRetryingHMSHandler.testRetryingHMSHandler
org.apache.hadoop.hive.ql.security.TestStorageBasedClientSideAuthorizationProvider.testSimplePrivileges
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/61/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/61/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 46 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642168;;;","30/Apr/14 03:52;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642510/HIVE-6901.7.patch

{color:red}ERROR:{color} -1 due to 14 failed/errored test(s), 5424 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unset_table_view_property
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/80/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/80/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 14 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642510;;;","30/Apr/14 21:12;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642600/HIVE-6901.8.patch

{color:red}ERROR:{color} -1 due to 12 failed/errored test(s), 5426 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unset_table_view_property
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/87/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/87/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 12 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642600;;;","02/May/14 01:42;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642733/HIVE-6901.9.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/99/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/99/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-99/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'conf/hive-default.xml.template'
Reverted 'hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java'
Reverted 'hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseSerDe.java'
Reverted 'common/src/java/org/apache/hadoop/hive/conf/HiveConf.java'
Reverted 'serde/src/test/org/apache/hadoop/hive/serde2/lazy/TestLazyPrimitive.java'
Reverted 'serde/src/test/org/apache/hadoop/hive/serde2/thrift_test/CreateSequenceFile.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/BinarySortableSerDe.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/OutputByteBuffer.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/columnar/ColumnarSerDe.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/columnar/LazyBinaryColumnarSerDe.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryUtils.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/objectinspector/LazyBinaryStructObjectInspector.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryStruct.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/DynamicSerDe.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorConverter.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/ByteStream.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/io/DateWritable.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java'
Reverted 'ql/src/test/results/clientpositive/mapjoin_mapjoin.q.out'
Reverted 'ql/src/test/results/clientpositive/tez/mapjoin_mapjoin.q.out'
Reverted 'ql/src/test/results/clientpositive/tez/mapjoin_decimal.q.out'
Reverted 'ql/src/test/org/apache/hadoop/hive/ql/exec/persistence/TestMapJoinEqualityTableContainer.java'
Reverted 'ql/src/test/org/apache/hadoop/hive/ql/exec/persistence/TestMapJoinTableContainer.java'
Reverted 'ql/src/test/queries/clientpositive/mapjoin_decimal.q'
Reverted 'ql/src/test/queries/clientpositive/mapjoin_mapjoin.q'
Reverted 'ql/src/test/queries/clientpositive/tez_union.q'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceSinkDesc.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HashMapWrapper.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKey.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinRowContainer.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinTableContainer.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinTableContainerSerDe.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/AbstractMapJoinTableContainer.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedColumnarSerDe.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapJoinOperator.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HashTableLoader.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/Driver.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToString.java'
++ awk '{print $2}'
++ egrep -v '^X|^Performing status on external'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-minikdc/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/core/target hcatalog/streaming/target hcatalog/server-extensions/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target hwi/target common/target common/src/gen contrib/target service/target serde/target serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target ql/src/test/org/apache/hadoop/hive/ql/exec/persistence/TestBytesBytesMultiHashMap.java ql/src/java/org/apache/hadoop/hive/ql/debug ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinPersistableTableContainer.java ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinBytesTableContainer.java
+ svn update
A    data/files/test1.txt
U    README.txt
A    ql/src/test/results/clientpositive/analyze_table_null_partition.q.out
U    ql/src/test/results/clientpositive/tez/tez_union.q.out
U    ql/src/test/results/clientpositive/tez/bucket_map_join_tez1.q.out
U    ql/src/test/results/clientpositive/tez/limit_pushdown.q.out
U    ql/src/test/results/clientpositive/tez/union3.q.out
U    ql/src/test/results/clientpositive/tez/union5.q.out
U    ql/src/test/results/clientpositive/tez/union7.q.out
U    ql/src/test/results/clientpositive/tez/union9.q.out
U    ql/src/test/results/clientpositive/tez/count.q.out
U    ql/src/test/results/clientpositive/tez/bucket_map_join_tez2.q.out
U    ql/src/test/results/clientpositive/tez/tez_dml.q.out
U    ql/src/test/results/clientpositive/tez/mrr.q.out
U    ql/src/test/results/clientpositive/tez/union2.q.out
U    ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out
A    ql/src/test/queries/clientpositive/analyze_table_null_partition.q
U    ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java

Fetching external item into 'hcatalog/src/test/e2e/harness'
Updated external to revision 1591803.

Updated to revision 1591803.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642733;;;","03/May/14 06:08;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12643097/HIVE-6109.10.patch

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 5430 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/109/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/109/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12643097;;;","12/May/14 18:23;xuefuz;[~ashutoshc] Would you mind reviewing the one-line change? Thanks.;;;","12/May/14 21:13;ashutoshc;+1;;;","13/May/14 16:36;ashutoshc;Committed to trunk. Thanks, Xuefu!;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HostUtil.getTaskLogUrl signature change causes compilation to fail,HIVE-6900,12708061,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,cdrome,cdrome,12/Apr/14 00:17,08/Jun/17 18:42,14/Jul/23 06:14,26/Apr/14 16:26,0.13.0,0.14.0,,,,,,,,0.14.0,,Shims,,,,0,,,"The signature for HostUtil.getTaskLogUrl has changed between Hadoop-2.3 and Hadoop-2.4.

Code in shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java works with Hadoop-2.3 method and causes compilation failure with Hadoop-2.4.",,cdrome,jdere,mdominguez@cloudera.com,navis,sushanth,szehon,thejas,vinodkv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6960,,HIVE-16860,,,,,,,,,MAPREDUCE-5857,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/14 04:44;navis;HIVE-6900.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12640202/HIVE-6900.1.patch.txt","26/Apr/14 00:32;jdere;HIVE-6900.2.patch;https://issues.apache.org/jira/secure/attachment/12642048/HIVE-6900.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386384,,,,Thu Jun 08 18:42:25 UTC 2017,,,,,,,,,,"0|i1uk93:",386649,,,,,,,,,,,,,,,,,,,,,"15/Apr/14 06:34;cdrome;[~navis] I spoke with some Hadoop core engineers and they commented that Hive should not use HostUtil as it is marked as private (and unstable).

Also, TaskLogServlet is not supported in MR2, so we just need to handle the case where mapreduce.framework.name != yarn (and yarn-tez I assume). We might want to add yarn-tez to the condition.;;;","15/Apr/14 06:41;navis;[~cdrome] Yes. I've seen that (the annotation). So I didn't assigned to me, believing there would be someone to know better about this. Could you handle this? I've been stuck still in hive-0.11.0 and I didn't know even that TaskLogServlet is not supported in MR2.;;;","23/Apr/14 22:25;vinodkv;I looked at the issue together with [~jdere]. Haven't reviewed the patch but overall this can let the compilation pass. The eventual link is used elsewhere in Hive to pull the logs and do some processing. The link used in the patch will still not work as the URLs changed completely.

We can do this in two halves
 - Fix compilation for now
 - And then follow up in YARN with a right API that can expose logs to users and change Hive to use that.

For the compilation fix, we can put back the previous API in YARN via MAPREDUCE-5830 or we can do the fix as done here in Hive.

Thoughts?;;;","24/Apr/14 00:14;jdere;Hadoop 2.4 is already released, right? So even Hadoop-2.5 restored the original API Hive would be broken against 2.4.  So I'd say we should just fix the compilation issue in Hive.  [~vinodkv] you mentioned that even with the compilation fix, whatever we're doing with HostUtil in Hive isn't going to work properly?  If so then maybe we should just remove this code completely from Hadoop23Shims, until we get the proper API from YARN to do what we want.

On the subject of getting a proper YARN API for this, is there already an existing mapreduce Jira for this?;;;","24/Apr/14 19:10;vinodkv;We can fix it in 2.4.1 and Hive can depend on that release if that is the route.

I see you filed MAPREDUCE-5857. It is strictly a YARN issue, I'll move it to the right sub-project.;;;","26/Apr/14 00:21;jdere;Would like to get this fixed sooner than later, as I'd like for Hive to be able to use 2.4.0.  Using Hadoop 2.4.0 in the Hive build will fix a number of unit test failures that we've been seeing.  I'm thinking to remove use of the HostUtil call since based on Vinod's comment it sounds like the URL we're generating isn't supposed to work anymore.

When MAPREDUCE-5857 is fixed we can add this functionality back to Hadoop23Shims.;;;","26/Apr/14 00:32;jdere;Patch v2 simply removes use of the HostUtil API.  If folks would still prefer to call into that API, we can use Navis' patch.;;;","26/Apr/14 00:48;jdere;[~hagleitn],[~ashutoshc] can you take a look?;;;","26/Apr/14 01:34;ashutoshc;url on hadoop-2 is incorrect anyway today, so not printing it makes things less-confusing to users. When right api is available from hadoop, we should switch to that. +1;;;","26/Apr/14 15:46;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642048/HIVE-6900.2.patch

{color:red}ERROR:{color} -1 due to 46 failed/errored test(s), 5418 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatCTAS
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_alter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_tblproperties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unset_table_view_property
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unset_table_property
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/52/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/52/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 46 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642048;;;","26/Apr/14 16:26;ashutoshc;Committed to trunk. Thanks, Jason!;;;","08/May/14 23:30;szehon;[~jdere] We were looking at this change and were wondering, why is the log talking about MR1 when it is a Hadoop23Shims?  Isn't the condition of the else statement still MR2, but to handle local mode case?  Seems like TaskServlet is in MR1.;;;","17/May/14 00:58;jdere;Hi Szehon, i don't really know much about the particulars of this issue, the logging statement made reference to MR1 because the original code had a comment about being in MR1 mode if it was in that particular point of the logic.  Definitely feel free to correct it if it is incorrect.;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;","08/Jun/17 18:29;sushanth;Btw, I think this issue is still not completely fixed, in that compilation succeeds, but this fails now at runtime because https://issues.apache.org/jira/secure/attachment/12651497/MAPREDUCE-5830.patch was committed in hadoop - The result is that our reflection check to see if the 23_METHOD exists succeeds, which means 23_METHOD will not be null, and 24_METHOD will be null, and thus, at runtime, we will wind up calling 23_METHOD. If hadoop had not ""fixed"" this bug, with this fix, our shim works, but with their fix, it looks like we still have a runtime problem. :)

I'm going to open a new jira for this and link them, but wanted to mention the rationale here, so that watchers of this jira are notified.;;;","08/Jun/17 18:42;sushanth;Actually, nvm - ignore my comment - that was based on looking at the code for .1.patch. .2.patch makes it so this should never happen. However, I saw that happening in a user's log and assumed that was still the problem. Clearly, though, that user has a weird hive version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Functions in hive are failing with java.lang.ClassNotFoundException on Tez,HIVE-6898,12708031,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vikram.dixit,vikram.dixit,vikram.dixit,11/Apr/14 21:38,09/Jun/14 06:39,14/Jul/23 06:14,25/Apr/14 22:11,0.13.0,0.14.0,,,,,,,,0.13.1,0.14.0,Tez,,,,0,,,"{code}
CREATE TABLE T1(key int, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

add jar /tmp/testudf.jar;
create temporary function square as 'org.apache.hive.udf.UDFSquare';
select square(key) from T1 limit 3;
{code}

Fails with 
{code}
Vertex failed, vertexName=Map 1, vertexId=vertex_1397230190905_0590_1_00, diagnostics=[Task failed, taskId=task_1397230190905_0590_1_00_000000, diagnostics=[AttemptID:attempt_1397230190905_0590_1_00_000000_0 Info:Error: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:145)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:163)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)
	at org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:564)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)
	at org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:553)
Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.hive.udf.UDFSquare
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.getUdfClass(GenericUDFBridge.java:133)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.isStateful(FunctionRegistry.java:1636)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.isDeterministic(FunctionRegistry.java:1599)
	at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.isDeterministic(ExprNodeGenericFuncEvaluator.java:132)
	at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.iterate(ExprNodeEvaluatorFactory.java:83)
	at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.toCachedEval(ExprNodeEvaluatorFactory.java:73)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:59)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:189)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:425)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:121)
	... 7 more
{code}",,sershe,thejas,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/14 22:03;vikram.dixit;HIVE-6898.1.patch;https://issues.apache.org/jira/secure/attachment/12639877/HIVE-6898.1.patch","11/Apr/14 23:54;vikram.dixit;HIVE-6898.2.patch;https://issues.apache.org/jira/secure/attachment/12639890/HIVE-6898.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386354,,,,Mon Jun 09 06:39:35 UTC 2014,,,,,,,,,,"0|i1uk2f:",386619,,,,,,,,,,,,,,,,,,,,,"11/Apr/14 23:00;sershe;Couple of nits on rb, can be fixed on commit, +1;;;","11/Apr/14 23:54;vikram.dixit;Address Sergey's comments.;;;","12/Apr/14 00:36;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639877/HIVE-6898.1.patch

{color:green}SUCCESS:{color} +1 5614 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2226/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2226/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639877;;;","12/Apr/14 04:51;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639890/HIVE-6898.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5614 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2230/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2230/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639890;;;","25/Apr/14 22:11;sershe;committed to trunk;;;","09/Jun/14 06:39;thejas;This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"hcatalog streaming version in 0.13 branch should be ""0.13""",HIVE-6894,12707852,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,thejas,thejas,thejas,11/Apr/14 01:39,11/Apr/14 01:45,14/Jul/23 06:14,11/Apr/14 01:45,,,,,,,,,,0.13.0,,,,,,0,,,,,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/14 01:40;thejas;HIVE-6894.1.patch;https://issues.apache.org/jira/secure/attachment/12639716/HIVE-6894.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386175,,,,Fri Apr 11 01:45:31 UTC 2014,,,,,,,,,,"0|i1uiyn:",386440,,,,,,,,,,,,,,,,,,,,,"11/Apr/14 01:43;rhbutani;+1
+1 for 0.13;;;","11/Apr/14 01:45;thejas;Patch committed to 0.13 branch.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
out of sequence error in HiveMetastore server,HIVE-6893,12707850,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ngangam,romainr,romainr,11/Apr/14 01:25,12/Sep/19 06:07,14/Jul/23 06:14,31/May/16 16:49,0.12.0,,,,,,,,,1.3.0,,HiveServer2,,,,5,,,"Calls listing databases or tables fail. It seems to be a concurrency problem.

{code}
014-03-06 05:34:00,785 ERROR hive.log: org.apache.thrift.TApplicationException: get_databases failed: out of sequence response
        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:76)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_databases(ThriftHiveMetastore.java:472)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_databases(ThriftHiveMetastore.java:459)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabases(HiveMetaStoreClient.java:648)
        at org.apache.hive.service.cli.operation.GetSchemasOperation.run(GetSchemasOperation.java:66)
        at org.apache.hive.service.cli.session.HiveSessionImpl.getSchemas(HiveSessionImpl.java:278)
        at sun.reflect.GeneratedMethodAccessor323.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:62)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
        at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:582)
        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:57)
        at com.sun.proxy.$Proxy9.getSchemas(Unknown Source)
        at org.apache.hive.service.cli.CLIService.getSchemas(CLIService.java:192)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.GetSchemas(ThriftCLIService.java:263)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$GetSchemas.getResult(TCLIService.java:1433)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$GetSchemas.getResult(TCLIService.java:1418)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.hive.service.cli.thrift.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:38)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:244)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
{code}",,amareshwari,arpanr,brocknoland,daisuke.kobayashi,erwaman,forrest_lv,giladwolff,glenn.strycker@gmail.com,hariprasad kuppuswamy,honglun,jinzheng,jonpyle,jxiang,mdominguez@cloudera.com,mohitsabharwal,nemon,nezihyigitbasi,ngangam,qiuzhuang.lian,qwertymaniac,romainr,sershe,szehon,veve,vgumashta,viji_r,wzheng,,,,,,,,,,,,,,,,,HIVE-10956,,,,,,,,HIVE-10404,,,HIVE-7594,,,,,,,,,,,,,,,,,,,,,"22/Apr/14 18:59;ngangam;HIVE-6893.1.patch;https://issues.apache.org/jira/secure/attachment/12641303/HIVE-6893.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386173,,,,Thu Sep 12 06:07:10 UTC 2019,,,,,,,,,,"0|i1uiy7:",386438,,,,,,,,,,,,,,,,,,,,,"11/Apr/14 20:47;ashutoshc;[~romainr] Can you provide more details about your setup? 
Was metastore running as separate process or embedded within HS2 ?
Were HS2 clients odbc or jdbc ? How many concurrent clients were there and how many queries they were firing?;;;","11/Apr/14 20:49;ashutoshc;Also, did you see any other error messages in your log like TException, TableAlreadyExistsException etc. or only the one you posted ?;;;","11/Apr/14 20:56;vgumashta;[~romainr] This seems like a metastore concurrency issue. It seems that the metastore client socket is reading the RPC response from a different call, hence the out of sequence exception. I think we'll have a better idea once we learn more about [~ashutoshc]'s queries.;;;","11/Apr/14 20:57;vgumashta;[~romainr] As a followup, can you also try starting HiveServer2 with -hiveconf hive.metastore.uris="" "" and see if you get the same error?;;;","15/Apr/14 14:16;ngangam;Mestastore is running as a standalone process/service outside the HS2 JVM. This issue was seen after the following occured (there is no indication of a problem prior)
1) Upgrade the HMS DB (mysql)
2) Running with slightly newer version of HUE.
3) About 15 parallel beeswax sessions to HS2 each running a query.

We haven't been able to reproduce this in-house using a JDBC Client that spins off threads that connect to HS2, run a query & exit. 

We suspected that it was one of the two causes
1) Socket timeouts during read by parallel invocations between HS2 (HMS Client) & HMS. 
2) Concurrency issues in HMS on invocations to HMS.

After noticing Socket timeout exceptions in the HS2 logs, we had the customer increase the socket read timeouts from 90s to 120s. Did not help, but also the hive logs indicate that the ""out of sequence"" errors occurred first then the socket  timeouts, pretty far apart in time. So the socket read timeout is not likely the root cause.

It must be concurrent access to the HMS by multiple beeswax sessions thats causing this issue. 

Hope this helps.;;;","22/Apr/14 18:59;ngangam;This proposed fix makes HS2 use a synchronized wrapper for the metastore client when the HMS is running as a standalone process. In embedded mode, we would not be reading sequenced responses over a socket.

This fix does NOT synchronize all the calls to the HMS. For example, Hive.getMSC() returns an metastore client instance that is not thread-safe. There is a bit of risk synchronizing these calls, as it might have a regressive performance impact. 

I will be measure performance impact of using a synchronized wrapper all over and see if the reward outweighs the risk. ;;;","22/Apr/14 19:02;vgumashta;[~ngangam] Can you also upload the patch to review board?;;;","22/Apr/14 19:27;ngangam;Review posted to the review board.
https://reviews.apache.org/r/20569/

;;;","23/Apr/14 04:55;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641303/HIVE-6893.1.patch

{color:red}ERROR:{color} -1 due to 41 failed/errored test(s), 5417 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/9/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/9/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 41 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641303;;;","23/Apr/14 05:51;szehon;As FYI, these are known test errors due to recent switch to hadoop-2.  Ref for a clean hadoop-2 build run two days back, see bottom of: [http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/precommit-hive-15/execution.txt|http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/precommit-hive-15/execution.txt];;;","24/May/14 23:42;ashutoshc;Patch in current form eschews underlying problem. I think its worth spending time to figure out whats going on. FWIW, I have seen {{TestRetryingHMSHandler.testRetryingHMSHandler}} test fail on trunk with HiveQA runs with following trace:
{code}
org.apache.thrift.TApplicationException: create_database failed: out of sequence response
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:76)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_create_database(ThriftHiveMetastore.java:511)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.create_database(ThriftHiveMetastore.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:534)
	at org.apache.hadoop.hive.metastore.TestRetryingHMSHandler.testRetryingHMSHandler(TestRetryingHMSHandler.java:76)
{code}

Above test failure which is independent of HS2 provides an independent repro test case. Though, in Hive QA runs it only fails rarely, hinting at some race condition. ;;;","24/Jun/14 00:13;giladwolff;I encountered the same issue, we get a socket read timeout and then out-of-sequence error. In one case we got an OOM in our client and I suspect it's the same underlying issue. Here is the metastore sequence of events. Our client tried to drop a table starting at 14:02:25. Note that we use a 20 seconds timeout for our client:
{code}
2014-06-23 14:02:25,181 INFO org.apache.hadoop.hive.metastore.HiveMetaStore: 11: source:/10.20.93.47 drop_table : db=cloudera_manager_metastore_canary_test_db tbl=CM_TEST_TABLE
2014-06-23 14:02:25,181 INFO org.apache.hadoop.hive.metastore.HiveMetaStore.audit: ugi=hue	ip=/10.20.93.47	cmd=source:/10.20.93.47 drop_table : db=cloudera_manager_metastore_canary_test_db tbl=CM_TEST_TABLE	
2014-06-23 14:02:25,182 INFO org.apache.hadoop.hive.metastore.HiveMetaStore: 11: source:/10.20.93.47 get_table : db=cloudera_manager_metastore_canary_test_db tbl=CM_TEST_TABLE
2014-06-23 14:02:25,182 INFO org.apache.hadoop.hive.metastore.HiveMetaStore.audit: ugi=hue	ip=/10.20.93.47	cmd=source:/10.20.93.47 get_table : db=cloudera_manager_metastore_canary_test_db tbl=CM_TEST_TABLE	
2014-06-23 14:02:46,596 INFO hive.metastore.hivemetastoressimpl: deleting  hdfs://jenkins-debian60-17.ent.cloudera.com:8020/user/hue/.cloudera_manager_hive_metastore_canary/HIVE_1_HIVEMETASTORE_627a77825bb851bf2db30317a698dded/2014_06_23_14_02_11/cm_test_table
2014-06-23 14:02:46,694 INFO hive.metastore.hivemetastoressimpl: Moved to trash: hdfs://jenkins-debian60-17.ent.cloudera.com:8020/user/hue/.cloudera_manager_hive_metastore_canary/HIVE_1_HIVEMETASTORE_627a77825bb851bf2db30317a698dded/2014_06_23_14_02_11/cm_test_table
{code}

On our client we get a socket timeout for the drop table call at 14:02:45:
{code}
2:02:45.209 PM 	WARN 	com.cloudera.cmon.firehose.polling.hive.HiveMetastoreCanary Metastore HIVE-1-HIVEMETASTORE-627a77825bb851bf2db30317a698dded: Failed to drop table 
com.cloudera.cmf.cdhclient.common.hive.MetaException: java.net.SocketTimeoutException: Read timed out
{code}
we then try to drop the database immediately afterwards and the next message in our logs is:
{code}
2:02:46.697 PM 	WARN 	com.cloudera.cmf.cdh4client.hive.MetastoreClientImpl 	Could not drop hive database: cloudera_manager_metastore_canary_test_db
com.cloudera.cdh4client.hive.shaded.org.apache.thrift.TApplicationException: get_database failed: out of sequence response
	at com.cloudera.cdh4client.hive.shaded.org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:76)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_database(ThriftHiveMetastore.java:412)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_database(ThriftHiveMetastore.java:399)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:736)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:479)
	at com.cloudera.cmf.cdh4client.hive.MetastoreClientImpl.dropDatabase(MetastoreClientImpl.java:160)
{code}

Note that the moved-to-trash message in the hive metastore is from 14:02:46,694 and the out-of-order exception is from 2:02:46.697. I know that order-in-time does not imply causation but is it possible that we are getting the drop-table acknowledgment message instead of the get_database message?;;;","31/May/16 16:49;ngangam;The fix in the proposed patch, is included in HIVE-10956. So this issue should be resolved by that fix. I am closing the jira.;;;","19/Feb/19 04:11;jinzheng;[~ngangam]

I encountered the same issue with [~giladwolff]

My beeline client always get a ""out of sequence response"" error after a metastore client timeout.

However, the current beeline client always works well after a ""reconnect()"" of metastore client.

And the patch in HIVE-10956 can not resolve this problem.

I found a jira HIVE-6390 with the same description, which can only be worked around by setting ""hive.metastore.failure.retries"" bigger than 0.;;;","19/Feb/19 15:32;ngangam;[~jinzheng] This jira is nearly 5 years old. Back then, the fix seems to have addressed the issue that was observed. The issue you are seeing is probably caused by a different scenario even though the exception might be the same. If this continuous to be an issue, I think we should file a new jira and investigate it with the latest codebase.;;;","20/Feb/19 02:06;jinzheng;[~ngangam].

I'm pretty sure my issue is the same as [~giladwolff], since I've seen the scenario as well as the code.

And our issue is not the same as this one indeed. I commented here only because many experts have discussed the related issue here.

I've worked around by adding ""reconnect()"" of IMetaStoreClient, in RetryingMetaStoreClient, after catching a SocketTimeoutException.

Thanks for you suggestions.;;;","20/Feb/19 02:30;ngangam;[~jinzheng] If you could post your fix to the jira as a patch, perhaps we could have it reviewed and committed to the source as a solution. Let me know your thoughts.;;;","21/Feb/19 09:15;jinzheng;[~ngangam] OK, shall I create a new jira? How about reopening HIVE-6390?;;;","12/Sep/19 06:07;honglun;I use hive-2.3.3，and the ""out of sequence response"" error occurs when two threads use the same HiveMetaStoreClient instance. 

In addition, I did not see this patch in 2.3.3, did I do it the wrong way?

 

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Alter rename partition Perm inheritance and general partition/table group inheritance,HIVE-6891,12707838,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,szehon,szehon,szehon,11/Apr/14 00:09,13/Nov/14 19:42,14/Jul/23 06:14,16/Apr/14 23:39,,,,,,,,,,0.14.0,,,,,,0,,,"Found this issue while looking at the method mentioned by HIVE-6648.

'alter table .. partition .. rename to ..' and other commands calling Warehouse.mkdirs() doesn't inherit permission on the partition directories and consequently the data, when ""hive.warehouse.subdir.inherit.perms"" is set.

Also, in these scenarios of directory creation, group is not being inherited.  Data files are already inheriting group by HIVE-3756.",,brocknoland,prasadm,szehon,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6892,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/14 23:43;szehon;HIVE-6891.2.patch;https://issues.apache.org/jira/secure/attachment/12639888/HIVE-6891.2.patch","12/Apr/14 08:00;szehon;HIVE-6891.3.patch;https://issues.apache.org/jira/secure/attachment/12639923/HIVE-6891.3.patch","12/Apr/14 23:51;szehon;HIVE-6891.4.patch;https://issues.apache.org/jira/secure/attachment/12639963/HIVE-6891.4.patch","11/Apr/14 00:50;szehon;HIVE-6891.patch;https://issues.apache.org/jira/secure/attachment/12639703/HIVE-6891.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386161,,,,Thu Nov 13 19:42:33 UTC 2014,,,,,,,,,,"0|i1uivj:",386426,,,,,,,,,,,,,,,,,,,,,"11/Apr/14 23:43;szehon;Addressing review comments.;;;","12/Apr/14 02:49;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639888/HIVE-6891.2.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2229/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2229/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hcatalog-it-unit ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hcatalog-it-unit ---
[INFO] 
[INFO] --- maven-jar-plugin:2.2:test-jar (default) @ hive-hcatalog-it-unit ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-0.14.0-SNAPSHOT-tests.jar
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hcatalog-it-unit ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/0.14.0-SNAPSHOT/hive-hcatalog-it-unit-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/0.14.0-SNAPSHOT/hive-hcatalog-it-unit-0.14.0-SNAPSHOT.pom
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-0.14.0-SNAPSHOT-tests.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/0.14.0-SNAPSHOT/hive-hcatalog-it-unit-0.14.0-SNAPSHOT-tests.jar
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - Testing Utilities 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-util ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/itests/util (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-it-util ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-it-util ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/util/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-it-util ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-it-util ---
[INFO] Compiling 45 source files to /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/classes
[WARNING] Note: Some input files use or override a deprecated API.
[WARNING] Note: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-it-util ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/util/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-it-util ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-it-util ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-it-util ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-it-util ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/hive-it-util-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-it-util ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-it-util ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/hive-it-util-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-it-util/0.14.0-SNAPSHOT/hive-it-util-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/util/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-it-util/0.14.0-SNAPSHOT/hive-it-util-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - Unit Tests 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-unit ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-it-unit ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-it-unit ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-it-unit ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-it-unit ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-it-unit ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-it-unit ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-metastore-scripts) @ hive-it-unit ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/tmp/scripts/metastore
     [copy] Copying 156 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/tmp/scripts/metastore
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-it-unit ---
[INFO] Compiling 56 source files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/test-classes
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] Note: Some input files use or override a deprecated API.
[WARNING] Note: Recompile with -Xlint:deprecation for details.
[WARNING] Note: Some input files use unchecked or unsafe operations.
[WARNING] Note: Recompile with -Xlint:unchecked for details.
[INFO] 4 warnings 
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[28,28] cannot find symbol
symbol  : class LocatedFileStatus
location: package org.apache.hadoop.fs
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[30,28] cannot find symbol
symbol  : class RemoteIterator
location: package org.apache.hadoop.fs
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[231,5] cannot find symbol
symbol  : class RemoteIterator
location: class org.apache.hadoop.hive.ql.security.TestFolderPermissions
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[231,20] cannot find symbol
symbol  : class LocatedFileStatus
location: class org.apache.hadoop.hive.ql.security.TestFolderPermissions
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[231,56] cannot find symbol
symbol  : method listFiles(org.apache.hadoop.fs.Path,boolean)
location: class org.apache.hadoop.fs.FileSystem
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[233,7] cannot find symbol
symbol  : class LocatedFileStatus
location: class org.apache.hadoop.hive.ql.security.TestFolderPermissions
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java:[396,16] mkdirs(org.apache.hadoop.fs.Path,boolean) in org.apache.hadoop.hive.metastore.Warehouse cannot be applied to (org.apache.hadoop.fs.Path)
[INFO] 7 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive Integration - Parent ......................... SUCCESS [9.419s]
[INFO] Hive Integration - Custom Serde ................... SUCCESS [11.176s]
[INFO] Hive Integration - HCatalog Unit Tests ............ SUCCESS [7.733s]
[INFO] Hive Integration - Testing Utilities .............. SUCCESS [9.561s]
[INFO] Hive Integration - Unit Tests ..................... FAILURE [10.414s]
[INFO] Hive Integration - Test Serde ..................... SKIPPED
[INFO] Hive Integration - QFile Tests .................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 50.065s
[INFO] Finished at: Fri Apr 11 22:49:18 EDT 2014
[INFO] Final Memory: 38M/108M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hive-it-unit: Compilation failure: Compilation failure:
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[28,28] cannot find symbol
[ERROR] symbol  : class LocatedFileStatus
[ERROR] location: package org.apache.hadoop.fs
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[30,28] cannot find symbol
[ERROR] symbol  : class RemoteIterator
[ERROR] location: package org.apache.hadoop.fs
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[231,5] cannot find symbol
[ERROR] symbol  : class RemoteIterator
[ERROR] location: class org.apache.hadoop.hive.ql.security.TestFolderPermissions
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[231,20] cannot find symbol
[ERROR] symbol  : class LocatedFileStatus
[ERROR] location: class org.apache.hadoop.hive.ql.security.TestFolderPermissions
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[231,56] cannot find symbol
[ERROR] symbol  : method listFiles(org.apache.hadoop.fs.Path,boolean)
[ERROR] location: class org.apache.hadoop.fs.FileSystem
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[233,7] cannot find symbol
[ERROR] symbol  : class LocatedFileStatus
[ERROR] location: class org.apache.hadoop.hive.ql.security.TestFolderPermissions
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java:[396,16] mkdirs(org.apache.hadoop.fs.Path,boolean) in org.apache.hadoop.hive.metastore.Warehouse cannot be applied to (org.apache.hadoop.fs.Path)
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-it-unit
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639888;;;","12/Apr/14 08:00;szehon;Fixing TestHiveMetaStore;;;","12/Apr/14 12:57;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639923/HIVE-6891.3.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2235/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2235/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hcatalog-it-unit ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hcatalog-it-unit ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hcatalog-it-unit ---
[INFO] 
[INFO] --- maven-jar-plugin:2.2:test-jar (default) @ hive-hcatalog-it-unit ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-0.14.0-SNAPSHOT-tests.jar
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hcatalog-it-unit ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/0.14.0-SNAPSHOT/hive-hcatalog-it-unit-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/0.14.0-SNAPSHOT/hive-hcatalog-it-unit-0.14.0-SNAPSHOT.pom
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-0.14.0-SNAPSHOT-tests.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/0.14.0-SNAPSHOT/hive-hcatalog-it-unit-0.14.0-SNAPSHOT-tests.jar
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - Testing Utilities 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-util ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/itests/util (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-it-util ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-it-util ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/util/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-it-util ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-it-util ---
[INFO] Compiling 45 source files to /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/classes
[WARNING] Note: Some input files use or override a deprecated API.
[WARNING] Note: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-it-util ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/util/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-it-util ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-it-util ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-it-util ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-it-util ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/hive-it-util-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-it-util ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-it-util ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/hive-it-util-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-it-util/0.14.0-SNAPSHOT/hive-it-util-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/util/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-it-util/0.14.0-SNAPSHOT/hive-it-util-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - Unit Tests 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-unit ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-it-unit ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-it-unit ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-it-unit ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-it-unit ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-it-unit ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-it-unit ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-metastore-scripts) @ hive-it-unit ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/tmp/scripts/metastore
     [copy] Copying 156 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/tmp/scripts/metastore
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-it-unit ---
[INFO] Compiling 56 source files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/test-classes
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] Note: Some input files use or override a deprecated API.
[WARNING] Note: Recompile with -Xlint:deprecation for details.
[WARNING] Note: Some input files use unchecked or unsafe operations.
[WARNING] Note: Recompile with -Xlint:unchecked for details.
[INFO] 4 warnings 
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[28,28] cannot find symbol
symbol  : class LocatedFileStatus
location: package org.apache.hadoop.fs
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[30,28] cannot find symbol
symbol  : class RemoteIterator
location: package org.apache.hadoop.fs
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[231,5] cannot find symbol
symbol  : class RemoteIterator
location: class org.apache.hadoop.hive.ql.security.TestFolderPermissions
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[231,20] cannot find symbol
symbol  : class LocatedFileStatus
location: class org.apache.hadoop.hive.ql.security.TestFolderPermissions
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[231,56] cannot find symbol
symbol  : method listFiles(org.apache.hadoop.fs.Path,boolean)
location: class org.apache.hadoop.fs.FileSystem
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[233,7] cannot find symbol
symbol  : class LocatedFileStatus
location: class org.apache.hadoop.hive.ql.security.TestFolderPermissions
[INFO] 6 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive Integration - Parent ......................... SUCCESS [9.652s]
[INFO] Hive Integration - Custom Serde ................... SUCCESS [12.170s]
[INFO] Hive Integration - HCatalog Unit Tests ............ SUCCESS [8.083s]
[INFO] Hive Integration - Testing Utilities .............. SUCCESS [9.127s]
[INFO] Hive Integration - Unit Tests ..................... FAILURE [10.498s]
[INFO] Hive Integration - Test Serde ..................... SKIPPED
[INFO] Hive Integration - QFile Tests .................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 51.341s
[INFO] Finished at: Sat Apr 12 08:57:22 EDT 2014
[INFO] Final Memory: 38M/96M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hive-it-unit: Compilation failure: Compilation failure:
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[28,28] cannot find symbol
[ERROR] symbol  : class LocatedFileStatus
[ERROR] location: package org.apache.hadoop.fs
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[30,28] cannot find symbol
[ERROR] symbol  : class RemoteIterator
[ERROR] location: package org.apache.hadoop.fs
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[231,5] cannot find symbol
[ERROR] symbol  : class RemoteIterator
[ERROR] location: class org.apache.hadoop.hive.ql.security.TestFolderPermissions
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[231,20] cannot find symbol
[ERROR] symbol  : class LocatedFileStatus
[ERROR] location: class org.apache.hadoop.hive.ql.security.TestFolderPermissions
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[231,56] cannot find symbol
[ERROR] symbol  : method listFiles(org.apache.hadoop.fs.Path,boolean)
[ERROR] location: class org.apache.hadoop.fs.FileSystem
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java:[233,7] cannot find symbol
[ERROR] symbol  : class LocatedFileStatus
[ERROR] location: class org.apache.hadoop.hive.ql.security.TestFolderPermissions
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-it-unit
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639923;;;","12/Apr/14 17:58;szehon;I see, was using hadoop-2 profile to build the test, and pre commit test is using hadoop-1..;;;","12/Apr/14 18:37;szehon;Using HadoopShims for listing the files works in both profiles works for me, lets see how far it goes.;;;","12/Apr/14 21:29;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639948/HIVE-6891.4.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5619 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2237/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2237/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639948;;;","12/Apr/14 23:51;szehon;That seems to work, fixing a minor whitespace missed in previous patch and uploading to rb.;;;","13/Apr/14 02:22;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639963/HIVE-6891.4.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5619 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_disable_merge_for_bucketing
org.apache.hadoop.hive.thrift.TestDBTokenStore.testDBTokenStore
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2238/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2238/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639963;;;","14/Apr/14 18:44;szehon;[~brocknoland] Thanks for the review, these test failures dont look like related to this patch.  

I ran and they passed locally.  TestDBTokenStore looks like a transient derby error, and the other looks like a transient error with the test itself. ;;;","14/Apr/14 22:32;brocknoland;+1;;;","16/Apr/14 04:47;brocknoland;I'd like to commit this, but I am locked out of my account. I am working on this but if someone else has the ability to commit this, please go ahead.;;;","16/Apr/14 23:39;prasadm;Patch committed to trunk.
Thanks [~szehon] for the contribution, thanks [~brocknoland] for the review.;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in HiveStreaming API causes problems if hive-site.xml is missing on streaming client side,HIVE-6890,12707831,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,roshan_naik,roshan_naik,roshan_naik,10/Apr/14 23:37,13/Nov/14 19:41,14/Jul/23 06:14,14/Apr/14 17:56,,,,,,,,,,0.14.0,,,,,,0,,,Incorrect conf object being passed to MetaStore client in AbstractRecordWriter  is causing the issue.,,gates,omalley,roshan_naik,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 23:38;roshan_naik;HIVE-6890.patch;https://issues.apache.org/jira/secure/attachment/12639683/HIVE-6890.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386154,,,,Thu Nov 13 19:41:52 UTC 2014,,,,,,,,,,"0|i1uitz:",386419,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 23:38;roshan_naik;uploading patch;;;","10/Apr/14 23:45;omalley;+1;;;","11/Apr/14 05:37;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639683/HIVE-6890.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5613 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_map_operators
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2217/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2217/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639683;;;","11/Apr/14 18:50;roshan_naik;Test failure is unrelated to patch.;;;","14/Apr/14 17:56;gates;Patch committed.  Thanks Roshan.;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive leaks MapWork objects via Utilities::gWorkMap,HIVE-6888,12707790,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,sershe,sershe,10/Apr/14 20:43,13/Nov/14 19:41,14/Jul/23 06:14,11/Apr/14 23:35,,,,,,,,,,0.14.0,,,,,,0,,,"When running multiple queries with hive on a single Application Master, we found that hive leaks a large number of MapWork objects which accumulate in the AM",,sershe,thejas,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 20:45;sershe;HIVE-6888.patch;https://issues.apache.org/jira/secure/attachment/12639650/HIVE-6888.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386113,,,,Thu Nov 13 19:41:05 UTC 2014,,,,,,,,,,"0|i1uikv:",386378,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 20:44;sershe;Found by [~t3rmin4t0r], and patch initially provided by [~hagleitn]. Both are on vacation so I will attach patch here so that this issue doesn't get lost;;;","10/Apr/14 23:20;vikram.dixit;+1;;;","11/Apr/14 01:36;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639650/HIVE-6888.patch

{color:green}SUCCESS:{color} +1 5613 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2214/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2214/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639650;;;","11/Apr/14 23:35;sershe;committed to trunk. Would be nice to get into 13 if there's another RC;;;","14/Apr/14 23:51;sershe;[~rhbutani] can you let me know if there's another RC? I'd like to get this patch in if possible;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add missing params to hive-default.xml.template ,HIVE-6887,12707767,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,,rhbutani,rhbutani,10/Apr/14 18:38,11/Apr/14 17:19,14/Jul/23 06:14,11/Apr/14 14:59,,,,,,,,,,0.13.0,,,,,,0,,,"Add in the ones that were added to HiveConf, but not the template.xml file; For 0.13 we will not be moving to HIVE-6037 style of genning the template file. ",,leftyl,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6503,,,,,,,,,,HIVE-6037,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 18:40;rhbutani;HIVE-6887.1.patch;https://issues.apache.org/jira/secure/attachment/12639623/HIVE-6887.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386090,,,,Fri Apr 11 14:59:24 UTC 2014,,,,,,,,,,"0|i1uifr:",386355,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 23:03;thejas;+1 LGTM

;;;","10/Apr/14 23:57;leftyl;[~ashutoshc] thinks *hive.serdes.using.metastore.for.schema* shouldn't be documented (see HIVE-6681 comment), but perhaps he'll agree with what the patch says:

<description>This an internal parameter. Check with the hive dev. team</description>

We have at least one other parameter that's deliberately undocumented.;;;","11/Apr/14 00:00;ashutoshc;yup.. description looks good to me. Additionally, this code may get redone on trunk, so this may no longer be there in future versions.;;;","11/Apr/14 00:08;thejas;We should probably adopt a naming convention to be used for such params in future.
Maybe ""hive.internal.*"" ?
;;;","11/Apr/14 00:40;rhbutani;[~leftylev] thanks for looking at this.

bq. We have at least one other parameter that's deliberately undocumented.

Does this need to be added? I am happy to add it with a similar comment.

Are you good with this patch, besides the 1 parameter issue?;;;","11/Apr/14 00:40;leftyl;Trivial edits, _all of which can be fixed post-0.13_:

* All-caps PAM in ""List of the underlying pam services ....""
* Init-cap Hive in ""Check with the hive dev. team""
* Init-cap Hive & Tez in ""... joins in hive when tez is used ...""
* *hive.txn.manager* could have a description such as ""Hive transaction manager"" (or something more useful) but it's pretty obvious from the parameter name
* Init-cap Time in ""time after which transactions are declared aborted ...""
* Add ""to be"" in ""Time in seconds between checks to see if any partitions need {to be} compacted.""
* Some lines are too long:
** value for *hive.serdes.using.metastore.for.schema* (unavoidable?)
** description for *hive.limit.query.max.table.partition*
** description for *hive.compactor.initiator.on*
** description for *hive.compactor.worker.timeout*
** description for *hive.compactor.check.interval* (2nd line)
** description for *hive.compactor.delta.num.threshold*
** description for *hive.compactor.delta.pct.threshold*
** description for *hive.compactor.abortedtxn.threshold*;;;","11/Apr/14 00:49;leftyl;+1, without any edits.  I can take care of the edit list later in trunk.

I'll look up the deliberately undocumented parameter, but it's nothing to hold up the show.  Most likely we have many accidentally undocumented parameters from previous releases -- that's a project on my to-do list.;;;","11/Apr/14 00:55;leftyl;[~rhbutani], what's the story with adding the value ""fs"" and changing the default value, which I forgot to mention, for hive.stats.dbclass (HIVE-6500)?  In the email thread you said it's ""already attached"" but I didn't understand.;;;","11/Apr/14 02:34;leftyl;bq. We have at least one other parameter that's deliberately undocumented.

*hive.stats.key.prefix* is what I had in mind (see comments by me & Navis on HIVE-6229, use link below).

I've been going through the patch for HIVE-6037 and found several parameters marked ""internal"" -- they all have descriptions except *hive.stats.key.prefix*.

I also found 46 parameters without descriptions in the first 2/3 of the parameter list.  Still working on that.

* link to comments on HIVE-6229:  [https://issues.apache.org/jira/browse/HIVE-6229?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13878510#comment-13878510];;;","11/Apr/14 14:59;rhbutani;Committed to trunk and 0.13
Lefty thanks for your help with this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveLockObject and enclosed HiveLockObjectData override equal() method but didn't do so for hashcode(),HIVE-6884,12707747,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,xuefuz,xuefuz,xuefuz,10/Apr/14 17:16,13/Nov/14 19:40,14/Jul/23 06:14,29/Apr/14 14:52,0.12.0,,,,,,,,,0.14.0,,HiveServer2,,,,0,,,"This breaches the JAVA contact that equal objects should have equal hash code, thus may cause unexpected results.",,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/14 22:14;xuefuz;HIVE-6884.patch;https://issues.apache.org/jira/secure/attachment/12642032/HIVE-6884.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,386070,,,,Thu Nov 13 19:40:20 UTC 2014,,,,,,,,,,"0|i1uibb:",386334,,,,,,,,,,,,,,,,,,,,,"26/Apr/14 12:02;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642032/HIVE-6884.patch

{color:red}ERROR:{color} -1 due to 46 failed/errored test(s), 5419 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_varchar2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatCTAS
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_alter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_tblproperties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unset_table_view_property
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_unset_table_property
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/50/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/50/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 46 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642032;;;","28/Apr/14 17:05;ashutoshc;+1;;;","29/Apr/14 14:52;ashutoshc;Committed to trunk.;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamic partitioning optimization does not honor sort order or order by,HIVE-6883,12707650,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,prasanth_j,prasanth_j,prasanth_j,10/Apr/14 06:51,19/Sep/14 02:30,14/Jul/23 06:14,12/Apr/14 00:39,0.13.0,,,,,,,,,0.13.1,0.14.0,,,,,0,,,"HIVE-6455 patch does not honor sort order of the output table or order by of select statement. The reason for the former is numDistributionKey in ReduceSinkDesc is set wrongly. It doesn't take into account the sort columns, because of this RSOp sets the sort columns to null in Key. Since nulls are set in place of sort columns in Key, the sort columns in Value are not sorted. 

The other issue is ORDER BY columns are not honored during insertion. For example
{code}
insert overwrite table over1k_part_orc partition(ds=""foo"", t) select si,i,b,f,t from over1k_orc where t is null or t=27 order by si;
{code}

the select query performs order by on column 'si' in the first MR job. The following MR job (inserted by HIVE-6455), sorts the input data on dynamic partition column 't' without taking into account the already sorted 'si' column. This results in out of order insertion for 'si' column.",,mdominguez@cloudera.com,prasanth_j,rhbutani,sershe,sushanth,thejas,vikram.dixit,wzc1989,Yibing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-8162,,,,,,,,,,,,,,,,,,,,,"06/May/14 00:21;prasanth_j;HIVE-6883-branch-0.13.3.patch;https://issues.apache.org/jira/secure/attachment/12643461/HIVE-6883-branch-0.13.3.patch","10/Apr/14 08:55;prasanth_j;HIVE-6883.1.patch;https://issues.apache.org/jira/secure/attachment/12639552/HIVE-6883.1.patch","10/Apr/14 20:30;prasanth_j;HIVE-6883.2.patch;https://issues.apache.org/jira/secure/attachment/12639645/HIVE-6883.2.patch","11/Apr/14 01:45;prasanth_j;HIVE-6883.3.patch;https://issues.apache.org/jira/secure/attachment/12639717/HIVE-6883.3.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385973,,,,Fri Sep 19 02:30:47 UTC 2014,,,,,,,,,,"0|i1uhpz:",386237,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 08:55;prasanth_j;The fix is small. I added more tests to verify sort by and order by cases. Also replicated the tests for vectorization and tez.;;;","10/Apr/14 08:59;prasanth_j;Review Board is flaky.. will upload the patch once it is back..;;;","10/Apr/14 09:00;prasanth_j;[~rhbutani] this is a critical issue as HIVE-6455 fails to honor sort order in DDL.. Will it be possible to include this in 0.13?;;;","10/Apr/14 09:15;prasanth_j;Attaching RB link;;;","10/Apr/14 19:36;rhbutani;I going to say, let's keep this off 0.13
The user can turn of the dyn partition optimization in the case of sort/order;;;","10/Apr/14 19:47;prasanth_j;Thanks. No problem. I will change the fix version then. And yes the user can turn off this optimization.;;;","10/Apr/14 20:30;prasanth_j;orc_analyze.q test was failing in hadoop-2. Due to inconsistency in between hadoop-1 and hadoop-2 added order by to the test cases.;;;","10/Apr/14 21:53;vikram.dixit;+1 LGTM;;;","10/Apr/14 22:22;rhbutani;+1 for 0.13;;;","10/Apr/14 23:36;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639645/HIVE-6883.2.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5571 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_dyn_part
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2209/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2209/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639645;;;","11/Apr/14 01:45;prasanth_j;Fixed infer_bucket_sort_dyn_part.q. Other failures are unrelated.;;;","11/Apr/14 09:35;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639717/HIVE-6883.3.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5613 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
org.apache.hive.jdbc.TestJdbcDriver2.testNewConnectionConfiguration
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2218/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2218/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639717;;;","11/Apr/14 09:39;prasanth_j;The tests are not related.;;;","12/Apr/14 00:39;sershe;committed to trunk;;;","06/May/14 00:21;prasanth_j;[~sushanth] fyi.. attaching patch for branch-0.13.;;;","06/May/14 00:26;sushanth;Thanks for the update, Prashant, I'll test it now.;;;","06/May/14 01:07;sushanth;Thanks, Prashant, this merges correctly.;;;","09/Jun/14 06:39;thejas;This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
;;;","16/Sep/14 09:10;wzc1989;@ [~prasanth_j] , this fix cause some problems when combine dynamic partitioning with group by. Consider the following case:
{code}
CREATE TABLE `t1`(  `a` int,`b` string) PARTITIONED BY (`dt` string);
create table src1 (
  `key` string,
  `val` string
);
explain insert overwrite table t1 partition(dt) select 1, ""hello"", ""20140901"" from src1 group by key;
{code}
The key expressions of RS in Stage-2 are wrong. The part of the patch which using the parent RS's keyCols needs more changes.
{code}
 if (parentRSOpOrder != null && !parentRSOpOrder.isEmpty() && sortPositions.isEmpty()) {
          newKeyCols.addAll(parentRSOp.getConf().getKeyCols());
          orderStr += parentRSOpOrder;
        }
{code}

;;;","16/Sep/14 22:12;prasanth_j;[~wzc1989] Thanks for reporting the issue. I will look at it and will post back a fix in a new jira.;;;","18/Sep/14 23:53;prasanth_j;[~wzc1989] HIVE-8162 has fix for the issue.;;;","19/Sep/14 02:26;wzc1989;@ [~prasanth_j] , thank you for the fix:);;;","19/Sep/14 02:30;prasanth_j;[~wzc1989] The fix needs more revision wrt subqueries. Currently it doesn't preserve the ordering of records if any subqueries has order by. I am working on it and will post an update to that fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make upgrade script schemaTool friendly,HIVE-6882,12707633,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,10/Apr/14 04:09,10/Apr/14 15:54,14/Jul/23 06:14,10/Apr/14 15:54,0.13.0,,,,,,,,,0.13.0,,Metastore,,,,0,,,"Current scripts work fine when invoked manually, but fails when invoked via schemaTool.",,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6874,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 15:33;ashutoshc;HIVE-6882.1.patch;https://issues.apache.org/jira/secure/attachment/12639592/HIVE-6882.1.patch","10/Apr/14 04:12;ashutoshc;HIVE-6882.patch;https://issues.apache.org/jira/secure/attachment/12639525/HIVE-6882.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385956,,,,Thu Apr 10 15:54:40 UTC 2014,,,,,,,,,,"0|i1uhm7:",386220,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 04:12;ashutoshc;Original issue is HIVE-6874 which needs to be fixed, but this patch works around it and make upgrade script work with schemaTool.;;;","10/Apr/14 04:19;rhbutani;+1
+1 for 0.13;;;","10/Apr/14 15:33;ashutoshc;Similar fix is also needed for oracle script. ;;;","10/Apr/14 15:54;ashutoshc;Committed to trunk & 0.13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Postgres Upgrade script for hive 0.13 is broken,HIVE-6881,12707627,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,rhbutani,rhbutani,rhbutani,10/Apr/14 02:55,10/Apr/14 15:56,14/Jul/23 06:14,10/Apr/14 15:56,,,,,,,,,,0.13.0,,,,,,0,,,The script added for HIVE-6757 didn't quote the identifiers,,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 02:56;rhbutani;HIVE-6881.1.patch;https://issues.apache.org/jira/secure/attachment/12639520/HIVE-6881.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385950,,,,Thu Apr 10 15:56:27 UTC 2014,,,,,,,,,,"0|i1uhkv:",386214,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 03:48;ashutoshc;+1;;;","10/Apr/14 15:56;ashutoshc;Committed to trunk & 0.13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestHWISessionManager fails with -Phadoop-2,HIVE-6880,12707622,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,10/Apr/14 01:56,13/Nov/14 19:43,14/Jul/23 06:14,10/Apr/14 22:35,0.13.0,,,,,,,,,0.14.0,,Tests,,,,0,,,"Looks like dependencies missing for -Phadoop-2

{noformat}
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.213 sec <<< FAILURE! - in org.apache.hadoop.hive.hwi.TestHWISessionManager
warning(junit.framework.TestSuite$1)  Time elapsed: 0.009 sec  <<< FAILURE!
junit.framework.AssertionFailedError: Exception in constructor: testHiveDriver (java.lang.NoClassDefFoundError: org/apache/hadoop/mapreduce/TaskAttemptContext
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:171)
	at org.apache.hadoop.hive.shims.ShimLoader.createShim(ShimLoader.java:120)
	at org.apache.hadoop.hive.shims.ShimLoader.loadShims(ShimLoader.java:115)
	at org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims(ShimLoader.java:80)
	at org.apache.hadoop.hive.conf.HiveConf$ConfVars.<clinit>(HiveConf.java:248)
	at org.apache.hadoop.hive.conf.HiveConf.<clinit>(HiveConf.java:81)
	at org.apache.hadoop.hive.hwi.TestHWISessionManager.<init>(TestHWISessionManager.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at junit.framework.TestSuite.createTest(TestSuite.java:65)
	at junit.framework.TestSuite.addTestMethod(TestSuite.java:294)
	at junit.framework.TestSuite.addTestsFromTestCase(TestSuite.java:150)
	at junit.framework.TestSuite.<init>(TestSuite.java:129)
	at org.junit.internal.runners.JUnit38ClassRunner.<init>(JUnit38ClassRunner.java:71)
	at org.junit.internal.builders.JUnit3Builder.runnerForClass(JUnit3Builder.java:14)
	at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:57)
	at org.junit.internal.builders.AllDefaultPossibilitiesBuilder.runnerForClass(AllDefaultPossibilitiesBuilder.java:29)
	at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:57)
	at org.junit.internal.requests.ClassRequest.getRunner(ClassRequest.java:24)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:262)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.mapreduce.TaskAttemptContext
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
	... 28 more
)
	at junit.framework.Assert.fail(Assert.java:50)
	at junit.framework.TestSuite$1.runTest(TestSuite.java:97)
{noformat}",,jdere,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 01:57;jdere;HIVE-6880.1.patch;https://issues.apache.org/jira/secure/attachment/12639512/HIVE-6880.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385945,,,,Thu Nov 13 19:43:45 UTC 2014,,,,,,,,,,"0|i1uhjr:",386209,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 04:53;ashutoshc;+1;;;","10/Apr/14 17:38;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639512/HIVE-6880.1.patch

{color:green}SUCCESS:{color} +1 5571 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2206/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2206/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639512;;;","10/Apr/14 22:35;ashutoshc;Committed to trunk. Thanks, Jason!;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vectorization: IsNull returns incorrect output column.,HIVE-6879,12707619,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jnp,jnp,jnp,10/Apr/14 01:51,11/Apr/14 02:31,14/Jul/23 06:14,11/Apr/14 02:31,,,,,,,,,,0.13.0,,,,,,0,,,IsNull returns -1 as output column.,,jnp,rhbutani,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 02:21;jnp;HIVE-6879.1.patch;https://issues.apache.org/jira/secure/attachment/12639519/HIVE-6879.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385942,,,,Fri Apr 11 02:31:31 UTC 2014,,,,,,,,,,"0|i1uhj3:",386206,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 04:28;vikram.dixit;LGTM +1;;;","10/Apr/14 19:37;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639519/HIVE-6879.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5571 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2207/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2207/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639519;;;","10/Apr/14 19:50;jnp;The failure is unrelated.

[~rhbutani] This should go to hive-0.13 as well. It causes some queries to crash.;;;","10/Apr/14 21:23;rhbutani;+1 for 0.13;;;","11/Apr/14 02:31;jnp;I have committed to trunk and branch-0.13.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MetaStoreDirectSql may not handle empty filter correctly,HIVE-6878,12707599,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,Fixed,sershe,sershe,sershe,09/Apr/14 23:41,10/Apr/14 01:08,14/Jul/23 06:14,10/Apr/14 01:07,,,,,,,,,,0.13.0,,,,,,0,,,""""" filter can happen in some queries",,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/14 23:42;sershe;HIVE-6878.patch;https://issues.apache.org/jira/secure/attachment/12639491/HIVE-6878.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385922,,,,Thu Apr 10 01:07:54 UTC 2014,,,,,,,,,,"0|i1uhen:",386186,,,,,,,,,,,,,,,,,,,,,"09/Apr/14 23:42;sershe;Trivial patch;;;","09/Apr/14 23:43;sershe;[~ashutoshc] can you take a look? 1-line change;;;","10/Apr/14 00:37;ashutoshc;+1;;;","10/Apr/14 01:07;sershe;committed to trunk and 13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestOrcRawRecordMerger is deleting test.tmp.dir,HIVE-6877,12707594,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,09/Apr/14 23:13,09/Jun/14 06:39,14/Jul/23 06:14,10/Apr/14 22:37,,,,,,,,,,0.13.1,0.14.0,Tests,,,,0,,,TestOrcRawRecordMerger seems to be deleting the directory pointed to by test.tmp.dir.  This can cause some failures in any tests that run after this test if they need to use any files in the tmp dir such as conf files or creating Hive tables.,,jdere,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6953,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/14 23:36;jdere;HIVE-6877.1.patch;https://issues.apache.org/jira/secure/attachment/12639488/HIVE-6877.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385917,,,,Mon Jun 09 06:39:38 UTC 2014,,,,,,,,,,"0|i1uhdj:",386181,,,,,,,,,,,,,,,,,,,,,"09/Apr/14 23:36;jdere;Path for output directories were not being generated properly, attaching patch.;;;","10/Apr/14 04:55;ashutoshc;+1;;;","10/Apr/14 13:38;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639488/HIVE-6877.1.patch

{color:green}SUCCESS:{color} +1 5570 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2203/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2203/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639488;;;","10/Apr/14 22:37;ashutoshc;Committed to trunk. Thanks, Jason!;;;","09/Jun/14 06:39;thejas;This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DISTINCT clause in aggregates is handled incorrectly by vectorized execution,HIVE-6873,12707443,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,rusanu,rusanu,rusanu,09/Apr/14 14:03,11/Apr/14 17:19,14/Jul/23 06:14,11/Apr/14 15:11,0.13.0,0.14.0,,,,,,,,0.13.0,,Query Processor,,,,0,,,"The vectorized aggregates ignore the DISTINCT clause. This cause incorrect results. Due to how GroupByOperatorDesc adds the DISTINCT keys to the overall aggregate keys the vectorized aggregates do account for the extra key, but they do not process the data correctly for the key. the reduce side the aggregates the input from the vectorized map side to results that are only sometimes correct but mostly incorrect. HIVE-4607 tracks the proper fix, but meantime I'm filing a bug to disable vectorized execution if DISTINCT is present. Fix is trivial.",,jnp,rhbutani,rusanu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/14 14:05;rusanu;HIVE-6873.1.patch;https://issues.apache.org/jira/secure/attachment/12639400/HIVE-6873.1.patch","09/Apr/14 18:39;rusanu;HIVE-6873.2.patch;https://issues.apache.org/jira/secure/attachment/12639447/HIVE-6873.2.patch","10/Apr/14 23:32;jnp;HIVE-6873.3.patch;https://issues.apache.org/jira/secure/attachment/12639682/HIVE-6873.3.patch","11/Apr/14 09:18;rusanu;image001.png;https://issues.apache.org/jira/secure/attachment/12639768/image001.png","11/Apr/14 09:18;rusanu;image002.png;https://issues.apache.org/jira/secure/attachment/12639769/image002.png","11/Apr/14 09:18;rusanu;image003.png;https://issues.apache.org/jira/secure/attachment/12639770/image003.png","11/Apr/14 09:18;rusanu;image004.png;https://issues.apache.org/jira/secure/attachment/12639771/image004.png",,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385766,,,,Fri Apr 11 15:11:18 UTC 2014,,,,,,,,,,"0|i1ugfz:",386030,,,,,,,,,,,,,,,,,,,,,"09/Apr/14 14:06;rusanu;[~rhbutani] please consider this for 0.13 release.;;;","09/Apr/14 17:07;jnp;Does the issue impact all aggregates? I have seen that count(distinct..) tests are passing, including tpcds queries.;;;","09/Apr/14 18:39;rusanu;Allow for COUNT(DISTINCT expr) wich happens to work, by accident, dur to how reduce side handles the aggregate merge for COUNT(DISTINCT);;;","09/Apr/14 20:39;jnp;+1;;;","10/Apr/14 05:47;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639447/HIVE-6873.2.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5570 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.ql.security.TestStorageBasedClientSideAuthorizationProvider.testSimplePrivileges
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2198/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2198/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639447;;;","10/Apr/14 06:18;rusanu;Failures are unrelated to the patch;;;","10/Apr/14 06:47;jnp;It seems to me that all aggregates are working fine. The reason is that in a query like:
select sum(distinct a) from T group by b;
The map-side aggregation treats (b, a) as the key, therefore distinct values of a are getting propagated to the reducer, and for the distinct scenario reducer only takes the key and discards the values. 
;;;","10/Apr/14 07:45;rusanu;[~jnp] From how I read GroupByOptimizer.java@227, I reckon there are some cases when the reduce side does expect the mapper to had been doing the correct aggregation:
{code}
          // Partial aggregation is not done for distincts on the mapper
          // However, if the data is bucketed/sorted on the distinct key, partial aggregation
          // can be performed on the mapper.
{code};;;","10/Apr/14 22:01;jnp;Here is a scenario where we get incorrect result. It shows up on sorted bucketed column with hive.map.groupby.sorted=true, and only on group by queries with no keys.

Here are the steps:

hive> Create table T(a int, b int) clustered by (a) sorted by (a) stored as orc;

load following data:
300  1
300  1
300  1
300  1
300  1

hive> set hive.map.groupby.sorted=true;

hive> select sum(distinct a) from T;  // Incorrect result.
hive> select count(distinct a) from T;  // This is also incorrect.

;;;","10/Apr/14 23:32;jnp;The attached patch fixes the issue by disabling the optimization for sorted/bucketed columns if vectorization is on. This change impacts only the specific scenario being hit in the example above. A test is also added to reproduce the above scenario, and another test for normal distinct case.;;;","10/Apr/14 23:43;ashutoshc;+1;;;","10/Apr/14 23:52;rhbutani;+1 for 0.13;;;","11/Apr/14 03:36;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639682/HIVE-6873.3.patch

{color:green}SUCCESS:{color} +1 5614 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2216/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2216/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639682;;;","11/Apr/14 09:16;rusanu;Thanks for finding the problem repro and fix!;;;","11/Apr/14 09:18;rusanu;set hive.map.groupby.sorted=true; was the missing part for me…

From: Jitendra Nath Pandey (JIRA) [mailto:jira@apache.org]
Sent: Friday, April 11, 2014 1:02 AM
To: Remus Rusanu
Subject: [jira] [Commented] (HIVE-6873) DISTINCT clause in aggregates is handled incorrectly by vectorized execution

[cid:image001.png@01CF557F.FDF58F90]

Jitendra Nath Pandey<https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jnp> commented on [Bug] HIVE-6873<https://issues.apache.org/jira/browse/HIVE-6873>




Re: DISTINCT clause in aggregates is handled incorrectly by vectorized execution<https://issues.apache.org/jira/browse/HIVE-6873>



Here is a scenario where we get incorrect result. It shows up on sorted bucketed column with hive.map.groupby.sorted=true, and only on group by queries with no keys.

Here are the steps:

hive> Create table T(a int, b int) clustered by (a) sorted by (a) stored as orc;

load following data:
300 1
300 1
300 1
300 1
300 1

hive> set hive.map.groupby.sorted=true;

hive> select sum(distinct a) from T; // Incorrect result.
hive> select count(distinct a) from T; // This is also incorrect.


[Add Comment]<https://issues.apache.org/jira/browse/HIVE-6873#add-comment>

Add Comment<https://issues.apache.org/jira/browse/HIVE-6873#add-comment>






This message was sent by Atlassian JIRA (v6.2#6252-sha1:aa34325)

[Atlassian logo]




;;;","11/Apr/14 15:11;rhbutani;Committed to trunk and 0.13
Thanks Jitendra, Remus and Ashutosh;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build fixes to allow Windows to run TestCliDriver,HIVE-6871,12707363,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,09/Apr/14 06:21,13/Nov/14 19:43,14/Jul/23 06:14,10/Apr/14 22:42,,,,,,,,,,0.14.0,,Build Infrastructure,Windows,,,0,,,"Some of the Java properties have been changed or set differently due to the Mavenization of the Hive build, and it looks like this is causing some issues with the Windows unit tests.",,jdere,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/14 06:23;jdere;HIVE-6871.1.patch;https://issues.apache.org/jira/secure/attachment/12639349/HIVE-6871.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385686,,,,Thu Nov 13 19:43:44 UTC 2014,,,,,,,,,,"0|i1ufy7:",385950,,,,,,,,,,,,,,,,,,,,,"09/Apr/14 06:23;jdere;Few fixes for Windows:
- Remove/change some test properties which have changed since Mavenization, which were used by Windows-specific code.
- Set hadoop binary to hadoop.cmd on Windows
- classpath environment variables exceeded 8K limit for windows command shell. Workaround by copying all dependencies to a single directory and doing $dir/* to shorten the environment variables.;;;","10/Apr/14 01:44;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639349/HIVE-6871.1.patch

{color:green}SUCCESS:{color} +1 5557 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2196/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2196/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639349;;;","10/Apr/14 05:03;ashutoshc;+1;;;","10/Apr/14 22:42;ashutoshc;Committed to trunk. Thanks, Jason!;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix maven.repo.local setting in Hive build,HIVE-6870,12707343,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,09/Apr/14 04:04,13/Nov/14 19:42,14/Jul/23 06:14,10/Apr/14 22:44,,,,,,,,,,0.14.0,,Build Infrastructure,,,,0,,,"The pom.xml currently assumes maven.repo.local should be ${user.home}/.m2/repository.  If the user has overridden the local repository through Maven settings, tests which assume the hive-exec JAR is at ${user.home}/.m2/repository will fail because the artifacts will not be installed at that location.",,jdere,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/14 04:06;jdere;HIVE-6870.1.patch;https://issues.apache.org/jira/secure/attachment/12639339/HIVE-6870.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385666,,,,Thu Nov 13 19:42:03 UTC 2014,,,,,,,,,,"0|i1uftr:",385930,,,,,,,,,,,,,,,,,,,,,"09/Apr/14 04:06;jdere;Use ${settings.localRepository} for the maven.repo.local property.;;;","09/Apr/14 19:40;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639339/HIVE-6870.1.patch

{color:green}SUCCESS:{color} +1 5556 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2193/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2193/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639339;;;","10/Apr/14 05:04;ashutoshc;+1;;;","10/Apr/14 22:44;ashutoshc;Committed to trunk. Thanks, Jason!;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create table in HCatalog sets different SerDe defaults than what is set through the CLI,HIVE-6868,12707280,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,rhbutani,rhbutani,rhbutani,08/Apr/14 22:25,13/Nov/14 19:43,14/Jul/23 06:14,02/May/14 15:51,,,,,,,,,,0.14.0,,HCatalog,,,,0,,,HCatCreateTableDesc doesn't invoke the getEmptyTable function on org.apache.hadoop.hive.ql.metadata.Table,,gates,rhbutani,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/14 22:33;rhbutani;HIVE-6868.1.patch;https://issues.apache.org/jira/secure/attachment/12639281/HIVE-6868.1.patch","09/Apr/14 23:39;rhbutani;HIVE-6868.2.patch;https://issues.apache.org/jira/secure/attachment/12639489/HIVE-6868.2.patch","11/Apr/14 01:58;sushanth;HIVE-6868.3.patch;https://issues.apache.org/jira/secure/attachment/12639720/HIVE-6868.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385603,,,,Thu Nov 13 19:43:04 UTC 2014,,,,,,,,,,"0|i1ufg7:",385869,,,,,,,,,,,,,,,,,,,,,"09/Apr/14 17:29;gates;+1 from the hcat side (non-binding on the hive metastore side).  It definitely is better to get the same defaults that Hive has for the table rather than having different defaults depending on the route you come in.;;;","09/Apr/14 23:39;rhbutani;didn't fix the HCatCreateTableDesc in hive.hcatalog.api;;;","11/Apr/14 01:50;sushanth;One more change is required on the HCat side - there's a new StorageDescriptor and a new SerDeInfo object being generated further down that clobber this. Also, org.apache.hcatalog.* is considered deprecated-frozen and are not supposed to be updated (to retain behaviour equivalence with 0.11).

I'll regenerate this patch with these changes.;;;","11/Apr/14 01:58;sushanth;Attached updated patch, also includes a test to check that properties are identical-ish to an empty table created by the metadata table.;;;","11/Apr/14 11:35;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639720/HIVE-6868.3.patch

{color:green}SUCCESS:{color} +1 5585 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2219/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2219/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639720;;;","12/Apr/14 16:45;sushanth;[~ashutoshc], could we get a review on this too? :);;;","15/Apr/14 04:01;ashutoshc;+1;;;","02/May/14 15:51;ashutoshc;Committed to trunk. Thanks, Harish!;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to load data into Hive from Pig using HCatStorer(),HIVE-6865,12707114,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,libing,libing,libing,08/Apr/14 06:20,05/Jun/15 20:03,14/Jul/23 06:14,03/Jun/15 17:32,0.12.0,,,,,,,,,1.2.0,,HCatalog,,,,0,,,"Reproduce steps:
1. create a hive table
hive> create table t1 (c1 int, c2 int, c3 int);

2. start pig shell
grunt> register $HIVE_HOME/lib/*.jar
grunt> register $HIVE_HOME/hcatalog/share/hcatalog/*.jar
grunt> A = load 'pig.txt' as (c1:int, c2:int, c3:int)
grunt> store A into 't1' using org.apache.hive.hcatalog.HCatSrorer();

Error Message:
ERROR [main] org.apache.pig.tools.pigstats.SimplePigStats     - ERROR 2997: Unable to recreate exception from backend error: org.apache.hcatalog.common.HCatException : 2004 : HCatOutputFormat not initialized, setOutput has to be called
        at org.apache.hcatalog.mapreduce.HCatBaseOutputFormat.getJobInfo(HCatBaseOutputFormat.java:111)
        at org.apache.hcatalog.mapreduce.HCatBaseOutputFormat.getJobInfo(HCatBaseOutputFormat.java:97)
        at org.apache.hcatalog.mapreduce.HCatBaseOutputFormat.getOutputFormat(HCatBaseOutputFormat.java:85)
        at org.apache.hcatalog.mapreduce.HCatBaseOutputFormat.checkOutputSpecs(HCatBaseOutputFormat.java:75)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.checkOutputSpecsHelper(PigOutputFormat.java:207)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.checkOutputSpecs(PigOutputFormat.java:187)
        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:1000)
        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:963)
        at java.security.AccessController.doPrivileged(AccessController.java:310)
        at javax.security.auth.Subject.doAs(Subject.java:573)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1502)
        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:963)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:616)
        at org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.submit(ControlledJob.java:336)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:60)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:37)
        at java.lang.reflect.Method.invoke(Method.java:611)
        at org.apache.pig.backend.hadoop23.PigJobControl.submit(PigJobControl.java:128)
        at org.apache.pig.backend.hadoop23.PigJobControl.run(PigJobControl.java:191)
        at java.lang.Thread.run(Thread.java:738)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:270)",,engrean,libing,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385437,,,,Fri Jun 05 20:03:20 UTC 2015,,,,,,,,,,"0|i1uefr:",385704,,,,,,,,,,,,,,,,,,,,,"08/Sep/14 21:19;engrean;Have you tried this by starting pig shell with ""pig -useHCatalog"" ? If I used ""-useHCatalog"", then I don't have to load the hive jars.;;;","01/Jun/15 06:08;libing;This issue has been resolved in Hive-1.2.0.;;;","03/Jun/15 17:32;libing;I tried the same queries in Hive 1.2.0, it could work well. 
Close it as fixed;;;","05/Jun/15 20:03;thejas;Closing jiras fixed in 1.2.0 release . 
Please open new jira if you find any related issue.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2 concurrency uses incorrect user information in unsecured mode,HIVE-6864,12707110,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,vgumashta,vgumashta,vgumashta,08/Apr/14 05:53,10/Apr/14 23:01,14/Jul/23 06:14,10/Apr/14 23:01,0.13.0,,,,,,,,,0.13.0,,HiveServer2,,,,0,,,Concurrent queries create table with wrong ownership,,rhbutani,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 04:54;vgumashta;HIVE-6864.1.patch;https://issues.apache.org/jira/secure/attachment/12639534/HIVE-6864.1.patch","10/Apr/14 19:36;vgumashta;HIVE-6864.2.patch;https://issues.apache.org/jira/secure/attachment/12639637/HIVE-6864.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385433,,,,Thu Apr 10 23:01:08 UTC 2014,,,,,,,,,,"0|i1ueev:",385700,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 04:54;vgumashta;[~thejas] [~rhbutani] Another last minute bug for 13!;;;","10/Apr/14 04:55;vgumashta;This is basically based on a patch provided by [~thejas] while testing a fix.;;;","10/Apr/14 07:26;thejas;Looks good. Just a minor comment about the exception in rb.

Harish, I think we should include this in 0.13 . This can result in action being done as a wrong user.
;;;","10/Apr/14 19:36;vgumashta;[~thejas] Patch includes feedback. Thanks!;;;","10/Apr/14 21:36;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639637/HIVE-6864.2.patch

{color:green}SUCCESS:{color} +1 5571 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2208/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2208/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639637;;;","10/Apr/14 22:40;rhbutani;+1 for 0.13;;;","10/Apr/14 23:01;thejas;Patch committed to 0.13 branch and trunk.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2 binary mode throws exception with PAM,HIVE-6863,12707109,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vgumashta,vgumashta,vgumashta,08/Apr/14 05:28,09/Apr/14 20:04,14/Jul/23 06:14,09/Apr/14 20:04,0.13.0,,,,,,,,,0.13.0,,HiveServer2,,,,0,,,Works fine in http mode,,rhbutani,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/14 05:35;vgumashta;HIVE-6863.1.patch;https://issues.apache.org/jira/secure/attachment/12639141/HIVE-6863.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385432,,,,Wed Apr 09 20:04:20 UTC 2014,,,,,,,,,,"0|i1ueen:",385699,,,,,,,,,,,,,,,,,,,,,"08/Apr/14 05:35;vgumashta;Minor patch. 

cc [~thejas] [~rhbutani] This is a bug for 13.;;;","08/Apr/14 20:16;thejas;+1;;;","08/Apr/14 22:39;rhbutani;+1 for 0.13;;;","09/Apr/14 01:30;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639141/HIVE-6863.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5555 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2181/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2181/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639141;;;","09/Apr/14 15:15;vgumashta;[~thejas] Failure looks unrelated. Thanks!;;;","09/Apr/14 20:04;thejas;Patch committed to 0.13 branch and trunk. Thanks Vaibhav!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add DB schema DDL and upgrade 12to13 scripts for MS SQL Server,HIVE-6862,12707107,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,08/Apr/14 05:16,19/Mar/21 17:09,14/Jul/23 06:14,17/Apr/14 21:42,0.13.0,,,,,,,,,0.13.1,0.14.0,Metastore,,,,0,,,"need to add a unifed 0.13 script and a separate script for ACID support

NO PRECOMMIT TESTS
",,ekoifman,leftyl,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-3764,HIVE-24911,,,,,,,,,,,,,,,,,,,,"15/Apr/14 20:44;ekoifman;HIVE-6862.2.patch;https://issues.apache.org/jira/secure/attachment/12640329/HIVE-6862.2.patch","16/Apr/14 18:19;ekoifman;HIVE-6862.3.patch;https://issues.apache.org/jira/secure/attachment/12640513/HIVE-6862.3.patch","09/Apr/14 02:18;ekoifman;HIVE-6862.patch;https://issues.apache.org/jira/secure/attachment/12639325/HIVE-6862.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385430,,,,Fri Mar 31 10:12:17 UTC 2017,,,,,,,,,,"0|i1uee7:",385697,,,,,,,,,,,,,,,,,,,,,"11/Apr/14 18:58;ashutoshc;[~eugene.koifman] All other DBs also have README providing instructions on how to effect upgrade. It will be good to have that for MS SQL as well.;;;","15/Apr/14 20:44;ekoifman;added readme;;;","16/Apr/14 05:22;leftyl;Nice README.  Just one question and some nits:

MetaStore Upgrade Steps

* Step 3 refers to creating a schema dump in the previous step, but I don't see it.  Did you mean in step 1 as part of best practices?
* Step 4 needs a period at the end.
* Step 5 ""statement"" should be plural and ""you"" should be ""your"" (and while I'm nitting, elsewhere you had only one space between sentences).

Upgrading from 0.11.0 to 0.13.0

* Step 2 has lots of trailing whitespace on the <property>, <name>, and <value> lines and repeats ""to create"" (and has two spaces between sentences again).;;;","16/Apr/14 18:19;ekoifman;HIVE-6862.3.patch to address [~leftylev]'s comments;;;","16/Apr/14 18:39;leftyl;Sorry to be a pest, but step 2 in Updating still repeats ""to create"" -- 

{quote}
+   in your hive-site.xml.  This will cause DataNucleus to create to
+   create tables which are missing from your database once metastore starts.
{quote};;;","17/Apr/14 18:46;ashutoshc;+1;;;","17/Apr/14 21:42;ashutoshc;Committed to trunk. Thanks, Eugene!
Lefty I edited that line while committing.;;;","18/Apr/14 03:13;leftyl;Thanks, Ashutosh.  (No more complaints about broken docs.);;;","09/Jun/14 06:39;thejas;This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
;;;","31/Mar/17 10:12;leftyl;Doc note:  This is now documented in the wiki.

* [Hive Schema Tool -- The schematool Command | https://cwiki.apache.org/confluence/display/Hive/Hive+Schema+Tool#HiveSchemaTool-TheschematoolCommand]

Thanks for the doc, [~asears].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
more hadoop2 only golden files to fix,HIVE-6861,12707095,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,08/Apr/14 02:30,13/Nov/14 19:41,14/Jul/23 06:14,10/Apr/14 04:20,,,,,,,,,,0.14.0,,Tests,,,,0,,,"More hadoop2 golden files to fix due to HIVE-6643, HIVE-6642, HIVE-6808, HIVE-6144.",,jdere,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/14 02:31;jdere;HIVE-6861.1.patch;https://issues.apache.org/jira/secure/attachment/12639124/HIVE-6861.1.patch","09/Apr/14 09:16;jdere;HIVE-6861.2.patch;https://issues.apache.org/jira/secure/attachment/12639372/HIVE-6861.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385418,,,,Thu Nov 13 19:41:51 UTC 2014,,,,,,,,,,"0|i1uebj:",385685,,,,,,,,,,,,,,,,,,,,,"08/Apr/14 02:31;jdere;patch v1.;;;","08/Apr/14 21:24;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639124/HIVE-6861.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5555 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_map_operators
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2179/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2179/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639124;;;","08/Apr/14 22:08;ashutoshc;+1;;;","09/Apr/14 09:16;jdere;I seem to be getting different ordering of results for union_remove_18.q on different runs of the test, looks like that test could benefit by adding an order-by to the last query.  Attaching patch v2 which makes that change.;;;","10/Apr/14 03:46;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639372/HIVE-6861.2.patch

{color:green}SUCCESS:{color} +1 5570 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2197/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2197/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639372;;;","10/Apr/14 04:20;ashutoshc;Committed to trunk. Thanks, Jason!;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issue with FS based stats collection on Tez,HIVE-6860,12707076,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,08/Apr/14 00:29,08/Apr/14 14:27,14/Jul/23 06:14,08/Apr/14 14:27,,,,,,,,,,0.13.0,,Statistics,Tez,,,0,,,Statistics from different tasks got overwritten while running on Tez.,,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/14 00:30;ashutoshc;HIVE-6860.patch;https://issues.apache.org/jira/secure/attachment/12639101/HIVE-6860.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385399,,,,Tue Apr 08 14:27:27 UTC 2014,,,,,,,,,,"0|i1ue7b:",385666,,,,,,,,,,,,,,,,,,,,,"08/Apr/14 00:32;vikram.dixit;+1 LGTM.;;;","08/Apr/14 14:27;ashutoshc;Committed to trunk & 0.13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Unit tests decimal_udf.q, vectorization_div0.q fail with jdk-7.",HIVE-6858,12707018,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jnp,jnp,jnp,07/Apr/14 20:10,13/Nov/14 19:40,14/Jul/23 06:14,09/Apr/14 20:41,0.13.0,,,,,,,,,0.14.0,,,,,,0,,,"Unit tests decimal_udf.q, vectorization_div0.q fail with jdk-7.

{noformat}
< -250.0	6583411.236	1.0	6583411.236	-0.004	-0.0048
---
> -250.0	6583411.236	1.0	6583411.236	-0.0040	-0.0048
{noformat}


Following code reproduces this behavior when run in jdk-7 vs jdk-6. Jdk-7 produces -0.004 while, jdk-6 produces -0.0040.
{code}
public class Main {
  public static void main(String[] a) throws Exception {
     double val = 0.004;
     System.out.println(""Value = ""+val);
  }
}
{code}

This happens to be a bug in jdk6, that has been fixed in jdk7.
http://bugs.java.com/bugdatabase/view_bug.do?bug_id=4511638

",,jdere,jnp,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/14 00:19;jnp;HIVE-6858.1.patch;https://issues.apache.org/jira/secure/attachment/12639098/HIVE-6858.1.patch","08/Apr/14 21:24;jnp;HIVE-6858.2.patch;https://issues.apache.org/jira/secure/attachment/12639270/HIVE-6858.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385341,,,,Thu Nov 13 19:40:38 UTC 2014,,,,,,,,,,"0|i1uduf:",385608,,,,,,,,,,,,,,,,,,,,,"08/Apr/14 00:19;jnp;Attached patch modifies the tests so that it doesn't run into the jdk bug.;;;","08/Apr/14 10:09;jdere;Would you be able to fix groupby3_map_skew.q as well, which looks like it also has a similar issue? For that one maybe you could replace:
SELECT dest1.* FROM dest1;
with:
SELECT c1, c2, c3, c4, c5, c6, c7, ROUND(c8, 5), ROUND(c9, 5) FROM dest1;

And hopefully the values generated do not show differences between the jdk6/7 formatting.;;;","08/Apr/14 17:19;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639098/HIVE-6858.1.patch

{color:green}SUCCESS:{color} +1 5550 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2175/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2175/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639098;;;","08/Apr/14 21:24;jnp;Updated patch with the fix to groupby3_map_skew.q as well, as suggested by [~jdere]. Verfied that it passes in both jdk6, jdk7.;;;","08/Apr/14 21:34;jdere;Thanks for tracking this one down. 
+1;;;","09/Apr/14 09:39;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639270/HIVE-6858.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5555 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_map_operators
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2185/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2185/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639270;;;","09/Apr/14 20:41;jnp;The failed test is not related, I have committed it to trunk.;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ddl commands fail with permissions issue when running using webhcat in secure Tez cluster,HIVE-6856,12707005,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,07/Apr/14 18:55,09/Apr/14 22:57,14/Jul/23 06:14,09/Apr/14 22:57,0.13.0,,,,,,,,,0.13.0,,WebHCat,,,,0,,,"curl -u : --negotiate -d ""exec=show tables;"" -X POST http://server:50111/templeton/v1/ddl

results in (when Tez is enabled in Secure cluster)
{noformat}
Exception in thread ""main"" java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=hrt_qa, access=WRITE, inode=""/user/hcat"":hcat:hcat:drwxr-xr-x
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkFsPermission(FSPermissionChecker.java:265)
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:251)
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:232)
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:176)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5497)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5479)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:5453)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:3596)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:3566)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3540)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:754)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:558)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:354)
        at org.apache.hive.hcatalog.cli.HCatCli.main(HCatCli.java:138)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
Caused by: org.apache.hadoop.security.AccessControlException: Permission denied: user=hrt_qa, access=WRITE, inode=""/user/hcat"":hcat:hcat:drwxr-xr-x
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkFsPermission(FSPermissionChecker.java:265)
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:251)
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:232)
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:176)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5497)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5479)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:5453)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:3596)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:3566)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3540)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:754)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:558)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
        at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2555)
        at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2524)
        at org.apache.hadoop.hdfs.DistributedFileSystem$16.doCall(DistributedFileSystem.java:827)
        at org.apache.hadoop.hdfs.DistributedFileSystem$16.doCall(DistributedFileSystem.java:823)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:823)
        at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:816)
        at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1815)
        at org.apache.hadoop.hive.ql.exec.tez.DagUtils.getDefaultDestDir(DagUtils.java:687)
        at org.apache.hadoop.hive.ql.exec.tez.DagUtils.getHiveJarDirectory(DagUtils.java:772)
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.createJarLocalResource(TezSessionState.java:288)
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:142)
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:116)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:352)
        ... 6 more
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=hrt_qa, access=WRITE, inode=""/user/hcat"":hcat:hcat:drwxr-xr-x
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkFsPermission(FSPermissionChecker.java:265)
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:251)
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:232)
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:176)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5497)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5479)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:5453)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:3596)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:3566)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3540)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:754)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:558)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

        at org.apache.hadoop.ipc.Client.call(Client.java:1410)
        at org.apache.hadoop.ipc.Client.call(Client.java:1363)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
        at $Proxy16.mkdirs(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
        at $Proxy16.mkdirs(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:500)
        at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2553)
        ... 19 more

{noformat}

NO PRECOMMIT TESTS",,ekoifman,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/14 19:10;ekoifman;HIVE-6856.patch;https://issues.apache.org/jira/secure/attachment/12639040/HIVE-6856.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385328,,,,Wed Apr 09 22:57:29 UTC 2014,,,,,,,,,,"0|i1udrj:",385595,,,,,,,,,,,,,,,,,,,,,"07/Apr/14 19:43;thejas;+1

Hcat cli never runs any query on the cluster, so it never needs a runtime engine. Always using mr as the engine in config works fine.
;;;","09/Apr/14 17:36;rhbutani;+1 for 0.13;;;","09/Apr/14 22:57;thejas;Patch committed to 0.13 and trunk.
Thanks for the contribution Eugene!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A couple of errors in MySQL db creation script for transaction tables,HIVE-6855,12706995,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,gates,gates,gates,07/Apr/14 17:55,08/Apr/14 14:29,14/Jul/23 06:14,08/Apr/14 14:29,0.13.0,,,,,,,,,0.13.0,,Metastore,,,,0,,,"There are a few small issues in the database creation scripts for mysql.  A couple of the tables don't set the engine to InnoDB.  None of the tables set default character set to latin1.  And the syntax ""CREATE INDEX...USING HASH"" doesn't work on older versions of MySQL.  Instead the index creation should be done without specifying a method (no USING clause).",,gates,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/14 17:57;gates;HIVE-6855.patch;https://issues.apache.org/jira/secure/attachment/12639028/HIVE-6855.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385318,,,,Tue Apr 08 14:29:37 UTC 2014,,,,,,,,,,"0|i1udpb:",385585,,,,,,,,,,,,,,,,,,,,,"07/Apr/14 17:59;gates;NO PRECOMMIT TESTS 

Updated the mysql scripts.;;;","07/Apr/14 18:05;ashutoshc;Seems like we also need to update hive-schema-0.14.0.mysql.sql;;;","07/Apr/14 18:33;gates;The transaction tables aren't currently in  hive-schema-0.14.0.mysql.sql because we wanted to figure out a better method than adding them by hand for 0.14.;;;","07/Apr/14 21:26;ashutoshc;+1;;;","08/Apr/14 14:29;ashutoshc;Committed to 0.13 & trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
show create table for hbase tables should exclude LOCATION ,HIVE-6853,12706975,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,mwc,mwc,mwc,07/Apr/14 15:54,13/Nov/14 19:42,14/Jul/23 06:14,25/May/14 01:01,0.10.0,,,,,,,,,0.14.0,,StorageHandler,,,,0,,,"If you create a table on top of hbase in hive and issue a ""show create table <hbase_table>"", it gives a bad DDL. It should not show LOCATION:                                                                                                    

[hive]$ cat /tmp/test_create.sql
CREATE EXTERNAL TABLE nba_twitter.hbase2(
key string COMMENT 'from deserializer',
name string COMMENT 'from deserializer',
pdt string COMMENT 'from deserializer',
service string COMMENT 'from deserializer',
term string COMMENT 'from deserializer',
update1 string COMMENT 'from deserializer')
ROW FORMAT SERDE
'org.apache.hadoop.hive.hbase.HBaseSerDe'
STORED BY
'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
'serialization.format'='1',
'hbase.columns.mapping'=':key,srv:name,srv:pdt,srv:service,srv:term,srv:update')
LOCATION
'hdfs://nameservice1/user/hive/warehouse/nba_twitter.db/hbase'
TBLPROPERTIES (
'hbase.table.name'='NBATwitter',
'transient_lastDdlTime'='1386172188')
Trying to create a table using the above fails:

[hive]$ hive -f /tmp/test_create.sql
cli ""-f /tmp/test_create.sql""
Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-4.4.0-1.cdh4.4.0.p0.39/lib/hive/lib/hive-common-0.10.0-cdh4.4.0.jar!/hive-log4j.properties
FAILED: Error in metadata: MetaException(message:LOCATION may not be specified for HBase.)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask

However, if I remove the LOCATION, then the DDL is valid.",,mwc,szehon,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/14 15:57;mwc;HIVE-6853-0.patch;https://issues.apache.org/jira/secure/attachment/12639002/HIVE-6853-0.patch","09/Apr/14 03:33;mwc;HIVE-6853.patch;https://issues.apache.org/jira/secure/attachment/12639336/HIVE-6853.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385298,,,,Thu Nov 13 19:42:31 UTC 2014,,,,,,,,,,"0|i1udkv:",385565,,,,,,,,,,,,,,,,,,,,,"07/Apr/14 15:56;mwc;Attaching a patch that checks if the table uses the HBaseStorageHandler and removes the LOCATION field in the show create table method. ;;;","08/Apr/14 19:12;szehon;Thanks for the fix, only one minor comment, is it needed to make a StringBuilder when there is only one string to return?

Also can you upload the patch in right name-format?  The precommit test takes patches in the form HIVE-XXXX.patch or HIVE-XXXX.n.patch only.;;;","09/Apr/14 03:33;mwc;bq: is it needed to make a StringBuilder when there is only one string to return?
Fixed. I removed it and just returned the string. 
;;;","09/Apr/14 04:58;szehon;Thanks , for the most part it LGTM.  I guess its not the cleanest, as its breaking the StorageHandler abstraction.  Probably cleaner to add some hook to StorageHandler interface, but due to backward compatibility, its probably not worth it for this use-case.

+1 (non-binding);;;","09/Apr/14 21:41;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639336/HIVE-6853.patch

{color:green}SUCCESS:{color} +1 5556 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2194/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2194/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639336;;;","24/May/14 23:53;ashutoshc;+1;;;","25/May/14 01:01;ashutoshc;Committed to trunk. Thanks, Miklos!;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"For FetchOperator, Driver uses the valid transaction list from the previous query",HIVE-6850,12706816,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,omalley,gates,omalley,05/Apr/14 23:58,09/Apr/14 19:41,14/Jul/23 06:14,09/Apr/14 19:41,,,,,,,,,,0.13.0,,Clients,,,,0,,,"The problem is two fold:
* FetchTask.initialize, which is called during parsing of the query, converts the HiveConf it is given into a JobConf by copying it.
* Driver.recordValidTxns, which runs after parsing, adds the valid transactions to the HiveConf.

Thus fetch operators will use the transactions from the previous command.",,omalley,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/14 00:01;omalley;HIVE-6850.patch;https://issues.apache.org/jira/secure/attachment/12638889/HIVE-6850.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385139,,,,Wed Apr 09 19:41:46 UTC 2014,,,,,,,,,,"0|i1uclj:",385406,,,,,,,,,,,,,,,,,,,,,"06/Apr/14 00:01;omalley;This patch moves the code to get a list of valid transactions before the command is parsed. Thus the transactions are correct when the fetch operator runs.;;;","06/Apr/14 21:16;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638889/HIVE-6850.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5548 tests executed
*Failed tests:*
{noformat}
org.apache.hive.jdbc.TestJdbcDriver2.testNewConnectionConfiguration
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2154/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2154/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638889;;;","08/Apr/14 00:02;rhbutani;+1 lgtm
+1 for 0.13;;;","09/Apr/14 19:41;omalley;Thanks for the review, Harish!

I've committed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Importing into an existing table fails,HIVE-6848,12706753,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,rhbutani,arpitgupta,rhbutani,05/Apr/14 01:28,07/Apr/14 17:21,14/Jul/23 06:14,06/Apr/14 18:35,,,,,,,,,,0.13.0,,Query Processor,,,,0,,,This is because ImportSemanticAnalyzer:checkTable doesn't account for the renaming of OutputFormat class and the setting of a default value for Serialization.Format,,rhbutani,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6854,,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/14 01:36;rhbutani;HIVE-6848.1.patch;https://issues.apache.org/jira/secure/attachment/12638820/HIVE-6848.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385076,,,,Mon Apr 07 17:21:21 UTC 2014,,,,,,,,,,"0|i1uc7j:",385343,,,,,,,,,,,,,,,,,,,,,"05/Apr/14 01:37;rhbutani;[~sushanth]/[~ashutoshc] can you please review;;;","05/Apr/14 15:05;ashutoshc;+1;;;","06/Apr/14 11:30;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638820/HIVE-6848.1.patch

{color:green}SUCCESS:{color} +1 5547 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2148/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2148/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638820;;;","06/Apr/14 18:35;rhbutani;Committed to trunk and 0.13
thanks Ashutosh for the review;;;","07/Apr/14 00:26;xuefuz;I was wondering if a test case would help prevent future breakage.;;;","07/Apr/14 17:21;rhbutani;Sure, added a new jira for this HIVE-6854;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve / fix bugs in Hive scratch dir setup,HIVE-6847,12706743,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vgumashta,vikram.dixit,vikram.dixit,04/Apr/14 23:56,24/Feb/15 09:50,14/Jul/23 06:14,04/Sep/14 20:27,0.14.0,,,,,,,,,0.14.0,,CLI,HiveServer2,,,0,TODOC14,,"Currently, the hive server creates scratch directory and changes permission to 777 however, this is not great with respect to security. We need to create user specific scratch directories instead. Also refer to HIVE-6782 1st iteration of the patch for approach.",,daisuke.kobayashi,leftyl,mdominguez@cloudera.com,qwertymaniac,szehon,thejas,vgumashta,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-8606,,,,,,,HIVE-8643,,,HIVE-4487,HIVE-8143,HIVE-6602,,,,,,,,,,,,,,,,,,,"16/May/14 01:40;vgumashta;HIVE-6847.1.patch;https://issues.apache.org/jira/secure/attachment/12645157/HIVE-6847.1.patch","03/Sep/14 22:01;vgumashta;HIVE-6847.10.patch;https://issues.apache.org/jira/secure/attachment/12666336/HIVE-6847.10.patch","04/Sep/14 07:38;vgumashta;HIVE-6847.11.patch;https://issues.apache.org/jira/secure/attachment/12666429/HIVE-6847.11.patch","11/Aug/14 01:10;vgumashta;HIVE-6847.2.patch;https://issues.apache.org/jira/secure/attachment/12660898/HIVE-6847.2.patch","22/Aug/14 07:38;vgumashta;HIVE-6847.3.patch;https://issues.apache.org/jira/secure/attachment/12663616/HIVE-6847.3.patch","23/Aug/14 06:46;vgumashta;HIVE-6847.4.patch;https://issues.apache.org/jira/secure/attachment/12663837/HIVE-6847.4.patch","25/Aug/14 17:52;vgumashta;HIVE-6847.5.patch;https://issues.apache.org/jira/secure/attachment/12664179/HIVE-6847.5.patch","26/Aug/14 22:50;vgumashta;HIVE-6847.6.patch;https://issues.apache.org/jira/secure/attachment/12664510/HIVE-6847.6.patch","02/Sep/14 23:17;vgumashta;HIVE-6847.7.patch;https://issues.apache.org/jira/secure/attachment/12666069/HIVE-6847.7.patch","03/Sep/14 00:56;vgumashta;HIVE-6847.8.patch;https://issues.apache.org/jira/secure/attachment/12666099/HIVE-6847.8.patch","03/Sep/14 01:50;vgumashta;HIVE-6847.9.patch;https://issues.apache.org/jira/secure/attachment/12666115/HIVE-6847.9.patch",,,,,11.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385066,,,,Thu Nov 13 19:41:46 UTC 2014,,,,,,,,,,"0|i1uc5b:",385333,,,,,,,,,,,,,,,,,,,,,"16/May/14 07:04;szehon;This seems like a nice addition, but wondering do we need to escape special characters in generating user-scratch dir, like the logic we use in generating the partition names (FileUtils.escapePathName)?  Also there is some extra formatting diffs, and it'd be clearer to see it in the review board if you could;;;","16/May/14 22:21;vgumashta;Thanks for taking a look [~szehon]. The review board link is already shared in the link section.;;;","17/May/14 03:01;szehon; [~vgumashta] Thanks,somehow the review board wasnt there when I made the comment, sorry about that.  Left some comments.;;;","19/May/14 18:43;hiveqa;Posting test-result manually

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12645157/HIVE-6847.1.patch

{color:red}ERROR:{color} -1 due to 20 failed/errored test(s), 5449 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_java_method
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_reflect
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_math_funcs
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input20
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input5
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
org.apache.hive.service.cli.TestScratchDir.testLocalScratchDirs
org.apache.hive.service.cli.TestScratchDir.testResourceDirs
org.apache.hive.service.cli.TestScratchDir.testScratchDirs
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/232/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/232/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 20 tests failed
{noformat};;;","11/Aug/14 01:10;vgumashta;Updated patch for review. 

cc [~szehon] [~jdere];;;","11/Aug/14 05:24;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12660898/HIVE-6847.2.patch

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 5883 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_histogram_numeric
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_opt_vectorization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.ql.parse.authorization.TestSessionUserName.testSessionConstructorUser
org.apache.hive.jdbc.TestJdbcWithMiniMr.org.apache.hive.jdbc.TestJdbcWithMiniMr
org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.testAllowedCommands
org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.testAuthorization1
org.apache.hive.service.cli.TestScratchDir.testLocalScratchDirs
org.apache.hive.service.cli.TestScratchDir.testResourceDirs
org.apache.hive.service.cli.TestScratchDir.testScratchDirs
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/247/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/247/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-247/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12660898;;;","11/Aug/14 09:43;vgumashta;I'll update the tests - errors seem related. The TestScratchDir tests are for the older HS2 scratch dir code.;;;","22/Aug/14 07:38;vgumashta;Fixes test failures;;;","22/Aug/14 19:12;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12663616/HIVE-6847.3.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/463/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/463/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-463/

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-hcatalog-it-unit ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-hcatalog-it-unit ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-hcatalog-it-unit ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hcatalog-it-unit ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/tmp/conf
     [copy] Copying 7 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hcatalog-it-unit ---
[INFO] Compiling 6 source files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/test-classes
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/hbase/ManyMiniCluster.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/hbase/ManyMiniCluster.java: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hcatalog-it-unit ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hcatalog-it-unit ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hcatalog-it-unit ---
[INFO] 
[INFO] --- maven-jar-plugin:2.2:test-jar (default) @ hive-hcatalog-it-unit ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-0.14.0-SNAPSHOT-tests.jar
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hcatalog-it-unit ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/0.14.0-SNAPSHOT/hive-hcatalog-it-unit-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/0.14.0-SNAPSHOT/hive-hcatalog-it-unit-0.14.0-SNAPSHOT.pom
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-0.14.0-SNAPSHOT-tests.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/0.14.0-SNAPSHOT/hive-hcatalog-it-unit-0.14.0-SNAPSHOT-tests.jar
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - Testing Utilities 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-util ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/itests/util (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-it-util ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-it-util ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/util/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-it-util ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-it-util ---
[INFO] Compiling 49 source files to /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/classes
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-it-util ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/util/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-it-util ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/tmp/conf
     [copy] Copying 7 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-it-util ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-it-util ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-it-util ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/hive-it-util-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-it-util ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-it-util ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/hive-it-util-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-it-util/0.14.0-SNAPSHOT/hive-it-util-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/util/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-it-util/0.14.0-SNAPSHOT/hive-it-util-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - Unit Tests 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-unit ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-it-unit ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-it-unit ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-it-unit ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-it-unit ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-it-unit ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-it-unit ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/tmp/conf
     [copy] Copying 7 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-metastore-scripts) @ hive-it-unit ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/tmp/scripts/metastore
     [copy] Copying 171 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/tmp/scripts/metastore
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-it-unit ---
[INFO] Compiling 70 source files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/target/test-classes
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hive/beeline/TestBeeLineWithArgs.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hive/beeline/TestBeeLineWithArgs.java: Recompile with -Xlint:deprecation for details.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java: Some input files use unchecked or unsafe operations.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java: Recompile with -Xlint:unchecked for details.
[INFO] 4 warnings 
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestUtilitiesDfs.java:[56,14] cannot find symbol
  symbol:   method createDirsWithPermission(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
  location: class org.apache.hadoop.hive.ql.exec.Utilities
[INFO] 1 error
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive Integration - Parent ......................... SUCCESS [7.366s]
[INFO] Hive Integration - Custom Serde ................... SUCCESS [11.249s]
[INFO] Hive Integration - HCatalog Unit Tests ............ SUCCESS [17.538s]
[INFO] Hive Integration - Testing Utilities .............. SUCCESS [13.056s]
[INFO] Hive Integration - Unit Tests ..................... FAILURE [13.538s]
[INFO] Hive Integration - Test Serde ..................... SKIPPED
[INFO] Hive Integration - QFile Tests .................... SKIPPED
[INFO] Hive Integration - Unit Tests - Hadoop 2 .......... SKIPPED
[INFO] Hive Integration - Unit Tests with miniKdc ........ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1:07.101s
[INFO] Finished at: Fri Aug 22 15:12:30 EDT 2014
[INFO] Final Memory: 60M/160M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hive-it-unit: Compilation failure
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestUtilitiesDfs.java:[56,14] cannot find symbol
[ERROR] symbol:   method createDirsWithPermission(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
[ERROR] location: class org.apache.hadoop.hive.ql.exec.Utilities
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-it-unit
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12663616;;;","23/Aug/14 08:27;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12663837/HIVE-6847.4.patch

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 6114 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_opt_vectorization
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_index_bitmap3
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_index_bitmap_auto
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority
org.apache.hadoop.hive.ql.parse.authorization.TestSessionUserName.testSessionConstructorUser
org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.testAllowedCommands
org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.testAuthorization1
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/475/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/475/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-475/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12663837;;;","25/Aug/14 19:28;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12664179/HIVE-6847.5.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 6114 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby8_noskew
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_opt_vectorization
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_index_bitmap3
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_index_bitmap_auto
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/488/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/488/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-488/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12664179;;;","26/Aug/14 17:58;vgumashta;The q file test pass locally (so does TestHiveServer2.testConnection).;;;","26/Aug/14 22:14;vgumashta;Whitespace cleanup got munged in the last 3 patches. I'll remove those and upload a new patch. ;;;","27/Aug/14 01:17;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12664510/HIVE-6847.6.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/515/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/515/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-515/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]
+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-maven-3.0.5/bin:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-maven-3.0.5/bin:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-TRUNK-Build-515/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'service/src/java/org/apache/hive/service/cli/CLIService.java'
++ egrep -v '^X|^Performing status on external'
++ awk '{print $2}'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit-hadoop2/target itests/hive-minikdc/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/core/target hcatalog/streaming/target hcatalog/server-extensions/target hcatalog/hcatalog-pig-adapter/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target accumulo-handler/target hwi/target common/target common/src/gen service/target service/src/java/org/apache/hive/service/cli/CLIService.java.orig contrib/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1620773.

At revision 1620773.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12664510;;;","02/Sep/14 23:18;vgumashta;[~thejas] V7 addresses review comments. Thanks!;;;","03/Sep/14 01:03;thejas;+1;;;","03/Sep/14 01:41;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12666099/HIVE-6847.8.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/607/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/607/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-607/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]
+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-maven-3.0.5/bin:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-maven-3.0.5/bin:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-TRUNK-Build-607/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnDbUtil.java'
++ egrep -v '^X|^Performing status on external'
++ awk '{print $2}'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit-hadoop2/target itests/hive-minikdc/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/core/target hcatalog/streaming/target hcatalog/server-extensions/target hcatalog/hcatalog-pig-adapter/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target accumulo-handler/target hwi/target common/target common/src/gen contrib/target service/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1622159.

At revision 1622159.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12666099;;;","03/Sep/14 01:50;vgumashta;Patch rebased on trunk.;;;","03/Sep/14 10:17;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12666115/HIVE-6847.9.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6134 tests executed
*Failed tests:*
{noformat}
org.apache.hive.jdbc.TestJdbcWithMiniMr.org.apache.hive.jdbc.TestJdbcWithMiniMr
org.apache.hive.service.TestHS2ImpersonationWithRemoteMS.org.apache.hive.service.TestHS2ImpersonationWithRemoteMS
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/614/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/614/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-614/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12666115;;;","04/Sep/14 02:42;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12666336/HIVE-6847.10.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6138 tests executed
*Failed tests:*
{noformat}
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
org.apache.hive.service.TestHS2ImpersonationWithRemoteMS.org.apache.hive.service.TestHS2ImpersonationWithRemoteMS
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/627/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/627/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-627/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12666336;;;","04/Sep/14 07:38;vgumashta;Addresses test failures.;;;","04/Sep/14 12:31;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12666429/HIVE-6847.11.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6138 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority
org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.testRevokeTimedOutWorkers
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/635/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/635/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-635/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12666429;;;","04/Sep/14 19:33;vgumashta;Just ran both the failed tests locally. None of the TestMinimrCliDriver qtests fail and neither does TestCompactionTxnHandler.;;;","04/Sep/14 20:26;vgumashta;Patch committed to trunk! Thanks [~thejas], [~szehon], [~vkorukanti] and [~jdere] for reviews.;;;","08/Sep/14 02:21;leftyl;Doc note:  This changes the default value and description for *hive.exec.scratchdir*, which is already documented in the wiki (in Configuring Hive as well as Configuration Properties).  It also gives a description for *hive.scratch.dir.permission*, which is not documented in the wiki yet.  No change is made to *hive.downloaded.resources.dir*, it's just removed in one place and added elsewhere.

Here are all the places in the wiki that *hive.exec.scratchdir* is documented:

* [AdminManual Configuration -- Configuring Hive | https://cwiki.apache.org/confluence/display/Hive/AdminManual+Configuration#AdminManualConfiguration-ConfiguringHive]
* [AdminManual Configuration -- Temporary Folders | https://cwiki.apache.org/confluence/display/Hive/AdminManual+Configuration#AdminManualConfiguration-TemporaryFolders]
* [AdminManual Configuration -- Configuration Variables | https://cwiki.apache.org/confluence/display/Hive/AdminManual+Configuration#AdminManualConfiguration-ConfigurationVariables]
* [Configuration Properties -- hive.exec.scratchdir | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.scratchdir]
* example of using hive.exec.scratchdir (2nd bullet):  [Hive CLI -- Examples | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli#LanguageManualCli-Examples];;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
allow safe set commands with sql standard authorization,HIVE-6846,12706734,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,thejas,thejas,thejas,04/Apr/14 23:16,03/Nov/14 00:43,14/Jul/23 06:14,08/Apr/14 17:38,0.13.0,,,,,,,,,0.13.0,,Authorization,,,,0,,,"HIVE-6827 disables all set commands when SQL standard authorization is turned on, but not all set commands are unsafe. We should allow safe set commands.
",,leftyl,rhbutani,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6827,HIVE-8534,HIVE-8660,HIVE-5837,,,,,,,,,,,,,,,,,,,,,"07/Apr/14 11:36;thejas;HIVE-6846.1.patch;https://issues.apache.org/jira/secure/attachment/12638964/HIVE-6846.1.patch","07/Apr/14 18:30;thejas;HIVE-6846.2.patch;https://issues.apache.org/jira/secure/attachment/12639034/HIVE-6846.2.patch","08/Apr/14 00:19;thejas;HIVE-6846.3.patch;https://issues.apache.org/jira/secure/attachment/12639099/HIVE-6846.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385057,,,,Mon Nov 03 00:43:27 UTC 2014,,,,,,,,,,"0|i1uc3b:",385324,,,,,,,,,,,,,,,,,,,,,"07/Apr/14 11:37;thejas;The patch also cleans up setting of various configuration options when sql std auth is used, and moves all that to one place.
;;;","07/Apr/14 11:43;thejas;[~rhbutani] This removes some of the restrictions placed in HIVE-6827. I think this is extremely valuable for usability of sql standard authorization. I think it makes sense to include it for 0.13 release.
;;;","07/Apr/14 13:59;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638964/HIVE-6846.1.patch

{color:red}ERROR:{color} -1 due to 27 failed/errored test(s), 5464 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_parallel_orderby
org.apache.hadoop.hive.ql.parse.authorization.TestSessionUserName.testSessionConstructorUser
org.apache.hadoop.hive.ql.parse.authorization.TestSessionUserName.testSessionDefaultUser
org.apache.hive.beeline.TestBeeLineWithArgs.org.apache.hive.beeline.TestBeeLineWithArgs
org.apache.hive.jdbc.TestJdbcDriver2.org.apache.hive.jdbc.TestJdbcDriver2
org.apache.hive.jdbc.TestJdbcWithMiniHS2.org.apache.hive.jdbc.TestJdbcWithMiniHS2
org.apache.hive.jdbc.TestJdbcWithMiniMr.org.apache.hive.jdbc.TestJdbcWithMiniMr
org.apache.hive.jdbc.TestSSL.testConnectionMismatch
org.apache.hive.jdbc.TestSSL.testInvalidConfig
org.apache.hive.jdbc.TestSSL.testSSLConnectionWithProperty
org.apache.hive.jdbc.TestSSL.testSSLConnectionWithURL
org.apache.hive.jdbc.TestSSL.testSSLFetch
org.apache.hive.jdbc.TestSSL.testSSLFetchHttp
org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testGetVariableValue
org.apache.hive.jdbc.miniHS2.TestMiniHS2.testConfInSession
org.apache.hive.service.auth.TestCustomAuthentication.org.apache.hive.service.auth.TestCustomAuthentication
org.apache.hive.service.auth.TestPlainSaslHelper.testDoAsSetting
org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService
org.apache.hive.service.cli.TestScratchDir.testLocalScratchDirs
org.apache.hive.service.cli.TestScratchDir.testResourceDirs
org.apache.hive.service.cli.TestScratchDir.testScratchDirs
org.apache.hive.service.cli.session.TestSessionHooks.testProxyUser
org.apache.hive.service.cli.session.TestSessionHooks.testSessionHook
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.org.apache.hive.service.cli.thrift.TestThriftHttpCLIService
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2161/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2161/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 27 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638964;;;","07/Apr/14 14:20;rhbutani;+1 for 0.13;;;","07/Apr/14 18:30;thejas;Fixing tests failures, added another jdbc test.
;;;","07/Apr/14 18:34;leftyl;What documentation does this need?;;;","07/Apr/14 21:23;thejas;I will add this to overall sql standard authorization document. I will work on that in a day or two.
;;;","07/Apr/14 22:11;ashutoshc;+1;;;","07/Apr/14 22:50;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639034/HIVE-6846.2.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5552 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.ql.parse.authorization.TestSessionUserName.testSessionConstructorUser
org.apache.hadoop.hive.ql.parse.authorization.TestSessionUserName.testSessionDefaultUser
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2165/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2165/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639034;;;","08/Apr/14 00:19;thejas;HIVE-6846.3.patch - test only changes. I have verified that the tests pass.
;;;","08/Apr/14 15:17;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639099/HIVE-6846.3.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5553 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2174/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2174/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639099;;;","08/Apr/14 17:38;ashutoshc;Committed to 0.13 & trunk.;;;","08/May/14 02:30;sushanth;FYI, on testing 0.13.0 RC0, this patch seemed to be the one git bisect identifies as causing a couple of test failures.
The test failures are as follows:

org.apache.hive.service.cli.TestScratchDir.testLocalScratchDirs

The logs for the git bisect run for this bug are available over at http://people.apache.org/~khorgath/releases/0.13.1_RC0/test_failures/HIVE-6846.bisect/

The .sh files there were the scripts used to test hive for the bugs in question. This issue is also present in the 0.13.0 released branch.;;;","08/May/14 21:04;thejas;That failure does not indicate a product problem. In fact there is no reason to set local scratch dirs to 777 . That change was part of  HIVE-5486. The idea is that in HS2, with doAs enabled, the files/subdirs under scratch dir happens as the end user. But in case of local file system, this is not true, all file creation happens as the HS2 server user.
There are some changes that Vaibhav and Vikram have been working on to create the base scratch dir as the actual user running the query. That will help address this issue. The test issue is already not there in trunk.

I don't think this should block 0.13.1 release.
;;;","08/May/14 21:21;sushanth;Awesome, good to hear. I will not consider this a blocker for 0.13.1.

Thanks!;;;","12/May/14 18:42;thejas;This is the default list of safe set command that this patch allows :

hive.exec.reducers.bytes.per.reducer
hive.exec.reducers.max
hive.map.aggr
hive.map.aggr.hash.percentmemory
hive.map.aggr.hash.force.flush.memory.threshold
hive.map.aggr.hash.min.reduction
hive.groupby.skewindata
hive.optimize.multigroupby.common.distincts
hive.optimize.index.groupby
hive.optimize.ppd
hive.optimize.ppd.storage
hive.optimize.ppd.storage
hive.ppd.recognizetransivity
hive.optimize.groupby
hive.optimize.sort.dynamic.partition
hive.optimize.skewjoin.compiletime
hive.optimize.union.remove
hive.multigroupby.singlereducer
hive.map.groupby.sorted
hive.map.groupby.sorted.testmode
hive.optimize.skewjoin
hive.optimize.skewjoin.compiletime
hive.mapred.mode
hive.enforce.bucketmapjoin
hive.exec.compress.output
hive.exec.compress.intermediate
hive.exec.parallel
hive.exec.parallel.thread.number
hive.exec.parallel.thread.number
hive.exec.rowoffset
hive.merge.mapfiles
hive.merge.mapredfiles
hive.merge.tezfiles
hive.ignore.mapjoin.hint
hive.auto.convert.join
hive.auto.convert.join.noconditionaltask
hive.auto.convert.join.noconditionaltask.size
hive.auto.convert.join.use.nonstaged
hive.auto.convert.join.noconditionaltask
hive.auto.convert.join.noconditionaltask.size
hive.auto.convert.join.use.nonstaged
hive.enforce.bucketing
hive.enforce.sorting
hive.enforce.sortmergebucketmapjoin
hive.auto.convert.sortmerge.join
hive.execution.engine
hive.vectorized.execution.enabled
hive.mapjoin.optimized.keys
hive.mapjoin.lazy.hashtable
hive.exec.check.crossproducts
hive.compat
hive.exec.dynamic.partition.mode
mapred.reduce.tasks
mapred.output.compression.codec
mapred.map.output.compression.codec
mapreduce.job.reduce.slowstart.completedmaps
mapreduce.job.queuename
;;;","09/Jul/14 08:07;leftyl;This adds *hive.security.authorization.sqlstd.confwhitelist* with a description in the HiveConf.java comment but not in hive-default.xml.template.

It's documented in the wiki here (please review, I made the assumption that it's a comma-separated list):

* [Configuration Properties -- hive.security.authorization.sqlstd.confwhitelist | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.security.authorization.sqlstd.confwhitelist]

with references from two places:

* [SQL Standard Based Hive Authorization -- Restrictions on Hive Commands and Statements | https://cwiki.apache.org/confluence/display/Hive/SQL+Standard+Based+Hive+Authorization#SQLStandardBasedHiveAuthorization-RestrictionsonHiveCommandsandStatements]
* [Configuration Properties -- Restricted List and Whitelist | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-RestrictedListandWhitelist]

I added a comment to HIVE-6586 so the parameter description won't get lost in the shuffle when HIVE-6037 changes HiveConf.java.;;;","03/Nov/14 00:43;leftyl;See HIVE-8534 and HIVE-8660 for updates in release 0.14.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestJdbcDriver.testShowRoleGrant can fail if TestJdbcDriver/TestJdbcDriver2 run together,HIVE-6845,12706722,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,04/Apr/14 22:22,08/Apr/14 18:41,14/Jul/23 06:14,08/Apr/14 18:41,,,,,,,,,,0.13.0,,Tests,,,,0,,,"Running both TestJdbcDriver/TestJdbcDriver2 together in the same run gives an error in testShowRoleGrant() because both tests create the role ""role1"".  When the 2nd test tries to create the role it fails:

{noformat}
testShowRoleGrant(org.apache.hive.jdbc.TestJdbcDriver2)  Time elapsed: 1.801 sec  <<< ERROR!
java.sql.SQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
	at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:275)
	at org.apache.hive.jdbc.TestJdbcDriver2.testShowRoleGrant(TestJdbcDriver2.java:2000)
{noformat}
",,jdere,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/14 22:34;jdere;HIVE-6845.1.patch;https://issues.apache.org/jira/secure/attachment/12638787/HIVE-6845.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385045,,,,Tue Apr 08 18:41:18 UTC 2014,,,,,,,,,,"0|i1uc0n:",385312,,,,,,,,,,,,,,,,,,,,,"04/Apr/14 22:34;jdere;Patch to drop the role at the start of testShowRoleGrant(), before creating the role as part of the test.;;;","06/Apr/14 03:38;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638787/HIVE-6845.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5547 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2140/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2140/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638787;;;","06/Apr/14 04:16;thejas;By run together, I assume you mean run in same setup/jvm (as against being run simultaneously). 

+1 
;;;","08/Apr/14 18:41;ashutoshc;Committed to 0.13 & trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
INSTR for UTF-8 returns incorrect position,HIVE-6843,12706697,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,szehon,ckran,ckran,04/Apr/14 20:09,13/Nov/14 19:42,14/Jul/23 06:14,24/Apr/14 01:09,0.11.0,0.12.0,,,,,,,,0.14.0,,UDF,,,,0,,,,,ckran,jdere,szehon,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/14 00:07;szehon;HIVE-6843.2.patch;https://issues.apache.org/jira/secure/attachment/12640922/HIVE-6843.2.patch","07/Apr/14 20:16;szehon;HIVE-6843.patch;https://issues.apache.org/jira/secure/attachment/12639052/HIVE-6843.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,385020,,,,Thu Nov 13 19:42:07 UTC 2014,,,,,,,,,,"0|i1ubv3:",385287,,,,,,,,,,,,,,,,,,,,,"04/Apr/14 20:12;ckran;Using the INSTR fuction to find the posistion of a substring for a UTF-8 returns zero

select INSTR (‘НАСТРОЕние’, ‘P’) from foo-bar 
;;;","06/Apr/14 22:24;szehon;Hi, I was going to look at this , but when I tried it looks like that your first P is a Cyrillic P (d0,a0), while the second is a English P (50).  Can you verify?  If you make the second a Cyrlilic P, than it works.;;;","07/Apr/14 16:02;ckran;Sorry, copy/paste got me. They _look_ the same. And sorry about the curly quotes, I don't know where they came from. 

The real issue is that for UTF-8 INSTR returns the position in bytes instead of characters. So this reutrns a 9 where by my count it should be a 5. Thank you for your support. 

select INSTR ('НАСТРОЕние', 'Р') from .... ;;;","07/Apr/14 20:16;szehon;This seems to work, lets see what folks think.

Original code was trying to avoid encoding the bytes and just doing byte-counting, but not sure if that is possible when doing unicode char calculations.;;;","08/Apr/14 05:04;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639052/HIVE-6843.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5549 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2170/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2170/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639052;;;","17/Apr/14 23:36;jdere;Should this also work for unicode characters which require more than one Java character? If you add these checks to TestGenericUDFUtils, the 2nd check fails:
{code}
    Assert.assertEquals(3, GenericUDFUtils.findText(new Text(""123\uD801\uDC00456""), new Text(""\uD801\uDC00""), 0));
    Assert.assertEquals(4, GenericUDFUtils.findText(new Text(""123\uD801\uDC00456""), new Text(""4""), 0));
{code}

This would require using String.codePointCount() on the indexOf() result.;;;","18/Apr/14 02:55;szehon;Thanks for the review.  As I understand, you are passing in a string literal to Text constructor, so it is not interpreting \uD801 as one char, so there is actually 5 chars there: '\', 'u', 'D', '8', '0', '1'.

I tried the following test and it seemed to work:

    char[] chararray = new char[] {'1', '2', '3', '\uD801', '\uDC00', '4', '5', '6'};
    String str = new String(chararray);
    Assert.assertEquals(5, GenericUDFUtils.findText(new Text(str), new Text(""4""), 0));

I guess the second check was supposed to be 5, not 4.;;;","18/Apr/14 08:21;jdere;The string literal does interpret ""\uD801"" as a single character, and ""\uD801\uDC00"" as a single code point (got the example character from http://www.oracle.com/technetwork/articles/javase/supplementary-142654.html):

{noformat}
    String str1 = ""123\uD801\uDC00456"";
    System.out.println(""str1 length="" + str1.length() + "", codePointCount="" + str1.codePointCount(0, str1.length()));

str1 length=8, codePointCount=7
{noformat}

So if we count things by unicode code points, the ""4"" would be at index 4 (for 0-based index).
;;;","18/Apr/14 08:22;jdere;From that link, ""\uD801\uDC00"" would be the representation for U+10400;;;","19/Apr/14 00:07;szehon;Sorry Jason I missed that.  Made the fix to handle supplementary characters (surrogate pairs), and updated the review board.;;;","19/Apr/14 00:27;jdere;I think it looks good, +1;;;","19/Apr/14 05:30;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12640922/HIVE-6843.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5406 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/precommit-hive/25/testReport
Console output: http://bigtop01.cloudera.org:8080/job/precommit-hive/25/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12640922;;;","24/Apr/14 01:09;jdere;Committed to trunk, thanks Szehon!;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vectorized execution throws NPE for partitioning columns with __HIVE_DEFAULT_PARTITION__,HIVE-6841,12706572,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,jnp,jnp,jnp,04/Apr/14 06:38,07/Apr/14 18:49,14/Jul/23 06:14,07/Apr/14 18:49,0.13.0,,,,,,,,,0.13.0,,,,,,0,,,"If partitioning columns have __HIVE_DEFAULT_PARTITION__ or null, vectorized execution throws NPE.",,hsubramaniyan,jnp,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/14 07:50;jnp;HIVE-6841.1.patch;https://issues.apache.org/jira/secure/attachment/12638646/HIVE-6841.1.patch","04/Apr/14 20:24;jnp;HIVE-6841.2.patch;https://issues.apache.org/jira/secure/attachment/12638754/HIVE-6841.2.patch","06/Apr/14 04:50;jnp;HIVE-6841.3.patch;https://issues.apache.org/jira/secure/attachment/12638899/HIVE-6841.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384895,,,,Mon Apr 07 18:49:59 UTC 2014,,,,,,,,,,"0|i1ub3b:",385162,,,,,,,,,,,,,,,,,,,,,"04/Apr/14 20:24;jnp;Updated patch maintains existing behavior for partitions on string columns.;;;","04/Apr/14 20:32;hsubramaniyan;+1
The projection of (ctinyint=__HIVE_DEFAULT_PARTITION__) to NULL  should be documented.;;;","04/Apr/14 21:16;ashutoshc;+1;;;","05/Apr/14 21:41;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638754/HIVE-6841.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5548 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_non_string_partition
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2131/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2131/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638754;;;","06/Apr/14 04:50;jnp;The test 'vector_non_string_partition.q' passes with hadoop-2, it failed because of different ordering in hadoop1. Updated patch adds order by clause in queries to produce same results in hadoop-1 and hadoop-2.;;;","07/Apr/14 01:07;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638899/HIVE-6841.3.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5549 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_dyn_part
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2157/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2157/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638899;;;","07/Apr/14 18:29;jnp;[~rhbutani] This is a critical issue in hive-0.13 and fails many queries on partitioned tables in vectorized execution. It should be fixed in branch-0.13 as well.;;;","07/Apr/14 18:45;rhbutani;+1 for 0.13;;;","07/Apr/14 18:49;jnp;The failed tests are not related to the patch and passed when run locally.

Committed to trunk and branch-0.13.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use Unordered Output for Bucket Map Joins on Tez,HIVE-6840,12706565,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sseth,sseth,sseth,04/Apr/14 06:00,06/Apr/14 23:10,14/Jul/23 06:14,06/Apr/14 23:10,,,,,,,,,,0.13.0,,,,,,0,,,"Tez 0.4 adds a placeholder UnorderedOutput. Once Hive is changed to use 0.4, it should be possible to make use of this.",,hagleitn,sseth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/14 06:01;sseth;HIVE-6840.1.patch;https://issues.apache.org/jira/secure/attachment/12638632/HIVE-6840.1.patch","05/Apr/14 10:53;sseth;HIVE-6840.2.patch;https://issues.apache.org/jira/secure/attachment/12638856/HIVE-6840.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384888,,,,Sun Apr 06 23:10:54 UTC 2014,,,,,,,,,,"0|i1ub1r:",385155,,,,,,,,,,,,,,,,,,,,,"04/Apr/14 06:01;sseth;Simple patch. Requires tez 0.4 dependency though, so will not compile at the moment.
[~vikram.dixit], [~hagleitn] - could you please review. Thanks;;;","04/Apr/14 15:51;hagleitn;+1 LGTM. Can you switch the pom to 0.4.0 in the same patch? Since it's needed for the patch.;;;","05/Apr/14 10:53;sseth;Updated patch with tez version changed to 0.4;;;","06/Apr/14 13:28;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638856/HIVE-6840.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5547 tests executed
*Failed tests:*
{noformat}
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2149/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2149/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638856;;;","06/Apr/14 20:54;hagleitn;Failure is unrelated.;;;","06/Apr/14 23:10;hagleitn;Committed to branch and trunk. Thanks [~sseth]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"q.out files need correction for stats properties - sample8,transform_ppr1,transform_ppr2,union_ppr",HIVE-6838,12706561,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,thejas,thejas,thejas,04/Apr/14 05:13,20/May/14 18:32,14/Jul/23 06:14,04/Apr/14 07:16,0.13.0,,,,,,,,,0.13.0,,Tests,,,,0,,,"HIVE-6808 updated stats information q.out of the following test files, causing failures -
sample8,transform_ppr1,transform_ppr2,union_ppr .
",,jnp,navis,prasanth_j,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6808,,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/14 05:18;thejas;HIVE-6838.1.patch;https://issues.apache.org/jira/secure/attachment/12638628/HIVE-6838.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384884,,,,Tue May 20 18:32:36 UTC 2014,,,,,,,,,,"0|i1ub0v:",385151,,,,,,,,,,,,,,,,,,,,,"04/Apr/14 05:29;thejas;Verified that the failed tests pass with this.
{code}
  <testcase name=""testCliDriver_sample8"" classname=""org.apache.hadoop.hive.cli.TestCliDriver"" time=""17.743""/>
  <testcase name=""testCliDriver_transform_ppr1"" classname=""org.apache.hadoop.hive.cli.TestCliDriver"" time=""15.931""/>
  <testcase name=""testCliDriver_transform_ppr2"" classname=""org.apache.hadoop.hive.cli.TestCliDriver"" time=""5.427""/>
  <testcase name=""testCliDriver_union_ppr"" classname=""org.apache.hadoop.hive.cli.TestCliDriver"" time=""5.617""/>
  <testcase name=""testCliDriver_shutdown"" classname=""org.apache.hadoop.hive.cli.TestCliDriver"" time=""6.063""/>
{code};;;","04/Apr/14 07:13;jnp;+1;;;","04/Apr/14 07:14;thejas;Committing it shortly to avoid the false alarms in precommit tests.

;;;","20/May/14 02:13;navis;[~thejas] Is precommit test runs on hadoop-2? Newly updated are all wrong in hadoop-1 and the result of hadoop-1 seemed right.;;;","20/May/14 18:02;thejas;[~navis] I should have generated this against hadoop-1. When you say wrong, do you mean the results are what were there before this patch or something different. 
[~prasanth_j] - I believe there is some difference in the stats results in some OS, is that right ?

;;;","20/May/14 18:25;prasanth_j;The diff in the attached patch doesn't seems to related to OS specific. OS specific diffs are mostly related to total file size.;;;","20/May/14 18:32;thejas;[~navis] Can you upload the diffs you see with hadoop-1 ? The switch to hadoop-2 in pre-commit tests was made only recently after 0.13 release. AFAIK, the precommit tests didn't show up failures for these testes with hadoop-1 after this was committed.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2 thrift/http mode & binary mode proxy user check fails reporting IP null for client,HIVE-6837,12706509,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vgumashta,darumugam,darumugam,03/Apr/14 21:36,07/Apr/14 21:21,14/Jul/23 06:14,07/Apr/14 21:21,0.13.0,,,,,,,,,0.13.0,,HiveServer2,,,,0,,,"Hive Server running thrift/http with Kerberos security.

Kinited user knox attempting to proxy as sam.

Beeline connection failed reporting error on hive server logs:
Caused by: org.apache.hadoop.security.authorize.AuthorizationException: Unauthorized connection for super-user: knox from IP null
",,darumugam,rhbutani,taksaito,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6738,,,,,,,,,,,,,,,,,HIVE-6857,,,,,,,,,,,,,,,,,,,,,"05/Apr/14 05:37;vgumashta;HIVE-6837.1.patch;https://issues.apache.org/jira/secure/attachment/12638843/HIVE-6837.1.patch","05/Apr/14 20:19;vgumashta;HIVE-6837.2.patch;https://issues.apache.org/jira/secure/attachment/12638880/HIVE-6837.2.patch","06/Apr/14 12:10;vgumashta;HIVE-6837.3.patch;https://issues.apache.org/jira/secure/attachment/12638909/HIVE-6837.3.patch","04/Apr/14 01:52;taksaito;hive.log;https://issues.apache.org/jira/secure/attachment/12638612/hive.log",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384832,,,,Mon Apr 07 21:21:36 UTC 2014,,,,,,,,,,"0|i1uapj:",385099,,,,,,,,,,,,,,,,,,,,,"04/Apr/14 01:51;taksaito;I see the same error when the following is specified in hive-site.xml:
<property>
<name>hadoop.proxyuser.hrt_qa.hosts</name>
<value>myhostname</value>
</property>
Workaround is to use * instead of myhostname.

The hive.log is attached.;;;","04/Apr/14 06:26;vgumashta;[~taksaito] Thanks for the log and additional testing info Tak!

This is caused by threadlocals lying all over the place. There are 2 set of threadlocals for storing ip address, usernames (one in TSetIpAddressProcessor and the other in HadoopThriftAuthBridge20S.Server). I think it makes sense to access those through one interface, otherwise similar problems will pop up again. ;;;","04/Apr/14 06:28;vgumashta;[~rhbutani] [~thejas] This is a bug for 13. I'll have a patch ready by tomorrow.;;;","05/Apr/14 05:37;vgumashta;[~thejas] Once HIVE-6738 is in, I can submit an RB request. Without this patch, ip address checks will fail for trusted proxy user (in both binary & http mode) unless the value is set to *. I've tested the fix on a secure cluster in both modes. Thanks!;;;","05/Apr/14 20:19;vgumashta;Patch based on trunk;;;","06/Apr/14 17:23;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638909/HIVE-6837.3.patch

{color:green}SUCCESS:{color} +1 5548 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2152/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2152/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638909;;;","06/Apr/14 19:39;thejas;+1

Vaibhav,
I remember you had suggested that TSetIpAddressProcessor also use SessionManager thread local, in a different jira. I think that makes sense, it would be cleaner. We should do that in a followup jira.
;;;","07/Apr/14 19:06;vgumashta;[~thejas] Thanks for taking a look.

Sure, I'll do that. There's another issue that I noticed caused in SessionManager#openSession as a result of this:
{code}
public SessionHandle openSession(TProtocolVersion protocol, String username, String password,
      Map<String, String> sessionConf, boolean withImpersonation, String delegationToken)
          throws HiveSQLException {
    HiveSession session;
    if (withImpersonation) {
      HiveSessionImplwithUGI hiveSessionUgi = new HiveSessionImplwithUGI(protocol, username, password,
        hiveConf, sessionConf, TSetIpAddressProcessor.getUserIpAddress(), delegationToken);
      session = HiveSessionProxy.getProxy(hiveSessionUgi, hiveSessionUgi.getSessionUgi());
      hiveSessionUgi.setProxySession(session);
    } else {
      session = new HiveSessionImpl(protocol, username, password, hiveConf, sessionConf,
          TSetIpAddressProcessor.getUserIpAddress());
    }
    session.setSessionManager(this);
    session.setOperationManager(operationManager);
    session.open();
    handleToSession.put(session.getSessionHandle(), session);

    try {
      executeSessionHooks(session);
    } catch (Exception e) {
      throw new HiveSQLException(""Failed to execute session hooks"", e);
    }
    return session.getSessionHandle();
  }
{code}

Notice that if withImpersonation is set to true, we're using TSetIpAddressProcessor.getUserIpAddress() to get the IP address which is wrong for a kerberized setup (should use HiveAuthFactory#getIpAddress).

Also, in case of a kerberized setup, we're wrapping the transport in a doAs (with UGI of the HiveServer2 process) which doesn't make sense to me: https://github.com/apache/hive/blob/trunk/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java#L335. ;;;","07/Apr/14 20:31;rhbutani;+1 for 0.13;;;","07/Apr/14 21:21;thejas;Patch committed to 0.13 branch and trunk. I made a minor edit to apply on 0.13 branch.
Thanks for the contribution Vaibhav. Thanks for the review Dilli.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading of partitioned Avro data fails if partition schema does not match table schema,HIVE-6835,12706464,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,erwaman,erwaman,erwaman,03/Apr/14 17:09,13/Nov/14 19:43,14/Jul/23 06:14,25/Apr/14 20:57,0.12.0,,,,,,,,,0.14.0,,,,,,0,,,"To reproduce:
{code}
create table testarray (a array<string>);

load data local inpath '/home/ahsu/test/array.txt' into table testarray;

# create partitioned Avro table with one array column
create table avroarray partitioned by (y string) row format serde 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' with serdeproperties ('avro.schema.literal'='{""namespace"":""test"",""name"":""avroarray"",""type"": ""record"", ""fields"": [ { ""name"":""a"", ""type"":{""type"":""array"",""items"":""string""} } ] }')  STORED as INPUTFORMAT  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'  OUTPUTFORMAT  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat';

insert into table avroarray partition(y=1) select * from testarray;

# add an int column with a default value of 0
alter table avroarray set serde 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' with serdeproperties('avro.schema.literal'='{""namespace"":""test"",""name"":""avroarray"",""type"": ""record"", ""fields"": [ {""name"":""intfield"",""type"":""int"",""default"":0},{ ""name"":""a"", ""type"":{""type"":""array"",""items"":""string""} } ] }');

# fails with ClassCastException
select * from avroarray;
{code}
The select * fails with:
{code}
Failed with exception java.io.IOException:java.lang.ClassCastException: org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector
{code}",,cwsteinbach,davidzchen,erwaman,mdominguez@cloudera.com,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-3953,HIVE-6131,,HIVE-8560,,,,,,,,,,,,,,,,,,,,,"07/Apr/14 19:19;erwaman;HIVE-6835.1.patch;https://issues.apache.org/jira/secure/attachment/12639043/HIVE-6835.1.patch","16/Apr/14 16:25;erwaman;HIVE-6835.2.patch;https://issues.apache.org/jira/secure/attachment/12640487/HIVE-6835.2.patch","17/Apr/14 01:16;erwaman;HIVE-6835.3.patch;https://issues.apache.org/jira/secure/attachment/12640573/HIVE-6835.3.patch","24/Apr/14 01:18;erwaman;HIVE-6835.4.patch;https://issues.apache.org/jira/secure/attachment/12641627/HIVE-6835.4.patch","24/Apr/14 17:44;erwaman;HIVE-6835.5.patch;https://issues.apache.org/jira/secure/attachment/12641763/HIVE-6835.5.patch",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384787,,,,Thu Nov 13 19:43:15 UTC 2014,,,,,,,,,,"0|i1uafj:",385054,,,,,,,,,,,,,,,,,,,,,"03/Apr/14 17:13;erwaman;Right now, when AvroSerDe.initialize() is called, the Properties it is passed include both table and partition properties, with the partition properties *overriding* the table properties.  The AvroSerDe needs the *latest* schema (which should be stored in the table properties) for proper initialization and to prevent the ClassCastException.  My proposal is to pass both the table and partition properties to SerDe.initialize() by prepending the table properties with ""table."", and let the SerDe decide which set of properties to use.

BTW, here's the full stack trace when you do the select *:
{code}
Failed with exception java.io.IOException:java.lang.ClassCastException: org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector
14/04/03 10:11:02 ERROR CliDriver: Failed with exception java.io.IOException:java.lang.ClassCastException: org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector
java.io.IOException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector
	at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:551)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:489)
	at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:136)
	at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1471)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:272)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:217)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:414)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:782)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:676)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:160)
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter(ObjectInspectorConverters.java:148)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters$StructConverter.<init>(ObjectInspectorConverters.java:304)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter(ObjectInspectorConverters.java:150)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:407)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:515)
	... 14 more
{code};;;","07/Apr/14 19:19;erwaman;Uploaded a patch with a fix.  Review Board link: https://reviews.apache.org/r/20096/;;;","08/Apr/14 00:52;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639043/HIVE-6835.1.patch

{color:green}SUCCESS:{color} +1 5550 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2167/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2167/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639043;;;","14/Apr/14 05:54;cwsteinbach;[~erwaman]: Please see my comments on reviewboard. Thanks.;;;","14/Apr/14 19:24;erwaman;Thanks for the very thorough code review, [~cwsteinbach].  I've uploaded a new patch that addresses your comments and also updated the Review Board request.;;;","15/Apr/14 06:59;cwsteinbach;+1. Will wait for tests to pass before committing.;;;","16/Apr/14 09:12;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12640124/HIVE-6835.2.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5402 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/precommit-hive/5/testReport
Console output: http://bigtop01.cloudera.org:8080/job/precommit-hive/5/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12640124;;;","16/Apr/14 16:25;erwaman;Reuploading patch version 2 to trigger the tests again.  I ran locally the tests that failed in the last pre-commit build run, and they both passed for me.;;;","16/Apr/14 16:47;ashutoshc;Please don't modify generated file serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde/serdeConstants.java Instead place new constant in serde/if/serde.thrift;;;","17/Apr/14 01:16;erwaman;Thanks for catching this, Ashutosh.  My bad for not noticing I was modifying a generated file.  I have updated my [Review Board request|https://reviews.apache.org/r/20096/] and also uploaded a new patch.;;;","17/Apr/14 02:30;cwsteinbach;[~ashutoshc]: Thanks for catching the Thrift codegen problem.

[~erwaman]: Updated patch looks good. +1;;;","17/Apr/14 16:52;xuefuz;Just curious. If the avro serde is initialized with the table schema (which is the latest), is there a problem for it to read the old data, that is, data that conforms to the partition level metadata? I have seen so many JIRAs about schema evolution, and isn't quite sure what is possible and what is not.

The example given here is adding a new column in the beginning. What about other cases, such as adding it at the end, or changing data type, etc?;;;","17/Apr/14 17:29;ashutoshc;I would also like to know answer for Xuefu's questions. It will be good to document what kind of schema evolution is supported by Avro Serde and more importantly what kinds are *not* supported.;;;","17/Apr/14 19:00;erwaman;The AvroSerDe handles schema evolution as described in http://avro.apache.org/docs/current/spec.html#Schema+Resolution.  However, in the Hive code, the AvroSerDe needs to always be initialized with the latest schema so that ObjectInspectorConverters.getConvertedOI() (in FetchOperator:getRecordReader()) will work.  When the AvroSerDe actually reads the Avro file, it will then compare the latest schema to the actual schema stored in the Avro file and do schema resolution/evolution.;;;","17/Apr/14 19:59;xuefuz;{quote}
 in the Hive code, the AvroSerDe needs to always be initialized with the latest schema so that ObjectInspectorConverters.getConvertedOI() (in FetchOperator:getRecordReader()) will work.
{quote}

[~erwaman] I guess I don't quite follow this. The exception stack shows that casting error happens when reading old data with partition schema which is old schema. If the schema matches the data, I'm not sure why we'd have this casting error? On the other hand, if we use the new schema and read old data, would it be possible that error might arise?

Anyway, I'm not fully understanding the real cause of the problem and how the change will address all other possible scenarios.;;;","18/Apr/14 00:22;erwaman;What happens is Hive tries to build ObjectInspectorConverters from the partition schema to the table schema.  If the partition schema is different from the table schema, you may get a ClassCastException like above.

When you add new columns at the end, this is not a problem because these new columns are chopped off.  See ObjectInspectorConverters:StructConverter:
{code}
int minFields = Math.min(inputFields.size(), outputFields.size());
fieldConverters = new ArrayList<Converter>(minFields);
{code}
It's only when you insert new columns at the beginning or in the middle that you might run into ClassCastExceptions.

For the AvroSerDe, if it always uses the latest schema (which should be the table-level schema), Hive will not get confused when constructing its ObjectInspectorConverters.  Then, later, when the AvroSerDe actually goes to read the Avro files, it can compare the latest schema with the (possibly old) schemas stored in the Avro data files themselves, and do the proper schema resolution, omitting fields or substituting default values, following the [schema resolution rules|http://avro.apache.org/docs/current/spec.html#Schema+Resolution].;;;","18/Apr/14 01:00;erwaman;On a side note: If you create an Avro table and store the schema in the TBLPROPERTIES -
{code}
CREATE TABLE ... TBLPROPERTIES ('avro.schema.literal'='...');
{code}
\- everything works fine with partitions because TBLPROPERTIES are NOT copied to the partition, so the partition will end using the TBLPROPERTIES for initializing the Avro SerDe.

It's only when you store the schema in the SERDEPROPERTIES -
{code}
CREATE TABLE ... WITH SERDEPROPERTIES ('avro.schema.literal'='...');
{code}
\- that problems arise.  SERDEPROPERTIES DO get copied to the partitions, so if you then end up changing the SERDEPROPERTIES stored at the table level, the SERDEPROPERTIES in the table and the partitions get out of sync and this sometimes leads to ClassCastExceptions with the AvroSerDe.;;;","18/Apr/14 02:39;xuefuz;[~erwaman] Thanks for the explanation. Now I see where the problem is. SERDEPROPERTIES and TBLPROPERTIES are for different purpose. I'm curious why user would put avro.schema.literal in the serde properties, as this is table specific and it should be put in TBLPROPERTIES. SERDEPROPERTIES, on the other hand, is used to control serde behavior (plugin level instead of table level), such as field delimiter which doesn't necessary vary from table to table. If you check AvroSerde documentation, schema is specified in TBLPROPERTIES. https://cwiki.apache.org/confluence/display/Hive/AvroSerDe. Thus, it seems that this fix is for an invalid use case. What's your thought on this?;;;","18/Apr/14 16:12;erwaman;I'm guessing the schema was specified in the SERDEPROPERTIES to work around HIVE-3953.  However, one issue with storing the schema in TBLPROPERTIES instead is that for partitioned tables, when you do a {{describe \[extended] <table_name> partition(...);}}, you get
{code}
error_error_error_error_error_error_error	string              	from deserializer   
cannot_determine_schema	string              	from deserializer   
check               	string              	from deserializer   
schema              	string              	from deserializer   
url                 	string              	from deserializer   
and                 	string              	from deserializer   
literal             	string              	from deserializer
{code}
because the AvroSerDe cannot find ""avro.schema.literal"" or ""avro.schema.url"".  If you store the schema in SERDEPROPERTIES, you don't get this issue, since the SERDEPROPERTIES get copied to the partition when it is created.

I do think it is useful to make both the table-level properties and the partition-level properties available separately to the SerDe when it's doing its .initalize().  The SerDe should be able to decide which set of properties it wants to use. From this point of view, I think my change is still useful and valid.;;;","18/Apr/14 17:43;xuefuz; If TBLPROPERTIES are NOT copied to the partition, We should probably fix that problem instead. In Hive, partition holds a snapshot of the table schema when the partition is created. This should be applicable to AvroSerde as well.

Making table properties and partition properties available for a serde seems a good idea to me in general. However, what I questioned is the way the problem in AvroSerde is fixed. Especially, we don't want to fix the problem in a workaround solution while avoiding the original problem.;;;","18/Apr/14 18:31;erwaman;If TBLPROPERTIES were copied to the partition, then you still might have the problem of the table-level Avro schema and the partition-level Avro schema getting out of sync, which might lead to ClassCastExceptions.  The Avro schema should always use the latest table-level schema, whether it is stored in TBLPROPERTIES or SERDEPROPERTIES.

The root of the problem is if an Avro schema somehow ends up in the partition properties, these could get out of sync with the table-level properties.  The Avro SerDe should always be using the table-level schema, and that's why my change was to (1) make the table-level properties available to the serde, and (2) change the Avro SerDe to use the table-level properties when present.;;;","18/Apr/14 19:07;xuefuz;Points taken. However, I'm a little concerned that the solution here is a little bit hacky, as a serde might have a property prefixed by ""table."". In that case, this may mess up. Instead, I'm thinking if it makes more sense to define a new serde initialize API, and deprecate the old one.
{code}
public void initialize(Configuration configuration, Properties tableProperties, Properties partitionProperties) throws SerDeException;
{code}
With that, a serde is free to do whatever they need .
;;;","18/Apr/14 20:52;ashutoshc;I agree with [~xuefuz]. Also, now that we have AbstractSerDe (which is an abstract class as oppose to interface Serde) adding new methods is easier from backward-compatibility point of view. Polluting one map object from various sources sounds like inviting trouble to me.;;;","19/Apr/14 00:53;erwaman;[~xuefuz] and [~ashutoshc], just to clarify, is this the alternative solution you're proposing?:
# Add
{code}
public void initialize(Configuration configuration, Properties tableProperties, Properties partitionProperties) throws SerDeException;
{code}
to AbstractSerDe and provide a default implementation that just calls {{initialize(configuration, partitionProperties)}}
# Change all calls of {{partitionSerde.initialize(conf, partProps)}} to {{partitionSerde.initialize(conf, tblProps, partProps)}}
# Add
{code}
@Override
public void initialize(Configuration configuration, Properties tableProperties, Properties partitionProperties) throws SerDeException;
{code}
to AvroSerDe and provide an implementation that just uses the tableProperties

I am okay with taking this approach, though it involves a lot more code changes and will change the public AbstractSerDe API.  Let me know what your thoughts on this approach are.;;;","21/Apr/14 18:21;xuefuz;I think that's pretty much what you need to do. While #2 may touch many files, it's fairly safe as #1 guarantees that the same code will be exercised. There isn't much API change. You add one with default implementation and deprecate the old one. In #3, you have both property sets and do whatever you need for Avro.;;;","21/Apr/14 22:53;erwaman;Great, sounds like we're on the same page. I'll implement this new approach and upload a new patch soon.;;;","22/Apr/14 20:17;erwaman;I started looking into this alternative and encountered an issue.  Most calls to serde.initialize() are treating serde as a Deserializer (interface).  I would either have to change the interface (and change all the implementations) or cast the Deserializer as an AbstractSerDe (whenever I want to use the new initialize() method), neither of which seems like a great solution. So I am back to supporting my original ""table."" prefix approach. Any thoughts on this?;;;","22/Apr/14 23:29;xuefuz;Not sure if I understand your problem correctly, but I do understand that the scope of the proposed change has got bigger than your original approach. For any caller of serde initialization, we should be able to find whether serde instance extends AbstractSerde. If so, we cast the serde instance to AbstractSerde and call initialize(arg1, arg2, arg3). Otherwise, call serde.initialize(arg1, arg2). Does this solve the problem?;;;","23/Apr/14 01:24;erwaman;Yes, this is possible, but I would have to add these ""instanceof AbstractSerde"" checks and then cast the Deserializer as an AbstractSerde before I can use the new initialize() method.  There are dozens of usages of .initialize() and adding all this type checking/casting code in so many places just for this new method doesn't seem very clean to me.

Also, if we add the new initialize() method, what should we do for table-level serde initialization?  When dealing with the table, there are no partition properties, so are we supposed to pass the table properties for both the tblProps and partProps arguments? If we leave partProps null, then the default new initialize() method implementation will just pass null to the old initialize() method.

There doesn't seem to be a very clean way of adding a new initialize() method without creating a lot of redundant boilerplate code and creating confusion which initialize() method to use and what values to pass in.  Given these concerns, I feel that prepending ""table."" might be a cleaner and less confusing approach.  What are your thoughts on this?;;;","23/Apr/14 01:38;xuefuz;bq. I feel that prepending ""table."" might be a cleaner and less confusing approach. What are your thoughts on this?

This doesn't seem to be a viable approach due to its hacky/problematic nature.

bq.  many places just for this new method doesn't seem very clean to me.

You could have a utility method somewhere so that you need to call instanceof only once. Something like this:

{code}
  public static void initializeSerde(SerDe serde, Properties tblProps, Properties partProps) {
    if (serde instanceof AbstractSerde) {
       ...
    } else {
      ...
    }
  }
{code}
Then, each caller just needs to switch to this method.

bq.  If we leave partProps null, then the default new initialize() method implementation will just pass null to the old initialize() method.

This sounds good to me.;;;","24/Apr/14 01:18;erwaman;Thanks for the suggestions and clarification, [~xuefuz].  I have uploaded a new patch (HIVE-6835.4.patch) using the new approach.;;;","24/Apr/14 01:25;erwaman;P.S.: I also updated my Review Board request: https://reviews.apache.org/r/20096/;;;","24/Apr/14 03:38;xuefuz;Thanks for the patch, which looks good. I have some comments on RB.;;;","24/Apr/14 17:44;erwaman;Uploaded a new patch addressing [~xuefuz]'s comments.  I removed the getOverlayedProperties() from PartitionDesc and added a new createOverlayedProperties() method in SerDeUtils.;;;","24/Apr/14 18:59;erwaman;Also updated [the RB|https://reviews.apache.org/r/20096/].;;;","24/Apr/14 19:24;xuefuz;Latest patch looks good to me. +1, pending on test result.;;;","25/Apr/14 08:28;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641763/HIVE-6835.5.patch

{color:red}ERROR:{color} -1 due to 40 failed/errored test(s), 5418 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/34/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/34/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 40 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641763;;;","25/Apr/14 16:45;xuefuz;[~erwaman] If you can confirm that these test failures are unrelated to your patch, I can commit it in a few hours.;;;","25/Apr/14 17:27;erwaman;I will do some local testing soon and let you know.;;;","25/Apr/14 18:40;erwaman;I tried all the failed union_remove TestCliDriver tests locally and they all passed.  Looking at some of the previous precommit builds, several of them also have the same test failures, so I believe these test failures are unrelated to my changes.;;;","25/Apr/14 18:44;erwaman;BTW, I have been doing all my development and testing against Hadoop 1.2.1 (-Phadoop-1).;;;","25/Apr/14 20:57;xuefuz;Patch committed to trunk. Thanks Anthony for the contribution.;;;","25/Apr/14 21:54;erwaman;Thanks, [~xuefuz], for all your help and guidance.;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,
Dynamic partition optimization bails out after removing file sink operator,HIVE-6834,12706463,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,prasanth_j,prasanth_j,prasanth_j,03/Apr/14 17:05,06/Apr/14 22:51,14/Jul/23 06:14,06/Apr/14 22:51,0.13.0,,,,,,,,,0.13.0,,,,,,0,,,HIVE-6455 introduced scalable dynamic partitioning optimization that bails out after removing the file sink operator. This causes union_remove_16.q test to fail by removing all the stages in the plan.,,prasanth_j,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/14 17:07;prasanth_j;HIVE-6834.1.patch;https://issues.apache.org/jira/secure/attachment/12638518/HIVE-6834.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384786,,,,Sun Apr 06 22:51:53 UTC 2014,,,,,,,,,,"0|i1uafb:",385053,,,,,,,,,,,,,,,,,,,,,"03/Apr/14 17:07;prasanth_j;This patch fixes the bug by moving the precondition check just before modify the operator tree.;;;","03/Apr/14 17:12;prasanth_j;[~rhbutani]/[~hagleitn] Can you please review this one?
;;;","03/Apr/14 20:57;rhbutani;+1;;;","05/Apr/14 09:53;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638518/HIVE-6834.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5547 tests executed
*Failed tests:*
{noformat}
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2121/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2121/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638518;;;","05/Apr/14 09:56;prasanth_j;The test failure is not related.;;;","06/Apr/14 22:51;rhbutani;Committed to trunk and 0.13
Thanks Prasanth;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
After major compaction unable to read from partition with MR job,HIVE-6830,12706389,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,omalley,gates,gates,03/Apr/14 09:41,08/Apr/14 15:05,14/Jul/23 06:14,08/Apr/14 15:03,0.13.0,,,,,,,,,0.13.0,,File Formats,,,,0,,,"After doing a major compaction any attempt to do read the data with an MR job (select count(*), subsequent compaction) fails with:
Caused by: java.lang.IllegalArgumentException: All base directories were ignored, such as hdfs://hdp.example.com:8020/apps/hive/warehouse/purchaselog/ds=201404031016/base_0044000 by 50000:4086:...
",,gates,omalley,rhbutani,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/14 10:19;omalley;HIVE-6830.patch;https://issues.apache.org/jira/secure/attachment/12638447/HIVE-6830.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384712,,,,Tue Apr 08 15:05:22 UTC 2014,,,,,,,,,,"0|i1u9yv:",384979,,,,,,,,,,,,,,,,,,,,,"03/Apr/14 10:19;omalley;This patch removes the restriction that the base must be completely covered by the valid transactions.;;;","03/Apr/14 20:25;rhbutani;+1 for adding to 0.13;;;","04/Apr/14 00:26;sershe;Now it adds bases to obsolete list unconditionally. Intended?;;;","05/Apr/14 04:02;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638447/HIVE-6830.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5547 tests executed
*Failed tests:*
{noformat}
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2119/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2119/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638447;;;","08/Apr/14 15:03;omalley;The test case passed locally.

I just committed this. Thanks for the review Harish.;;;","08/Apr/14 15:05;omalley;Sergey, if bestBase is defined it adds which ever is older (either bestBase or child) to obsolete.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
alter table foo compact gives an error ,HIVE-6829,12706382,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,gates,gates,gates,03/Apr/14 09:02,05/Apr/14 05:33,14/Jul/23 06:14,05/Apr/14 05:33,0.13.0,,,,,,,,,0.13.0,,SQL,,,,0,,,"Doing ""alter table foo compact 'minor'"" results in the error:
ERROR ql.Driver (SessionState.java:printError(550)) - FAILED: HiveException Operation should not be null
org.apache.hadoop.hive.ql.metadata.HiveException: Operation should not be null

""alter table foo partition (key='value') compact 'minor'"" works fine.",,gates,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/14 09:11;gates;HIVE-6829.patch;https://issues.apache.org/jira/secure/attachment/12638444/HIVE-6829.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384705,,,,Sat Apr 05 05:33:52 UTC 2014,,,,,,,,,,"0|i1u9xb:",384972,,,,,,,,,,,,,,,,,,,,,"04/Apr/14 00:26;ashutoshc;+1;;;","04/Apr/14 22:01;rhbutani;+1 for 0.13;;;","05/Apr/14 02:07;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638444/HIVE-6829.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5547 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_groupby2
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2118/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2118/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638444;;;","05/Apr/14 05:33;ashutoshc;Committed to 0.13 & trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive tez bucket map join conversion interferes with map join conversion,HIVE-6828,12706365,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vikram.dixit,vikram.dixit,vikram.dixit,03/Apr/14 05:04,09/Jun/14 06:39,14/Jul/23 06:14,02/May/14 23:07,0.13.0,0.14.0,,,,,,,,0.13.1,0.14.0,Tez,,,,0,,,The issue is that bucket count is used for checking the scaled down size of the hash tables but is used later on to convert to the map join as well which may be incorrect in cases where the entire hash table does not fit in the specified size.,,hagleitn,rajesh.balamohan,sershe,thejas,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/14 05:10;vikram.dixit;HIVE-6828.1.patch;https://issues.apache.org/jira/secure/attachment/12638424/HIVE-6828.1.patch","02/May/14 00:48;hagleitn;HIVE-6828.2.patch;https://issues.apache.org/jira/secure/attachment/12642970/HIVE-6828.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384688,,,,Mon Jun 09 06:39:38 UTC 2014,,,,,,,,,,"0|i1u9tj:",384955,,,,,,,,,,,,,,,,,,,,,"04/Apr/14 16:17;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638424/HIVE-6828.1.patch

{color:green}SUCCESS:{color} +1 5518 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2112/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2112/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638424;;;","25/Apr/14 21:29;hagleitn;+1;;;","02/May/14 00:48;hagleitn;Uploading same patch to run test suite one more time.;;;","02/May/14 09:08;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642970/HIVE-6828.2.patch

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 5427 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/101/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/101/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642970;;;","02/May/14 22:50;hagleitn;No new test failures (build 100 had the same errors - plus one more).
;;;","02/May/14 23:07;hagleitn;Committed to trunk. Thanks Vikram!;;;","09/Jun/14 06:39;thejas;This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive-tez has issues when different partitions work off of different input types,HIVE-6826,12706355,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vikram.dixit,vikram.dixit,vikram.dixit,03/Apr/14 03:23,09/Jun/14 06:39,14/Jul/23 06:14,06/May/14 00:24,0.13.0,0.14.0,,,,,,,,0.13.1,0.14.0,Tez,,,,0,,,"create table test (key int, value string) partitioned by (p int) stored as textfile;

insert into table test partition (p=1) select * from src limit 10;

alter table test set fileformat orc;

insert into table test partition (p=2) select * from src limit 10;

describe test;

select * from test where p=1 and key > 0;
select * from test where p=2 and key > 0;
select * from test where key > 0;


throws a classcast exception",,hagleitn,sushanth,thejas,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7071,,,,,,,,,,,,,,,,,,,,,"03/Apr/14 04:38;vikram.dixit;HIVE-6826.1.patch;https://issues.apache.org/jira/secure/attachment/12638420/HIVE-6826.1.patch","03/May/14 00:32;vikram.dixit;HIVE-6826.2.patch;https://issues.apache.org/jira/secure/attachment/12643166/HIVE-6826.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384678,,,,Mon Jun 09 06:39:36 UTC 2014,,,,,,,,,,"0|i1u9rb:",384945,,,,,,,,,,,,,,,,,,,,,"04/Apr/14 14:22;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638420/HIVE-6826.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5546 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_minimr_broken_pipe
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2110/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2110/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638420;;;","25/Apr/14 21:26;hagleitn;Some comments on rb.

The code says this is a stop gap fix, and I agree. Can you open a follow up jira for the fix using custom input initializer?;;;","03/May/14 00:16;vikram.dixit;Address Gunther's comments.;;;","04/May/14 03:13;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12643166/HIVE-6826.2.patch

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 5430 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/117/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/117/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12643166;;;","05/May/14 00:48;hagleitn;LGTM +1. There are two small review items, but I can fix those up when checking in.;;;","05/May/14 22:44;sushanth;Hi guys,

could you commit this patch to trunk and comment on whether it's stable to backport to 0.13 ?;;;","06/May/14 00:18;vikram.dixit;Committed to trunk. It is stable to back port to 0.13.

Thanks
Vikram.;;;","06/May/14 00:24;vikram.dixit;committed to trunk.;;;","08/May/14 02:27;sushanth;FYI, on testing 0.13.0 RC0, this patch seemed to be the one git bisect identifies as causing a couple of test failures.
The test failures are as follows:

org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6

The logs for the git bisect run for this bug are available over at http://people.apache.org/~khorgath/releases/0.13.1_RC0/test_failures/HIVE-6926.bisect/

The .sh files there were the scripts used to test hive for the bugs in question, and the file prior.patch available there is the patch that you will need to apply on top of 0.13.0 if you want to arrive at the last clean state before this patch was introduced and the issue was observed.;;;","08/May/14 02:29;sushanth;Correction on the URL above, should be http://people.apache.org/~khorgath/releases/0.13.1_RC0/test_failures/HIVE-6826.bisect/;;;","08/May/14 03:23;vikram.dixit;This passes on my setup. Could be an intermittently failing test as I have seen this fail in other jiras as well.;;;","08/May/14 10:51;sushanth;Yup, on re-testing, it seems to pass on my setup as well. I'm going to ignore the initial failure report. Thanks for checking up on it!;;;","09/Jun/14 06:39;thejas;This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sql std auth - database authorization does not check for role ownership,HIVE-6823,12706339,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,thejas,deepesh,thejas,03/Apr/14 00:52,03/Apr/14 22:17,14/Jul/23 06:14,03/Apr/14 22:17,0.13.0,,,,,,,,,0.13.0,,Authorization,,,,0,,,"A role can own the database, but when the authorization checks are determining the privileges for a user, they are not checking if one of the roles the user belongs to is an owner of the database.
",,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/14 01:24;thejas;HIVE-6823.1.patch;https://issues.apache.org/jira/secure/attachment/12638389/HIVE-6823.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384662,,,,Thu Apr 03 22:17:09 UTC 2014,,,,,,,,,,"0|i1u9nz:",384930,,,,,,,,,,,,,,,,,,,,,"03/Apr/14 00:55;thejas;From [~deepesh] -
Steps to reproduce:
# As admin user adminuser,
{noformat}
0: jdbc:hive2://localhost:10> create role testrole;
No rows affected (0.063 seconds)
0: jdbc:hive2://localhost:10> grant role testrole to user hrt_1;
No rows affected (0.07 seconds)
0: jdbc:hive2://localhost:10> create database testdb;
No rows affected (0.13 seconds)
0: jdbc:hive2://localhost:10> alter database testdb set owner role testrole;
No rows affected (0.111 seconds)
0: jdbc:hive2://localhost:10> desc database testdb;
+----------+----------+-----------------------------------------------------------------------+-------------+
| db_name  | comment  |                               location                                | parameters  |
+----------+----------+-----------------------------------------------------------------------+-------------+
| testdb   |          | hdfs://localhost:8020/apps/hive/warehouse/testdb.db  | testrole    |
+----------+----------+-----------------------------------------------------------------------+-------------+
1 row selected (0.038 seconds)
{noformat}
# As a user hrt_1 in role testrole,
{noformat}
0: jdbc:hive2://localhost:10> set role testrole;
No rows affected (0.055 seconds)
0: jdbc:hive2://localhost:10> drop database testdb;
Error: Error while compiling statement: FAILED: HiveAccessControlException Permission denied. Principal [name=hrt_1, type=USER] does not have following privileges on Object [type=DATABASE, name=testdb] : [OBJECT OWNERSHIP] (state=42000,code=40000)
0: jdbc:hive2://localhost:10> use testdb;
No rows affected (0.032 seconds)
0: jdbc:hive2://localhost:10> create table foobar (foo string, bar string);
Error: Error while compiling statement: FAILED: HiveAccessControlException Permission denied. Principal [name=hrt_1, type=USER] does not have following privileges on Object [type=DATABASE, name=testdb] : [OBJECT OWNERSHIP] (state=42000,code=40000)
{noformat}
I see that commands work when i set ownership to user hrt_1 directly. I expect to see the same behavior with role ownership as well.;;;","03/Apr/14 05:18;ashutoshc;+1;;;","03/Apr/14 20:13;thejas;[~rhbutani] This is an important security fix, which I think we should include in 0.13.
;;;","03/Apr/14 20:56;rhbutani;+1 for 0.13;;;","03/Apr/14 22:11;thejas;Ran tests locally and they passed.
;;;","03/Apr/14 22:17;thejas;Patch committed to 0.13 branch and trunk.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestAvroSerdeUtils fails with -Phadoop-2,HIVE-6822,12706338,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,03/Apr/14 00:50,13/Nov/14 19:42,14/Jul/23 06:14,08/Apr/14 22:42,,,,,,,,,,0.14.0,,Tests,,,,0,,,"Works fine with -Phadoop-1, but with -Phadoop-2 hits the following error:

{noformat}
Running org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils
Tests run: 10, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.603 sec <<< FAILURE! - in org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils
determineSchemaCanReadSchemaFromHDFS(org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils)  Time elapsed: 0.688 sec  <<< ERROR!
java.lang.NoClassDefFoundError: com/sun/jersey/spi/container/servlet/ServletContainer
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
	at org.apache.hadoop.http.HttpServer2.addJerseyResourcePackage(HttpServer2.java:564)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.initWebHdfs(NameNodeHttpServer.java:84)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:121)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:601)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:500)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1259)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:914)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:805)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:663)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:603)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:474)
	at org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.determineSchemaCanReadSchemaFromHDFS(TestAvroSerdeUtils.java:189)
{noformat}",,erwaman,jdere,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6588,,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/14 00:52;jdere;HIVE-6822.1.patch;https://issues.apache.org/jira/secure/attachment/12638386/HIVE-6822.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384661,,,,Thu Nov 13 19:42:56 UTC 2014,,,,,,,,,,"0|i1u9nr:",384929,,,,,,,,,,,,,,,,,,,,,"03/Apr/14 00:52;jdere;Attaching patch, add test dependency on jersey-servlet.;;;","04/Apr/14 02:39;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638386/HIVE-6822.1.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 5547 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.ql.processors.TestCommandProcessorFactory.testAvailableCommands
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2102/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2102/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638386;;;","04/Apr/14 05:21;thejas;FYI sample8,transform_ppr1,transform_ppr2,union_ppr failures are caused by a bad commit, created HIVE-6838 to address it.;;;","08/Apr/14 18:43;ashutoshc;+1;;;","08/Apr/14 22:42;ashutoshc;Committed to trunk.;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix some non-deterministic tests ,HIVE-6821,12706327,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,03/Apr/14 00:04,08/Apr/14 00:21,14/Jul/23 06:14,08/Apr/14 00:21,,,,,,,,,,0.13.0,,Tests,,,,0,,,A bunch of qfile tests look like they need an ORDER-BY added to the queries so that the output looks repeatable when testing with hadoop1/hadoop2.,,jdere,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6588,,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/14 00:27;jdere;HIVE-6821.1.patch;https://issues.apache.org/jira/secure/attachment/12638382/HIVE-6821.1.patch","04/Apr/14 20:57;jdere;HIVE-6821.2.patch;https://issues.apache.org/jira/secure/attachment/12638764/HIVE-6821.2.patch","07/Apr/14 00:27;jdere;HIVE-6821.3.patch;https://issues.apache.org/jira/secure/attachment/12638932/HIVE-6821.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384650,,,,Tue Apr 08 00:21:46 UTC 2014,,,,,,,,,,"0|i1u9lj:",384918,,,,,,,,,,,,,,,,,,,,,"03/Apr/14 00:27;jdere;Attaching patch. Also updating compile_processor.q to drop the temp function at the end of the test, because if the qfile tests are run serially this extra function causes show_functions.q to fail.;;;","03/Apr/14 01:03;jdere;RB https://reviews.apache.org/r/19978/;;;","04/Apr/14 00:39;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638382/HIVE-6821.1.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5545 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge3
org.apache.hadoop.hive.ql.processors.TestCommandProcessorFactory.testAvailableCommands
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2101/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2101/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638382;;;","04/Apr/14 18:46;jdere;Looks like I missed a couple of .q.out files in patch, will regenerate patch.
Not sure how TestCommandProcessorFactory.testAvailableCommands would be affected by this patch, since it's only modifying .q files.;;;","04/Apr/14 20:57;jdere;patch v2 - generate diff using ""git diff -a"" to include the missing .q.out files (which git treated as binary files). Also regenerated merge3.q.out since it had changed due to HIVE-6808.;;;","05/Apr/14 23:43;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638764/HIVE-6821.2.patch

{color:green}SUCCESS:{color} +1 5547 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2138/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2138/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638764;;;","07/Apr/14 00:27;jdere;patch v3 - rebase patch with trunk, also include list_bucket_dml_2.q as I've seen the results fail for this on both Mac/Linux.;;;","07/Apr/14 02:16;ashutoshc;+1;;;","07/Apr/14 03:03;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638932/HIVE-6821.3.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5548 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_dyn_part
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2159/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2159/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638932;;;","08/Apr/14 00:21;ashutoshc;Committed to 0.13 & trunk. Thanks, Jason!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer(2) ignores HIVE_OPTS,HIVE-6820,12706308,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,libing,rding,rding,02/Apr/14 21:59,13/Nov/14 19:39,14/Jul/23 06:14,12/May/14 18:03,0.12.0,,,,,,,,,0.14.0,,HiveServer2,,,,0,,,"In hiveserver2.sh:

{code}
exec $HADOOP jar $JAR $CLASS ""$@""
{code}

While cli.sh having:

{code}
exec $HADOOP jar ${HIVE_LIB}/hive-cli-*.jar $CLASS $HIVE_OPTS ""$@""
{code}

Hence some hive commands that run properly in Hive shell fail in HiveServer.",,libing,rding,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6948,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/14 08:29;libing;HIVE-6820.1.patch;https://issues.apache.org/jira/secure/attachment/12640403/HIVE-6820.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384631,,,,Thu Nov 13 19:39:47 UTC 2014,,,,,,,,,,"0|i1u9hb:",384899,,,,,,,,,,,,,,,,,,,,,"16/Apr/14 08:29;libing;Append $HIVE_OPTS to hiveserver2.sh and hiveserver.sh;;;","16/Apr/14 08:29;libing;The patch is created based on trunk branch;;;","05/May/14 06:35;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12640403/HIVE-6820.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5428 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/125/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/125/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12640403;;;","09/May/14 23:47;thejas;+1;;;","12/May/14 18:03;thejas;Patch committed to trunk.
Thanks for the contribution [~libing]
;;;","13/Nov/14 19:39;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Array out of bounds when ORC is used with ACID and predicate push down,HIVE-6818,12706283,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,omalley,omalley,omalley,02/Apr/14 20:54,09/Apr/14 19:55,14/Jul/23 06:14,09/Apr/14 19:55,,,,,,,,,,0.13.0,,File Formats,,,,0,,,"The users gets an ArrayOutOfBoundsException when using ORC, ACID, and predicate push down.",,omalley,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/14 21:12;omalley;HIVE-6818.patch;https://issues.apache.org/jira/secure/attachment/12638338/HIVE-6818.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384606,,,,Wed Apr 09 19:55:59 UTC 2014,,,,,,,,,,"0|i1u9br:",384874,,,,,,,,,,,,,,,,,,,,,"02/Apr/14 21:12;omalley;This patch fixes the problem. It also adds a serialization interface for SearchArguments, which is needed for testing the fix.;;;","03/Apr/14 20:51;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638338/HIVE-6818.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5543 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_smb_mapjoin_8
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2099/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2099/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638338;;;","04/Apr/14 00:28;sershe;Is new argument solely for testing? It seems like a bad idea to add to production code;;;","07/Apr/14 22:56;omalley;Sergey,
  My intention is to replace the current xml ast with the serialized SearchArgument. The serialized SearchArgument is much more compact and focused on predicate pushdown. However, in order for that to happen, we need to transition the clients from the old format to the new one. So, yes, the immediate patch only uses it for testing, but it should over time become the mainline path.;;;","07/Apr/14 23:26;sershe;Can you add comment about that on commit? Otherwise +1;;;","08/Apr/14 15:53;omalley;The three failures are unrelated and pass when I run it locally. I'll commit this after the 24 hours.;;;","09/Apr/14 19:55;omalley;Thanks for the review, Sergey!

I just committed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some hadoop2-only tests need diffs to be updated,HIVE-6817,12706280,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,02/Apr/14 20:36,13/Nov/14 19:43,14/Jul/23 06:14,04/Apr/14 23:44,,,,,,,,,,0.14.0,,Tests,,,,0,,,expected output needs updating due to pre/post hook messages from the authorization changes,,jdere,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6588,,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/14 20:39;jdere;HIVE-6817.1.patch;https://issues.apache.org/jira/secure/attachment/12638327/HIVE-6817.1.patch","04/Apr/14 19:15;jdere;HIVE-6817.2.patch;https://issues.apache.org/jira/secure/attachment/12638747/HIVE-6817.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384603,,,,Thu Nov 13 19:43:11 UTC 2014,,,,,,,,,,"0|i1u9b3:",384871,,,,,,,,,,,,,,,,,,,,,"03/Apr/14 17:01;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638327/HIVE-6817.1.patch

{color:green}SUCCESS:{color} +1 5542 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2096/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2096/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638327;;;","03/Apr/14 17:10;ashutoshc;+1;;;","04/Apr/14 19:14;jdere;Looks like the diff for combine2.q has changed due to HIVE-6808, will need to regenerate the expected output.  Also want to include TestHBaseNegativeCliDriver.cascade_dbdrop.q to this patch.;;;","04/Apr/14 19:15;jdere;patch v2;;;","04/Apr/14 19:16;jdere;[~ashutoshc], does this still look ok?;;;","04/Apr/14 21:43;ashutoshc;yup.. LGTM;;;","04/Apr/14 23:44;jdere;Committed to Hive trunk.;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jar upload path w/o schema is not handled correctly,HIVE-6816,12706246,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sershe,sershe,sershe,02/Apr/14 17:49,10/Apr/14 19:59,14/Jul/23 06:14,10/Apr/14 19:59,,,,,,,,,,0.13.0,,,,,,0,,,"{noformat}
java.io.IOException: java.net.URISyntaxException: Expected scheme name at index 0: :///user/sershe/hive-exec-0.14.0-SNAPSHOT-5a31b3483b29ad46db705a47893898b2b9f5b7ce3c65f0641bbecca2b1201d81.jar
	at org.apache.tez.client.TezClientUtils.setupDAGCredentials(TezClientUtils.java:304)
	at org.apache.tez.client.TezSession.submitDAG(TezSession.java:202)
	at org.apache.tez.client.TezSession.submitDAG(TezSession.java:154)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.submit(TezTask.java:294)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:147)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1473)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1240)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1058)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:885)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:875)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:424)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:793)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:687)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:626)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
Caused by: java.net.URISyntaxException: Expected scheme name at index 0: :///user/sershe/hive-exec-0.14.0-SNAPSHOT-5a31b3483b29ad46db705a47893898b2b9f5b7ce3c65f0641bbecca2b1201d81.jar
	at java.net.URI$Parser.fail(URI.java:2829)
	at java.net.URI$Parser.failExpecting(URI.java:2835)
	at java.net.URI$Parser.parse(URI.java:3027)
	at java.net.URI.<init>(URI.java:753)
	at org.apache.hadoop.yarn.util.ConverterUtils.getPathFromYarnURL(ConverterUtils.java:80)
	at org.apache.tez.client.TezClientUtils.setupDAGCredentials(TezClientUtils.java:296)
	... 22 more
{noformat}",,sershe,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 18:30;sershe;HIVE-6816.01.patch;https://issues.apache.org/jira/secure/attachment/12639621/HIVE-6816.01.patch","02/Apr/14 17:57;sershe;HIVE-6816.patch;https://issues.apache.org/jira/secure/attachment/12638299/HIVE-6816.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384569,,,,Thu Apr 10 19:59:44 UTC 2014,,,,,,,,,,"0|i1u93j:",384837,,,,,,,,,,,,,,,,,,,,,"02/Apr/14 17:57;sershe;Remove some duplicate code and make sure the path is normalized. Seems to work for me on the cluster, I'll run minitez tests;;;","02/Apr/14 17:58;sershe;[~vikram.dixit] fyi;;;","02/Apr/14 18:04;sershe;Ideally I'd like to get this into 0.13...;;;","03/Apr/14 11:15;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638299/HIVE-6816.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5542 tests executed
*Failed tests:*
{noformat}
org.apache.hive.jdbc.TestJdbcDriver2.testNewConnectionConfiguration
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2093/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2093/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638299;;;","03/Apr/14 17:00;sershe;test failure is unrelated;;;","10/Apr/14 18:30;sershe;Update the patch to correspond to recent changes;;;","10/Apr/14 19:41;vikram.dixit;+1 LGTM;;;","10/Apr/14 19:59;sershe;committed to trunk and 0.13, I ran some tests locally; only affects Tez path. This is an important bug to fix in 13.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
show compactions returns error when there are no compactions,HIVE-6812,12706131,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,gates,gates,gates,02/Apr/14 09:27,08/Apr/14 19:16,14/Jul/23 06:14,08/Apr/14 19:16,0.13.0,,,,,,,,,0.13.0,,SQL,,,,0,,,"Doing ""show compactions"" when there are no current transactions in process or in the queue results in: 
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. null",,gates,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/14 09:45;gates;HIVE-6812.patch;https://issues.apache.org/jira/secure/attachment/12638225/HIVE-6812.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384454,,,,Tue Apr 08 19:16:27 UTC 2014,,,,,,,,,,"0|i1u8e7:",384722,,,,,,,,,,,,,,,,,,,,,"03/Apr/14 05:26;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638225/HIVE-6812.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5542 tests executed
*Failed tests:*
{noformat}
org.apache.hcatalog.mapreduce.TestHCatMultiOutputFormat.org.apache.hcatalog.mapreduce.TestHCatMultiOutputFormat
org.apache.hive.jdbc.TestJdbcDriver2.testNewConnectionConfiguration
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2088/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2088/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638225;;;","03/Apr/14 06:26;ashutoshc;+1;;;","08/Apr/14 19:16;ashutoshc;Committed to 0.13 & trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOAD command does not work with relative paths on Windows,HIVE-6811,12706111,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,02/Apr/14 06:28,13/Nov/14 19:44,14/Jul/23 06:14,05/Apr/14 00:04,,,,,,,,,,0.14.0,,,,,,0,,,"qfile tests on Windows fail when trying to load data, with URISyntaxException: Relative path in absolute URI",,ckadner,jdere,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/14 06:34;jdere;HIVE-6811.1.patch;https://issues.apache.org/jira/secure/attachment/12638204/HIVE-6811.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384434,,,,Thu Nov 13 19:44:21 UTC 2014,,,,,,,,,,"0|i1u89r:",384702,,,,,,,,,,,,,,,,,,,,,"02/Apr/14 06:32;jdere;Looks like LoadSemanticAnalyzer changes in HIVE-6048 may have caused this, when path string in windows is generated with Path.toUri(), ""C:/path"" becomes ""/C:/path"" which seems to have allowed things to work before.  Without the toUri() call, the path remains ""C:/path"" and the we see the error. ;;;","02/Apr/14 06:34;jdere;Patch to use toUri() again when generating the path string.  [~xuefuz] does this break any of the other changes in HIVE-6048, or will this work ok?;;;","02/Apr/14 17:03;xuefuz;URIUtil.decode() is the evil that HIVE-6048 and others were trying to avoid. Thus, the patch is likely to break things. One way to try out is to have a file with '+' in its name.;;;","02/Apr/14 18:25;jdere;Thanks for the input Xuefu.  Actually it looks like the 2 test cases with space/+ in the name pass (load_file_with_space_in_the_name.q, load_hdfs_file_with_space_in_the_name.q), is it ok for this change to be done? Or would you prefer that we do something Windows-specific here and prepend a ""/"" to the path string if on Windows?;;;","02/Apr/14 18:42;xuefuz;I think I'd be surprised, but I'm fine with the current patch if the tests do pass.;;;","02/Apr/14 19:48;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638204/HIVE-6811.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5539 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_minimr_broken_pipe
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2082/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2082/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638204;;;","02/Apr/14 21:18;xuefuz;+1.
I guess URIUtil.decode() is okay to be used here because later the load work is constructed using Path instead of previous String as the source location.;;;","05/Apr/14 00:03;jdere;TestNegativeMinimrCliDriver.testNegativeCliDriver_minimr_broken_pipe passes for me locally on Linux VM. Will go ahead and commit this.;;;","05/Apr/14 00:04;jdere;Committed to trunk.;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"sql std auth - describe table, show partitions are not being authorized",HIVE-6808,12706081,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,thejas,thejas,thejas,02/Apr/14 02:07,04/Apr/14 06:14,14/Jul/23 06:14,03/Apr/14 23:03,0.13.0,,,,,,,,,0.13.0,,Authorization,,,,0,,,"Only users with SELECT privilege on the table should be able to do 'describe table' and 'show partitions'.
",,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6838,,,,,,,,,,,,,,,,,,,,,"02/Apr/14 06:38;thejas;HIVE-6808.1.patch;https://issues.apache.org/jira/secure/attachment/12638205/HIVE-6808.1.patch","02/Apr/14 20:43;thejas;HIVE-6808.2.patch;https://issues.apache.org/jira/secure/attachment/12638329/HIVE-6808.2.patch","02/Apr/14 20:56;thejas;HIVE-6808.3.patch;https://issues.apache.org/jira/secure/attachment/12638334/HIVE-6808.3.patch","03/Apr/14 20:23;thejas;HIVE-6808.4.patch;https://issues.apache.org/jira/secure/attachment/12638551/HIVE-6808.4.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384404,,,,Thu Apr 03 23:03:11 UTC 2014,,,,,,,,,,"0|i1u83b:",384672,,,,,,,,,,,,,,,,,,,,,"02/Apr/14 06:40;thejas;Several .q.out files need to be updated, I am regenerating those files. Uploading patch without that for now, for ease of review (HIVE-6808.1.patch).
;;;","02/Apr/14 20:43;thejas;HIVE-6808.2.patch - fix to handle cases where the tablename includes db name. Does not contain all the q.out changes to keep it easy to review.


;;;","02/Apr/14 20:43;thejas;HIVE-6808.3.patch - includes all q.out file changes
;;;","03/Apr/14 05:11;ashutoshc;+1;;;","03/Apr/14 18:57;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638334/HIVE-6808.3.patch

{color:red}ERROR:{color} -1 due to 15 failed/errored test(s), 5544 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_sahooks
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatCTAS
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats13
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket5
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_list_bucket_dml_10
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority2
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2097/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2097/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 15 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638334;;;","03/Apr/14 20:11;thejas;HIVE-6808.4.patch - generated with ""git diff -a"" so that some .q.out files don't get treated as binary. Also updated results for merge4,multi_sahooks,nullformatCTAS.
;;;","03/Apr/14 20:11;thejas;[~rhbutani] This is an important security fix, which I think we should include in 0.13.
;;;","03/Apr/14 20:33;rhbutani;+1 for 0.13;;;","03/Apr/14 22:58;thejas;Ran the failed tests with updated .q.out files and they pass.
;;;","03/Apr/14 23:03;thejas;Patch committed to trunk and 0.13 branch.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add HCatStorer ORC test to test missing columns,HIVE-6807,12706069,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,02/Apr/14 00:48,01/Oct/19 22:07,14/Jul/23 06:14,21/Apr/14 17:13,0.13.0,,,,,,,,,0.14.0,,HCatalog,,,,0,,,,,ekoifman,gates,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-4975,HIVE-6766,,,,,,,,,,,,,,,,,,,,"02/Apr/14 00:54;ekoifman;HIVE-6807.patch;https://issues.apache.org/jira/secure/attachment/12638168/HIVE-6807.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384392,,,,Thu Nov 13 19:41:42 UTC 2014,,,,,,,,,,"0|i1u80n:",384660,,,,,,,,,,,,,,,,,,,,,"02/Apr/14 00:49;ekoifman;enable a test introduced in HIVE-6766 now that HIVE-4975 is fixed;;;","02/Apr/14 13:57;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638168/HIVE-6807.patch

{color:green}SUCCESS:{color} +1 5540 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2079/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2079/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638168;;;","18/Apr/14 22:58;gates;This patch seems to do nothing, since it adds a test that is marked ignore.  Also it appears to reference an internal bug number in the ignore statement.;;;","19/Apr/14 00:39;ekoifman;[~gates] You misread the patch.  It removes the method with Ignore annotation, thus the 'super' of it will actually run with ORC;;;","21/Apr/14 17:13;gates;Thanks Eugene for correcting my misread of the patch.  Patch checked in.;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sql std auth - granting existing table privilege to owner should result in error,HIVE-6804,12705883,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,thejas,deepesh,thejas,01/Apr/14 08:54,02/Apr/14 18:19,14/Jul/23 06:14,02/Apr/14 18:19,,,,,,,,,,0.13.0,,Authorization,,,,0,,,"Table owner gets all privileges on the table at the time of table creation.
But granting some or all of the privileges using grant statement still works resulting in duplicate privileges. 
",,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6805,,,,,,,,,,,,,,,,,,,,,"01/Apr/14 09:06;thejas;HIVE-6804.1.patch;https://issues.apache.org/jira/secure/attachment/12638029/HIVE-6804.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384207,,,,Wed Apr 02 18:19:46 UTC 2014,,,,,,,,,,"0|i1u6vr:",384475,,,,,,,,,,,,,,,,,,,,,"01/Apr/14 08:56;thejas;From [~deepesh]
Steps to reproduce:
# Login as a public user (eg. hrt_1).
{noformat}
0: jdbc:hive2://localhost:10> create table foobar (foo string, bar string);
No rows affected (0.167 seconds)
0: jdbc:hive2://localhost:10> show grant on table foobar;
+-----------+---------+------------+---------+-----------------+-----------------+------------+---------------+--------+
| database  |  table  | partition  | column  | principal_name  | principal_type  | privilege  | grant_option  |   gran |
+-----------+---------+------------+---------+-----------------+-----------------+------------+---------------+--------+
| default   | foobar  |            |         | hrt_1           | USER            | DELETE     | true          | 139629 |
| default   | foobar  |            |         | hrt_1           | USER            | INSERT     | true          | 139629 |
| default   | foobar  |            |         | hrt_1           | USER            | SELECT     | true          | 139629 |
| default   | foobar  |            |         | hrt_1           | USER            | UPDATE     | true          | 139629 |
+-----------+---------+------------+---------+-----------------+-----------------+------------+---------------+--------+
4 rows selected (0.043 seconds)
0: jdbc:hive2://localhost:10> grant all on table foobar to user hrt_1 with grant option;
No rows affected (0.171 seconds)
0: jdbc:hive2://localhost:10> show grant on table foobar;
+-----------+---------+------------+---------+-----------------+-----------------+------------+---------------+--------+
| database  |  table  | partition  | column  | principal_name  | principal_type  | privilege  | grant_option  |   gran |
+-----------+---------+------------+---------+-----------------+-----------------+------------+---------------+--------+
| default   | foobar  |            |         | hrt_1           | USER            | DELETE     | true          | 139629 |
| default   | foobar  |            |         | hrt_1           | USER            | DELETE     | true          | 139629 |
| default   | foobar  |            |         | hrt_1           | USER            | INSERT     | true          | 139629 |
| default   | foobar  |            |         | hrt_1           | USER            | INSERT     | true          | 139629 |
| default   | foobar  |            |         | hrt_1           | USER            | SELECT     | true          | 139629 |
| default   | foobar  |            |         | hrt_1           | USER            | SELECT     | true          | 139629 |
| default   | foobar  |            |         | hrt_1           | USER            | UPDATE     | true          | 139629 |
| default   | foobar  |            |         | hrt_1           | USER            | UPDATE     | true          | 139629 |
+-----------+---------+------------+---------+-----------------+-----------------+------------+---------------+--------+
8 rows selected (0.046 seconds)
{noformat}
I would not expect duplicate entries, either we should error out when we try to grant privileges on a table where user already has privileges or the command become a NOOP.
# Now try grant another time and revoke.
{noformat}
0: jdbc:hive2://localhost:10> grant all on table foobar to user hrt_1 with grant option;
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Error granting privileges: Internal error processing grant_privileges (state=08S01,code=1)
0: jdbc:hive2://localhost:10> show grant on table foobar;
+-----------+---------+------------+---------+-----------------+-----------------+------------+---------------+--------+
| database  |  table  | partition  | column  | principal_name  | principal_type  | privilege  | grant_option  |   gran |
+-----------+---------+------------+---------+-----------------+-----------------+------------+---------------+--------+
| default   | foobar  |            |         | hrt_1           | USER            | DELETE     | true          | 139629 |
| default   | foobar  |            |         | hrt_1           | USER            | DELETE     | true          | 139629 |
| default   | foobar  |            |         | hrt_1           | USER            | INSERT     | true          | 139629 |
| default   | foobar  |            |         | hrt_1           | USER            | INSERT     | true          | 139629 |
| default   | foobar  |            |         | hrt_1           | USER            | SELECT     | true          | 139629 |
| default   | foobar  |            |         | hrt_1           | USER            | SELECT     | true          | 139629 |
| default   | foobar  |            |         | hrt_1           | USER            | UPDATE     | true          | 139629 |
| default   | foobar  |            |         | hrt_1           | USER            | UPDATE     | true          | 139629 |
+-----------+---------+------------+---------+-----------------+-----------------+------------+---------------+--------+
8 rows selected (0.045 seconds)
0: jdbc:hive2://localhost:10> revoke all on table foobar from user hrt_1;
No rows affected (0.156 seconds)
0: jdbc:hive2://localhost:10> show grant on table foobar;
+-----------+---------+------------+---------+-----------------+-----------------+------------+---------------+--------+
| database  |  table  | partition  | column  | principal_name  | principal_type  | privilege  | grant_option  |   gran |
+-----------+---------+------------+---------+-----------------+-----------------+------------+---------------+--------+
| default   | foobar  |            |         | hrt_1           | USER            | DELETE     | true          | 139629 |
| default   | foobar  |            |         | hrt_1           | USER            | INSERT     | true          | 139629 |
| default   | foobar  |            |         | hrt_1           | USER            | SELECT     | true          | 139629 |
| default   | foobar  |            |         | hrt_1           | USER            | UPDATE     | true          | 139629 |
+-----------+---------+------------+---------+-----------------+-----------------+------------+---------------+--------+
4 rows selected (0.039 seconds)
{noformat}
So we see two things here. First, the user cannot grant privileges again which is right and should have been the case even earlier. Second, revoking privileges removes only the duplicate set of privileges. This can be bad for end user who might think that he already revoked privileges but the system still preserves a set.;;;","01/Apr/14 09:10;thejas;[~rhbutani] This is an important bug fix for sql std auth. I think we should include this in 0.13 .
;;;","01/Apr/14 14:36;ashutoshc;+1;;;","01/Apr/14 14:36;rhbutani;+1 for 0.13;;;","02/Apr/14 12:02;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638029/HIVE-6804.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5540 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_smb_mapjoin_8
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2078/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2078/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638029;;;","02/Apr/14 18:19;thejas;Patch committed to 0.13 branch and trunk.
Thanks for the review Ashutosh!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix metastore.thrift: add partition_columns.types constant,HIVE-6802,12705827,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,rhbutani,rhbutani,rhbutani,01/Apr/14 00:50,01/Apr/14 14:42,14/Jul/23 06:14,01/Apr/14 14:42,,,,,,,,,,0.13.0,,,,,,0,,,"HIVE-6642 edited the hive_metastoreConstants.java genned file. 
Need to add constant to thrift file and regen thrift classes.",,hsubramaniyan,jdere,rhbutani,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/14 00:53;rhbutani;HIVE-6802.1.patch;https://issues.apache.org/jira/secure/attachment/12637968/HIVE-6802.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384151,,,,Tue Apr 01 14:42:35 UTC 2014,,,,,,,,,,"0|i1u6jb:",384419,,,,,,,,,,,,,,,,,,,,,"01/Apr/14 00:57;jdere;+1;;;","01/Apr/14 00:57;sershe;+1;;;","01/Apr/14 01:09;hsubramaniyan;+1. Thanks [~rhbutani] for incorporating this missing change.;;;","01/Apr/14 10:26;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637968/HIVE-6802.1.patch

{color:green}SUCCESS:{color} +1 5513 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2062/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2062/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637968;;;","01/Apr/14 14:42;rhbutani;Committed to trunk and 0.13
thanks Jason, Sergey, Hari for reviewing;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2 is not passing proxy user setting through hive-site,HIVE-6800,12705815,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vgumashta,vgumashta,vgumashta,31/Mar/14 23:28,06/Apr/14 00:48,14/Jul/23 06:14,06/Apr/14 00:09,0.13.0,,,,,,,,,0.13.0,,HiveServer2,,,,0,,,"Setting the following in core-site.xml works fine in a secure cluster with hive.server2.allow.user.substitution set to true:
{code}
<property>
  <name>hadoop.proxyuser.user1.groups</name>
  <value>users</value>
</property>
    
<property>
  <name>hadoop.proxyuser.user1.hosts</name>
  <value>*</value>
</property>
{code}

where user1 will be proxying for user2:
{code}
!connect jdbc:hive2:/myhostname:10000/;principal=hive/_HOST@EXAMPLE.COM;hive.server2.proxy.user=user2 user1 fakepwd org.apache.hive.jdbc.HiveDriver
{code}

However, setting this in hive-site.xml throws ""Failed to validate proxy privilage"" exception.",,brett_s_r,prasadm,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6851,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/14 00:34;vgumashta;HIVE-6800.1.patch;https://issues.apache.org/jira/secure/attachment/12637963/HIVE-6800.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384139,,,,Sun Apr 06 00:48:20 UTC 2014,,,,,,,,,,"0|i1u6gn:",384407,,,,,,,,,,,,,,,,,,,,,"31/Mar/14 23:33;vgumashta;cc [~thejas] [~prasadm]

[~rhbutani] Bug for 13!

Thanks!;;;","01/Apr/14 08:29;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637963/HIVE-6800.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5513 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_dyn_part
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2059/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2059/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637963;;;","01/Apr/14 13:54;prasadm;[~vaibhavgumashta] Thanks for fixing the issue. Looks fine to me.
+1
;;;","01/Apr/14 16:48;vgumashta;[~prasadm] Thanks for taking a look. The failure looks unrelated.;;;","05/Apr/14 20:33;thejas;There is a thread safety issue here,  ProxyUsers uses HashMap internally, and refreshSuperUserGroupsConfiguration updates the hashmaps. If it gets updated from two places, it could result in issues like getting stuck in an infinite loop.
Also, when one thread is calling ProxyUsers.authorize, another thread might clear the hashmap entries as it enters ProxyUsers.refreshSuperUserGroupsConfiguration.
refreshSuperUserGroupsConfiguration also looks like an expensive operation. I think we would be better to do it just once from HadoopShimsSecure .
;;;","05/Apr/14 20:35;thejas;Sorry, I take that back, refreshSuperUserGroupsConfiguration and authorize are actually synchronized methods. 
Patch looks good to me.

I think we should look at doing the refreshSuperUserGroupsConfiguration only once, but that can be part of a follow up jira.
;;;","06/Apr/14 00:09;thejas;Patch committed to trunk and 0.13 branch.
Thanks for the contribution Vaibhav, and thanks for the review Prasad!
;;;","06/Apr/14 00:48;thejas;Created HIVE-6851 for the optimization suggested above.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create/drop roles is case-sensitive whereas 'set role' is case insensitive,HIVE-6796,12705800,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,deepesh,thejas,31/Mar/14 22:16,05/Apr/14 22:59,14/Jul/23 06:14,03/Apr/14 21:21,,,,,,,,,,0.13.0,,,,,,0,,,"Create/drop role operations should be case insensitive.
",,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-5837,,,,,,,,,,,,,,,,,,,,,"02/Apr/14 18:24;ashutoshc;HIVE-6796.2.patch;https://issues.apache.org/jira/secure/attachment/12638303/HIVE-6796.2.patch","03/Apr/14 06:18;ashutoshc;HIVE-6796.3.patch;https://issues.apache.org/jira/secure/attachment/12638430/HIVE-6796.3.patch","01/Apr/14 21:57;ashutoshc;HIVE-6796.patch;https://issues.apache.org/jira/secure/attachment/12638137/HIVE-6796.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384124,,,,Thu Apr 03 21:21:28 UTC 2014,,,,,,,,,,"0|i1u6db:",384392,,,,,,,,,,,,,,,,,,,,,"31/Mar/14 22:18;thejas;From [~deepesh]

Steps to reproduce:
# Login as admin user (eg hrt_qa) and create a new role.
{noformat}
0: jdbc:hive2://hor8n19.gq1.ygridcore.net:100> set role ADMIN;
No rows affected (0.048 seconds)
0: jdbc:hive2://:10> create role TESTROLE;
No rows affected (0.081 seconds)
0: jdbc:hive2://:10> grant role TESTROLE to user hrt_1;
No rows affected (0.086 seconds)
0: jdbc:hive2://:10> show roles;
+-----------+
|   role    |
+-----------+
| ADMIN     |
| PUBLIC    |
| TESTROLE  |
|           |
+-----------+
4 rows selected (0.05 seconds)
{noformat}
# Login as public user hrt_1 and assume above role.
{noformat}
beeline> !connect jdbc:hive2://:10000 hrt_1 pwd
Connected to: Apache Hive (version 0.13.0.2.1.1.0-261)
Driver: Hive JDBC (version 0.13.0.2.1.1.0-261)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://:10> show current roles;
+-----------+
|   role    |
+-----------+
| PUBLIC    |
| TESTROLE  |
|           |
+-----------+
3 rows selected (0.096 seconds)
0: jdbc:hive2://:10> set role testrole;
No rows affected (0.025 seconds)
0: jdbc:hive2://:10> show current roles;
+-----------+
|   role    |
+-----------+
| TESTROLE  |
|           |
+-----------+
2 rows selected (0.019 seconds)
{noformat}
This seems convenient as to not worry about case-sensitivity.
# But here is the problem. Try to create role ""testrole"" (earlier we created TESTROLE). On admin session (hrt_qa):
{noformat}
0: jdbc:hive2://:10> create role testrole;
No rows affected (0.15 seconds)
0: jdbc:hive2://:10> show roles;
+-----------+
|   role    |
+-----------+
| ADMIN     |
| PUBLIC    |
| TESTROLE  |
| testrole  |
|           |
+-----------+
5 rows selected (0.026 seconds)
0: jdbc:hive2://:10> drop role TESTROLE;
No rows affected (0.094 seconds)
0: jdbc:hive2://:10> show roles;
+-----------+
|   role    |
+-----------+
| ADMIN     |
| PUBLIC    |
| testrole  |
|           |
+-----------+
4 rows selected (0.026 seconds)
{noformat}

Above shows that create/drop role is case sensitive but ""set role"" is not. They should be all consistent (either they all are case sensitive or they are all case insensitive).;;;","01/Apr/14 21:58;ashutoshc;https://reviews.apache.org/r/19889/;;;","02/Apr/14 08:11;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638137/HIVE-6796.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5539 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_roles
org.apache.hadoop.hive.jdbc.TestJdbcDriver.testShowRoleGrant
org.apache.hive.jdbc.TestJdbcDriver2.testShowRoleGrant
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2076/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2076/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638137;;;","03/Apr/14 13:10;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638430/HIVE-6796.3.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5543 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2094/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2094/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638430;;;","03/Apr/14 15:40;thejas;+1;;;","03/Apr/14 21:21;ashutoshc;Committed to 0.13 & trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"metastore initialization should add default roles with default, SBA",HIVE-6795,12705785,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,thejas,deepesh,thejas,31/Mar/14 20:57,01/Apr/14 15:16,14/Jul/23 06:14,01/Apr/14 15:16,0.13.0,,,,,,,,,0.13.0,,Authorization,,,,0,,,"Hiveserver2 running sql standard authorization can connect to a metastore running storage based authorization. Currently metastore is not adding the standard roles to the db in such cases.
It would be better to add them in these cases as well.

",,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/14 22:03;thejas;HIVE-6795.1.patch;https://issues.apache.org/jira/secure/attachment/12637932/HIVE-6795.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384109,,,,Tue Apr 01 15:16:00 UTC 2014,,,,,,,,,,"0|i1u69z:",384377,,,,,,,,,,,,,,,,,,,,,"31/Mar/14 22:11;ashutoshc;+1;;;","01/Apr/14 02:46;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637932/HIVE-6795.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5513 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket5
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2057/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2057/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637932;;;","01/Apr/14 09:09;thejas;[~rhbutani] I think we should include this in 0.13, this is a small change that makes new authorization setup more flexible.
;;;","01/Apr/14 14:36;rhbutani;+1 for 0.13;;;","01/Apr/14 15:16;ashutoshc;Committed to trunk & 0.13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
select * from parquet hive table containing map columns runs into exception,HIVE-6794,12705769,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,,tongjie,tongjie,31/Mar/14 19:12,31/Mar/14 21:29,14/Jul/23 06:14,31/Mar/14 21:29,0.13.0,,,,,,,,,,,File Formats,Serializers/Deserializers,,,0,,,"select * from parquet hive table containing map columns run into exception ""FAILED: RuntimeException java.lang.ClassCastException: parquet.hive.serde.DeepParquetHiveMapInspector cannot be cast to parquet.hive.serde.StandardParquetHiveMapInspector""

To reproduce, use the following steps (both regular_map_table and parquet_map_table contains schema ""c1 , c2 int""):

hive> insert overwrite table parquet_map_table select * from regular_map_table;
hive> select * from parquet_map_table; (the first query to select * works fine )

OK
{""key1"":""value1""} 1
{""key2"":""value2""} 2
{""key3"":""value3""} 3
Time taken: 2.669 seconds, Fetched: 3 row(s)
hive> select * from parquet_map_table; (the second and all subsequent exact same query breaks)

FAILED: RuntimeException java.lang.ClassCastException: parquet.hive.serde.DeepParquetHiveMapInspector cannot be cast to parquet.hive.serde.StandardParquetHiveMapInspector

Interestingly ""select c1 from parquet_map_column"" query works fine though.",,szehon,tongjie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6575,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384093,,,,Mon Mar 31 21:29:15 UTC 2014,,,,,,,,,,"0|i1u66f:",384361,,,,,,,,,,,,,,,,,,,,,"31/Mar/14 21:29;szehon;This is already solved by HIVE-6575, can you try with trunk?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DDLSemanticAnalyzer.analyzeShowRoles() should use HiveAuthorizationTaskFactory,HIVE-6793,12705734,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,xuefuz,xuefuz,xuefuz,31/Mar/14 18:26,13/Nov/14 19:43,14/Jul/23 06:14,01/Apr/14 15:10,0.13.0,,,,,,,,,0.14.0,,Authorization,Query Processor,,,0,,,"Currently DDLSemanticAnalyzer.analyzeShowRoles() isn't using HiveAuthorizationTaskFactory to create task, at odds with other Authorization related task creations such as for analyzeShowRolePrincipals(). This JIRA is to make it consistent.",,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/14 19:02;xuefuz;HIVE-6793.patch;https://issues.apache.org/jira/secure/attachment/12637896/HIVE-6793.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,384068,,,,Thu Nov 13 19:43:01 UTC 2014,,,,,,,,,,"0|i1u60v:",384336,,,,,,,,,,,,,,,,,,,,,"31/Mar/14 22:44;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637896/HIVE-6793.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5513 tests executed
*Failed tests:*
{noformat}
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2052/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2052/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637896;;;","31/Mar/14 22:53;xuefuz;the above test failure isn't related to the patch. It also appears in other test runs.;;;","31/Mar/14 22:57;ashutoshc;+1;;;","01/Apr/14 15:10;ashutoshc;Committed to trunk. Thanks, Xuefu!;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hive.warehouse.subdir.inherit.perms doesn't work correctly in CTAS,HIVE-6792,12705576,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ctang,ctang,ctang,31/Mar/14 01:04,13/Nov/14 19:44,14/Jul/23 06:14,05/Apr/14 19:56,0.14.0,,,,,,,,,0.14.0,,Authorization,Security,,,0,,,"hive.warehouse.subdir.inherit.perms doesn't work correctly in CTAS. When it is set to true, the table created using create table .. as select.. does not inherit its parent directory's group and permission mode. It can be easily reproduced:
==
hive> dfs -ls -R /user/hive/warehouse;
drwxrwx--T   - hive   hive                0 2014-03-30 17:44 /user/hive/warehouse/ctas.db
drwxr-xr-x   - hive   hive                0 2014-03-30 17:20 /user/hive/warehouse/ctas_src_tbl
-rw-r--r--   3 hive   hive            46059 2014-03-30 17:20 /user/hive/warehouse/ctas_src_tbl/000000_0

hive> create table ctas.test_perm as select * from ctas_src_tbl;
                        
hive> dfs -ls -R /user/hive/warehouse;                          
drwxrwx--T   - hive   hive                0 2014-03-30 17:46 /user/hive/warehouse/ctas.db
drwxr-xr-x   - hive   supergroup          0 2014-03-30 17:46 /user/hive/warehouse/ctas.db/test_perm
-rw-r--r--   3 hive   supergroup      46059 2014-03-30 17:46 /user/hive/warehouse/ctas.db/test_perm/000000_0
drwxr-xr-x   - hive   hive                0 2014-03-30 17:20 /user/hive/warehouse/ctas_src_tbl
-rw-r--r--   3 hive   hive            46059 2014-03-30 17:20 /user/hive/warehouse/ctas_src_tbl/000000_0
==
The created table does not inherit its database ctas's group hive and permission mode 770, instead it takes the default group (supergroup) and permission mode (755) in hdfs",,brocknoland,ctang,daisuke.kobayashi,mdominguez@cloudera.com,szehon,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6892,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/14 21:38;ctang;HIVE-6792-1.patch;https://issues.apache.org/jira/secure/attachment/12637924/HIVE-6792-1.patch","04/Apr/14 04:22;brocknoland;HIVE-6792.1.patch;https://issues.apache.org/jira/secure/attachment/12638621/HIVE-6792.1.patch","31/Mar/14 01:52;ctang;HIVE-6792.patch;https://issues.apache.org/jira/secure/attachment/12637756/HIVE-6792.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,383905,,,,Thu Nov 13 19:44:28 UTC 2014,,,,,,,,,,"0|i1u50n:",384173,,,,,,,,,,,,,,,,,,,,,"31/Mar/14 01:52;ctang;Please review attached patch. With this, the new created table (test_perm) with its data file inherit group and permission mode from their parent cats.db.
==
hive> dfs -ls -R /user/hive/warehouse;                          
drwxrwx--T   - hive   hive                0 2014-03-30 17:56 /user/hive/warehouse/ctas.db
drwxrwx--T   - hive   hive                0 2014-03-30 17:56 /user/hive/warehouse/ctas.db/test_perm
-rw-rw----   3 hive   hive            46059 2014-03-30 17:56 /user/hive/warehouse/ctas.db/test_perm/000000_0
drwxr-xr-x   - hive   hive                0 2014-03-30 17:20 /user/hive/warehouse/ctas_src_tbl
-rw-r--r--   3 hive   hive            46059 2014-03-30 17:20 /user/hive/warehouse/ctas_src_tbl/000000_0;;;","31/Mar/14 05:35;szehon;Mostly looks good on first pass, one nit: is it necessary to pass in 'conf' variable to 'MoveTask.createTargetPath'?  It is already a member variable.

Also, do you want to submit the patch to trigger the pre-commit test?;;;","31/Mar/14 21:38;ctang;Thanks, Szehon, for pointing out. I changed to use member variable conf instead and please see attached HIVE-6792-1.patch;;;","04/Apr/14 01:59;szehon;Please upload in the format HIVE-6792.1.patch to trigger the pre commit test.;;;","04/Apr/14 04:21;brocknoland;+1 pending tests.;;;","04/Apr/14 04:22;brocknoland;Uploading the same patch with a name precommits like.;;;","05/Apr/14 19:42;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638621/HIVE-6792.1.patch

{color:green}SUCCESS:{color} +1 5547 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2129/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2129/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638621;;;","05/Apr/14 19:56;brocknoland;Thank you Chaoyu! I have committed this to trunk.;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveStatement client transport lock should unlock in finally block.,HIVE-6789,12705503,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vgumashta,vgumashta,vgumashta,29/Mar/14 21:26,01/Apr/14 21:22,14/Jul/23 06:14,01/Apr/14 21:22,0.13.0,,,,,,,,,0.13.0,,JDBC,,,,0,,,,,rhbutani,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/14 21:39;vgumashta;HIVE-6789.1.patch;https://issues.apache.org/jira/secure/attachment/12637670/HIVE-6789.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382780,,,,Tue Apr 01 21:22:00 UTC 2014,,,,,,,,,,"0|i1ty2n:",383048,,,,,,,,,,,,,,,,,,,,,"29/Mar/14 21:39;vgumashta;cc [~thejas] [~rhbutani] bug for 13!;;;","30/Mar/14 03:29;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637670/HIVE-6789.1.patch

{color:green}SUCCESS:{color} +1 5502 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2040/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2040/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637670;;;","31/Mar/14 22:32;thejas;+1;;;","01/Apr/14 14:35;rhbutani;+1 for .13;;;","01/Apr/14 21:22;thejas;Patch committed to 0.13 branch and trunk.
Thanks for the contribution Vaibhav!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Abandoned opened transactions not being timed out,HIVE-6788,12705492,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,gates,gates,gates,29/Mar/14 18:48,13/Nov/14 19:40,14/Jul/23 06:14,03/Apr/14 06:31,0.13.0,,,,,,,,,0.14.0,,Locking,,,,0,,,If a client abandons an open transaction it is never closed.  This does not cause any immediate problems (as locks are timed out) but it will eventually lead to high levels of open transactions in the lists that readers need to be aware of when reading tables or partitions.,,gates,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/14 17:04;gates;HIVE-6788.patch;https://issues.apache.org/jira/secure/attachment/12638090/HIVE-6788.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382769,,,,Thu Nov 13 19:40:55 UTC 2014,,,,,,,,,,"0|i1ty07:",383037,,,,,,,,,,,,,,,,,,,,,"01/Apr/14 17:04;gates;This patch adds logic to getOpenTxns to check for any abandoned transactions and move them from open to aborted before returning the list of open transactions.;;;","02/Apr/14 00:27;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638090/HIVE-6788.patch

{color:green}SUCCESS:{color} +1 5539 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2072/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2072/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638090;;;","02/Apr/14 18:40;ashutoshc;+1;;;","03/Apr/14 06:31;ashutoshc;Committed to trunk. Thanks, Alan!;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ORC+ACID assumes all missing buckets are in ACID structure,HIVE-6787,12704415,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,omalley,gopalv,gopalv,29/Mar/14 05:15,08/Apr/14 15:12,14/Jul/23 06:14,08/Apr/14 15:12,0.13.0,,,,,,,,,0.13.0,,File Formats,,,,0,,,"ORC+ACID creates ACID structure splits for all missing buckets in a table

{code}
java.lang.RuntimeException: java.io.IOException: java.io.IOException: Vectorization and ACID tables are incompatible.
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:996)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:240)
	... 15 more
{code}

The tables are normal ORC tables and are not using ACID structure at all.

{code}
@@ -539,7 +539,7 @@ public void run() {
         for(int b=0; b < context.numBuckets; ++b) {
           if (!covered[b]) {
             context.splits.add(new OrcSplit(dir, b, 0, new String[0], null,
-                               false, false, deltas));
+                               isOriginal, false, deltas));
           }
         }
{code}

seems to fix the issue. [~owen.omalley], please confirm if that is what I should be doing.",,gopalv,omalley,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/14 19:33;omalley;HIVE-6787.patch;https://issues.apache.org/jira/secure/attachment/12638317/HIVE-6787.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382749,,,,Tue Apr 08 15:12:03 UTC 2014,,,,,,,,,,"0|i1txvr:",383017,,,,,,,,,,,,,,,,,,,,,"02/Apr/14 19:33;omalley;This fixes the problem, adds a test case, and also makes a log message that was confusing users a debug level.;;;","03/Apr/14 15:05;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638317/HIVE-6787.patch

{color:green}SUCCESS:{color} +1 5543 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2095/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2095/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638317;;;","04/Apr/14 00:24;sershe;+1;;;","08/Apr/14 15:12;omalley;Thanks for the review, Sergey!

I just committed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Off by one error in ORC PPD ,HIVE-6786,12704413,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,prasanth_j,gopalv,gopalv,29/Mar/14 05:00,31/Jul/14 18:42,14/Jul/23 06:14,01/Apr/14 21:22,0.13.0,,,,,,,,,0.13.0,,,,,,0,,,"Turning on ORC PPD makes split computation fail for a 10Tb benchmark.

Narrowed down to the following code fragment

https://github.com/apache/hive/blob/branch-0.13/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java#L757

{code}
                includeStripe[i] = (i > stripeStats.size()) ||
                    isStripeSatisfyPredicate(stripeStats.get(i), sarg,
                                             filterColumns);
{code}

I would guess that should be a >=, but [~prasanth_j], can you comment if that is the right fix?

Stack trace will look like: 
{code}
Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at java.util.ArrayList.rangeCheck(ArrayList.java:604)
	at java.util.ArrayList.get(ArrayList.java:382)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.run(OrcInputFormat.java:761)
	... 3 more
{code}",,gopalv,prasanth_j,rhbutani,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/14 00:30;prasanth_j;HIVE-6786.1.patch;https://issues.apache.org/jira/secure/attachment/12637678/HIVE-6786.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382747,,,,Tue Apr 01 21:22:45 UTC 2014,,,,,,,,,,"0|i1txvb:",383015,,,,,,,,,,,,,,,,,,,,,"30/Mar/14 00:29;prasanth_j;[~gopalv] Thats looks like an off-by-one error. It should be >=. I think this case will only be hit when the stripe statistics is missing, in which case the stripeStatistics.size() will be 0 and all stripes should be included.
The attached patch fixes it.

I am curious how is it missing stripe statistics? Is it an old ORC which did not have stripe statistics or is the ORC file generated via other means (apart from hive)?;;;","30/Mar/14 00:34;prasanth_j;[~gopalv] Was it throwing ArrayIndexOutOfBounds exception when PPD is enabled?;;;","30/Mar/14 05:24;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637678/HIVE-6786.1.patch

{color:green}SUCCESS:{color} +1 5502 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2041/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2041/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637678;;;","30/Mar/14 15:32;gopalv;[~prasanth_j]: This is an old ORC 10Tb database which hasn't been modified for a while now.;;;","31/Mar/14 18:10;sershe;+1;;;","01/Apr/14 01:03;rhbutani;+1 for 0.13;;;","01/Apr/14 21:22;rhbutani;Committed to trunk and 0.13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
query fails when partitioned table's table level serde is ParquetHiveSerDe and partition level serde is of different SerDe,HIVE-6785,12704409,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,tongjie,tongjie,tongjie,29/Mar/14 02:32,13/Nov/14 19:42,14/Jul/23 06:14,27/Apr/14 18:49,0.13.0,,,,,,,,,0.14.0,,File Formats,Serializers/Deserializers,,,0,,,"When a hive table's SerDe is ParquetHiveSerDe, while some partitions are of other SerDe, AND if this table has string column[s], hive generates confusing error message:

""Failed with exception java.io.IOException:java.lang.ClassCastException: parquet.hive.serde.primitive.ParquetStringInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableTimestampObjectInspector""

This is confusing because timestamp is mentioned even if it is not been used by the table. The reason is when there is SerDe difference between table and partition, hive tries to convert objectinspector of two SerDes. ParquetHiveSerDe's object inspector for string type is ParquetStringInspector (newly introduced), neither a subclass of WritableStringObjectInspector nor JavaStringObjectInspector, which ObjectInspectorConverters expect for string category objector inspector. There is no break statement in STRING case statement, hence the following TIMESTAMP case statement is executed, generating confusing error message.

see also in the following parquet issue:
https://github.com/Parquet/parquet-mr/issues/324

To fix that it is relatively easy, just make ParquetStringInspector subclass of JavaStringObjectInspector instead of AbstractPrimitiveJavaObjectInspector. But because constructor of class JavaStringObjectInspector is package scope instead of public or protected, we would need to move ParquetStringInspector to the same package with JavaStringObjectInspector.

Also ArrayWritableObjectInspector's setStructFieldData needs to also accept List data, since the corresponding setStructFieldData and create methods return a list. This is also needed when table SerDe is ParquetHiveSerDe, and partition SerDe is something else.


",,brocknoland,lukas.nalezenec,szehon,thejas,tongjie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/14 02:07;tongjie;HIVE-6785.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12638183/HIVE-6785.1.patch.txt","05/Apr/14 22:55;tongjie;HIVE-6785.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12638885/HIVE-6785.2.patch.txt","11/Apr/14 17:35;tongjie;HIVE-6785.3.patch;https://issues.apache.org/jira/secure/attachment/12639834/HIVE-6785.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382743,,,,Thu Nov 13 19:42:11 UTC 2014,,,,,,,,,,"0|i1txuf:",383011,,,,,,,,,,,,,,,,,,,,,"29/Mar/14 15:32;brocknoland;FYI [~jcoffey] [~xuefuz] [~szehon];;;","02/Apr/14 02:10;tongjie;This patch involves deleting file and adding new files (mv),  and there is no instruction to delete/add if using git in https://cwiki.apache.org/confluence/display/Hive/HowToContribute; however my patch is using git diff, if that does not work, I will resubmit a patch using svn.

https://reviews.apache.org/r/19896/;;;","02/Apr/14 08:19;szehon;Shouldn't need to use svn for that, you can try 'git add'/ 'git rm'.  

I left couple comments on the rb for consideration.  Also when you are ready, click 'submit patch' to trigger the pre commit test.;;;","02/Apr/14 18:24;szehon;In my opinion, it is better to change the javastringobjectinspector's Ctor to protected , so we can keep all the parquet inspector in the same package.

Also it would be good to add a q-test for this case. You can write one following the example of ""parquet_create.q"" and then generate and verify the result using the following command (maven version) on your q-test file: [https://cwiki.apache.org/confluence/display/Hive/HiveDeveloperFAQ#HiveDeveloperFAQ-HowdoupdatetheoutputofaCliDrivertestcase?|https://cwiki.apache.org/confluence/display/Hive/HiveDeveloperFAQ#HiveDeveloperFAQ-HowdoupdatetheoutputofaCliDrivertestcase?];;;","02/Apr/14 18:37;tongjie;If we change javastringobjectinspector's Ctor to be protected, that should work fine for 0.13.   But when we backport this jira to parquet-hive, it will break unless we also make the same change in hive 0.12 and hive 0.10, right?
;;;","02/Apr/14 19:05;szehon;Yea thats right, do you think it would be a huge issue though if its a hive 0.13 only fix?;;;","02/Apr/14 19:11;tongjie;The only issue I see is that parquet-hive cannot backport this jira.;;;","02/Apr/14 19:27;szehon;OK I'm not a huge fan of moving that inspector to a unnatural place because it will be stuck like that going forward in hive, but we can let others also chime in.  

If its really important to support for earlier hive, maybe one option is to back-port a different version of the patch into parquet?;;;","02/Apr/14 19:33;tongjie;If we can be flexible by back-porting a different version of the patch into parquet-hive, that would be great!

I like to keep parquet related stuff in parquet package as well.;;;","05/Apr/14 20:42;tongjie;When I add a qtest, I realized that this bug is resolved with this patch in hive-trunk. But it is still a bug in Hive-0.11 now.

Digging a little bit, I found that when Partition SerDe and Table SerDe are different, hive 0.11 would try to convert object inspector as long as they are not equals; however, in hive-trunk (0.13 or 0.14), if output ObjectInspector's all fields are all settable, there is no conversion happening, hence the bug presented in this jira does not show in hive-trunk any more.

However, I do think that ParquetStringInspector should be subclass of JavaStringObjectInspector, so that Hive 0.11 would have no problem as well.

related Hive Jiras:

HIVE-5202
HIVE-5394

------------------- HIVE-trunk (0.13, 0.14 etc) code snippet for ObjectInspectorConverters -------------------------
    // 1. If equalsCheck is true and the inputOI is the same as the outputOI OR
    // 2. If the outputOI has all fields settable, return it
    if ((equalsCheck && inputOI.equals(outputOI)) ||
        ObjectInspectorUtils.hasAllFieldsSettable(outputOI, oiSettableProperties) == true) {
      return outputOI;
    }   

------------------- HIVE-0.11 code snippet for ObjectInspectorConverters -------------------------
    // If the inputOI is the same as the outputOI, just return it
    if (inputOI.equals(outputOI)) {
      return outputOI;
    }   ;;;","05/Apr/14 21:19;tongjie;In my previous comment, I mean that this bug is not reproducible in hive trunk due to patches from HIVE-5202 and HIVE-5394.

But the patch introduced in this Jira is still an enhancement.

Btw, how can I edit my own previous comment once submitted?;;;","05/Apr/14 22:55;tongjie;add a new qtest;;;","06/Apr/14 19:20;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638885/HIVE-6785.2.patch.txt

{color:green}SUCCESS:{color} +1 5549 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2153/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2153/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638885;;;","08/Apr/14 00:54;szehon;+1 (non-binding) , thanks for adding the q-test and address comments.

FYI [~brocknoland];;;","08/Apr/14 01:09;brocknoland;Hi,

LGTM except I see we are using the parquet... class names when creating a table, which are soon to be removed.;;;","08/Apr/14 01:26;szehon;Good catch Brock, I missed that.;;;","11/Apr/14 01:01;tongjie;Hi [~brocknoland], 

are you suggesting using:

ALTER TABLE parquet_mixed_fileformat SET FILEFORMAT PARQUET;  ( after I execute this statement, the table become ""not found"")

instead of:

ALTER TABLE parquet_mixed_fileformat set SERDE 'parquet.hive.serde.ParquetHiveSerDe';
ALTER TABLE parquet_mixed_fileformat
     SET FILEFORMAT
     INPUTFORMAT 'parquet.hive.DeprecatedParquetInputFormat'
     OUTPUTFORMAT 'parquet.hive.DeprecatedParquetOutputFormat';

Please advise.;;;","11/Apr/14 09:15;szehon;Hi Tonjie, these are deprecated now and will be removed.  See the discussion on HIVE-6757, for the current state.

Use 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe',
'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',
'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat';;;","11/Apr/14 17:35;tongjie;replace deprecated parquet class.;;;","11/Apr/14 20:21;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639834/HIVE-6785.3.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5615 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_dyn_part
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_reduce_deduplicate
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority2
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2221/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2221/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639834;;;","11/Apr/14 20:36;tongjie;[~szehon], are those failure transient? the new patch only changes the parquet class, which has nothing to do with these test cases.;;;","11/Apr/14 20:54;szehon;Yea, it doesnt look related to this patch.;;;","11/Apr/14 23:55;szehon;Looked at the new patch, looks fine with me;;;","15/Apr/14 22:18;szehon;+1 (non-binding) +  [~brocknoland] ;;;","16/Apr/14 12:32;brocknoland;+1;;;","27/Apr/14 18:49;brocknoland;Committed to trunk! Thank you for the contribution!

Thank you Szehon for the review!;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incompatible schema for maps between parquet-hive and parquet-pig,HIVE-6783,12704406,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,,tongjie,tongjie,29/Mar/14 02:16,09/Jun/14 06:39,14/Jul/23 06:14,02/Apr/14 21:57,0.13.0,,,,,,,,,0.13.1,0.14.0,File Formats,,,,0,,,"see also in following parquet issue:
https://github.com/Parquet/parquet-mr/issues/290

The schema written for maps isn't compatible between hive and pig. This means any files written in one cannot be properly read in the other.

More specifically,  for the same map column c1, parquet-pig generates schema:

message pig_schema {
  optional group c1 (MAP) {
    repeated group map (MAP_KEY_VALUE) {
      required binary key (UTF8);
      optional binary value;
    }   
  }
}

while parquet-hive generates schema:
message hive_schema {
   optional group c1 (MAP_KEY_VALUE) {
     repeated group map {
       required binary key;
       optional binary value;
   }
 }
}
",,brocknoland,sushanth,szehon,thejas,tongjie,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/14 05:55;tongjie;HIVE-6783.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12637609/HIVE-6783.1.patch.txt","29/Mar/14 18:14;tongjie;HIVE-6783.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12637660/HIVE-6783.2.patch.txt","29/Mar/14 18:32;tongjie;HIVE-6783.3.patch.txt;https://issues.apache.org/jira/secure/attachment/12637661/HIVE-6783.3.patch.txt","29/Mar/14 18:36;tongjie;HIVE-6783.4.patch.txt;https://issues.apache.org/jira/secure/attachment/12637662/HIVE-6783.4.patch.txt",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382740,,,,Mon Jun 09 06:39:39 UTC 2014,,,,,,,,,,"0|i1txtr:",383008,,,,,,,,,,,,,,,,,,,,,"29/Mar/14 05:55;tongjie;this patch will correctly convert hive map type (consistent with Pig map);;;","29/Mar/14 14:28;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637609/HIVE-6783.1.patch.txt

{color:green}SUCCESS:{color} +1 5503 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2031/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2031/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637609;;;","29/Mar/14 15:30;brocknoland;Hi,

The patch has tabs where as Hive requires two spaces. Can you fix that and then put a Review Board item up? (reviews.apache.org)

FYI [~jcoffey] [~xuefuz];;;","29/Mar/14 15:37;brocknoland;FYI [~szehon];;;","29/Mar/14 18:14;tongjie;remove tab and clean up some format;;;","29/Mar/14 18:41;tongjie;https://reviews.apache.org/r/19825/;;;","30/Mar/14 01:33;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637662/HIVE-6783.4.patch.txt

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5503 tests executed
*Failed tests:*
{noformat}
org.apache.hive.jdbc.TestJdbcDriver2.testNewConnectionConfiguration
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2036/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2036/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637662;;;","30/Mar/14 02:07;tongjie;Hmm, why formatting code would cause one previously passed test to fail. The failure is related to jdbc, which is unrelated at all.

Is that transient error?;;;","31/Mar/14 04:46;szehon;Hi Tongjie yea it looks like a transient error, a few pre commit builds around that time also failed with same failure.

About this patch, my question is, will it affect backward compatibility of reading existing stored maps in hive-parquet?;;;","31/Mar/14 19:25;tongjie;I think it should be backward compatible, I am able to read existing stored maps with this fix.;;;","31/Mar/14 22:06;szehon;Thanks, that is my only concern to not break existing stored maps, I'm ok if thats the case.  Although I wonder, it would seem pig should be able to read hive's maps if that is the case, because we are using pig's schema now to read?  Or is there some difference there?
;;;","31/Mar/14 22:56;tongjie;The fix presented in this jira is to tag map type with the appropriate OriginalType.

The reason hive is backward compatible is that when parquet-hive converts parquet fields map.key and map.value back to hive map column, it does not check MAP_KEY_VALUE OriginalType.  Also, the equals method of GroupType does not check OriginalType at all, hence hive_schema and pig_schema shown in description section are treated as equal (the only different is OriginalType).

However, parquet-pig's PigSchemaConverter checks to make sure map's OriginalType is of correct type, hence it breaks when it reads Hive's map.

With the fix, pig now can read hive's map since hive converts it right.
;;;","31/Mar/14 23:00;szehon;Thanks looks good to me.  + [~brocknoland], [~xuefuz];;;","01/Apr/14 17:53;xuefuz;+1;;;","02/Apr/14 21:57;xuefuz;Patch committed to trunk. Thanks to Tongjie for the contribution.

BTW, it seems that you're not in the contributor list so I'm not able to assign this JIRA to your credit. You might request this on dev email list.;;;","17/Apr/14 00:12;szehon;Hi [~rhbutani], the Parquet community is wondering if this can be also in 0.13.1, or some release of 0.13?  That would be helpful , as this fixes parquet format compatibility between hive and pig.  Thanks.;;;","12/May/14 21:47;sushanth;(Btw, to be explicit, this has been accepted for inclusion in 0.13.1, and will be part of it);;;","09/Jun/14 06:39;thejas;This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HiveServer2Concurrency issue when running with tez intermittently, throwing ""org.apache.tez.dag.api.SessionNotRunning: Application not running"" error",HIVE-6782,12704402,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vikram.dixit,vikram.dixit,vikram.dixit,29/Mar/14 01:05,13/Nov/14 19:41,14/Jul/23 06:14,08/Apr/14 18:36,,,,,,,,,,0.13.0,0.14.0,Tez,,,,0,,,"HiveServer2 concurrency is failing intermittently when using tez, throwing ""org.apache.tez.dag.api.SessionNotRunning: Application not running"" error",,cdrome,hagleitn,leftyl,rhbutani,thejas,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6962,,,,,,,,,,,,,,,,,,,,,"29/Mar/14 01:13;vikram.dixit;HIVE-6782.1.patch;https://issues.apache.org/jira/secure/attachment/12637594/HIVE-6782.1.patch","07/Apr/14 22:30;vikram.dixit;HIVE-6782.10.patch;https://issues.apache.org/jira/secure/attachment/12639082/HIVE-6782.10.patch","08/Apr/14 07:49;vikram.dixit;HIVE-6782.11.patch;https://issues.apache.org/jira/secure/attachment/12639155/HIVE-6782.11.patch","29/Mar/14 02:48;vikram.dixit;HIVE-6782.2.patch;https://issues.apache.org/jira/secure/attachment/12637599/HIVE-6782.2.patch","03/Apr/14 03:27;vikram.dixit;HIVE-6782.3.patch;https://issues.apache.org/jira/secure/attachment/12638404/HIVE-6782.3.patch","03/Apr/14 06:28;vikram.dixit;HIVE-6782.4.patch;https://issues.apache.org/jira/secure/attachment/12638432/HIVE-6782.4.patch","04/Apr/14 02:35;vikram.dixit;HIVE-6782.5.patch;https://issues.apache.org/jira/secure/attachment/12638617/HIVE-6782.5.patch","04/Apr/14 19:07;vikram.dixit;HIVE-6782.6.patch;https://issues.apache.org/jira/secure/attachment/12638746/HIVE-6782.6.patch","04/Apr/14 23:48;vikram.dixit;HIVE-6782.7.patch;https://issues.apache.org/jira/secure/attachment/12638801/HIVE-6782.7.patch","05/Apr/14 02:14;vikram.dixit;HIVE-6782.8.patch;https://issues.apache.org/jira/secure/attachment/12638827/HIVE-6782.8.patch","07/Apr/14 20:59;vikram.dixit;HIVE-6782.9.patch;https://issues.apache.org/jira/secure/attachment/12639059/HIVE-6782.9.patch",,,,,11.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382736,,,,Thu Nov 13 19:41:26 UTC 2014,,,,,,,,,,"0|i1txsv:",383004,,,,,,,,,,,,,,,,,,,,,"29/Mar/14 02:29;hagleitn;I think for hive 13 it'd be safer to just do what MR does and set the perms to 777 on the root folder. We should open a ticket for the next release though to do the proper fix (since it involves changing config vars we might have to keep 2 ways for a release or so). Thoughts?;;;","29/Mar/14 02:48;vikram.dixit;Address Gunther's comments. Going with the simpler approach.;;;","29/Mar/14 11:34;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637599/HIVE-6782.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5502 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority2
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2029/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2029/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637599;;;","03/Apr/14 03:27;vikram.dixit;Latest iteration fixes more concurrency issues.;;;","03/Apr/14 06:28;vikram.dixit;Missed a file.;;;","04/Apr/14 01:55;vikram.dixit;Some unit tests are failing.;;;","04/Apr/14 12:24;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638617/HIVE-6782.5.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5546 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.testGetNonDefaultSession
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2109/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2109/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638617;;;","04/Apr/14 22:11;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638746/HIVE-6782.6.patch

{color:green}SUCCESS:{color} +1 5546 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2116/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2116/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638746;;;","04/Apr/14 23:09;vikram.dixit;Addressed review comments.;;;","04/Apr/14 23:56;vikram.dixit;Created HIVE-6847 for longer term fix.;;;","05/Apr/14 02:14;vikram.dixit;Address review comments.;;;","05/Apr/14 02:37;thejas;+1;;;","05/Apr/14 03:04;rhbutani;+1 for 0.13;;;","05/Apr/14 17:42;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638827/HIVE-6782.8.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5547 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2128/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2128/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638827;;;","07/Apr/14 20:59;vikram.dixit;Fix the case where a tez session is launched without a query.;;;","07/Apr/14 22:01;thejas;+1 to the update as well.
;;;","07/Apr/14 22:30;vikram.dixit;Needed rebase after HIVE-6739.;;;","08/Apr/14 03:42;leftyl;This adds *hive.localize.resource.wait.interval* and *hive.localize.resource.num.wait.attempts* to HiveConf.java.

They need descriptions in hive-default.xml.template or in a release note (since hive-default.xml.template will be generated from the new HiveConf.java after HIVE-6037 gets committed). When the time comes, I'll add them to the post-HIVE-6037 list (in HIVE-6586) and put them in the Configuration Properties wikidoc.;;;","08/Apr/14 07:49;vikram.dixit;Address Lefty's comment.;;;","08/Apr/14 09:08;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639082/HIVE-6782.10.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5549 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.testSimplePrivileges
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2171/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2171/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639082;;;","08/Apr/14 11:11;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639155/HIVE-6782.11.patch

{color:green}SUCCESS:{color} +1 5549 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2172/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2172/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639155;;;","08/Apr/14 18:36;vikram.dixit;Committed to both trunk and branch-0.13;;;","01/Jul/14 08:44;leftyl;*hive.localize.resource.wait.interval* & *hive.localize.resource.num.wait.attempts* are documented in the wiki here:

* [Configuration Properties -- Tez -- hive.localize.resource.wait.interval | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.localize.resource.wait.interval]
* [Configuration Properties -- Tez -- hive.localize.resource.num.wait.attempts | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.localize.resource.num.wait.attempts]

I also added a comment to HIVE-6586 so they won't get lost in the shuffle when HIVE-6037 changes HiveConf.java.;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive JDBC in http mode is using HiveConf - should be removed ,HIVE-6781,12704400,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vgumashta,vgumashta,vgumashta,29/Mar/14 00:39,30/Mar/14 23:46,14/Jul/23 06:14,30/Mar/14 23:46,0.13.0,,,,,,,,,0.13.0,,JDBC,,,,0,,,"This change is needed so that in unsecured mode, the jdbc driver does not depend on HiveConf which is derived from hadoop's Configuration class, continue being a thin client. ",,rhbutani,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/14 00:52;vgumashta;HIVE-6781.1.patch;https://issues.apache.org/jira/secure/attachment/12637586/HIVE-6781.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382734,,,,Sun Mar 30 23:46:22 UTC 2014,,,,,,,,,,"0|i1txsf:",383002,,,,,,,,,,,,,,,,,,,,,"29/Mar/14 00:52;vgumashta;cc [~thejas] Minor patch.

cc [~rhbutani] Bug for 13!;;;","30/Mar/14 00:33;thejas;+1
[~rhbutani] It will be very useful to have this patch in 0.13. Otherwise, we will need to add hadoop as a dependency for jdbc applications, even for the unsecure mode.
;;;","30/Mar/14 09:14;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637586/HIVE-6781.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5502 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_parallel_orderby
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2042/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2042/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637586;;;","30/Mar/14 16:28;rhbutani;+1 for 0.13;;;","30/Mar/14 23:46;thejas;Patch committed to 0.13 branch and trunk.
Thanks for the contribution Vaibhav!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set tez credential file property along with MR conf property for Tez jobs,HIVE-6780,12704395,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,29/Mar/14 00:01,11/Apr/14 18:20,14/Jul/23 06:14,02/Apr/14 21:18,0.13.0,,,,,,,,,0.13.0,,WebHCat,,,,0,,,"webhcat should set the additional property - ""tez.credentials.path"" to the same value as the MapReduce property.
WebHCat should always proactively set this tez.credentials.path property to the same value and in the same cases where it is setting the MR equivalent property.
NO PRECOMMIT TESTS",,ekoifman,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6896,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/14 21:48;ekoifman;HIVE-6780.2.patch;https://issues.apache.org/jira/secure/attachment/12638136/HIVE-6780.2.patch","29/Mar/14 00:05;ekoifman;HIVE-6780.patch;https://issues.apache.org/jira/secure/attachment/12637580/HIVE-6780.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382729,,,,Wed Apr 02 21:18:43 UTC 2014,,,,,,,,,,"0|i1txrb:",382997,,,,,,,,,,,,,,,,,,,,,"29/Mar/14 00:05;ekoifman;NO PRECOMMIT TESTS;;;","29/Mar/14 01:18;thejas;+1;;;","01/Apr/14 08:43;thejas;This needs rebasing post HIVE-6546 . We should probably include similar fix for the new param as well. [~ekoifman] Can you please take a look ?
;;;","01/Apr/14 21:48;ekoifman;rebased and addressed Thejas' comments;;;","01/Apr/14 22:21;thejas;+1;;;","02/Apr/14 21:08;thejas;[~rhbutani] It will be very useful to have this bug fix in 0.13, it is needed to get tez jobs from webhcat working.
;;;","02/Apr/14 21:09;rhbutani;+1 for 0.13;;;","02/Apr/14 21:18;thejas;Patch committed to 0.13 branch and trunk.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive cli may get into inconsistent state when Ctrl-C is hit on hadoop2,HIVE-6779,12704393,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,28/Mar/14 23:40,31/Mar/14 22:36,14/Jul/23 06:14,31/Mar/14 22:36,,,,,,,,,,0.13.0,,Diagnosability,,,,0,,,,,jdere,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/14 23:41;ashutoshc;HIVE-6779.patch;https://issues.apache.org/jira/secure/attachment/12637573/HIVE-6779.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382727,,,,Mon Mar 31 22:36:35 UTC 2014,,,,,,,,,,"0|i1txqv:",382995,,,,,,,,,,,,,,,,,,,,,"28/Mar/14 23:45;ashutoshc;https://reviews.apache.org/r/19810/;;;","29/Mar/14 05:50;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637573/HIVE-6779.patch

{color:green}SUCCESS:{color} +1 5502 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2027/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2027/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637573;;;","31/Mar/14 19:02;ashutoshc;To add more context: Important part of patch is removal of interrupting of main thread for SignalHandler. In hadop-1 java io with sockets were used to communicate between client and server. So, interrupting main thread wasn't an issue. However,  Hadoop-2 uses java nio based interruptible channels, which throws up if current thread is interrupted. ;;;","31/Mar/14 20:46;jdere;+1;;;","31/Mar/14 22:36;ashutoshc;Committed to trunk & 0.13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ql/src/test/queries/clientpositive/pcr.q covers the test which generate 1.0 =1 predicate in partition pruner. ,HIVE-6778,12704392,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,rhbutani,hsubramaniyan,hsubramaniyan,28/Mar/14 23:20,02/Apr/14 03:45,14/Jul/23 06:14,02/Apr/14 03:45,,,,,,,,,,0.13.0,,,,,,0,,,"select key, value, ds from pcr_foo where (ds % 2 == 1);
ql/src/test/queries/clientpositive/pcr.q

The test generates 1.0==1 predicate in the pruner which cannot be evaluated since a double cannot be converted to int.",,hsubramaniyan,jnp,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6642,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/14 02:00;rhbutani;HIVE-6778.1.patch;https://issues.apache.org/jira/secure/attachment/12637977/HIVE-6778.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382726,,,,Wed Apr 02 03:45:49 UTC 2014,,,,,,,,,,"0|i1txqn:",382994,,,,,,,,,,,,,,,,,,,,,"01/Apr/14 02:15;hsubramaniyan;+1;;;","01/Apr/14 18:33;jnp;+1;;;","02/Apr/14 02:23;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637977/HIVE-6778.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5539 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_dyn_part
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2073/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2073/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637977;;;","02/Apr/14 03:32;rhbutani;this failure is not related.
Ran locally and validated that the test passes.;;;","02/Apr/14 03:45;rhbutani;Committed to trunk and 0.13
thanks Hari, Jitendra for reviewing;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update readme for ptest2 framework,HIVE-6773,12704344,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,szehon,szehon,szehon,28/Mar/14 19:15,13/Nov/14 19:40,14/Jul/23 06:14,08/Apr/14 19:24,,,,,,,,,,0.14.0,,Testing Infrastructure,,,,0,,,"Approvals dependency is needed for testing.  Need to add instructions.

NO PRECOMMIT TESTS",,brocknoland,szehon,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/14 19:16;szehon;HIVE-6773.patch;https://issues.apache.org/jira/secure/attachment/12637480/HIVE-6773.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382678,,,,Thu Nov 13 19:40:44 UTC 2014,,,,,,,,,,"0|i1txfr:",382946,,,,,,,,,,,,,,,,,,,,,"28/Mar/14 19:17;szehon;Hi [~brocknoland], adding some missing instruction as we discussed.;;;","29/Mar/14 15:15;brocknoland;+1;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Update WebHCat E2E tests now that comments is reported correctly in ""describe table"" output",HIVE-6771,12704195,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,deepesh,deepesh,deepesh,28/Mar/14 05:46,28/Mar/14 16:25,14/Jul/23 06:14,28/Mar/14 16:25,0.13.0,,,,,,,,,0.13.0,,Tests,,,,0,,,"HIVE-6681 corrected the comments in the describe table output, earlier it would show ""from deserializer"" in comments.
Some WebHCat E2E tests are checking for the string ""from deserializer"" even overshadowing the actual comments.",,deepesh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6681,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/14 05:49;deepesh;HIVE-6771.patch;https://issues.apache.org/jira/secure/attachment/12637339/HIVE-6771.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382529,,,,Fri Mar 28 16:25:26 UTC 2014,,,,,,,,,,"0|i1twin:",382797,,,,,,,,,,,,,,,,,,,,,"28/Mar/14 05:49;deepesh;Attaching a patch to correct the behavior in the 7 impacted tests.;;;","28/Mar/14 05:51;deepesh;Would be great if it makes it in with 0.13.0 as HIVE-6681 is already in 0.13.0.;;;","28/Mar/14 06:08;ashutoshc;+1;;;","28/Mar/14 16:25;ashutoshc;Committed to 0.13 & trunk. Thanks, Deepesh!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove hcatalog/webhcat/svr/src/main/config/override-container-log4j.properties,HIVE-6768,12704150,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,28/Mar/14 00:32,24/Mar/15 20:36,14/Jul/23 06:14,12/May/14 19:00,0.13.0,,,,,,,,,0.14.0,,WebHCat,,,,0,,,"now that MAPREDUCE-5806 is fixed we can remove override-container-log4j.properties and and all the logic around this which was introduced in HIVE-5511 to work around MAPREDUCE-5806

NO PRECOMMIT TESTS",,ekoifman,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-5511,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/14 01:01;ekoifman;HIVE-6768.patch;https://issues.apache.org/jira/secure/attachment/12641624/HIVE-6768.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382484,,,,Thu Nov 13 19:39:54 UTC 2014,,,,,,,,,,"0|i1tw8v:",382753,,,,,,,,,,,,,,,,,,,,,"17/Apr/14 18:51;ashutoshc;In addition to this file, I assume we also need to revert changes introduced in HIVE-5511 [~ekoifman] would you like to attach a patch for this?;;;","08/May/14 18:42;thejas;[~ekoifman] Can you respond on Ashutosh's comment ? 

Looks like most of the changes in HIVE-5511 are not specific to the issue, but were general cleanup.  And the attached patch reverts changes that were specific to the log handling.
;;;","09/May/14 18:25;ekoifman;[~thejas],[~hashutosh] the attached patch reverts all changes that were part of HIVE-5511 needed to handle the 'special' override-container-log4j, just like the bug description says.  HIVE-5511 also included some refactoring, which should not be reverted.;;;","09/May/14 21:39;thejas;+1
;;;","12/May/14 19:00;thejas;Patch committed to trunk. Thanks for the contribution Eugene!
;;;","13/Nov/14 19:39;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HCatLoader always returns Char datatype with maxlength(255)  when table format is ORC,HIVE-6766,12704075,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,ekoifman,ekoifman,ekoifman,27/Mar/14 19:51,01/Oct/19 22:07,14/Jul/23 06:14,01/Apr/14 21:10,0.13.0,,,,,,,,,0.13.0,,HCatalog,,,,0,,,"attached patch contains
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer#testWriteChar()

which shows that char(5) value written to Hive (ORC) table using HCatStorer will come back as char(255) when read with HCatLoader.",,ekoifman,rhbutani,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6807,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/14 02:35;thejas;HIVE-6766.1.patch;https://issues.apache.org/jira/secure/attachment/12637984/HIVE-6766.1.patch","27/Mar/14 20:01;ekoifman;HIVE-6766.patch;https://issues.apache.org/jira/secure/attachment/12637226/HIVE-6766.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382409,,,,Tue Apr 01 21:10:47 UTC 2014,,,,,,,,,,"0|i1tvsv:",382679,,,,,,,,,,,,,,,,,,,,,"27/Mar/14 20:01;ekoifman;ensured that when DefaultRecordWriterContainer constructs an ObjectInspector, it creates PrimitiveObjectInspectors based on type (char(5)) rather than category.;;;","27/Mar/14 23:35;sushanth;Patch looks good to me. [~gates], could we please have a precommit test kicked off on this?;;;","01/Apr/14 02:35;thejas;HIVE-6766.1.patch - attaching file again for precommit tests ;;;","01/Apr/14 16:23;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637984/HIVE-6766.1.patch

{color:green}SUCCESS:{color} +1 5539 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2066/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2066/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637984;;;","01/Apr/14 18:15;thejas;[~rhbutani] This is a very useful bug fix to have in hive 0.13 .
;;;","01/Apr/14 20:54;rhbutani;+1 for 0.13;;;","01/Apr/14 21:10;thejas;Patch committed to trunk and 0.13 branch.
Thanks for the contribution Eugene, and for the review Sushanth!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2 in http mode might send same kerberos client ticket in case of concurrent requests resulting in server throwing a replay exception,HIVE-6763,12703882,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vgumashta,vgumashta,vgumashta,27/Mar/14 02:14,01/Oct/19 22:06,14/Jul/23 06:14,30/Mar/14 00:36,0.13.0,,,,,,,,,0.13.0,,HiveServer2,,,,0,,,,,gates,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/14 02:19;vgumashta;HIVE-6763.1.patch;https://issues.apache.org/jira/secure/attachment/12637078/HIVE-6763.1.patch","29/Mar/14 01:12;vgumashta;HIVE-6763.2.patch;https://issues.apache.org/jira/secure/attachment/12637592/HIVE-6763.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382216,,,,Sun Mar 30 00:36:29 UTC 2014,,,,,,,,,,"0|i1tum7:",382487,,,,,,,,,,,,,,,,,,,,,"27/Mar/14 02:19;vgumashta;cc [~thejas] [~rhbutani] This is a bug for 13. Thanks!
;;;","27/Mar/14 18:30;thejas;[~vaibhavgumashta] Can you create a reviewboard link ? The indentation seems to have some issues.
It is recommended practice to put the unlock in a finally block. Alternatively, should we just use a simpler synchronized block instead of ReentrantLock ? ;;;","28/Mar/14 00:30;gates;I see one failure that I'm not sure if it is related or not:
2014-03-28 00:02:12 WARN  PTest:205 - org.apache.hadoop.hive.metastore.TestMetaStoreEventListenerOnlyOnCommit.testEventStatus;;;","28/Mar/14 20:06;vgumashta;[~gates] Thanks a lot for running the tests. The issue is unrelated but I need to incorporate some feedback.;;;","29/Mar/14 01:12;vgumashta;[~thejas] Addressed the feedback; linked the rb url. Thanks!;;;","29/Mar/14 01:35;thejas;+1
;;;","29/Mar/14 09:41;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637592/HIVE-6763.2.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5502 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority2
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2028/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2028/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637592;;;","29/Mar/14 17:03;thejas;Vaibhav,
Can you please check if the test failures are related ?
;;;","29/Mar/14 21:09;vgumashta;[~thejas] The failures are unrelated. Thanks!;;;","30/Mar/14 00:36;thejas;Patch committed to 0.13 branch and trunk.
Thanks for the contribution Vaibhav!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove deprecated parquet classes from outside of org.apache package,HIVE-6757,12703808,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,rhbutani,omalley,omalley,26/Mar/14 20:03,23/Jul/14 07:08,14/Jul/23 06:14,08/Apr/14 15:47,,,,,,,,,,0.13.0,,,,,,0,TODOC13,,Apache shouldn't release projects with files outside of the org.apache namespace.,,brocknoland,busbey,cutting,gates,jcoffey,leftyl,omalley,prasadm,rhbutani,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/14 02:29;rhbutani;HIVE-6757.2.patch;https://issues.apache.org/jira/secure/attachment/12638894/HIVE-6757.2.patch","27/Mar/14 16:38;omalley;HIVE-6757.patch;https://issues.apache.org/jira/secure/attachment/12637179/HIVE-6757.patch","27/Mar/14 23:09;omalley;parquet-hive.patch;https://issues.apache.org/jira/secure/attachment/12637271/parquet-hive.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382142,,,,Wed Apr 09 05:37:15 UTC 2014,,,,,,,,,,"0|i1tu67:",382415,,,,,,,,,,,,,,,,,,,,,"26/Mar/14 20:03;omalley;I assume they are there for compatibility, but they should be put into the parquet jars if they are necessary.;;;","26/Mar/14 20:10;brocknoland;bq. I assume they are there for compatibility

That is correct.

bq. Apache shouldn't release projects with files outside of the org.apache namespace.

The shell classes simply for compatibility and are ASL 2.0 licensed. Is there documentation saying this is against Apache policy? ;;;","26/Mar/14 20:38;cutting;There's no ASF-wide policy requiring every Java source code file to be in an org.apache sub-package.  Certainly that is preferred, but sometimes, usually for back-compatibility, code in non-Apache packages continues to be released from Apache TLPs for some time.  For example, Subversion still releases org.tigris code.

http://subversion.apache.org/docs/javahl/1.7/org/tigris/subversion/javahl/package-summary.html
;;;","26/Mar/14 22:25;omalley;There is no pressing need for them to be in Hive's jar. They are brand new classes and can easily be put in to the parquet jars that are distributed by parquet.;;;","26/Mar/14 23:33;brocknoland;bq. There is no pressing need for them to be in Hive's jar

There is no pressing need to remove them either.;;;","26/Mar/14 23:46;thejas;If we look at precedence for this in hive community, there has lot of work put into keeping the package structure clean.  All the hcatalog classes were moved into org.apache.hive. They old packaging is being deprecated in hive 0.14. And that already in org.apache.* . 

cc [~cwsteinbach]
;;;","27/Mar/14 00:58;xuefuz;I'm not sure what the motivation is. The inclusion of the code violates no bylaws of either Hive or Apache. For those who don't use Parquet, they probably never deal with these classes, but for those who do, these are important to them. I'm not sure what we gain by removing them.

If we think this is important for Hive, I'd be open to consider amending the current by-laws to apply going forward. In absence of such bylaw, I don't see a strong argument for the removal.

As to the case of HCat, I'm wondering how many releases have passed before it lands on the current package structure.;;;","27/Mar/14 01:39;thejas;I am not sure if we need to have by-laws for things like this. For example we don't have by-law for indentation in code, or adding tests with patches.

;;;","27/Mar/14 02:01;thejas;Regarding hcat, it was added in 0.11 , there is a patch out there to remove the old deprecated classes in 0.14 .
;;;","27/Mar/14 02:56;xuefuz;Yeah, I think it makes more sense to remove these deprecated parquet classes in certain future release. ;;;","27/Mar/14 16:36;omalley;From Hive's point of view, they are unused classes that have never been released. The right time to remove them is now before 0.13 is released.;;;","27/Mar/14 16:38;omalley;Just delete the entire directory. No other code change is necessary.;;;","27/Mar/14 16:43;brocknoland;bq. From Hive's point of view, they are unused classes that have never been released. 

I disagree. These class names are stored by many hive users in the metastore.;;;","27/Mar/14 17:19;omalley;{quote}
I disagree. These class names are stored by many hive users in the metastore.
{quote}

Actually, these have never been released so there are *NO* Hive users yet. That is exactly why this needs to be fixed before 0.13.;;;","27/Mar/14 17:27;brocknoland;bq. Actually, these have never been released so there are NO Hive users yet.

This is not true. Many Hive users used the Parquet Serde before it was contributed to the Hive project. Those Hive users are extremely interested in having their existing tables work when they go to 0.13.;;;","27/Mar/14 17:43;omalley;{quote}
 Many Hive users used the Parquet Serde before it was contributed to the Hive project. 
{quote}
They can continue to use their jar and it will continue to work. That isn't motivation for putting these jars into Hive.;;;","27/Mar/14 17:53;brocknoland;bq. They can continue to use their jar and it will continue to work. That isn't motivation for putting these jars into Hive.

They cannot continue to use their jars because many of the Hive interfaces changed in 0.12 and 0.13. This was one of the reasons that the Parquet developers agreed to contribute their work to Hive. I am quite surprised you marked this as a blocker considering:

* There is no apache or hive policy against this code
* This work was done a long time ago
* You are watching the JIRA in which this work was completed
* It's a tiny amount of code (all wrappers), impacting no one

I do not agree with removing this code for the 0.13 release.;;;","27/Mar/14 20:29;omalley;Hive does *not* have a need to maintain backwards compatibility with third party jars. The user installed third party jars and needs new versions to work with the current version of Hive. That doesn't mean that Hive should start publishing source code in the parquet namespace. 

There can't be any technical reason to block this patch. 
  * It removes unused java files. 
  * It does not break compatibility with any release of Hive
  * It prevents creating a new public API that starts deprecated.

This is straight forward goodness. It breaks no one and prevents downstream problems. I strongly encourage you to work with the parquet team to create a new version of their jar with these four compatibility classes.;;;","27/Mar/14 20:53;brocknoland;bq. Hive does not have a need to maintain backwards compatibility with third party jars. 

Simply because we do not have to, does not mean we cannot. More simply there is no policy saying we cannot maintain backwards compatibility with existing Parquet users. The work was done by the Hive developer community for the Hive user community.

bq. There can't be any technical reason to block this patch.

Breaking Hive users, part of the Hive community, is a technical reason.;;;","27/Mar/14 22:16;omalley;{quote}
Breaking Hive users, part of the Hive community, is a technical reason.
{quote}

You haven't demonstrated how it breaks any Hive users. Any user that downloaded the Parquet jars still has the Parquet jars and should download updated ones that work with their current release. This doesn't present a problem. It certainly isn't worth creating more confusion by releasing files outside of the org.apache namespace.;;;","27/Mar/14 22:38;gates;[~brocknoland], sorry, I don't quite follow the issue here.  I get that you want current Hive users who are using Parquet to not experience a non-backward compatible change.  That's reasonable.  If these parquet.hive.... classes are placed in the parquet jars and pulled via maven that should still be achieved, correct?  I'm missing how having these in Hive rather than parquet is a backwards compatibility issue.;;;","27/Mar/14 23:09;omalley;Brock, here is a patch that creates a separate project parquet-hive.jar that users can slip into replace their old ones.;;;","28/Mar/14 09:28;jcoffey;Owen, the solution your proposing means that there is no seamless upgrade path for existing parquet-hive users and that somewhere on the hive wiki there will have to be a call out ""attention existing parquet users, you must include the parquet-hive.jar when upgrading to hive 13.  we're sorry, but this is the price you have to pay for being an early adopter and driving functionality"".

One of the goals of the #HIVE-5783 patch was to make the lives of parquet users easier (there were of course many other reasons, but ease of use is a good goal in and of itself).  The classes as they are do no harm and it's hard to see how they pollute the code base of Hive in any significant way.  This patch kinda sorta seems a tiny bit punitive if you ask me.

Please don't take any of this the wrong way, but I believe this is what a fair chunk of the parquet-hive community might think if this patch is committed.;;;","28/Mar/14 15:12;brocknoland;Great points Justin. Many folks in the Hive community want this code, which is not against any Apache or Hive policy.;;;","28/Mar/14 15:43;omalley;Justin,
   They already have parquet-hive.jar that they've manually added to their installation. Giving them an upgraded jar to work with Hive 0.13 is a better answer than making conflicting classes in Hive itself.

In fact, the way that HIVE-5783 was done imposes a significant chance that class conflicts will occur for users that have manually installed the parquet jars. I'm not trying to force reverting HIVE-5783 out of Hive 0.13, but leaving these classes in the parquet jars and not in Hive is a much better answer. ;;;","28/Mar/14 15:49;xuefuz;If removing the code helps Hive functionally or performance-widely, I may be convinced by the proposal of removing this small piece of code. Based on what we gain by doing this removal, it's hard to be convincing that this benefits anything if at all, while discouraging some hive/parquet users who really care. For most of other Hive users, who cares about the extra two classes they don't need to bother with.;;;","28/Mar/14 16:38;jcoffey;I guess my point is simply that early adopters are penalized for life whereas new users get the full benefit of the patch.  I agree that the penalty is pretty small, but the two classes kicking around in the parquet package are even less of a penalty to the hive code base.  Thus I remain against pulling them out.;;;","28/Mar/14 18:45;omalley;The point is that these files are *CREATING* a new *PUBLIC* api for Hive. That API is starting deprecated. That just creates confusion and noise. The users already need to update their manually installed parquet jars. This is the time that imposes the *LEAST* disruption on the users of Apache Hive. If we release them then there is user confusion over duplicated classes. Hive users won't expect to see classes in parquet.* in the hive-exec jar. *THAT* will create brand new user confusion.;;;","28/Mar/14 22:23;rhbutani;Hi Justin, Brock,

Couple of questions/thoughts:

1.
What if we include the parquet-hive.jar in the hive-exec shaded jar? Does this mitigate the upgrade issues for existing users? 

2. 
If they choose to how will existing users migrate to the new classes? Do we provide metadata upgrade scripts? Do we have to support their existing sql code: for e.g. we add checks in the hive parsing layer to replace old parquet class references with new classes.
So the migration process when we remove(now or in the future) the deprecated classes is not clear. Can you guys please help me understand how this will play out.  ;;;","29/Mar/14 15:51;brocknoland;Hi,

The work that was done in HIVE-5783, by the Hive community, ensured that it was backwards compatible for Parquet Hive users, also members of the Hive community. There would be no issue with a patch that kept the backwards compatibility work.

The simplest solution would be to update the serde, input, and output class names in the metastore via the upgrade scripts.

Brock;;;","31/Mar/14 12:08;jcoffey;I can +1 [~brocknoland]'s solution if that flies for everyone else.  Actually, we joked about this in one of our review sessions here thinking that it was a bit of a brute force solution, but if this works for everyone it works for us (FYI, for one table we expect to have 47K partitions to update).;;;","03/Apr/14 20:23;omalley;Having the upgrade script convert the names is great as long as we can remove the deprecated classes outside of org.apache.;;;","03/Apr/14 20:35;jcoffey;I find that to be an acceptable compromise.

consensus :).;;;","06/Apr/14 02:29;rhbutani;Took a stab at creating the upgrade script.
Please review. This is the first time I am doing an upgrade script, so I apologize if I missed something.  

I tested this on derby: 
- created a table with the old syntax and classes.
- applied the upgrade on my instance
- With this patch applied could run a select on the table.

Hopefully this is good enough for existing parquet users.;;;","06/Apr/14 23:12;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638894/HIVE-6757.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5548 tests executed
*Failed tests:*
{noformat}
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2156/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2156/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638894;;;","07/Apr/14 02:32;xuefuz;+1. The patch looks good to me. Thanks to Harish for taking on this.;;;","07/Apr/14 02:59;brocknoland;+1
Thank you Harish!!;;;","07/Apr/14 14:52;jcoffey;much appreciated Harish!;;;","07/Apr/14 23:09;omalley;+1 thanks Harish!
;;;","08/Apr/14 15:47;rhbutani;Committed to trunk and 0.13
thanks Owen, Xuefu, Brock, Justin.;;;","09/Apr/14 05:37;leftyl;This should have a release note, and the wiki needs usage information.

* [LanguageManual - File Formats - Parquet |https://cwiki.apache.org/confluence/display/Hive/Parquet];;;",,,,,,,,,,,,,,,,,,,,
alter table set fileformat should set serde too,HIVE-6756,12703743,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,chinnalalam,omalley,omalley,26/Mar/14 16:10,13/Nov/14 19:42,14/Jul/23 06:14,27/May/14 18:49,0.13.0,,,,,,,,,0.14.0,,,,,,0,TODOC14,,Currently doing alter table set fileformat doesn't change the serde. This is unexpected by customers because the serdes are largely file format specific.,,chinnalalam,leftyl,omalley,szehon,thejas,vasanthkumar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/May/14 09:18;vasanthkumar;HIVE-6756.1.patch;https://issues.apache.org/jira/secure/attachment/12645402/HIVE-6756.1.patch","19/May/14 12:15;chinnalalam;HIVE-6756.2.patch;https://issues.apache.org/jira/secure/attachment/12645546/HIVE-6756.2.patch","25/May/14 11:04;chinnalalam;HIVE-6756.3.patch;https://issues.apache.org/jira/secure/attachment/12646708/HIVE-6756.3.patch","17/Apr/14 18:34;chinnalalam;HIVE-6756.patch;https://issues.apache.org/jira/secure/attachment/12640671/HIVE-6756.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,382077,,,,Thu Nov 13 19:42:59 UTC 2014,,,,,,,,,,"0|i1tts7:",382352,,,,,,,,,,,,,,,,,,,,,"17/Apr/14 18:34;chinnalalam;In alert table file format for the ORC and RC  file formats are setting the corresponding serdes, reaming file formats are not setting the corresponding serde.

In create table if we are not specifying the serde other than ORC and RC file formats it is setting with LazySimpleSerDe, like create table in alert table set file format added this.;;;","17/Apr/14 19:01;ashutoshc;I think instead of always defaulting to LazySimpleSerde, better is to set LazySimpleSerde for Textfile and SequenceFile format only and throw exception in cases where serde is not specified. We cant assume other file format uses LazySimpleSerde.;;;","22/Apr/14 18:05;chinnalalam;With out the patch, current code is taken care for the RC,ORC and PARQUET file formats (ALTER TATBLE SET FILEFORMT configuring the proper serde for RC,ORC and PARQUET file formats)

TEXTFILE, SEQUENCE file formats are not handled. This patch will address by configuring LazySimpleSerde for these file formats.

Apart from this in ALTER TATBLE SET FILEFORMT can use INPUTFORMAT,OUTPUTFORMAT classes. In this scenario not sure which serde need to be configure?

If throws exception he cannot use INPUTFORMAT,OUTPUTFORMAT classes in ALTER TATBLE SET FILEFORMT.

Any suggestions..;;;","09/May/14 18:15;ashutoshc;Sorry [~chinnalalam] for being late on this. My suggestion is:
1. Instead of setting LazySimpleSerde as default for all cases, set LazySimpleSerde only for TEXTFILE & SEQUENCE (similiar to RC, ORC etc.)
2. For Alter table set fileformat IF OF case, throw exception if user doesnt specify serde. ie if user is specifying IF & OF she must also specify serde.;;;","17/May/14 09:27;vasanthkumar;Hi [~ashutoshc],
Implemented as per your suggestion.
Sorry [~chinnalalam] for taking over this JIRA.

Kindly verify.

Thanks,
Vasanth kumar;;;","18/May/14 03:28;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12645402/HIVE-6756.1.patch

{color:red}ERROR:{color} -1 due to 31 failed/errored test(s), 5526 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_04_evolved_parts
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_java_method
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_reflect
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_math_funcs
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_script_pipe
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_transform1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input20
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input5
org.apache.hive.hcatalog.cli.TestUseDatabase.testAlterTablePass
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/224/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/224/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 31 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12645402;;;","19/May/14 06:58;ashutoshc;While some of the failures in Hive QA run are also on trunk, but some other failures look relevant.;;;","19/May/14 08:14;chinnalalam;Hi Ashutosh, I am working on this. I will upload patch today.;;;","19/May/14 12:16;chinnalalam;Added positive and negative testcases for these scenarios.
Corrected parquet_serde.q this test case and remaining testcases are not related to this change.;;;","25/May/14 00:52;ashutoshc;Patch looks good, accept for textfile & seqfile 
+      serde = conf.getVar(HiveConf.ConfVars.HIVESCRIPTSERDE);

scriptserde config is used for other purposes, I think its better just to do 
serde = LazySimpleSerDe.getClass().getName() 
since thats equivalent behavior with create table stored as textfile / sequencefile

Looks good otherwise ;;;","25/May/14 11:07;chinnalalam;Hi Ashutosh, Thanks for reviewing the patch. Reworked the patch.;;;","25/May/14 17:00;ashutoshc;+1;;;","26/May/14 13:10;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646708/HIVE-6756.3.patch

{color:red}ERROR:{color} -1 due to 12 failed/errored test(s), 5465 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_04_evolved_parts
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat3
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hive.hcatalog.cli.TestUseDatabase.testAlterTablePass
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/298/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/298/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-298/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 12 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646708;;;","27/May/14 18:49;ashutoshc;Committed to trunk. Thanks, Chinna!;;;","28/May/14 19:14;szehon;[~ashutoshc] the commit message has a typo and says HIVE-3756 and might cause confusion grepping git log, is it too late to fix?;;;","28/May/14 20:01;ashutoshc;Thanks for catching that. Updated svn commit message. Its correctly reflected in svn log now. However, I think svn-git bridge doesnt recognize commit message edits, so in git repo it will still show old one.;;;","29/May/14 05:41;leftyl;Does this need any user doc?;;;","29/May/14 17:41;chinnalalam;Hi Lefty Leverenz,

Need to update this in Alter Table/Partition File Format section:

Alter table set fileformat with INPUTFORMAT & OUTPUTFORMAT case, throw exception if user doesn't specify serde. ie if user is specifying INPUTFORMAT & OUTPUTFORMAT she must also specify serde.;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unions on Tez NPE when there's a mapjoin the union work,HIVE-6753,12703663,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,26/Mar/14 07:12,27/Mar/14 22:03,14/Jul/23 06:14,27/Mar/14 22:03,,,,,,,,,,0.13.0,,,,,,0,,,In some cases when there's a mapjoin in union work we need to broadcast the same result set to multiple downstream work items. This causes a vertex failure right now.,,hagleitn,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/14 07:14;hagleitn;HIVE-6753.1.patch;https://issues.apache.org/jira/secure/attachment/12636867/HIVE-6753.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381997,,,,Thu Mar 27 22:03:46 UTC 2014,,,,,,,,,,"0|i1ttaf:",382272,,,,,,,,,,,,,,,,,,,,,"26/Mar/14 07:14;hagleitn;patch also addresses two more issues. a) some golden files needed update b) we're masking exceptions in some cases in tez.;;;","26/Mar/14 19:08;vikram.dixit;LGTM +1. Pending test run.;;;","27/Mar/14 07:57;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12636867/HIVE-6753.1.patch

{color:green}SUCCESS:{color} +1 5491 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1976/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1976/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12636867;;;","27/Mar/14 22:03;hagleitn;Committed to trunk and branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Vectorized Between and IN expressions don't work with decimal, date types.",HIVE-6752,12703647,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jnp,jnp,jnp,26/Mar/14 03:20,29/Mar/14 00:00,14/Jul/23 06:14,28/Mar/14 23:44,0.13.0,,,,,,,,,0.13.0,,,,,,0,,,"Vectorized Between and IN expressions don't work with decimal, date types.",,ehans,jnp,rhbutani,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/14 06:59;jnp;HIVE-6752.1.patch;https://issues.apache.org/jira/secure/attachment/12637102/HIVE-6752.1.patch","27/Mar/14 23:19;jnp;HIVE-6752.2.patch;https://issues.apache.org/jira/secure/attachment/12637279/HIVE-6752.2.patch","28/Mar/14 17:45;jnp;HIVE-6752.3.patch;https://issues.apache.org/jira/secure/attachment/12637458/HIVE-6752.3.patch","28/Mar/14 21:17;jnp;HIVE-6752.4.patch;https://issues.apache.org/jira/secure/attachment/12637548/HIVE-6752.4.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381981,,,,Sat Mar 29 00:00:13 UTC 2014,,,,,,,,,,"0|i1tt6v:",382256,I have committed this to trunk and branch-0.13,,,,,,,,,,,,,,,,,,,,"27/Mar/14 07:03;jnp;Review board entry: https://reviews.apache.org/r/19718/;;;","27/Mar/14 18:40;ehans;+1

Conditional on addressing my comments in the code review. All of them are minor.;;;","27/Mar/14 23:19;jnp;Updated patch addressing the comments. Also updated test to make with pass with hadoop-2 because hadoop-2 produces results in different order.;;;","28/Mar/14 17:45;jnp;Patch updated with a small bug fix identified in testing.;;;","28/Mar/14 20:08;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637458/HIVE-6752.3.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5499 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testBetweenFilters
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2017/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2017/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637458;;;","28/Mar/14 21:18;jnp;Latest patch fixes the test.;;;","28/Mar/14 22:04;sershe;+1 on most recent changes ;;;","28/Mar/14 22:47;ehans;Please see my comments on review board;;;","28/Mar/14 23:11;ehans;+1

Thanks for the response on review board. I agree that it is reasonable to take up the issues raised in separate JIRAs, which are not time-critical at this point.;;;","28/Mar/14 23:19;jnp;Thanks for the review Eric, I have filed following jiras:
HIVE-6776 : To optimize timestamp comparisons.
HIVE-6777: Comments and readability in VectorizedHashKeyWrapper.;;;","28/Mar/14 23:25;jnp;[~rhbutani] This bug affects hive-0.13 and is important for decimal/date to work correctly in vectorization, therefore the patch should be ported to branch-0.13 as well.;;;","28/Mar/14 23:25;jnp;I ran the tests locally for the latest patch, and tests pass.;;;","28/Mar/14 23:28;rhbutani;+1 for 0.13;;;","29/Mar/14 00:00;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637548/HIVE-6752.4.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5499 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority2
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2022/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2022/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637548;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive printing debug information in stdout after the end of CLI session,HIVE-6750,12703636,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vgumashta,vgumashta,vgumashta,26/Mar/14 00:31,27/Mar/14 00:21,14/Jul/23 06:14,27/Mar/14 00:07,0.14.0,,,,,,,,,0.13.0,,CLI,,,,0,,,"{noformat}
$ hive -e ""show tables;""

OK
table1
table2
table3
Time taken: 0.694 seconds, Fetched: 3 row(s)
classLoader = java.net.URLClassLoader@29978933
SharedSecrets.getJavaNetAccess()=java.net.URLClassLoader$7@4b7d03c5
{noformat}
Observe the last two lines that are printed upon the end of Hive CLI session. 

JavaUtils#closeClassLoader uses reflection to invoke sun.misc.ClassLoaderUtil.releaseLoader. Just peeking at the implementation of this method clearly showed this:
{code}
            System.out.println (""classLoader = "" + classLoader);
            System.out.println (""SharedSecrets.getJavaNetAccess()=""+SharedSecrets.getJavaNetAccess());
{code}
See line 80-81 at http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/6-b14/sun/misc/ClassLoaderUtil.java.",Open JDK6,gates,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/14 00:38;vgumashta;HIVE-6750.1.patch;https://issues.apache.org/jira/secure/attachment/12636831/HIVE-6750.1.patch","26/Mar/14 01:57;vgumashta;HIVE-6750.2.patch;https://issues.apache.org/jira/secure/attachment/12636843/HIVE-6750.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381970,,,,Thu Mar 27 00:21:18 UTC 2014,,,,,,,,,,"0|i1tt4f:",382245,,,,,,,,,,,,,,,,,,,,,"26/Mar/14 00:39;vgumashta;cc [~navis] [~ashutoshc];;;","26/Mar/14 01:57;vgumashta;New patch with review comments.;;;","26/Mar/14 02:04;ashutoshc;+1;;;","26/Mar/14 23:30;gates;Ran the tests, all looks good.;;;","27/Mar/14 00:07;ashutoshc;Committed to trunk & 0.13. Thanks, Vaibhav!;;;","27/Mar/14 00:21;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12636843/HIVE-6750.2.patch

{color:green}SUCCESS:{color} +1 5488 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1970/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1970/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12636843;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Turn hive.auto.convert.join.use.nonstaged off by default,HIVE-6749,12703632,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,26/Mar/14 00:02,02/Jul/14 08:21,14/Jul/23 06:14,03/Apr/14 21:54,0.13.0,,,,,,,,,0.13.0,,Configuration,,,,0,,,,,leftyl,navis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6144,,,,,,,,,,,,,,,,,,,,,"02/Apr/14 15:24;ashutoshc;HIVE-6749.1.patch;https://issues.apache.org/jira/secure/attachment/12638270/HIVE-6749.1.patch","26/Mar/14 00:12;ashutoshc;HIVE-6749.patch;https://issues.apache.org/jira/secure/attachment/12636822/HIVE-6749.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381966,,,,Wed Jul 02 08:21:41 UTC 2014,,,,,,,,,,"0|i1tt3j:",382241,,,,,,,,,,,,,,,,,,,,,"26/Mar/14 00:08;ashutoshc;It should be turned off by default because :
* Its not clear it will always be more performant than local hashtable generation. When this kicks in, there will be three DataNode (which has replicas of smaller table) serving 100s of map tasks which are doing remote reads. Compare to this, DistributedCache distributes small file as well as localize, which will be local read. There might be some case that nonstaged is faster, but I dont think that will be a general case.
* Seems like this will not work with security. Because we are reading smaller table directly now from hdfs, whose tokens are not obtained.
* Its a new feature, so lets give it some time to mature.;;;","26/Mar/14 00:12;ashutoshc;Attaching patch. [~navis] Let me know what you think?;;;","26/Mar/14 22:27;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12636822/HIVE-6749.patch

{color:red}ERROR:{color} -1 due to 109 failed/errored test(s), 5488 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join29
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin_negative
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin_negative2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin_negative3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_explain_rearrange
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_convert_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join27
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join29
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32_lessSize
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join35
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join36
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join37
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_reorder4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_star
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_hook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_memcheck
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_subquery
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_subquery2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_join_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_reduce_deduplicate_exclude_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_transform_hint
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subq_where_serialization
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_in_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_multiinsert
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin7
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_bucket_mapjoin_mismatch1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_sortmerge_mapjoin_mismatch_1
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1968/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1968/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 109 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12636822;;;","01/Apr/14 01:12;navis;Distributing small aliases(about 500M max) through DistributedCache makes busy works between DNs, and it was not good for the whole system. But +1 for the idea. It's not matured, yet.;;;","03/Apr/14 09:19;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638270/HIVE-6749.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5542 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cross_product_check_2
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2092/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2092/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638270;;;","03/Apr/14 21:54;ashutoshc;Committed to 0.13 & branch. Thanks, Navis for review.;;;","02/Jul/14 08:21;leftyl;The changed default value for *hive.auto.convert.join.use.nonstaged* is documented in the wiki here:

* [Configuration Properties -- hive.auto.convert.join.use.nonstaged | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.auto.convert.join.use.nonstaged]

I added a comment to HIVE-6586 so the correct default value of *hive.auto.convert.join.use.nonstaged* won't get lost in the shuffle when HIVE-6037 changes HiveConf.java.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSinkOperator needs to cleanup held references for container reuse,HIVE-6748,12703619,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,gopalv,gopalv,gopalv,25/Mar/14 22:26,28/Mar/14 05:52,14/Jul/23 06:14,28/Mar/14 05:52,0.13.0,,,,,,,,,0.13.0,,Tez,,,,0,,,"The current implementation of FileSinkOperator runs into trouble when reusing the same query pipeline aggressively with container reuse.

This is due to a prevFSP writer which is left referenced after closeOp() and which is not reset even for initializeOp().

{code}
014-03-25 14:46:31,744 FATAL [main] org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor: org.apache.hadoop.hive.ql.metadata.HiveException: java.nio.channels.ClosedChannelException
        at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.closeWriters(FileSinkOperator.java:170)
        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynOutPaths(FileSinkOperator.java:758)
        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.startGroup(FileSinkOperator.java:833)
        at org.apache.hadoop.hive.ql.exec.Operator.defaultStartGroup(Operator.java:497)
        at org.apache.hadoop.hive.ql.exec.Operator.startGroup(Operator.java:520)
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processKeyValues(ReduceRecordProcessor.java:296)
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:223)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:159)
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:306)
        at org.apache.hadoop.mapred.YarnTezDagChild$4.run(YarnTezDagChild.java:549)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
        at org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:538)
Caused by: java.nio.channels.ClosedChannelException
        at org.apache.hadoop.hdfs.DFSOutputStream.checkClosed(DFSOutputStream.java:1526)
        at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:98)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)
        at java.io.DataOutputStream.write(DataOutputStream.java:107)
        at org.apache.hadoop.hive.ql.io.orc.WriterImpl$DirectStream.output(WriterImpl.java:316)
        at org.apache.hadoop.hive.ql.io.orc.OutStream.flush(OutStream.java:242)
        at org.apache.hadoop.hive.ql.io.orc.WriterImpl.writeMetadata(WriterImpl.java:1923)
        at org.apache.hadoop.hive.ql.io.orc.WriterImpl.close(WriterImpl.java:2017)
        at org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat$OrcRecordWriter.close(OrcOutputFormat.java:98)
        at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.closeWriters(FileSinkOperator.java:167)
        ... 13 more
{code}",,gopalv,hagleitn,prasanth_j,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6735,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/14 22:42;gopalv;HIVE-6748.1.patch;https://issues.apache.org/jira/secure/attachment/12636802/HIVE-6748.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381953,,,,Fri Mar 28 05:52:50 UTC 2014,,,,,,,,,,"0|i1tt0n:",382228,FileSinkOperator cleanliness of references on closeOp/initializeOp,,,,,,,,,,,,,,,,,,,,"25/Mar/14 23:06;hagleitn;+1;;;","26/Mar/14 18:34;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12636802/HIVE-6748.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5457 tests executed
*Failed tests:*
{noformat}
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1966/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1966/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12636802;;;","27/Mar/14 19:49;prasanth_j;Test failure is unrelated.;;;","28/Mar/14 01:27;rhbutani;+1 for 0.13;;;","28/Mar/14 05:52;hagleitn;Committed to trunk and branch. Thanks [~gopalv]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HCat MultiOutputFormat hardcodes DistributedCache keynames,HIVE-6745,12703560,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sushanth,sushanth,sushanth,25/Mar/14 19:29,13/Nov/14 19:40,14/Jul/23 06:14,14/Apr/14 17:07,0.13.0,0.14.0,,,,,,,,0.14.0,,HCatalog,,,,0,,,"There's a bug in how MultiOutputFormat deals with DistributedCache, in that it hardcodes the parameter name to merge for distributed cache entries in the jobconf. This parameter name has changed with recent builds of 2.x, thus causing a test failure. These parameters need to be properly shimmed out.",,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/14 19:31;sushanth;HIVE-6745.patch;https://issues.apache.org/jira/secure/attachment/12636762/HIVE-6745.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381894,,,,Thu Nov 13 19:40:31 UTC 2014,,,,,,,,,,"0|i1tsnz:",382169,,,,,,,,,,,,,,,,,,,,,"25/Mar/14 19:31;sushanth;Patch attached.;;;","11/Apr/14 23:08;sushanth;Marking as patch-available.;;;","11/Apr/14 23:24;ashutoshc;+1;;;","12/Apr/14 02:35;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12636762/HIVE-6745.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5614 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_dyn_part
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2227/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2227/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12636762;;;","12/Apr/14 16:45;sushanth;Note : the failed tests are not connected to this patch.;;;","14/Apr/14 17:07;ashutoshc;Committed to trunk. Thanks, Sushanth!;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Permanent UDF lookup fails when current DB has uppercase letters,HIVE-6744,12703530,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,25/Mar/14 17:54,29/Mar/14 02:29,14/Jul/23 06:14,29/Mar/14 00:17,0.13.0,,,,,,,,,0.13.0,,UDF,,,,0,,,"If defaulting to current DB/schema name for resolving UDF name, the DB name should be lowercased before doing the UDF lookup.",,jdere,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6047,,,,,,,,,,,,,,,,,,,,,"25/Mar/14 19:14;jdere;HIVE-6744.1.patch;https://issues.apache.org/jira/secure/attachment/12636756/HIVE-6744.1.patch","25/Mar/14 17:58;jdere;HIVE-6744.1.patch;https://issues.apache.org/jira/secure/attachment/12636739/HIVE-6744.1.patch","29/Mar/14 00:02;jdere;HIVE-6744.2.patch;https://issues.apache.org/jira/secure/attachment/12637579/HIVE-6744.2.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381864,,,,Sat Mar 29 00:17:29 UTC 2014,,,,,,,,,,"0|i1tshb:",382139,,,,,,,,,,,,,,,,,,,,,"25/Mar/14 17:58;jdere;Attaching patch v1. Also including test case for this issue and HIVE-6672.
[~rhbutani] is this something we can add to Hive 0.13?;;;","25/Mar/14 18:03;jdere;RB https://reviews.apache.org/r/19623/;;;","25/Mar/14 19:06;rhbutani;+1 for 0.13;;;","25/Mar/14 21:29;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12636739/HIVE-6744.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5447 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_smb_mapjoin_8
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1957/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1957/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12636739;;;","25/Mar/14 23:15;jdere;I think these 2 failures are flaky tests - I've seen testExecuteStatementAsync showing up in a few recent builds.  I've run both tests locally on Mac/Linux and they pass.;;;","26/Mar/14 03:19;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12636756/HIVE-6744.1.patch

{color:green}SUCCESS:{color} +1 5458 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1961/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1961/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12636756;;;","27/Mar/14 01:32;thejas;+1 .
Lets just remove that commented line in test. We don't need to run tests again for that.
;;;","29/Mar/14 00:02;jdere;Rebasing patch with trunk (looks like MiniHS2.java has moved), and removing commented code.;;;","29/Mar/14 00:17;jdere;Committed to 0.13 branch and trunk. Thanks for review Thejas.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tez Outputs need to be started before accessing the writer,HIVE-6742,12703397,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sseth,sseth,sseth,25/Mar/14 06:07,26/Mar/14 08:04,14/Jul/23 06:14,26/Mar/14 08:04,,,,,,,,,,0.13.0,,,,,,0,,,Accessing the writer on a Tez Output may not be safe until the Output has been started.,,hagleitn,sseth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/14 06:16;sseth;HIVE-6742.1.patch;https://issues.apache.org/jira/secure/attachment/12636539/HIVE-6742.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381735,,,,Wed Mar 26 08:04:55 UTC 2014,,,,,,,,,,"0|i1tron:",382010,,,,,,,,,,,,,,,,,,,,,"25/Mar/14 06:16;sseth;Trivial patch to start the outputs before initializing the OutputCollector wrapper which accesses the writer.;;;","25/Mar/14 06:19;hagleitn;+1;;;","26/Mar/14 07:53;hagleitn;Ran tests locally. Passed.;;;","26/Mar/14 08:04;hagleitn;Committed to trunk and branch. Thanks [~sseth].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2 startup fails in secure (kerberos) mode due to backward incompatible hadoop change,HIVE-6741,12703391,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,vgumashta,vgumashta,vgumashta,25/Mar/14 04:56,10/Feb/16 03:25,14/Jul/23 06:14,01/May/14 01:20,0.14.0,,,,,,,,,0.14.0,,HiveServer2,,,,0,,," [HADOOP-10211|https://issues.apache.org/jira/browse/HADOOP-10211] made a backward incompatible change due to which the following hive call returns a null map ([HiveAuthFactory-old|https://github.com/apache/hive/blob/fc3fdb19668369c56994d11df3207e14f2c5dba8/service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java#L115]):
{code}
Map<String, String> hadoopSaslProps =  ShimLoader.getHadoopThriftAuthBridge().
        getHadoopSaslProperties(conf); 
SaslQOP hadoopSaslQOP = SaslQOP.fromString(hadoopSaslProps.get(Sasl.QOP));
if(hadoopSaslQOP.ordinal() > saslQOP.ordinal()) {
LOG.warn(MessageFormat.format(""\""hadoop.rpc.protection\"" is set to higher security level "" +
          ""{0} then {1} which is set to {2}"", hadoopSaslQOP.toString(),
          ConfVars.HIVE_SERVER2_THRIFT_SASL_QOP.varname, saslQOP.toString()));
}
{code}

Since this code path is only used for logging hadoop sasl qop values in case hadoop's qop > hive's qop, we can do away with this and add a general log message.",,arp,erwaman,jdere,kedarsdixit,leftyl,prasadm,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6960,,,,HIVE-8154,,,,,,,HIVE-7620,HADOOP-10451,,HIVE-6987,,,,,,,,,,,,,,,,,,,,,"25/Mar/14 05:05;vgumashta;HIVE-6741.1.patch;https://issues.apache.org/jira/secure/attachment/12636522/HIVE-6741.1.patch","29/Apr/14 17:56;vgumashta;HIVE-6741.2.patch;https://issues.apache.org/jira/secure/attachment/12642502/HIVE-6741.2.patch","29/Apr/14 19:43;vgumashta;HIVE-6741.3.patch;https://issues.apache.org/jira/secure/attachment/12642517/HIVE-6741.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381729,,,,Thu Nov 13 19:41:45 UTC 2014,,,,,,,,,,"0|i1trnb:",382004,,,,,,,,,,,,,,,,,,,,,"25/Mar/14 04:57;vgumashta;Marking this as blocker for 13. Will submit a patch in few mins.;;;","25/Mar/14 05:05;vgumashta;cc [~rhbutani]: This should be a blocker for 13. Trivial patch to fix it.
[~thejas] [~prasadm]: Patch is up for review.;;;","25/Mar/14 05:25;rhbutani;+1 for 0.13;;;","25/Mar/14 16:32;prasadm;[~vaibhavgumashta] Thanks for looking into the problem. 
[HADOOP-10211|https://issues.apache.org/jira/browse/HADOOP-10211] seems to be fixed in Hadoop 2.4 and trunk (3.0), which are both unreleased hadoop versions. 
Hive is still using last hadoop release 2.3.0. Shouldn't this wait till we upgrade hadoop dependencies ?;;;","25/Mar/14 20:25;vgumashta;[~prasadm] Sure, we can wait. However, the problem will manifest whenever we upgrade so I thought fixing it sooner is no harm. But since 13 won't be using hadoop 2.4, let me tag this for 14.

[~thejas] Had chat with hadoop folks, and it seems they won't revert the change. Check the logic for the change in this comment: https://issues.apache.org/jira/browse/HADOOP-10221?focusedCommentId=13923205&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13923205.
;;;","25/Mar/14 20:58;prasadm;[~vaibhavgumashta] This is an incompatible change and hence would be better to do this along with the hadoop upgrade. The code being removed is applicable to Hadoop 2.3. We might not switch to Hadoop 2.4 at all for Hive 0.14 as well, or may be Hadoop 2.4 will change the logic later during it's release cycle etc. In such cases the patch won't be adding any value.
Also [https://issues.apache.org/jira/browse/HIVE-6657|HIVE-6657] is proposing to add miniKdc based test in Hive. I guess that would catch this problem once we upgrade, so won't miss out this patch.

Regarding the actual patch, is the new unconditional log message needed at all ? It's getting printed be printed regardless the hadoop.rpc.protection and hive.server2.thrift.sasl.qop config. Would it make sense to just add it to docs and remove the message ?
;;;","25/Mar/14 21:08;vgumashta;[~prasadm] Makes sense.

I also think we can get away with the log message and document it instead. Let me post a new patch. ;;;","29/Apr/14 17:57;vgumashta;Incorporated [~prasadm]'s feedback.

cc [~jdere];;;","29/Apr/14 18:36;jdere;I think it looks good - can you also add a comment to HadoopThriftAuthBridge20S.getHadoopSaslProperties() saying that method no longer works with Hadoop 2.4.0?;;;","29/Apr/14 19:43;vgumashta;[~jdere] Done. Also added a note in hive-default. Since this call is used in metastore client/server, I'll create a new jira for that. Thanks!;;;","29/Apr/14 22:39;jdere;+1;;;","30/Apr/14 00:08;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642517/HIVE-6741.3.patch

{color:red}ERROR:{color} -1 due to 19 failed/errored test(s), 5490 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_binary_search
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_count
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_limit_pushdown
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mrr
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/79/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/79/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 19 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642517;;;","01/May/14 01:12;jdere;These test failures do not look related to this patch.;;;","01/May/14 01:20;jdere;Committed to trunk. Thanks for fixing Vaibhav.;;;","01/May/14 02:13;vgumashta;Thanks for review [~jdere].;;;","01/May/14 23:57;leftyl;For the record:  This adds information to the description of *hive.server2.thrift.sasl.qop* which was introduced in Hive 0.12.0, so I've put a comment in HIVE-6586 (for HIVE-6037) and updated the wiki here:

* [Configuration Properties -- hive.server2.thrift.sasl.qop  |https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.server2.thrift.sasl.qop]
* [HIVE-6586 comment  |https://issues.apache.org/jira/browse/HIVE-6586?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13987186#comment-13987186];;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in src/ql/pom.xml,HIVE-6740,12703387,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,szehon,grisha,grisha,25/Mar/14 03:59,15/Apr/14 20:47,14/Jul/23 06:14,15/Apr/14 20:46,0.12.0,0.13.0,,,,,,,,0.13.0,,,,,,0,,,"Line 502 of http://svn.apache.org/viewvc/hive/trunk/ql/pom.xml?view=markup

I think it should say ""avro-mapred"", not ""arvro-mapred"":

{code}
<include>org.apache.avro:arvro-mapred</include>
{code}

This causes hive-exec not to have avro-mapred stuff, and thus you will get errors querying avro tables such as:
{code}
Caused by: java.lang.ClassNotFoundException: org.apache.avro.mapred.FsInput
{code}

NO PRECOMMIT TESTS",,grisha,rhbutani,szehon,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/14 18:29;szehon;HIVE-6740.patch;https://issues.apache.org/jira/secure/attachment/12636747/HIVE-6740.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381725,,,,Tue Apr 15 20:46:30 UTC 2014,,,,,,,,,,"0|i1trmf:",382000,,,,,,,,,,,,,,,,,,,,,"25/Mar/14 05:35;szehon;Good catch, seems to affect 0.13 as well.  Did you want to submit a fix, or let others take a look ?;;;","25/Mar/14 14:12;grisha;Nah, I don't have a fix, I figured someone could just correct this at their convenience :);;;","25/Mar/14 18:29;szehon;Thanks Grisha for the analysis, attaching the fix.

Built + verified that org.apache.avro.mapred classes including FsInput is in the hive-exec uber-jar.;;;","25/Mar/14 18:33;szehon;Skip precommit tests, verify by building normally.;;;","25/Mar/14 18:44;xuefuz;+1

Does this sound bad enough to be included in 0.13?;;;","25/Mar/14 18:47;szehon;Thanks Xuefu, yea it would be good to include in 0.13 if possible, its looks very similar to the typo you fixed about the parquet dependency and would have similar affect.;;;","26/Mar/14 18:29;rhbutani;+1 for 0.13;;;","15/Apr/14 20:46;rhbutani;Committed to trunk and 0.13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive HBase query fails on Tez due to missing jars and then due to NPE in getSplits,HIVE-6739,12703368,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sershe,sershe,sershe,25/Mar/14 01:22,07/Apr/14 22:19,14/Jul/23 06:14,07/Apr/14 22:19,,,,,,,,,,0.13.0,,,,,,0,,,"Tez paths in Hive never call configure on the input/output operators, so (among other things, potentially) requisite files never get added to the job",,ndimiduk,sershe,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6825,HIVE-6824,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/14 00:29;sershe;HIVE-6739.01.patch;https://issues.apache.org/jira/secure/attachment/12637961/HIVE-6739.01.patch","03/Apr/14 02:24;sershe;HIVE-6739.02.patch;https://issues.apache.org/jira/secure/attachment/12638400/HIVE-6739.02.patch","04/Apr/14 18:50;sershe;HIVE-6739.03.patch;https://issues.apache.org/jira/secure/attachment/12638743/HIVE-6739.03.patch","07/Apr/14 18:14;sershe;HIVE-6739.04.patch;https://issues.apache.org/jira/secure/attachment/12639030/HIVE-6739.04.patch","28/Mar/14 01:05;sershe;HIVE-6739.patch;https://issues.apache.org/jira/secure/attachment/12637303/HIVE-6739.patch","25/Mar/14 16:18;sershe;HIVE-6739.preliminary.patch;https://issues.apache.org/jira/secure/attachment/12636721/HIVE-6739.preliminary.patch",,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381706,,,,Mon Apr 07 22:19:14 UTC 2014,,,,,,,,,,"0|i1tri7:",381981,,,,,,,,,,,,,,,,,,,,,"25/Mar/14 01:28;sershe;preliminary patch. Need to set up cluster to test this (and later remove logging), haven't done so far;;;","25/Mar/14 01:30;sershe;Nm, it iterates work graph incorrectly on review now. ;;;","27/Mar/14 22:50;sershe;Will attach updated patch today which fixes the issue and also another one with NPE in getSplits;;;","28/Mar/14 01:05;sershe;patch that provides files to AM and vertices, and also makes sure correct configs are set to avoid NPE. 
The part for AM may change in response to better Tez APIs...;;;","01/Apr/14 00:29;sershe;Updated patch. Note that this depends on a feature in yet-unreleased Tez 0.4; if that ships first I will update pom, otherwise it will be split into two patches. We will commit restart-AM path here, and relocalize path separately in Hive 0.14;;;","01/Apr/14 08:30;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637961/HIVE-6739.01.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2060/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2060/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-2060/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'shims/common-secure/src/main/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java'
++ awk '{print $2}'
++ egrep -v '^X|^Performing status on external'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/storage-handlers/hbase/target hcatalog/server-extensions/target hcatalog/core/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target hwi/target common/target common/src/gen service/target contrib/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1583571.

At revision 1583571.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637961;;;","03/Apr/14 01:42;sershe;Pacth with the test. Test with one of the Tez's own files (or guava, which is what I tried first) passes. However, the conflict test fails, so it's not really valid. DAG passes despite having guava-named, or tez-named, bogus jar added to resources. 
Given that I saw it work in cluster I wonder if some test setup issues are causing this. Do you guys have an idea? I will look tomorrow otherwise.;;;","03/Apr/14 02:24;sershe;Splitting the patch; this part has the original workaround and doesn't depend on new tez apis;;;","03/Apr/14 02:26;sershe;this is the diff btw https://reviews.apache.org/r/19789/;;;","04/Apr/14 08:31;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638400/HIVE-6739.02.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 5547 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.ql.processors.TestCommandProcessorFactory.testAvailableCommands
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2104/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2104/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638400;;;","04/Apr/14 18:55;vikram.dixit;LGTM +1;;;","04/Apr/14 19:04;vikram.dixit;Fix failing test.;;;","04/Apr/14 19:05;vikram.dixit;Ignore previous comment. Wrong jira.;;;","05/Apr/14 21:45;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638743/HIVE-6739.03.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2134/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2134/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
Decision can match input such as ""LPAREN KW_NULL EQUAL_NS"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_NULL BITWISEOR"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN CharSetName CharSetLiteral"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_NULL NOTEQUAL"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:115:5: 
Decision can match input such as ""KW_CLUSTER KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:127:5: 
Decision can match input such as ""KW_PARTITION KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:138:5: 
Decision can match input such as ""KW_DISTRIBUTE KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:149:5: 
Decision can match input such as ""KW_SORT KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:166:7: 
Decision can match input such as ""STAR"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:179:5: 
Decision can match input such as ""KW_STRUCT"" using multiple alternatives: 4, 6

As a result, alternative(s) 6 were disabled for that input
warning(200): IdentifiersParser.g:179:5: 
Decision can match input such as ""KW_ARRAY"" using multiple alternatives: 2, 6

As a result, alternative(s) 6 were disabled for that input
warning(200): IdentifiersParser.g:179:5: 
Decision can match input such as ""KW_UNIONTYPE"" using multiple alternatives: 5, 6

As a result, alternative(s) 6 were disabled for that input
warning(200): IdentifiersParser.g:261:5: 
Decision can match input such as ""KW_TRUE"" using multiple alternatives: 3, 8

As a result, alternative(s) 8 were disabled for that input
warning(200): IdentifiersParser.g:261:5: 
Decision can match input such as ""KW_DATE StringLiteral"" using multiple alternatives: 2, 3

As a result, alternative(s) 3 were disabled for that input
warning(200): IdentifiersParser.g:261:5: 
Decision can match input such as ""KW_NULL"" using multiple alternatives: 1, 8

As a result, alternative(s) 8 were disabled for that input
warning(200): IdentifiersParser.g:261:5: 
Decision can match input such as ""KW_FALSE"" using multiple alternatives: 3, 8

As a result, alternative(s) 8 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_MAP LPAREN"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_SORT KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_INSERT KW_OVERWRITE"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_DISTRIBUTE KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_GROUP KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_LATERAL KW_VIEW"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""KW_BETWEEN KW_MAP LPAREN"" using multiple alternatives: 8, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_ORDER KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_CLUSTER KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_UNION KW_ALL"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_INSERT KW_INTO"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:518:5: 
Decision can match input such as ""{AMPERSAND..BITWISEXOR, DIV..DIVIDE, EQUAL..EQUAL_NS, GREATERTHAN..GREATERTHANOREQUALTO, KW_AND, KW_ARRAY, KW_BETWEEN..KW_BOOLEAN, KW_CASE, KW_DOUBLE, KW_FLOAT, KW_IF, KW_IN, KW_INT, KW_LIKE, KW_MAP, KW_NOT, KW_OR, KW_REGEXP, KW_RLIKE, KW_SMALLINT, KW_STRING..KW_STRUCT, KW_TINYINT, KW_UNIONTYPE, KW_WHEN, LESSTHAN..LESSTHANOREQUALTO, MINUS..NOTEQUAL, PLUS, STAR, TILDE}"" using multiple alternatives: 1, 3

As a result, alternative(s) 3 were disabled for that input
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-exec ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-exec ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-exec ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-exec ---
[INFO] Compiling 1681 source files to /data/hive-ptest/working/apache-svn-trunk-source/ql/target/classes
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] Note: Some input files use or override a deprecated API.
[WARNING] Note: Recompile with -Xlint:deprecation for details.
[WARNING] Note: Some input files use unchecked or unsafe operations.
[WARNING] Note: Recompile with -Xlint:unchecked for details.
[INFO] 4 warnings 
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java:[134,44] cannot find symbol
symbol  : method localizeTempFile(java.lang.String,org.apache.hadoop.hive.conf.HiveConf,java.lang.String[])
location: class org.apache.hadoop.hive.ql.exec.tez.DagUtils
[INFO] 1 error
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [10.008s]
[INFO] Hive Ant Utilities ................................ SUCCESS [12.350s]
[INFO] Hive Shims Common ................................. SUCCESS [4.321s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [3.178s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.255s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [3.074s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [8.170s]
[INFO] Hive Shims ........................................ SUCCESS [1.246s]
[INFO] Hive Common ....................................... SUCCESS [11.049s]
[INFO] Hive Serde ........................................ SUCCESS [9.812s]
[INFO] Hive Metastore .................................... SUCCESS [34.063s]
[INFO] Hive Query Language ............................... FAILURE [55.777s]
[INFO] Hive Service ...................................... SKIPPED
[INFO] Hive JDBC ......................................... SKIPPED
[INFO] Hive Beeline ...................................... SKIPPED
[INFO] Hive CLI .......................................... SKIPPED
[INFO] Hive Contrib ...................................... SKIPPED
[INFO] Hive HBase Handler ................................ SKIPPED
[INFO] Hive HCatalog ..................................... SKIPPED
[INFO] Hive HCatalog Core ................................ SKIPPED
[INFO] Hive HCatalog Pig Adapter ......................... SKIPPED
[INFO] Hive HCatalog Server Extensions ................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client ................. SKIPPED
[INFO] Hive HCatalog Webhcat ............................. SKIPPED
[INFO] Hive HCatalog HBase Storage Handler ............... SKIPPED
[INFO] Hive HWI .......................................... SKIPPED
[INFO] Hive ODBC ......................................... SKIPPED
[INFO] Hive Shims Aggregator ............................. SKIPPED
[INFO] Hive TestUtils .................................... SKIPPED
[INFO] Hive Packaging .................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 2:42.022s
[INFO] Finished at: Sat Apr 05 17:44:56 EDT 2014
[INFO] Final Memory: 74M/564M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-exec: Compilation failure
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java:[134,44] cannot find symbol
[ERROR] symbol  : method localizeTempFile(java.lang.String,org.apache.hadoop.hive.conf.HiveConf,java.lang.String[])
[ERROR] location: class org.apache.hadoop.hive.ql.exec.tez.DagUtils
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-exec
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638743;;;","07/Apr/14 18:14;sershe;The previous patch was incomplete, updating. The tests that failed on 02 pass locally for me, which stands to reason as only the Tez path is changed here. This patch does not need Tez 0.4;;;","07/Apr/14 18:14;sershe;Will commit later today;;;","07/Apr/14 20:48;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639030/HIVE-6739.04.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5549 tests executed
*Failed tests:*
{noformat}
org.apache.hive.hcatalog.mapreduce.TestHCatMutableDynamicPartitioned.testHCatDynamicPartitionedTable
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2164/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2164/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639030;;;","07/Apr/14 22:19;sershe;in trunk and 13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DDL locking too course grained in new db txn manager,HIVE-6734,12703348,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,gates,gates,gates,24/Mar/14 23:18,01/Oct/19 22:07,14/Jul/23 06:14,28/Mar/14 16:16,0.13.0,,,,,,,,,0.13.0,,Locking,,,,0,,,"All DDL operations currently acquire an exclusive lock.  This is too course grained, as some operations like alter table add partition shouldn't get an exclusive lock on the entire table.",,gates,wanchang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/14 21:33;gates;HIVE-6734.patch;https://issues.apache.org/jira/secure/attachment/12637249/HIVE-6734.patch","26/Mar/14 23:52;gates;HIVE-6734.patch;https://issues.apache.org/jira/secure/attachment/12637053/HIVE-6734.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381686,,,,Thu Sep 18 03:15:22 UTC 2014,,,,,,,,,,"0|i1trdr:",381961,,,,,,,,,,,,,,,,,,,,,"24/Mar/14 23:18;gates;[~rhbutani] This needs to be in 0.13 as well.;;;","27/Mar/14 04:44;gates;Ran tests locally, all looks good.;;;","27/Mar/14 17:20;ashutoshc;[~gates] Can you create RB entry for it ?;;;","27/Mar/14 17:31;gates;I changed the grain of locking for DDL statements.  In the initial checkin I had set all DDL statements to exclusive lock.  But this results in a whole table being locked so that a partition can be added.  This is excessive.  So I went back and added a DDL_EXCLUSIVE, DDL_SHARED, and DDL_NO_LOCK so that different DDL statements could obtain appropriate grains of locks.  For example, add partition is now a shared lock, drop partition is an exclusive lock, and add function is no lock.;;;","27/Mar/14 17:31;gates;Created review board https://reviews.apache.org/r/19736/;;;","27/Mar/14 20:59;ashutoshc;Left some comments on RB.;;;","27/Mar/14 21:33;gates;New patch that addresses Ashutosh's comments.;;;","27/Mar/14 22:21;ashutoshc;+1;;;","28/Mar/14 05:23;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637249/HIVE-6734.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5496 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1992/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1992/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637249;;;","28/Mar/14 16:16;ashutoshc;Committed to 0.13 & trunk.;;;","18/Sep/14 03:15;wanchang;I use DummyTxnManager as the default value of hive.txn.manager. I noticed that the CREATE TEMPORARY FUNCTION requires a EXCLUSIVE lock while it's writetype is DLL_NO_LOCK in the DummyTxnManager. Maybe it's a good idea to check the DLL's writetype in DummyTxnManager too.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Driver context logs every query in the ""warn"" level",HIVE-6733,12703289,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,24/Mar/14 19:30,26/Mar/14 22:16,14/Jul/23 06:14,26/Mar/14 22:16,0.13.0,,,,,,,,,0.13.0,,,,,,0,,,"Trivial, just lower the log level on one statement.",,hagleitn,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/14 19:31;hagleitn;HIVE-6733.1.patch;https://issues.apache.org/jira/secure/attachment/12636409/HIVE-6733.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381627,,,,Wed Mar 26 22:16:04 UTC 2014,,,,,,,,,,"0|i1tr0n:",381902,,,,,,,,,,,,,,,,,,,,,"24/Mar/14 19:31;thejas;+1;;;","26/Mar/14 22:12;hagleitn;ran tests locally. passed.;;;","26/Mar/14 22:16;hagleitn;Committed to trunk and branch. Thanks Thejas!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Release Notes for Hive 0.13,HIVE-6732,12703280,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,Fixed,rhbutani,rhbutani,rhbutani,24/Mar/14 19:09,15/Apr/14 00:55,14/Jul/23 06:14,15/Apr/14 00:55,0.13.0,0.14.0,,,,,,,,0.13.0,,,,,,0,,,NO PRECOMMIT TESTS,,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/14 19:11;rhbutani;HIVE-6732.1.patch;https://issues.apache.org/jira/secure/attachment/12636406/HIVE-6732.1.patch","09/Apr/14 17:55;rhbutani;HIVE-6732.2.patch;https://issues.apache.org/jira/secure/attachment/12639439/HIVE-6732.2.patch","11/Apr/14 17:21;rhbutani;HIVE-6732.3.patch;https://issues.apache.org/jira/secure/attachment/12639833/HIVE-6732.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381618,,,,Tue Apr 15 00:55:23 UTC 2014,,,,,,,,,,"0|i1tqyn:",381893,,,,,,,,,,,,,,,,,,,,,"24/Mar/14 20:18;ashutoshc;+1;;;","11/Apr/14 19:36;rhbutani;Committed to trunk and 0.13;;;","15/Apr/14 00:44;rhbutani;Add in jiras committed to 0.13 today.;;;","15/Apr/14 00:55;rhbutani;Just going to open a new jira for the delta change.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing file override-container-log4j.properties in Hcatalog ,HIVE-6728,12702993,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,21/Mar/14 23:31,28/Mar/14 23:27,14/Jul/23 06:14,28/Mar/14 23:27,0.13.0,,,,,,,,,0.13.0,,WebHCat,,,,0,,,,,ekoifman,gates,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/14 23:42;ekoifman;HIVE-6728.patch;https://issues.apache.org/jira/secure/attachment/12636142/HIVE-6728.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381330,,,,Fri Mar 28 23:27:06 UTC 2014,,,,,,,,,,"0|i1tp7r:",381607,,,,,,,,,,,,,,,,,,,,,"21/Mar/14 23:42;ekoifman;added missing file to assembly descriptor;;;","21/Mar/14 23:42;ekoifman;NO PRECOMMIT TESTS;;;","25/Mar/14 21:49;gates;Sorry, can you explain what this does?;;;","25/Mar/14 22:01;ekoifman;this file was added in HIVE-5511.  It used to be included in the tar during 'ant' build but was missed in Mavenization.  Basically this enables 'percentComplete' reporting in WebHCat job status (with Hadoop 2);;;","27/Mar/14 20:23;thejas;+1;;;","27/Mar/14 21:57;thejas;[~rhbutani] It would be great to have this bug fix in hive 0.13.
;;;","27/Mar/14 22:10;rhbutani;+1 for 0.13;;;","28/Mar/14 23:27;thejas;Patch committed to 0.13 branch and trunk.
Thanks for the contributions Eugene!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table level stats for external tables are set incorrectly,HIVE-6727,12702991,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,libing,rhbutani,rhbutani,21/Mar/14 23:08,16/Feb/16 23:51,14/Jul/23 06:14,01/Jun/15 20:18,0.13.0,0.13.1,0.14.0,1.0.0,1.1.0,1.2.0,,,,1.3.0,2.0.0,Statistics,,,,0,,,"if you do the following:
{code}
CREATE EXTERNAL TABLE anaylyze_external (a INT) LOCATION 'data/files/ext_test';
describe formatted anaylyze_external;
{code}
The table level stats are:
{noformat}
Table Parameters:
	COLUMN_STATS_ACCURATE	true
	EXTERNAL            	TRUE
	numFiles            	0
	numRows             	6
	rawDataSize         	6
	totalSize           	0
{noformat}
numFiles and totalSize is always 0.
Issue is:
MetaStoreUtils:updateUnpartitionedTableStatsFast attempts to set table level stats from FileStatus. But it doesn't account for External tables, it always calls Warehouse.getFileStatusesForUnpartitionedTable",,libing,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/14 13:20;libing;HIVE-6727.2.patch;https://issues.apache.org/jira/secure/attachment/12675820/HIVE-6727.2.patch","01/Jun/15 08:34;libing;HIVE-6727.3.patch;https://issues.apache.org/jira/secure/attachment/12736513/HIVE-6727.3.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381328,,,,Thu Jun 04 00:37:48 UTC 2015,,,,,,,,,,"0|i1tp7b:",381605,,,,,,,,,,,,,,,,,,,,,"20/Oct/14 09:03;libing;This issue also happens when the table is managed but specified a location which is not in hive warehouse directory on hdfs.;;;","20/Oct/14 09:07;libing;This patch is generated based on the latest trunk code;;;","20/Oct/14 09:09;libing;The patch is generated based on the latest trunk code.;;;","20/Oct/14 12:13;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12675790/HIVE-6727.1.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1349/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1349/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1349/

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/common/src/java/org/apache/hadoop/hive/common/FileUtils.java: Recompile with -Xlint:deprecation for details.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/common/src/java/org/apache/hadoop/hive/common/ObjectPair.java: /data/hive-ptest/working/apache-svn-trunk-source/common/src/java/org/apache/hadoop/hive/common/ObjectPair.java uses unchecked or unsafe operations.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/common/src/java/org/apache/hadoop/hive/common/ObjectPair.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-common ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 4 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-common ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/common/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/common/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/common/target/tmp/conf
     [copy] Copying 7 files to /data/hive-ptest/working/apache-svn-trunk-source/common/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-common ---
[INFO] Compiling 13 source files to /data/hive-ptest/working/apache-svn-trunk-source/common/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-common ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-common ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/common/target/hive-common-0.15.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-common ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-common ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/common/target/hive-common-0.15.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-common/0.15.0-SNAPSHOT/hive-common-0.15.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/common/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-common/0.15.0-SNAPSHOT/hive-common-0.15.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Serde 0.15.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-serde ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/serde (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-source (add-source) @ hive-serde ---
[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/serde/src/gen/protobuf/gen-java added.
[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/serde/src/gen/thrift/gen-javabean added.
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-serde ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-serde ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/serde/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-serde ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-serde ---
[INFO] Compiling 368 source files to /data/hive-ptest/working/apache-svn-trunk-source/serde/target/classes
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/serde/src/java/org/apache/hadoop/hive/serde2/SerDe.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/serde/src/java/org/apache/hadoop/hive/serde2/SerDe.java: Recompile with -Xlint:deprecation for details.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/Complex.java: Some input files use unchecked or unsafe operations.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/Complex.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-serde ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-serde ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/serde/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/serde/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/serde/target/tmp/conf
     [copy] Copying 7 files to /data/hive-ptest/working/apache-svn-trunk-source/serde/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-serde ---
[INFO] Compiling 46 source files to /data/hive-ptest/working/apache-svn-trunk-source/serde/target/test-classes
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableSerDe.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableSerDe.java: Recompile with -Xlint:deprecation for details.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestStandardObjectInspectors.java: Some input files use unchecked or unsafe operations.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestStandardObjectInspectors.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-serde ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-serde ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/serde/target/hive-serde-0.15.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-serde ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-serde ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/serde/target/hive-serde-0.15.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-serde/0.15.0-SNAPSHOT/hive-serde-0.15.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/serde/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-serde/0.15.0-SNAPSHOT/hive-serde-0.15.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Metastore 0.15.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-metastore ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/metastore (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-source (add-source) @ hive-metastore ---
[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/model added.
[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/gen/thrift/gen-javabean added.
[INFO] 
[INFO] --- antlr3-maven-plugin:3.4:antlr (default) @ hive-metastore ---
[INFO] ANTLR: Processing source directory /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/java
ANTLR Parser Generator  Version 3.4
org/apache/hadoop/hive/metastore/parser/Filter.g
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-metastore ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-metastore ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-metastore ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-metastore ---
[INFO] Compiling 222 source files to /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/classes
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java: Recompile with -Xlint:deprecation for details.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/MetaException.java: Some input files use unchecked or unsafe operations.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/MetaException.java: Recompile with -Xlint:unchecked for details.
[INFO] 4 warnings 
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java:[542,10] variable tablePath is already defined in method getFileStatusesForUnpartitionedTable(org.apache.hadoop.hive.metastore.api.Database,org.apache.hadoop.hive.metastore.api.Table)
[INFO] 1 error
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [13.425s]
[INFO] Hive Shims Common ................................. SUCCESS [9.765s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [3.491s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [5.821s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.748s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [7.472s]
[INFO] Hive Shims ........................................ SUCCESS [1.838s]
[INFO] Hive Common ....................................... SUCCESS [24.309s]
[INFO] Hive Serde ........................................ SUCCESS [20.794s]
[INFO] Hive Metastore .................................... FAILURE [27.909s]
[INFO] Hive Ant Utilities ................................ SKIPPED
[INFO] Hive Query Language ............................... SKIPPED
[INFO] Hive Service ...................................... SKIPPED
[INFO] Hive Accumulo Handler ............................. SKIPPED
[INFO] Hive JDBC ......................................... SKIPPED
[INFO] Hive Beeline ...................................... SKIPPED
[INFO] Hive CLI .......................................... SKIPPED
[INFO] Hive Contrib ...................................... SKIPPED
[INFO] Hive HBase Handler ................................ SKIPPED
[INFO] Hive HCatalog ..................................... SKIPPED
[INFO] Hive HCatalog Core ................................ SKIPPED
[INFO] Hive HCatalog Pig Adapter ......................... SKIPPED
[INFO] Hive HCatalog Server Extensions ................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client ................. SKIPPED
[INFO] Hive HCatalog Webhcat ............................. SKIPPED
[INFO] Hive HCatalog Streaming ........................... SKIPPED
[INFO] Hive HWI .......................................... SKIPPED
[INFO] Hive ODBC ......................................... SKIPPED
[INFO] Hive Shims Aggregator ............................. SKIPPED
[INFO] Hive TestUtils .................................... SKIPPED
[INFO] Hive Packaging .................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 2:00.679s
[INFO] Finished at: Mon Oct 20 08:12:54 EDT 2014
[INFO] Final Memory: 56M/426M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-metastore: Compilation failure
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java:[542,10] variable tablePath is already defined in method getFileStatusesForUnpartitionedTable(org.apache.hadoop.hive.metastore.api.Database,org.apache.hadoop.hive.metastore.api.Table)
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-metastore
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12675790
 - PreCommit-HIVE-TRUNK-Build;;;","20/Oct/14 13:20;libing;Fix the error in HIVE-6727.1.patch;;;","20/Oct/14 15:13;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12675820/HIVE-6727.2.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1353/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1353/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1353/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]
+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-TRUNK-Build-1353/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/optimizer/SkewJoinOptimizer.java'
++ egrep -v '^X|^Performing status on external'
++ awk '{print $2}'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit-hadoop2/target itests/hive-minikdc/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/core/target hcatalog/streaming/target hcatalog/server-extensions/target hcatalog/hcatalog-pig-adapter/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target accumulo-handler/target hwi/target common/target common/src/gen contrib/target service/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1633169.

At revision 1633169.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
patch: **** malformed patch at line 12:  

patch: **** malformed patch at line 12:  

patch: **** malformed patch at line 12:  

The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12675820
 - PreCommit-HIVE-TRUNK-Build;;;","12/Nov/14 21:42;ashutoshc;[~libing] Would you like to rebase the patch and also include a test case for this ?;;;","01/Jun/15 08:34;libing;This patch is created based on the latest Hive source code and included the fix in test case.;;;","01/Jun/15 08:36;libing;Hi, [~ashutoshc]
The current test cases in Hive already cover this case but the result is wrong.
HIVE-6727.3.patch fixes the output file of the case.

Thank you for your review.;;;","01/Jun/15 11:53;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12736513/HIVE-6727.3.patch

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 8993 tests executed
*Failed tests:*
{noformat}
TestUtil - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fold_case
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_noscan_2
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver_encryption_join_unencrypted_tbl
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx_cbo_2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats_noscan_2
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4125/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4125/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4125/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12736513 - PreCommit-HIVE-TRUNK-Build;;;","01/Jun/15 16:02;ashutoshc;+1;;;","01/Jun/15 20:18;ashutoshc;Pushed to master. Thanks, Bing!;;;","04/Jun/15 00:37;libing;Thank you, Ashutosh!;;;","04/Jun/15 00:37;libing;Thank you, Ashutosh!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hcat cli does not close SessionState,HIVE-6726,12702964,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sushanth,sushanth,sushanth,21/Mar/14 20:24,13/Nov/14 19:43,14/Jul/23 06:14,15/Apr/14 20:24,0.13.0,0.14.0,,,,,,,,0.14.0,,,,,,0,,,"When running HCat E2E tests, it was observed that hcat cli left Tez sessions on the RM which ultimately die upon timeout. Expected behavior is to clean the Tez sessions immediately upon exit. This is causing slowness in system tests as over time lot of orphan Tez sessions hang around.

On looking through code, it seems obvious in retrospect because HCatCli starts a SessionState, but does not explicitly call close on them, exiting the jvm through System.exit instead. This needs to be changed to explicitly call SessionState.close() before exiting.",,ashutoshc,deepesh,hagleitn,kedarsdixit,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/14 20:26;sushanth;HIVE-6726.patch;https://issues.apache.org/jira/secure/attachment/12636099/HIVE-6726.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381301,,,,Thu Nov 13 19:43:05 UTC 2014,,,,,,,,,,"0|i1tp13:",381577,,,,,,,,,,,,,,,,,,,,,"21/Mar/14 20:26;sushanth;Attaching patch.;;;","21/Mar/14 20:26;sushanth;[~hagleitn], could I bug you to have a quick look at this patch to see if this is sufficient?;;;","24/Mar/14 04:53;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12636099/HIVE-6726.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5443 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1939/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1939/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12636099;;;","24/Mar/14 22:31;sushanth;Note - the test failures listed here are not related to this hcat cli fix.;;;","28/Mar/14 21:32;sushanth;[~thejas]/[~hagleitn], can I bother either of you for a review for this?;;;","11/Apr/14 22:43;thejas;+1
;;;","15/Apr/14 20:24;sushanth;Thanks, Thejas!

Going ahead and committing as I have Thejas' approval to do so - he's having issues with his svn credentials currently.;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HCatStorer throws ClassCastException while storing tinyint/smallint data,HIVE-6724,12702951,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,21/Mar/14 19:26,13/Nov/14 19:41,14/Jul/23 06:14,25/Mar/14 21:36,0.12.0,,,,,,,,,0.13.0,0.14.0,HCatalog,,,,0,,,"given Hive tables:
1) create table pig_hcatalog_1 (si smallint)  STORED AS TEXTFILE;
2) create table all100k (si smallint, ti tinyint) STORED ....;

the following sequence of steps (assuming there is data in all100k)

{noformat}
a=load 'all100k' using org.apache.hive.hcatalog.pig.HCatLoader();
b = foreach a generate si;
store b into 'pig_hcatalog_1' using org.apache.hive.hcatalog.pig.HCatStorer();
{noformat}
produces 
{noformat}
org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.ClassCastException: java.lang.Short cannot be cast to java.lang.Integer
	at org.apache.hive.hcatalog.pig.HCatBaseStorer.getJavaObj(HCatBaseStorer.java:372)
	at org.apache.hive.hcatalog.pig.HCatBaseStorer.putNext(HCatBaseStorer.java:306)
	at org.apache.hive.hcatalog.pig.HCatStorer.putNext(HCatStorer.java:61)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:139)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:98)
	at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:635)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.collect(PigMapOnly.java:48)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:284)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:277)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)
{noformat}

",,ekoifman,gates,rhbutani,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/14 20:50;ekoifman;HIVE-6724.patch;https://issues.apache.org/jira/secure/attachment/12636106/HIVE-6724.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381288,,,,Thu Nov 13 19:41:34 UTC 2014,,,,,,,,,,"0|i1toy7:",381564,,,,,,,,,,,,,,,,,,,,,"21/Mar/14 20:47;ekoifman;The use case is Hive-Pig-Hive.
HCatLoader automatically sets hcat.data.tiny.small.int.promotion=true as that is required by Pig. HCatStorer did the opposite for no good reason. Since Pig doesn't evaluate each statement separately, the Storer action clobbered Loader action (the context that contains the configuration is shared). I changed Storer not to do that.;;;","24/Mar/14 08:20;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12636106/HIVE-6724.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5444 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1940/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1940/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12636106;;;","24/Mar/14 14:41;ekoifman;the patch is HCatalog only chang - the error above is not related.;;;","24/Mar/14 20:51;sushanth;+1, HCatStorer should definitely not be force-setting the promotions to false, that changes behaviour in cases where it was previously set to true. This is especially a problem since HCatContext winds up being shared across a pig job, and will affect further read attempts.;;;","25/Mar/14 21:27;sushanth;Committed to trunk. Thanks, Eugene.

Also, re-setting fix version as 0.14, as fix version tracks the version the patch has already been committed to. [~rhbutani] will be the one to backport to 0.13 after he's satisfied with it, and he can set it to 0.13.;;;","25/Mar/14 21:33;sushanth;Spoke to [~rhbutani], he's okay with me committing to 0.13 as well, will do so, and mark the fix version to 0.13 once that happens.;;;","25/Mar/14 21:36;sushanth;Committed to 0.13 as well. Thanks, Eugene!;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming ingest needs to be able to send many heartbeats together,HIVE-6721,12702940,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,gates,gates,gates,21/Mar/14 18:31,01/Oct/19 22:07,14/Jul/23 06:14,30/Mar/14 18:41,0.13.0,,,,,,,,,0.13.0,,Locking,,,,0,,,The heartbeat method added to HiveMetaStoreClient is intended for SQL operations where the user will have one transaction and a hand full of locks.  But in the streaming ingest case the client opens a batch of transactions together.  In this case we need a way for the client to send a heartbeat for this batch of transactions rather than being forced to send the heartbeats one at a time.,,appodictic,gates,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-5687,,,,"29/Mar/14 17:59;gates;HIVE-6721.patch;https://issues.apache.org/jira/secure/attachment/12637659/HIVE-6721.patch","28/Mar/14 23:48;gates;HIVE-6721.patch;https://issues.apache.org/jira/secure/attachment/12637576/HIVE-6721.patch","28/Mar/14 23:48;gates;HIVE-6721.src-only.patch;https://issues.apache.org/jira/secure/attachment/12637575/HIVE-6721.src-only.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381277,,,,Sun Mar 30 18:41:04 UTC 2014,,,,,,,,,,"0|i1tovr:",381553,,,,,,,,,,,,,,,,,,,,,"21/Mar/14 18:31;gates;[~rhbutani] This should go into 0.13 as well, as HIVE-5687 will depend on it.;;;","28/Mar/14 23:52;gates;This patch adds a call to allow a range of heartbeats to be sent together.  This way the streaming client can send a range for all open transactions.  If a transaction is aborted or missing that is in the range it is reported back to the caller, but the valid transactions still have a heartbeat recorded.  The caller must determine if it is acceptable for those transactions to be missing or aborted.

Review board posted: https://reviews.apache.org/r/19812/;;;","28/Mar/14 23:56;ashutoshc;+1;;;","29/Mar/14 00:11;rhbutani;+1 for 0.13;;;","29/Mar/14 03:54;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637576/HIVE-6721.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2025/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2025/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-2025/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java'
Reverted 'metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/hive_metastoreConstants.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaHiveVarcharObjectInspector.java'
Reverted 'ql/src/test/results/clientpositive/groupby_ppr.q.out'
Reverted 'ql/src/test/results/clientpositive/input_part7.q.out'
Reverted 'ql/src/test/results/clientpositive/pcr.q.out'
Reverted 'ql/src/test/results/clientpositive/join33.q.out'
Reverted 'ql/src/test/results/clientpositive/input_part2.q.out'
Reverted 'ql/src/test/results/clientpositive/alter_partition_coltype.q.out'
Reverted 'ql/src/test/results/clientpositive/load_dyn_part8.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby_map_ppr.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby_sort_6.q.out'
Reverted 'ql/src/test/results/clientpositive/push_or.q.out'
Reverted 'ql/src/test/results/clientpositive/stats13.q.out'
Reverted 'ql/src/test/results/clientpositive/combine2_hadoop20.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby_map_ppr_multi_distinct.q.out'
Reverted 'ql/src/test/results/clientpositive/filter_join_breaktask.q.out'
Reverted 'ql/src/test/results/clientpositive/sort_merge_join_desc_5.q.out'
Reverted 'ql/src/test/results/clientpositive/input_part9.q.out'
Reverted 'ql/src/test/results/clientpositive/join26.q.out'
Reverted 'ql/src/test/results/clientpositive/bucketmapjoin_negative.q.out'
Reverted 'ql/src/test/results/clientpositive/join_map_ppr.q.out'
Reverted 'ql/src/test/results/clientpositive/join9.q.out'
Reverted 'ql/src/test/results/clientpositive/smb_mapjoin_11.q.out'
Reverted 'ql/src/test/results/clientpositive/sample1.q.out'
Reverted 'ql/src/test/results/clientpositive/rand_partitionpruner3.q.out'
Reverted 'ql/src/test/results/clientpositive/merge3.q.out'
Reverted 'ql/src/test/results/clientpositive/sort_merge_join_desc_7.q.out'
Reverted 'ql/src/test/results/clientpositive/bucketmapjoin9.q.out'
Reverted 'ql/src/test/results/clientpositive/union22.q.out'
Reverted 'ql/src/test/results/clientpositive/join32.q.out'
Reverted 'ql/src/test/results/clientpositive/input_part1.q.out'
Reverted 'ql/src/test/results/clientpositive/columnstats_partlvl.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_sortmerge_join_3.q.out'
Reverted 'ql/src/test/results/clientpositive/sample8.q.out'
Reverted 'ql/src/test/results/clientpositive/transform_ppr2.q.out'
Reverted 'ql/src/test/results/clientpositive/union_ppr.q.out'
Reverted 'ql/src/test/results/clientpositive/ppd_vc.q.out'
Reverted 'ql/src/test/results/clientpositive/dynamic_partition_skip_default.q.out'
Reverted 'ql/src/test/results/clientpositive/stats12.q.out'
Reverted 'ql/src/test/results/clientpositive/router_join_ppr.q.out'
Reverted 'ql/src/test/results/clientpositive/input42.q.out'
Reverted 'ql/src/test/results/clientpositive/sample10.q.out'
Reverted 'ql/src/test/results/clientpositive/louter_join_ppr.q.out'
Reverted 'ql/src/test/results/clientpositive/rand_partitionpruner2.q.out'
Reverted 'ql/src/test/results/clientpositive/bucket3.q.out'
Reverted 'ql/src/test/results/clientpositive/sort_merge_join_desc_6.q.out'
Reverted 'ql/src/test/results/clientpositive/bucketmapjoin8.q.out'
Reverted 'ql/src/test/results/clientpositive/annotate_stats_part.q.out'
Reverted 'ql/src/test/results/clientpositive/smb_mapjoin_12.q.out'
Reverted 'ql/src/test/results/clientpositive/metadataonly1.q.out'
Reverted 'ql/src/test/results/clientpositive/join32_lessSize.q.out'
Reverted 'ql/src/test/results/clientpositive/outer_join_ppr.q.out'
Reverted 'ql/src/test/results/clientpositive/bucketmapjoin_negative2.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_sortmerge_join_2.q.out'
Reverted 'ql/src/test/results/clientpositive/transform_ppr1.q.out'
Reverted 'ql/src/test/results/clientpositive/ppd_union_view.q.out'
Reverted 'ql/src/test/results/clientpositive/stats11.q.out'
Reverted 'ql/src/test/results/clientpositive/input23.q.out'
Reverted 'ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorizedRowBatchCtx.java'
Reverted 'ql/src/test/queries/clientpositive/alter_partition_coltype.q'
Reverted 'ql/src/test/queries/clientpositive/pcr.q'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java'
++ egrep -v '^X|^Performing status on external'
++ awk '{print $2}'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/storage-handlers/hbase/target hcatalog/server-extensions/target hcatalog/core/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target hwi/target common/target common/src/gen service/target contrib/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target ql/src/test/results/clientpositive/vectorized_non_string_part.q.out ql/src/test/results/clientpositive/input23.q.out.orig ql/src/test/queries/clientpositive/vectorized_non_string_part.q
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1582949.

At revision 1582949.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637576;;;","29/Mar/14 17:00;gates;Ok, I'm not sure how to proceed here.  All the merge failures are in the generated code.  I don't know how to make thrift generate code that will merge properly.  If the committer applies the src-only patch and re-runs the thrift generation all should work.  Please let me know how to proceed.;;;","29/Mar/14 17:37;thejas;Alan, I suspect the merge failures are because of commits with thrift changes that went in yesterday. The patches with generated code usually applies fine in my experience. Can you try regenerating the patch on latest trunk ? I think it might just work.
;;;","29/Mar/14 17:43;appodictic;[~gates]

One trick I notice about thrift is you should always add structs at the end. If you insert structs in the middle it tends to chance some internal numbering and make more noise then it should when mergin the generated code.;;;","29/Mar/14 17:59;gates;Thanks [~thejas] and [~appodictic].  Re-running the thrift generation myself and rebuilding the patch as Thejas proposed seemed to do it.;;;","29/Mar/14 18:00;gates;Canceling and resubmitting patch to get test run.;;;","29/Mar/14 23:36;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637659/HIVE-6721.patch

{color:green}SUCCESS:{color} +1 5505 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2035/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2035/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637659;;;","30/Mar/14 18:41;ashutoshc;Committed to trunk & 0.13. Thanks, Alan!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ORC struct throws NPE for tables with inner structs having null values ,HIVE-6716,12702790,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,prasanth_j,prasanth_j,prasanth_j,21/Mar/14 02:00,24/Mar/14 19:18,14/Jul/23 06:14,24/Mar/14 19:18,0.13.0,0.14.0,,,,,,,,0.13.0,,,,,,0,orcfile,,"ORCStruct should return null when object passed to getStructFieldsDataAsList(Object obj) is null.

{code}
public List<Object> getStructFieldsDataAsList(Object object) {
      OrcStruct struct = (OrcStruct) object;
      List<Object> result = new ArrayList<Object>(struct.fields.length);
{code}

In the above code struct.fields will throw NPE if struct is NULL.",,hagleitn,prasanth_j,rhbutani,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6631,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/14 02:06;prasanth_j;HIVE-6716.1.patch;https://issues.apache.org/jira/secure/attachment/12635937/HIVE-6716.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381128,,,,Mon Mar 24 19:18:08 UTC 2014,,,,,,,,,,"0|i1tnyn:",381404,,,,,,,,,,,,,,,,,,,,,"21/Mar/14 02:16;hagleitn;+1 LGTM, wondering though if it would be easy to write a unit test for it.;;;","22/Mar/14 02:05;yhuai;[~prasanth_j] It is the same bug as I mentioned in https://issues.apache.org/jira/browse/HIVE-6631, right? If so, I will mark that one as duplicate.;;;","22/Mar/14 02:12;prasanth_j;[~yhuai] yes it looks like the same one.. in your description if cg1 inner struct is null then it will throw NPE.. You can mark it as duplicate.;;;","22/Mar/14 02:15;yhuai;ok. Have marked that one. Thanks.;;;","23/Mar/14 22:00;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635937/HIVE-6716.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5442 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1930/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1930/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635937;;;","23/Mar/14 22:19;prasanth_j;Test failures are not related.;;;","24/Mar/14 18:51;rhbutani;+1 for 0.13;;;","24/Mar/14 19:18;hagleitn;Committed to trunk and branch. Thanks [~prasanth_j]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive JDBC should include username into open session request for non-sasl connection,HIVE-6715,12702783,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,prasadm,srinathshankar,srinathshankar,21/Mar/14 01:43,13/Nov/14 19:41,14/Jul/23 06:14,16/Oct/14 19:33,,,,,,,,,,0.14.0,,JDBC,,,,1,,,"The only parameter from sessVars that's being set in HiveConnection.openSession() is HS2_PROXY_USER. 
HIVE_AUTH_USER must also be set.",,leftyl,prasadm,srinathshankar,thejas,vgumashta,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/14 19:06;prasadm;HIVE-6715.1.patch;https://issues.apache.org/jira/secure/attachment/12636083/HIVE-6715.1.patch","15/Oct/14 15:04;thejas;HIVE-6715.2.patch;https://issues.apache.org/jira/secure/attachment/12675012/HIVE-6715.2.patch","15/Oct/14 15:54;thejas;HIVE-6715.3.patch;https://issues.apache.org/jira/secure/attachment/12675019/HIVE-6715.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381121,,,,Thu Nov 13 19:41:27 UTC 2014,,,,,,,,,,"0|i1tnx3:",381397,,,,,,,,,,,,,,,,,,,,,"21/Mar/14 19:11;vgumashta;[~prasadm] The change looks good. Is it possible to stick the tests in one of the existing test case classes though? 
;;;","21/Mar/14 19:21;prasadm;[~vaibhavgumashta] Thanks for taking a look.
The problem is the nosasl transport mode. It needs to be set via system property (till HIVE-6665 is committed) and 
restart the server. That's what I had to add it to a separate test.
;;;","15/Oct/14 15:03;thejas;[~prasadm] Thanks for the patch. Looks good to me.  +1

I have rebased it for the latest trunk.
;;;","15/Oct/14 15:54;thejas;HIVE-6715.3.patch - In current trunk, setting the auth property via conf for MiniHS2 is what works  (HiveAuthFactory no longer creates a new hiveconf). Updating test case.
;;;","15/Oct/14 20:52;thejas;[~vikram.dixit] This is a useful bug fix. It is also small and should not be destabilizing. If it is not too late, would like to include it in 0.14 .
;;;","15/Oct/14 21:59;vikram.dixit;+1 for 0.14;;;","15/Oct/14 23:34;prasadm;Thanks [~thejas]!;;;","16/Oct/14 08:32;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12675019/HIVE-6715.3.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 6560 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_custom_key
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_custom_key2
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_custom_key3
org.apache.hive.beeline.TestSchemaTool.testSchemaInit
org.apache.hive.beeline.TestSchemaTool.testSchemaUpgrade
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1294/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1294/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1294/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12675019
 - PreCommit-HIVE-TRUNK-Build;;;","16/Oct/14 19:33;thejas;Patch committed to trunk and 0.14 branch.
Thanks  [~prasadm]!
;;;","17/Oct/14 02:18;leftyl;This looks like it doesn't need any user documentation ... but I want to be sure.;;;","17/Oct/14 02:20;thejas;It is a bug fix. No documentation required for this one.
;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ORC maps uses getMapSize() from MapOI which is unreliable,HIVE-6711,12702685,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,prasanth_j,prasanth_j,prasanth_j,20/Mar/14 18:39,13/Nov/14 19:44,14/Jul/23 06:14,24/Mar/14 00:23,0.11.0,0.12.0,0.13.0,,,,,,,0.13.0,0.14.0,,,,,0,orcfile,,"HIVE-6707 had issues with map size. getMapSize() of LazyMap and LazyBinaryMap does not deserialize the keys and count the number of unique keys. Since getMapSize() may return non-distinct count of keys, the length of maps stored using ORC's map tree writer will not be in sync with actual map size. As a result of this RLE reader will try to read beyond the disk range expecting more map entries and will throw exception.

Stack trace will look like:
{code}
Caused by: java.io.EOFException: Read past end of RLE integer from compressed stream Stream for column 2 kind DATA position: 22059699 length: 22059699 range: 0 offset: 22359014 limit: 22359014 range 0 = 0 to 22059699 uncompressed: 53370 to 53370
        at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readValues(RunLengthIntegerReaderV2.java:54)
        at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.next(RunLengthIntegerReaderV2.java:301)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StringDictionaryTreeReader.next(RecordReaderImpl.java:1572)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StringTreeReader.next(RecordReaderImpl.java:1330)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$MapTreeReader.next(RecordReaderImpl.java:2041)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StructTreeReader.next(RecordReaderImpl.java:1772)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.next(RecordReaderImpl.java:2963)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$OrcRecordReader.next(OrcInputFormat.java:121)
{code}",,hagleitn,prasanth_j,rhbutani,thejas,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 18:44;prasanth_j;HIVE-6711.1.patch;https://issues.apache.org/jira/secure/attachment/12635836/HIVE-6711.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381023,,,,Thu Nov 13 19:44:31 UTC 2014,,,,,,,,,,"0|i1tnbz:",381301,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 18:51;hagleitn;makes sense for orc but should still fix teh getMapSize api in the case of duplicated entries (separate jira). +1;;;","23/Mar/14 18:34;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635836/HIVE-6711.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5440 tests executed
*Failed tests:*
{noformat}
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1925/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1925/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635836;;;","23/Mar/14 22:19;prasanth_j;Test failure is not related.;;;","24/Mar/14 00:23;vikram.dixit;Committed to trunk. Thanks Prasanth!;;;","24/Mar/14 04:08;rhbutani;+1 for 0.13;;;","24/Mar/14 10:20;vikram.dixit;Committed to branch 0.13.;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlocks seen in transaction handler using mysql,HIVE-6710,12702673,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,gates,gates,gates,20/Mar/14 17:20,01/Oct/19 22:07,14/Jul/23 06:14,28/Mar/14 16:20,0.13.0,,,,,,,,,0.13.0,,Locking,,,,1,,,When multiple clients attempt to obtain locks a deadlock on the mysql database occasionally occurs.,,gates,kedarsdixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/14 23:24;gates;HIVE-6710.patch;https://issues.apache.org/jira/secure/attachment/12637046/HIVE-6710.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,381011,,,,Fri Mar 28 16:20:05 UTC 2014,,,,,,,,,,"0|i1tn9j:",381289,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 17:23;gates;The issue is two fold:
# The code is not disciplined about ordering the access to the TXNS and HIVE_LOCKS tables, this needs to be fixed
# Per the [mysql documentation | https://dev.mysql.com/doc/refman/5.0/en/innodb-deadlocks.html], even when tables are accessed in the right order deadlocks can be seen due to row locking issues and thus the application should always be ready to re-start the transaction.  This is observed in the TxnManager.lock function when two clients are obtaining locks at the same time.;;;","20/Mar/14 17:23;gates;[~rhbutani] This needs to be included in Hive 0.13;;;","27/Mar/14 04:30;gates;Ran tests locally, all looks good.;;;","27/Mar/14 17:19;ashutoshc;[~gates] Can you create RB entry for this ?;;;","27/Mar/14 17:27;gates;Review board entry created, https://reviews.apache.org/r/19735/;;;","27/Mar/14 17:27;gates;It should be noted that this looks like I rewrote large sections of the code, but I did not.  Each public method was wrapped in a try/catch block to handle deadlocks and retry.;;;","27/Mar/14 23:33;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637046/HIVE-6710.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5491 tests executed
*Failed tests:*
{noformat}
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1988/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1988/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637046;;;","28/Mar/14 00:05;ashutoshc;+1;;;","28/Mar/14 16:20;ashutoshc;Committed to trunk & 0.13. Thanks, Alan!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2 help command is not recognizing properly.,HIVE-6709,12702591,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,sreenivasulureddy,sreenivasulureddy,sreenivasulureddy,20/Mar/14 09:35,13/Nov/14 19:43,14/Jul/23 06:14,20/Mar/14 21:22,0.11.0,0.12.0,,,,,,,,0.14.0,,HiveServer2,,,,0,,,"Install the HiveServer,
Then Execute the following command for help in Hive_home

./hive --service hiveserver2 --help
{code}
Starting HiveServer2
Unrecognized option: -h
usage: hiveserver2
 -H,--help                        Print help information
    --hiveconf <property=value>   Use value for given property
{code}
need to provide in -H instead of -h in hiverserver2.sh file
{code}
hiveserver2_help() {
  hiveserver2 -H
}
{code}",,leftyl,sreenivasulureddy,szehon,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6321,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 09:55;sreenivasulureddy;HIVE-6709.patch;https://issues.apache.org/jira/secure/attachment/12635760/HIVE-6709.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,380930,,,,Thu Nov 13 19:43:21 UTC 2014,,,,,,,,,,"0|i1tmrz:",381209,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 17:29;szehon;Yea I see it now too, good catch.  +1 (non-binding);;;","20/Mar/14 20:23;leftyl;Since this affects versions 0.11 and 0.12, the wiki should mention it.  

Should the wiki just say that --help doesn't work in those versions?  The usage message isn't documented yet, but it could go in this section:

* [HiveServer2:  How to Start |https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2#SettingUpHiveServer2-HowtoStart];;;","20/Mar/14 20:37;szehon;Sreenivasulu can correct me if wrong, but looks like running './hive --service hiveserver2 --help' will print an 'unrecognized option' error, but as a side effect of that error, still displays the usage.  In other words, any parse error (even one trying to display usage) displays the usage.  

Up to you whether we want to document the defect, as it works in a weird way, but definitely the usage in general is good to document. ;;;","20/Mar/14 20:46;leftyl;Oh, right.  But there's only one option besides help (--hiveconf) and that's standard for Hive, so does it really need to be documented in the wiki?  Well, no harm putting it in.;;;","20/Mar/14 21:20;ashutoshc;+1;;;","20/Mar/14 21:22;ashutoshc;Committed to trunk. Thanks, Sreenivasulu!;;;","20/Mar/14 21:28;leftyl;Added a Usage Message section:

* [Setting Up HiveServer2:  How to Start |https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2#SettingUpHiveServer2-HowtoStart];;;","21/Mar/14 04:31;sreenivasulureddy;Thanks Ashutosh Chauhan, szehon and Lefty Leverenz  for Review the Patch.;;;","21/Mar/14 05:24;sreenivasulureddy;[~szehon]
{code}
 public boolean process(String[] argv) {
    try {
      commandLine = new GnuParser().parse(options, argv);
      if (commandLine.hasOption('H')) {
        printUsage();
        return false;
      }
{code}
In the above code passing argument is handled by GnuParser class, there parse exception we are getting, in catch block any error we are getting, showing usage again.
{code}
catch (ParseException e) {
      System.err.println(e.getMessage());
      printUsage();
      return false;
    }
    return true;
  }
{code}
Better will throw parse Exception only.
Instead of printing usage again.
Based on Exception end user will come to know.;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConstantVectorExpression should create copies of data objects rather than referencing them,HIVE-6708,12702587,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hsubramaniyan,hsubramaniyan,hsubramaniyan,20/Mar/14 09:10,26/Mar/14 22:54,14/Jul/23 06:14,26/Mar/14 22:54,,,,,,,,,,0.13.0,,,,,,0,,,"1. ConstantVectorExpression vector should be updated for bytecolumnvectors and decimalColumnVectors. The current code changes the reference to the vector which might be shared across multiple columns

2. VectorizationContext.foldConstantsForUnaryExpression(ExprNodeDesc exprDesc) has a minor bug as to when to constant fold the expression.
The following code should replace the corresponding piece of code in the trunk.
..
    GenericUDF gudf = ((ExprNodeGenericFuncDesc) exprDesc).getGenericUDF();
    if (gudf instanceof GenericUDFOPNegative || gudf instanceof GenericUDFOPPositive
        || castExpressionUdfs.contains(gudf.getClass())
... ",,gates,hsubramaniyan,jnp,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 09:14;hsubramaniyan;HIVE-6708-1.patch;https://issues.apache.org/jira/secure/attachment/12635750/HIVE-6708-1.patch","25/Mar/14 07:45;hsubramaniyan;HIVE-6708-3.patch;https://issues.apache.org/jira/secure/attachment/12636548/HIVE-6708-3.patch","26/Mar/14 00:20;hsubramaniyan;HIVE-6708-4.patch;https://issues.apache.org/jira/secure/attachment/12636826/HIVE-6708-4.patch","21/Mar/14 19:47;hsubramaniyan;HIVE-6708.2.patch;https://issues.apache.org/jira/secure/attachment/12636090/HIVE-6708.2.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,380926,,,,Wed Mar 26 22:54:25 UTC 2014,,,,,,,,,,"0|i1tmr3:",381205,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 09:14;hsubramaniyan;cc-ing [~jnp] for review.;;;","20/Mar/14 21:05;hsubramaniyan;https://reviews.apache.org/r/19456/;;;","21/Mar/14 19:27;hsubramaniyan;updated patch as per [~jnp] 's comments;;;","21/Mar/14 20:46;jnp;+1;;;","24/Mar/14 03:09;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12636090/HIVE-6708.2.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5444 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_expressions
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testIfConditionalExprs
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1936/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1936/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12636090;;;","26/Mar/14 06:29;gates;Ran tests locally for patch 4, everything passed.;;;","26/Mar/14 18:14;jnp;+1 for patch 4.;;;","26/Mar/14 18:23;jnp;[~rhbutani] This bug is causing some correctness issues, therefore this fix should also go to hive-0.13.;;;","26/Mar/14 20:52;rhbutani;I see its already in; +1 for 0.13;;;","26/Mar/14 22:54;jnp;I have committed this to trunk and branch-0.13. Thanks to [~hari.s]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lazy maps are broken (LazyMap and LazyBinaryMap),HIVE-6707,12702570,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,prasanth_j,prasanth_j,prasanth_j,20/Mar/14 07:24,13/Nov/14 19:40,14/Jul/23 06:14,24/Mar/14 00:20,0.10.0,0.11.0,0.12.0,0.13.0,0.5.0,0.6.0,0.7.0,0.8.0,0.9.0,0.13.0,0.14.0,Serializers/Deserializers,,,,0,serde,,"LazyPrimitive and LazyBinaryPrimitive overrides hashcode method in HIVE-949. But it failed to override equals() method. As a result, LazyMap and LazyBinaryMap will end up having multiple values for the same key. Both LazyMap and LazyBinaryMap uses LinkedHashMap, so the expected behaviour is to have a single value per unique key.

In the following code from LazyMap (LazyBinaryMap also has same code segment)
{code}
LazyPrimitive<?, ?> lazyKey = uncheckedGetKey(i);
if (lazyKey == null) {
  continue;
}
Object key = lazyKey.getObject();
if (key != null && !cachedMap.containsKey(key)) {
{code}

lazyKey.hashcode() returns the writable object's hashcode. The containsKeys() method of hash map (http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/6-b14/java/util/HashMap.java#366) checks if the hashcode are same, if so then it uses equals() method to verify if the key already exists. Since LazyPrimitive does not override equals() method it falls back to use Object equals(). Object equals() will return true only if both object are exactly the same (this == obj).
So in the above code segment, even if the key already exists, the new value will be inserted with hash collision resulting in more number of map entries.",,hagleitn,prasanth_j,rhbutani,thejas,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6714,,,,"20/Mar/14 07:25;prasanth_j;HIVE-6707.1.patch;https://issues.apache.org/jira/secure/attachment/12635736/HIVE-6707.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,380909,,,,Thu Nov 13 19:40:46 UTC 2014,,,,,,,,,,"0|i1tmnb:",381188,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 18:04;prasanth_j;getMapSize() api is broken as well. It does not reported the number of distinct keys. I will fix that as well and will upload a new patch.;;;","20/Mar/14 18:21;hagleitn;+1;;;","20/Mar/14 18:42;prasanth_j;Making it patch available for precommit tests.;;;","23/Mar/14 20:17;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635736/HIVE-6707.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5443 tests executed
*Failed tests:*
{noformat}
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1926/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1926/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635736;;;","23/Mar/14 22:19;prasanth_j;Test failure is not related.;;;","24/Mar/14 00:20;vikram.dixit;Committed to trunk. Thanks Prasanth!;;;","24/Mar/14 04:06;rhbutani;+1 for 0.13;;;","24/Mar/14 09:58;vikram.dixit;Committed to branch-0.13.;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tez queries fail when there are no input paths,HIVE-6706,12702562,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,20/Mar/14 06:13,24/Mar/14 19:11,14/Jul/23 06:14,24/Mar/14 19:11,,,,,,,,,,0.13.0,,,,,,0,,,"Need to disable check for empty input paths in the InputFormat in Tez. Tez allows ""empty"" vertices to make it easier to handle empty tables, partitions, buckets, etc.",,hagleitn,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 06:14;hagleitn;HIVE-6706.1.patch;https://issues.apache.org/jira/secure/attachment/12635726/HIVE-6706.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,380901,,,,Sun Mar 23 11:40:54 UTC 2014,,,,,,,,,,"0|i1tmlb:",381180,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 19:01;vikram.dixit;LGTM +1.;;;","20/Mar/14 19:01;vikram.dixit;pending tests.;;;","23/Mar/14 11:40;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635726/HIVE-6706.1.patch

{color:green}SUCCESS:{color} +1 5440 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1918/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1918/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635726;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
date_add()/date_sub()/datediff() fail with NPE with null input,HIVE-6704,12702538,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,20/Mar/14 02:37,23/Mar/14 17:05,14/Jul/23 06:14,23/Mar/14 17:05,,,,,,,,,,0.13.0,,UDF,,,,0,,,"Similar to HIVE-6645, the following functions also get NPE error if the input is null:
date_add
date_sub
datediff

{noformat}
Diagnostic Messages for this Task:
Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""t"":null,""si"":null,""i"":null,""b"":null,""f"":null,""d"":null,""dc"":null,""bo"":null,""s"":null,""s2"":null,""ts"":null,""ts2"":null,""dt"":null}
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:195)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""t"":null,""si"":null,""i"":null,""b"":null,""f"":null,""d"":null,""dc"":null,""bo"":null,""s"":null,""s2"":null,""ts"":null,""ts2"":null,""dt"":null}
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:534)
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)
	... 8 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating date_add(ts, 2)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:791)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:791)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:524)
	... 9 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateAdd.evaluate(GenericUDFDateAdd.java:144)
	at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:166)
	at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:77)
	at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:65)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:79)
	... 13 more


FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
{noformat}",,jdere,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 02:40;jdere;HIVE-6704.1.patch;https://issues.apache.org/jira/secure/attachment/12635713/HIVE-6704.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,380877,,,,Sun Mar 23 17:05:38 UTC 2014,,,,,,,,,,"0|i1tmg7:",381156,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 02:44;jdere;RB at https://reviews.apache.org/r/19447/
Also this patch adds support for char/varchar argument types.;;;","20/Mar/14 19:42;ashutoshc;+1;;;","23/Mar/14 02:28;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635713/HIVE-6704.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5440 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_disable_merge_for_bucketing
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1916/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1916/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635713;;;","23/Mar/14 17:05;ashutoshc;Committed to trunk & 0.13 Thanks, Jason!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TezMergedLogicalInput needs to inform the framework when it is ready,HIVE-6702,12702524,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sseth,sseth,sseth,20/Mar/14 01:09,24/Mar/14 20:59,14/Jul/23 06:14,24/Mar/14 20:59,,,,,,,,,,0.13.0,,,,,,0,,,Required for unions to work with Tez - since Hive now waits for Inputs to become ready. The Inputs are required to inform Tez when they're ready.,,hagleitn,sseth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 01:09;sseth;HIVE-6702.1.patch;https://issues.apache.org/jira/secure/attachment/12635704/HIVE-6702.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,380863,,,,Mon Mar 24 20:59:58 UTC 2014,,,,,,,,,,"0|i1tmd3:",381142,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 01:09;sseth;Simple patch which waits for all underlying inputs to become ready before informing Tez.;;;","20/Mar/14 01:14;hagleitn;LGTM +1;;;","23/Mar/14 00:43;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635704/HIVE-6702.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5440 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1913/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1913/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635704;;;","24/Mar/14 20:54;hagleitn;failures are unrelated.;;;","24/Mar/14 20:59;hagleitn;Committed to branch and trunk. Thank you [~sseth]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Analyze table compute statistics for decimal columns.,HIVE-6701,12702506,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sershe,jnp,jnp,19/Mar/14 23:18,06/Dec/22 13:29,14/Jul/23 06:14,27/Mar/14 23:43,,,,,,,,,,0.13.0,,,,,,0,,,Analyze table should compute statistics for decimal columns as well.,,jnp,sershe,shreepadma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/14 21:39;sershe;HIVE-6701.02.patch;https://issues.apache.org/jira/secure/attachment/12636116/HIVE-6701.02.patch","21/Mar/14 01:13;jnp;HIVE-6701.1.patch;https://issues.apache.org/jira/secure/attachment/12635934/HIVE-6701.1.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,380845,,,,Thu Mar 27 23:43:08 UTC 2014,,,,,,,,,,"0|i1tm93:",381124,,,,,,,,,,,,,,,,,,,,,"21/Mar/14 01:13;jnp;This patch only adds decimal support in the udf GenericUDAFComputeStats.
The changes to metastore and thrift APIs are still needed.;;;","21/Mar/14 02:43;sershe;I have most of the metastore and API changes... will finish tomorrow and post and addon patch. Perhaps a q file to test will also be needed;;;","21/Mar/14 21:39;sershe;the metastore work and the the q file.
Note that fields for decimal (varchar) already existed in the schema since HIVE-1362, they just weren't used. So upgrade scripts are not necessary.
Most of the patch is generated code...;;;","21/Mar/14 21:44;sershe;https://reviews.apache.org/r/19552;;;","21/Mar/14 22:04;shreepadma;The extra unused field were added in HIVE-1362 precisely to avoid upgrading the schema.;;;","24/Mar/14 10:02;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12636116/HIVE-6701.02.patch

{color:green}SUCCESS:{color} +1 5444 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1942/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1942/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12636116;;;","25/Mar/14 23:59;ashutoshc;+1 LGTM;;;","27/Mar/14 16:59;sershe;will commit to trunk and 13 later today;;;","27/Mar/14 23:43;sershe;in trunk and 13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
In some queries inputs are closed on Tez before the operator pipeline is flushed,HIVE-6700,12702492,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,19/Mar/14 21:31,24/Mar/14 19:12,14/Jul/23 06:14,24/Mar/14 19:12,,,,,,,,,,0.13.0,,,,,,0,,,Group by operators won't flush their last row until operator is closed. In Tez it's possible that the input is already closed at this point.,,hagleitn,sseth,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TEZ-955,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/14 21:33;hagleitn;HIVE-6700.1.patch;https://issues.apache.org/jira/secure/attachment/12635647/HIVE-6700.1.patch","19/Mar/14 22:08;hagleitn;HIVE-6700.2.patch;https://issues.apache.org/jira/secure/attachment/12635657/HIVE-6700.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,380831,,,,Sun Mar 23 13:23:38 UTC 2014,,,,,,,,,,"0|i1tm5z:",381110,,,,,,,,,,,,,,,,,,,,,"19/Mar/14 21:43;sseth;Looks good, only question I have is whether the try block needs to enclose setting up Inputs in the ReduceProcessor.
{code}
  if(shuffleInputs.size() == 1){
        //no merging of inputs required
        kvsReader = (KeyValuesReader) shuffleInputs.get(0).getReader();
      }else {
        //get a sort merged input
        kvsReader = new InputMerger(shuffleInputs);
      }
{code};;;","19/Mar/14 21:45;vikram.dixit;LGTM +1 pending tests run.;;;","19/Mar/14 22:08;hagleitn;.2 addresses sid's comment. good point need to include more in the try block.;;;","19/Mar/14 22:17;sseth;Looks good. +1;;;","23/Mar/14 13:23;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635657/HIVE-6700.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5440 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_bucketed_table
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1919/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1919/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635657;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hcat.py script does not correctly load the hbase storage handler jars,HIVE-6698,12702461,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,deepesh,deepesh,deepesh,19/Mar/14 19:12,25/Feb/15 15:21,14/Jul/23 06:14,21/Mar/14 20:33,0.13.0,,,,,,,,,0.14.0,,HCatalog,,,,0,,,"Currently queries using the HBaseHCatStorageHandler when run using hcat.py fail. Example query
{code}
create table pig_hbase_1(key string, age string, gpa string)
STORED BY 'org.apache.hcatalog.hbase.HBaseHCatStorageHandler'
TBLPROPERTIES ('hbase.columns.mapping'=':key,info:age,info:gpa');
{code}
Following error is seen in the hcat logs:
{noformat}
2014-03-18 08:25:49,437 ERROR ql.Driver (SessionState.java:printError(541)) - FAILED: SemanticException java.io.IOException: Error in loading storage handler.org.apache.hcatalog.hbase.HBaseHCatStorageHandler
org.apache.hadoop.hive.ql.parse.SemanticException: java.io.IOException: Error in loading storage handler.org.apache.hcatalog.hbase.HBaseHCatStorageHandler
	at org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook.postAnalyze(CreateTableHook.java:208)
	at org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.postAnalyze(HCatSemanticAnalyzer.java:242)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:402)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:295)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:949)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:997)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:885)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:875)
	at org.apache.hive.hcatalog.cli.HCatDriver.run(HCatDriver.java:43)
	at org.apache.hive.hcatalog.cli.HCatCli.processCmd(HCatCli.java:259)
	at org.apache.hive.hcatalog.cli.HCatCli.processLine(HCatCli.java:213)
	at org.apache.hive.hcatalog.cli.HCatCli.main(HCatCli.java:172)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
Caused by: java.io.IOException: Error in loading storage handler.org.apache.hcatalog.hbase.HBaseHCatStorageHandler
	at org.apache.hive.hcatalog.common.HCatUtil.getStorageHandler(HCatUtil.java:432)
	at org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook.postAnalyze(CreateTableHook.java:199)
	... 16 more
Caused by: java.lang.ClassNotFoundException: org.apache.hcatalog.hbase.HBaseHCatStorageHandler
	at java.net.URLClassLoader$1.run(Unknown Source)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(Unknown Source)
	at java.lang.ClassLoader.loadClass(Unknown Source)
	at java.lang.ClassLoader.loadClass(Unknown Source)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Unknown Source)
	at org.apache.hive.hcatalog.common.HCatUtil.getStorageHandler(HCatUtil.java:426)
	... 17 more
{noformat}
The problem is that the hbaseStorageJar is incorrect with the merging of hcat into hive. Also as per HIVE-6695 we should add the HBASE_LIB in the classpath.",,deepesh,pouic,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/14 19:13;deepesh;HIVE-6698.patch;https://issues.apache.org/jira/secure/attachment/12635619/HIVE-6698.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,380800,,,,Wed Feb 25 15:21:24 UTC 2015,,,,,,,,,,"0|i1tlz3:",381079,,,,,,,,,,,,,,,,,,,,,"19/Mar/14 19:13;deepesh;Attaching the patch with the fix.;;;","20/Mar/14 22:28;sushanth;Looks good to me, +1.;;;","21/Mar/14 20:33;sushanth;Committed. Thanks, Deepesh!;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;","25/Feb/15 15:21;pouic;Hi,

I have the same issue using HDP 2.2 (which should embed hive 0.14).
I cannot find patch lines in my hcat.py nor on https://apache.googlesource.com/hive/+/trunk/hcatalog/bin/hcat.py
???


;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bin/hcat should include hbase jar and dependencies in the classpath [followup/clone of HCATALOG-621],HIVE-6695,12702275,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ndimiduk,sushanth,sushanth,18/Mar/14 22:48,03/Jul/18 20:52,14/Jul/23 06:14,18/Mar/14 23:44,,,,,,,,,,0.14.0,,,,,,0,,,"This is to address the addendum of HCATALOG-621, now that the HCatalog jira seems to be in read-only mode. To quote Nick from the original bug:

I'm not sure how this fixes anything for the error listed above. The find command in the script we merged is broken, at least on linux. Maybe it worked with BSD find and we both tested on Macs?

From the patch we committed:
{noformat}
if [ -d ${HBASE_HOME} ] ; then
   for jar in $(find $HBASE_HOME -name *.jar -not -name thrift\*.jar); do
      HBASE_CLASSPATH=$HBASE_CLASSPATH:${jar}
   done
   export HADOOP_CLASSPATH=""${HADOOP_CLASSPATH}:${HBASE_CLASSPATH}""
fi
{noformat}
The find command syntax is wrong – it returns no jars ever.

{noformat}
$ find /usr/lib/hbase -name *.jar
$ find /usr/lib/hbase -name *.jar -not -name thrift\*.jar
$
{noformat}

What we need is more like:

{noformat}
$ find /usr/lib/hbase -name '*.jar'
... // prints lots of jars
$ find /usr/lib/hbase -name '*.jar' | grep thrift
/usr/lib/hbase/lib/libthrift-0.9.0.jar
$ find /usr/lib/hbase -name '*.jar' -not -name '*thrift*' | grep thrift
$
{noformat}
",,ndimiduk,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HCATALOG-621,,,,,,,,,,,,,HIVE-20077,,,,,,,,"18/Mar/14 22:51;sushanth;HIVE-6695.patch;https://issues.apache.org/jira/secure/attachment/12635427/HIVE-6695.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,380615,,,,Thu Nov 13 19:42:57 UTC 2014,,,,,,,,,,"0|i1tktz:",380894,,,,,,,,,,,,,,,,,,,,,"18/Mar/14 22:51;sushanth;(Attaching addendum patch that was uploaded to HCATALOG-621);;;","18/Mar/14 23:00;sushanth;[~ndimiduk], I created this hive jira since I was not able to respond on HCATALOG-621, since that seems like it's been locked down.


+1 to the change, I'll go ahead and commit it.

I've experimented with both versions of the find command, and both work for me (with and without quotes, and in fact, I'm more used to the backslash notation). I'm using findutils-4.4.2-6.el6.x86_64. The main difference though, was that our offending jar is libthrift\*jar, not thrift\*jar.
;;;","18/Mar/14 23:44;sushanth;Committed. Thanks, Nick!;;;","18/Mar/14 23:53;ndimiduk;Thanks [~sushanth]!;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in tez session state,HIVE-6690,12702028,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,sershe,sershe,sershe,18/Mar/14 00:19,19/Mar/14 16:52,14/Jul/23 06:14,19/Mar/14 16:52,0.13.0,,,,,,,,,0.13.0,,,,,,0,,,"If hive.jar.directory isn't set hive will throw NPE in startup with tez:

Exception in thread ""main"" java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:344)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:682)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:626)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.createHiveExecLocalResource(TezSessionState.java:303)
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:130)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:342)
        ... 7 more",,hagleitn,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/14 00:29;sershe;HIVE-6690.patch;https://issues.apache.org/jira/secure/attachment/12635207/HIVE-6690.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,380368,,,,Wed Mar 19 16:52:29 UTC 2014,,,,,,,,,,"0|i1tjcf:",380649,,,,,,,,,,,,,,,,,,,,,"18/Mar/14 00:30;sershe;[~hagleitn] fyi;;;","18/Mar/14 20:53;hagleitn;+1 LGTM;;;","19/Mar/14 16:52;sershe;committed to trunk and 13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide an option to not display partition columns separately in describe table output ,HIVE-6689,12702010,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,17/Mar/14 23:14,22/Nov/16 22:44,14/Jul/23 06:14,21/Mar/14 15:48,0.11.0,0.12.0,,,,,,,,0.13.0,,,,,,0,,,"In ancient Hive partition columns were not displayed differently, in newer version they are displayed differently. This has resulted in backward incompatible change for upgrade scenarios. ",,jdere,leftyl,Tauruzzz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/14 23:57;ashutoshc;HIVE-6689.1.patch;https://issues.apache.org/jira/secure/attachment/12635203/HIVE-6689.1.patch","21/Mar/14 04:35;ashutoshc;HIVE-6689.2.patch;https://issues.apache.org/jira/secure/attachment/12635956/HIVE-6689.2.patch","17/Mar/14 23:16;ashutoshc;HIVE-6689.patch;https://issues.apache.org/jira/secure/attachment/12635193/HIVE-6689.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,380350,,,,Tue Nov 22 22:44:50 UTC 2016,,,,,,,,,,"0|i1tj8f:",380631,,,,,,,,,,,,,,,,,,,,,"17/Mar/14 23:16;ashutoshc;Provide a config knob to revert to old behavior, if needed. Default is to keep current behavior.;;;","17/Mar/14 23:26;ashutoshc;Ready for review at: https://reviews.apache.org/r/19329/;;;","18/Mar/14 07:37;jdere;+1;;;","21/Mar/14 15:34;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635956/HIVE-6689.2.patch

{color:green}SUCCESS:{color} +1 5437 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1882/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1882/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635956;;;","21/Mar/14 15:48;ashutoshc;Committed to trunk & 0.13;;;","30/Jun/14 06:02;leftyl;This adds configuration parameter *hive.display.partition.cols.separately* which is documented here:

* [Language Manual -- DDL -- Describe Table/View/Column | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-DescribeTable/View/Column]
* [Configuration Properties -- hive.display.partition.cols.separately | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.display.partition.cols.separately]

I've added a comment to HIVE-6586 so *hive.display.partition.cols.separately* won't get lost in the shuffle when HIVE-6037 changes HiveConf.java.;;;","22/Nov/16 22:44;Tauruzzz;This flag does not seem to work with ""DESCRIBE FORMATTED"". Is it expected?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC ResultSet fails to get value by qualified projection name,HIVE-6687,12701993,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jpullokkaran,jpullokkaran,jpullokkaran,17/Mar/14 21:26,10/Aug/16 06:25,14/Jul/23 06:14,24/Mar/14 05:28,0.12.0,,,,,,,,,0.13.0,,HiveServer2,,,,0,documentation,,"Getting value from result set using fully qualified name would throw exception. Only solution today is to use position of the column as opposed to column label.
{code}
String sql = ""select r1.x, r2.x from r1 join r2 on r1.y=r2.y"";
ResultSet res = stmt.executeQuery(sql);
res.getInt(""r1.x"");
{code}
res.getInt(""r1.x""); would throw exception unknown column even though sql specifies it.

Fix is to fix resultsetschema in semantic analyzer.

",,jpullokkaran,leftyl,mdominguez@cloudera.com,prasadm,rhbutani,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-14387,,,,,,,,,,HIVE-11614,,,,,,,,,,,,,,,,,,,,,"24/Mar/14 04:42;jpullokkaran;HIVE-6687.4.patch;https://issues.apache.org/jira/secure/attachment/12636295/HIVE-6687.4.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,380333,,,,Tue Mar 25 08:13:41 UTC 2014,,,,,,,,,,"0|i1tj4v:",380615,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 09:04;vgumashta;[~jpullokkaran] Can you upload the patch to review board - makes an easier read?

Quickly went through and noticed that some tests are added to TestJdbcDriver. Actually that class is used to tests the JDBC-1 driver compatible with HiveServer1. I don't think there's much development there. TestJdbcDriver2 supersedes it. Thanks!;;;","20/Mar/14 22:25;rhbutani;Review is at https://reviews.apache.org/r/19503/;;;","21/Mar/14 19:01;jpullokkaran;Apparently view schema also uses same result set schema.
Modified patch to:
1. Separate out View Schema vs Result Set Schema.
2. View Schema won't use qualified table names. View schema would also ensure that column names are unique.
3. ResultSet schema by default would use table aliases if provided (select *, or user provided qualified projections select r1.x..)
4. To get old behavior for result set schema, introduced a config param ""hive.resultset.use.unique.column.names""; this is set to true by default. User will have to set this to false for old behavior.;;;","21/Mar/14 19:02;jpullokkaran;Vaibhav, I modified the test cases that seems like could get affected. If we are not using JDBC1 then its a no-op.;;;","21/Mar/14 21:11;ashutoshc;[~jpullokkaran] Can you also update RB with latest patch?;;;","21/Mar/14 21:18;leftyl;The documentation of *hive.resultset.use.unique.column.names* looks good to me.  When the time comes I'll add it to the wiki.;;;","21/Mar/14 21:32;jpullokkaran;Review Board: https://reviews.apache.org/r/19551/;;;","21/Mar/14 21:52;ashutoshc;+1 LGTM;;;","22/Mar/14 03:23;prasadm;+1
;;;","22/Mar/14 04:53;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12636089/HIVE-6687.3.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5437 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_print_header
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1898/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1898/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12636089;;;","22/Mar/14 06:11;ashutoshc;Seems like checked-in .q.out file is incorrect, what we are getting now seems more appropriate.;;;","24/Mar/14 04:45;jpullokkaran;Modified unit test ""print_header.q"" log file.;;;","24/Mar/14 05:28;ashutoshc;Committed to trunk & 0.13;;;","25/Mar/14 08:13;leftyl;Added the new configuration parameter to the wiki here:

* [Configuration Properties:  hive.resultset.use.unique.column.names |https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.resultset.use.unique.column.names]

After HIVE-6037 gets committed, this parameter will have to be added to HiveConf.java again -- see HIVE-6586:

* [comment for hive.resultset.use.unique.column.names |https://issues.apache.org/jira/browse/HIVE-6586?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13946274#comment-13946274];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
webhcat does not honour -Dlog4j.configuration=$WEBHCAT_LOG4J of log4j.properties file on local filesystem.,HIVE-6686,12701989,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,17/Mar/14 21:16,28/Mar/14 22:57,14/Jul/23 06:14,28/Mar/14 22:57,0.13.0,,,,,,,,,0.13.0,,WebHCat,,,,0,,,,,deepesh,ekoifman,gates,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/14 21:23;ekoifman;HIVE-6686.patch;https://issues.apache.org/jira/secure/attachment/12635168/HIVE-6686.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,380329,,,,Fri Mar 28 22:57:21 UTC 2014,,,,,,,,,,"0|i1tj3z:",380611,,,,,,,,,,,,,,,,,,,,,"17/Mar/14 21:23;ekoifman;no precommit test;;;","21/Mar/14 13:39;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635168/HIVE-6686.patch

{color:green}SUCCESS:{color} +1 5436 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1878/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1878/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635168;;;","25/Mar/14 21:30;gates;Have we confirmed that this change works both when Hive is installed as a tarball and when installed via bigtop rpms?;;;","25/Mar/14 21:56;ekoifman;[~deepesh] said that none of the locations this script attempted to use were used by rpm.  Deepesh, could you confirm?;;;","26/Mar/14 20:48;deepesh;That seems accurate to my knowledge.;;;","27/Mar/14 20:25;thejas;Does that mean that a different fix is required with bigtop rpm ?
;;;","27/Mar/14 20:35;ekoifman;no, it's exactly the same;;;","27/Mar/14 20:58;thejas;+1;;;","27/Mar/14 21:57;thejas;[~rhbutani] It would be great to have this bug fix in hive 0.13.
;;;","28/Mar/14 18:08;rhbutani;+1 for 0.13;;;","28/Mar/14 22:57;thejas;Patch committed to 0.13 branch and trunk.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Beeline throws ArrayIndexOutOfBoundsException for mismatched arguments,HIVE-6685,12701980,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,szehon,szehon,szehon,17/Mar/14 20:52,30/Mar/14 15:18,14/Jul/23 06:14,30/Mar/14 15:18,0.12.0,,,,,,,,,,,CLI,,,,0,,,"Noticed that there is an ugly ArrayIndexOutOfBoundsException for mismatched arguments in beeline prompt.  It would be nice to cleanup.

Example:
{noformat}
beeline -u szehon -p
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 3
	at org.apache.hive.beeline.BeeLine.initArgs(BeeLine.java:560)
	at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:628)
	at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:366)
	at org.apache.hive.beeline.BeeLine.main(BeeLine.java:349)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
{noformat}",,szehon,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/14 01:31;szehon;HIVE-6685.2.patch;https://issues.apache.org/jira/secure/attachment/12635452/HIVE-6685.2.patch","26/Mar/14 19:43;szehon;HIVE-6685.3.patch;https://issues.apache.org/jira/secure/attachment/12636986/HIVE-6685.3.patch","26/Mar/14 20:33;szehon;HIVE-6685.4.patch;https://issues.apache.org/jira/secure/attachment/12636996/HIVE-6685.4.patch","17/Mar/14 20:57;szehon;HIVE-6685.patch;https://issues.apache.org/jira/secure/attachment/12635162/HIVE-6685.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,380320,,,,Sun Mar 30 15:18:56 UTC 2014,,,,,,,,,,"0|i1tj1z:",380602,,,,,,,,,,,,,,,,,,,,,"17/Mar/14 20:57;szehon;Attaching a simple fix.  Review board is not responding for me, will try again later.;;;","17/Mar/14 21:40;szehon;Added review board.;;;","17/Mar/14 21:45;xuefuz;[~szehon] Thanks for working on this. I have some review comments on review board for your consideration.;;;","19/Mar/14 01:31;szehon;Thanks for the review and suggestion.  I refactored Beeline to use the GNU Parser, it is a much cleaner solution.;;;","21/Mar/14 11:44;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635452/HIVE-6685.2.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1877/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1877/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-1877/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
++ awk '{print $2}'
++ egrep -v '^X|^Performing status on external'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/storage-handlers/hbase/target hcatalog/server-extensions/target hcatalog/core/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target hwi/target common/target common/src/gen service/target contrib/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1579927.

At revision 1579927.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635452;;;","21/Mar/14 16:54;szehon;As this change became a refactoring, will need a rebase as the code has changed.  Will take a look a bit later.;;;","26/Mar/14 19:43;szehon;Rebasing and incorporating HIVE-6652 fix in the refactored code.  

[~xuefuz] can you take a look when you get the chance?  Thanks.;;;","26/Mar/14 20:04;xuefuz;The patch looks good for me. I left a minor comment for consideration.;;;","26/Mar/14 20:33;szehon;Good catch, addressing the comment.;;;","27/Mar/14 03:10;xuefuz;+1;;;","27/Mar/14 13:40;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12636996/HIVE-6685.4.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5499 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1980/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1980/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12636996;;;","30/Mar/14 15:18;xuefuz;Patch committed to trunk. Thanks to Szehon for the contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Beeline does not accept comments that are preceded by spaces,HIVE-6684,12701952,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,rem120,rem120,rem120,17/Mar/14 18:15,30/Apr/15 20:23,14/Jul/23 06:14,22/May/14 22:14,0.10.0,,,,,,,,,0.14.0,,CLI,,,,0,TODOC14,,"Beeline throws an error if single-line comments are indented with spaces. This works in the embedded Hive CLI.

For example:

SELECT
   -- this is the field we want
   field
FROM
   table;

Error: Error while processing statement: FAILED: ParseException line 1:71 cannot recognize input near '<EOF>' '<EOF>' '<EOF>' in select clause (state=42000,code=40000)",,leftyl,mdominguez@cloudera.com,rem120,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/May/14 21:48;rem120;HIVE-6684.1.patch;https://issues.apache.org/jira/secure/attachment/12643133/HIVE-6684.1.patch","21/May/14 16:42;rem120;HIVE-6684.2.patch;https://issues.apache.org/jira/secure/attachment/12646040/HIVE-6684.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,380292,,,,Thu Nov 13 19:41:08 UTC 2014,,,,,,,,,,"0|i1tivz:",380575,,,,,,,,,,,,,,,,,,,,,"17/Mar/14 18:19;rem120;The description doesn't render properly on my browser - the 2nd, 3rd and 5th lines of the example start with three spaces.;;;","02/May/14 21:48;rem120;Added patch with unit test - please review.;;;","03/May/14 02:52;xuefuz;Patch looks good. Minor nit though, is that in the line
{code}
+    return line.trim().startsWith(""#"") || line.trim().startsWith(""--"");
{code}
line.trim() may be called twice, which is unnecessary. Better if just to trim once.;;;","03/May/14 13:34;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12643133/HIVE-6684.1.patch

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 5431 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/111/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/111/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12643133;;;","03/May/14 22:20;leftyl;This bug seems like a good thing to document in the wiki for releases 0.12.0 through 0.13.1, especially considering how cryptic the error message is.  Does it also apply to release 0.11.0 without the -f option?  (In other words, can a query provided with the -e option have multiple lines?)

Currently the Beeline user doc doesn't even mention comments.  This bug & jira could be another note for the -f option here:

* [HiveServer2 Clients -- Beeline Command Options |https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-BeelineCommandOptions];;;","21/May/14 16:41;rem120;Good point Xuefu. Added patch that only trims once.;;;","21/May/14 17:22;xuefuz;+1

[~leftylev] Comments are applicable to script from -e, -f, jdbc, or beeline. The bug was probably there since day one.;;;","22/May/14 12:40;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646040/HIVE-6684.2.patch

{color:red}ERROR:{color} -1 due to 11 failed/errored test(s), 5527 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_nested_types
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/262/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/262/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-262/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 11 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646040;;;","22/May/14 17:05;xuefuz;[~rem120] Are those test failures above relevant?;;;","22/May/14 17:26;rem120;[~xuefuz] Those failures do not involve the beeline component, so I would say not.;;;","22/May/14 22:14;xuefuz;Patch committed to trunk. Thanks to Jeremy for the contribution.;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Beeline does not accept comments at end of line,HIVE-6683,12701950,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,spena,rem120,rem120,17/Mar/14 18:13,26/May/16 17:57,14/Jul/23 06:14,15/Nov/14 01:19,0.10.0,,,,,,,,,1.1.0,,CLI,,,,1,TODOC15,,"Beeline fails to read queries where lines have comments at the end. This works in the embedded Hive CLI.

Example:

SELECT
1 -- this is a comment about this value
FROM
table;

Error: Error while processing statement: FAILED: ParseException line 1:36 mismatched input '<EOF>' expecting FROM near '1' in from clause (state=42000,code=40000)",,brocknoland,ctang,jbeard,manickmuthu,rem120,spena,SudarshanS,szehon,Tauruzzz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/14 15:51;spena;HIVE-6683.1.patch;https://issues.apache.org/jira/secure/attachment/12671244/HIVE-6683.1.patch","23/Sep/14 19:07;spena;HIVE-6683.1.patch;https://issues.apache.org/jira/secure/attachment/12670778/HIVE-6683.1.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,380290,,,,Thu May 26 17:57:46 UTC 2016,,,,,,,,,,"0|i1tivj:",380573,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 21:12;nicothieb;I am also affected by this. It would be nice it it were fixed server side so clients do not have to implement comment parsing. It would also make reusing queries across beeline / hue / jdbc much easier;;;","23/Sep/14 17:21;spena;This is working and fixed on hive 0.13

{noformat}
0: jdbc:hive2://localhost:10000> SELECT *
. . . . . . . . . . . . . . . .> -- this is a comment about this value
. . . . . . . . . . . . . . . .> FROM
. . . . . . . . . . . . . . . .> persons;
+-------------+---------------+--+
| persons.id  | persons.name  |
+-------------+---------------+--+
| 1           | name1         |
+-------------+---------------+--+
{noformat};;;","23/Sep/14 17:35;jbeard;[~spena] That example is for a comment at the start of the line. Could you please confirm the example as listed in the JIRA description?;;;","23/Sep/14 18:17;spena;It does not work. 
It does not throw an error though, but the results are not correct.

The example is run in a table 'persons' with two rows only.

{noformat}
0: jdbc:hive2://localhost:10000> select 1 from persons;
+------+--+
| _c0  |
+------+--+
| 1    |
| 1    |
+------+--+

0: jdbc:hive2://localhost:10000> select  
. . . . . . . . . . . . . . . .> 1 -- comment
. . . . . . . . . . . . . . . .> from persons;
+------+--+
| _c0  |
+------+--+
| 1    |
+------+--+
{noformat}

Although there are some different errors when using the column names or the *.

{noformat}
0: jdbc:hive2://localhost:10000> select
. . . . . . . . . . . . . . . .> * -- comment
. . . . . . . . . . . . . . . .> from persons;
Error: Error while compiling statement: FAILED: SemanticException Line 0:-1 Invalid column reference 'TOK_ALLCOLREF' (state=42000,code=40000)

0: jdbc:hive2://localhost:10000> select
. . . . . . . . . . . . . . . .> id -- comment
. . . . . . . . . . . . . . . .> from persons;
Error: Error while compiling statement: FAILED: SemanticException [Error 10004]: Line 1:7 Invalid table alias or column reference 'id': (possible column names are: ) (state=42000,code=10004)
{noformat};;;","23/Sep/14 19:07;spena;Here's the patch that allows you to add comments at the end of the line.

All the following examples worked correctly:

{noformat}
select 
id --id 
from persons;

select 
id, 
name --name 
from persons;

select 
id --id
, name from persons;

select 
id --id
, name --name 
from persons;
{noformat};;;","24/Sep/14 02:34;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12670778/HIVE-6683.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6325 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_dynamic_partition_pruning
org.apache.hadoop.hive.ql.parse.TestParse.testParse_union
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/950/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/950/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-950/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12670778;;;","25/Sep/14 15:51;spena;Re-submit patch to run tests non-related with this fix.;;;","26/Sep/14 10:24;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12671244/HIVE-6683.1.patch

{color:green}SUCCESS:{color} +1 6353 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/992/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/992/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-992/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12671244;;;","14/Nov/14 20:43;brocknoland;+1;;;","15/Nov/14 01:19;brocknoland;Thank you Sergio! I have committed this to trunk!;;;","15/Nov/14 01:41;szehon;Need to add to this page for Hive 0.15.

[https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients|https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients];;;","25/May/16 21:05;Tauruzzz;Sergio, I believe the following still fails in beeline

select 
id
from persons; -- a comment after semicolon;;;","26/May/16 16:05;spena;Thanks [~Tauruzzz]
Could you create a JIRA, and include necessary information to reproduce and versions?;;;","26/May/16 17:57;manickmuthu;I have created a JIRA for the issue that Sergey reported.

https://issues.apache.org/jira/browse/HIVE-13864
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nonstaged mapjoin table memory check may be broken,HIVE-6682,12701949,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sershe,sershe,sershe,17/Mar/14 18:12,24/Mar/14 23:25,14/Jul/23 06:14,24/Mar/14 23:25,0.13.0,,,,,,,,,0.13.0,,,,,,0,,,"We are getting the below error from task while the staged load works correctly. 
We don't set the memory threshold so low so it seems the settings are just not handled correctly. This seems to always trigger on the first check. Given that map task might have bunch more stuff, not just the hashmap, we may also need to adjust the memory check (e.g. have separate configs).

{noformat}
Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionException: 2014-03-14 08:11:21	Processing rows:	200000	Hashtable size:	199999	Memory usage:	204001888	percentage:	0.197
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:195)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionException: 2014-03-14 08:11:21	Processing rows:	200000	Hashtable size:	199999	Memory usage:	204001888	percentage:	0.197
	at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.load(HashTableLoader.java:104)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:150)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp(MapJoinOperator.java:165)
	at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1026)
	at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1030)
	at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1030)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:489)
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)
	... 8 more
Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionException: 2014-03-14 08:11:21	Processing rows:	200000	Hashtable size:	199999	Memory usage:	204001888	percentage:	0.197
	at org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionHandler.checkMemoryStatus(MapJoinMemoryExhaustionHandler.java:91)
	at org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.processOp(HashTableSinkOperator.java:248)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:791)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)
	at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.startForward(MapredLocalTask.java:375)
	at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.startForward(MapredLocalTask.java:346)
	at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.loadDirectly(HashTableLoader.java:147)
	at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.load(HashTableLoader.java:82)
	... 15 more
{noformat}",,navis,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6144,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/14 17:00;sershe;HIVE-6682.01.patch;https://issues.apache.org/jira/secure/attachment/12635575/HIVE-6682.01.patch","20/Mar/14 01:27;sershe;HIVE-6682.02.patch;https://issues.apache.org/jira/secure/attachment/12635706/HIVE-6682.02.patch","21/Mar/14 22:47;sershe;HIVE-6682.03.patch;https://issues.apache.org/jira/secure/attachment/12636133/HIVE-6682.03.patch","17/Mar/14 20:45;sershe;HIVE-6682.patch;https://issues.apache.org/jira/secure/attachment/12635160/HIVE-6682.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,380289,,,,Mon Mar 24 23:25:47 UTC 2014,,,,,,,,,,"0|i1tivb:",380572,,,,,,,,,,,,,,,,,,,,,"17/Mar/14 18:18;sershe;I suspect that hashtableMemoryUsage is desc is 0;;;","17/Mar/14 18:19;sershe;*in;;;","17/Mar/14 20:45;sershe;Fix. Attached q file fails w/o fix for me but passes with fix;;;","17/Mar/14 20:45;sershe;[~navis] [~hagleitn] do you guys mind taking a look?;;;","18/Mar/14 01:48;navis;Ah, I've missed that. Thanks for finding this. Could make a review board entry?

On TemporaryHashSinkOperator, If ""hashtableMemoryUsage"" is already added to MapJoinDesc, would it be enough to just copying it in HashTableSinkDesc(MapJoinDesc clone)?;;;","18/Mar/14 18:21;sershe;https://reviews.apache.org/r/19363/ although the patch is really small, it's just the q file and result.
What do you mean by the question? That is what is done, right? I added config to make sure it's set, because if it's not the job is going to fail on any real data;;;","19/Mar/14 17:00;sershe;RB feedback;;;","20/Mar/14 01:40;navis;+1, let's see the test results.;;;","20/Mar/14 16:58;sershe;hmm, is HiveQA broken?;;;","21/Mar/14 09:47;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635706/HIVE-6682.02.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5431 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_truncate_column_buckets
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1876/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1876/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635706;;;","21/Mar/14 22:46;sershe;2 tests are unrelated. For one, broken golden file was committed with original JIRA. Let me update. I hope +1 still stands;;;","22/Mar/14 14:11;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12636133/HIVE-6682.03.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5440 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1903/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1903/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12636133;;;","24/Mar/14 23:25;sershe;in trunk and 13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Describe table sometimes shows ""from deserializer"" for column comments",HIVE-6681,12701769,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,16/Mar/14 19:55,21/Apr/15 22:33,14/Jul/23 06:14,21/Mar/14 04:24,0.11.0,0.12.0,,,,,,,,0.13.0,,Metastore,Serializers/Deserializers,,,0,,,,,cwlaird3,decster,hagleitn,leftyl,qwertymaniac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6771,HIVE-10428,,,,,,,,,,,,,,,,,,,,"17/Mar/14 01:39;ashutoshc;HIVE-6681.2.patch;https://issues.apache.org/jira/secure/attachment/12635023/HIVE-6681.2.patch","17/Mar/14 07:13;ashutoshc;HIVE-6681.3.patch;https://issues.apache.org/jira/secure/attachment/12635040/HIVE-6681.3.patch","17/Mar/14 18:20;ashutoshc;HIVE-6681.4.patch;https://issues.apache.org/jira/secure/attachment/12635135/HIVE-6681.4.patch","17/Mar/14 21:54;ashutoshc;HIVE-6681.5.patch;https://issues.apache.org/jira/secure/attachment/12635175/HIVE-6681.5.patch","18/Mar/14 05:43;ashutoshc;HIVE-6681.6.patch;https://issues.apache.org/jira/secure/attachment/12635244/HIVE-6681.6.patch","19/Mar/14 17:35;ashutoshc;HIVE-6681.7.patch;https://issues.apache.org/jira/secure/attachment/12635590/HIVE-6681.7.patch","19/Mar/14 18:24;ashutoshc;HIVE-6681.8.patch;https://issues.apache.org/jira/secure/attachment/12635608/HIVE-6681.8.patch","16/Mar/14 20:03;ashutoshc;HIVE-6681.patch;https://issues.apache.org/jira/secure/attachment/12635004/HIVE-6681.patch",,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,380114,,,,Tue Sep 30 15:25:05 UTC 2014,,,,,,,,,,"0|i1thsn:",380398,,,,,,,,,,,,,,,,,,,,,"16/Mar/14 20:03;ashutoshc;Initial patch to get Hive QA run.;;;","17/Mar/14 16:30;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635040/HIVE-6681.3.patch

{color:red}ERROR:{color} -1 due to 180 failed/errored test(s), 5397 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_groupby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_select
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_reordering_values
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binary_output_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin_negative
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin_negative2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin_negative3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_columnstats_partlvl
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_columnstats_tbllvl
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_combine2_hadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_like_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynamic_partition_skip_default
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_map_ppr_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_ppr_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input42
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32_lessSize
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join35
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_filters_overlap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_louter_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadataonly1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatCTAS
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_outer_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pcr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_vc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_allchildsarenull
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_push_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_regexp_extract
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_router_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_serde_user_properties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_reflect2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_serde
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_fileformat_base64
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats2
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats3
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats_empty_partition
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket4
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket5
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin7
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_disable_merge_for_bucketing
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_list_bucket_dml_10
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_reduce_deduplicate
org.apache.hadoop.hive.jdbc.TestJdbcDriver.testMetaDataGetColumns
org.apache.hadoop.hive.ql.parse.TestParse.testParse_case_sensitivity
org.apache.hadoop.hive.ql.parse.TestParse.testParse_cast1
org.apache.hadoop.hive.ql.parse.TestParse.testParse_groupby1
org.apache.hadoop.hive.ql.parse.TestParse.testParse_groupby2
org.apache.hadoop.hive.ql.parse.TestParse.testParse_groupby3
org.apache.hadoop.hive.ql.parse.TestParse.testParse_groupby4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_groupby5
org.apache.hadoop.hive.ql.parse.TestParse.testParse_groupby6
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input1
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input2
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input20
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input3
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input5
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input6
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input7
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input8
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input9
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input_part1
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input_testsequencefile
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input_testxpath
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input_testxpath2
org.apache.hadoop.hive.ql.parse.TestParse.testParse_join1
org.apache.hadoop.hive.ql.parse.TestParse.testParse_join2
org.apache.hadoop.hive.ql.parse.TestParse.testParse_join3
org.apache.hadoop.hive.ql.parse.TestParse.testParse_join4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_join5
org.apache.hadoop.hive.ql.parse.TestParse.testParse_join6
org.apache.hadoop.hive.ql.parse.TestParse.testParse_join7
org.apache.hadoop.hive.ql.parse.TestParse.testParse_join8
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample1
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample2
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample3
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample5
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample6
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample7
org.apache.hadoop.hive.ql.parse.TestParse.testParse_subq
org.apache.hadoop.hive.ql.parse.TestParse.testParse_udf1
org.apache.hadoop.hive.ql.parse.TestParse.testParse_udf4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_udf6
org.apache.hadoop.hive.ql.parse.TestParse.testParse_udf_case
org.apache.hadoop.hive.ql.parse.TestParse.testParse_udf_when
org.apache.hadoop.hive.ql.parse.TestParse.testParse_union
org.apache.hive.jdbc.TestJdbcDriver2.testMetaDataGetColumns
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1859/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1859/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 180 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635040;;;","17/Mar/14 20:03;ashutoshc;https://reviews.apache.org/r/19319/;;;","18/Mar/14 02:49;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635175/HIVE-6681.5.patch

{color:red}ERROR:{color} -1 due to 89 failed/errored test(s), 5403 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binary_output_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin_negative
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin_negative2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_columnstats_partlvl
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_columnstats_tbllvl
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_map_ppr_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_ppr_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input42
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32_lessSize
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join35
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_filters_overlap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_louter_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadataonly1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatCTAS
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_analyze
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_outer_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pcr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_vc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_allchildsarenull
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_push_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_regexp_extract
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_router_join_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_serde_user_properties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket4
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket5
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_disable_merge_for_bucketing
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_list_bucket_dml_10
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_parallel_orderby
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_reduce_deduplicate
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1865/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1865/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 89 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635175;;;","20/Mar/14 15:27;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635608/HIVE-6681.8.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5416 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_part
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1874/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1874/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635608;;;","21/Mar/14 02:40;hagleitn;LGTM +1;;;","21/Mar/14 04:24;ashutoshc;Committed to trunk & 0.13;;;","21/Mar/14 07:05;leftyl;This adds *hive.serdes.using.metastore.for.schema* to HiveConf.java, but it needs a description.  (Perhaps it's self-evident.)  How about a release note?;;;","30/Mar/14 08:04;leftyl;A gentle doc nudge for [~ashutoshc].;;;","10/Apr/14 23:30;ashutoshc;I think this config is too specific for a particular scenario. Its useful only for hive-devs. I dont think its useful to be exposed to end users, so I will avoid documenting it. ;;;","10/Apr/14 23:39;leftyl;Fair enough.  Thanks.;;;","12/Apr/14 04:27;leftyl;HIVE-6887 added this description for *hive.serdes.using.metastore.for.schema*:  ""This an internal parameter. Check with the hive dev. team"" (see [Ashutosh's comment|https://issues.apache.org/jira/browse/HIVE-6887?focusedCommentId=13966036&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13966036]) but I won't add it to Configuration Properties in the wiki.;;;","30/Sep/14 15:25;cwlaird3;I believe this fix has caused another issue: https://issues.apache.org/jira/browse/HIVE-8307

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Decimal128#update(Decimal128 o, short scale) should adjust the unscaled value.",HIVE-6680,12701662,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jnp,jnp,jnp,15/Mar/14 09:10,17/Mar/14 22:15,14/Jul/23 06:14,17/Mar/14 22:15,,,,,,,,,,0.13.0,,,,,,0,,,"Decimal128#update(Decimal128 o, short scale) should adjust the unscaled value.",,jnp,rusanu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/14 21:43;jnp;HIVE-6680.1.patch;https://issues.apache.org/jira/secure/attachment/12634939/HIVE-6680.1.patch","15/Mar/14 09:31;jnp;HIVE-6680.1.patch;https://issues.apache.org/jira/secure/attachment/12634906/HIVE-6680.1.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,380008,,,,Mon Mar 17 21:28:26 UTC 2014,,,,,,,,,,"0|i1th5b:",380292,,,,,,,,,,,,,,,,,,,,,"15/Mar/14 09:31;jnp;Attached patch fixes the issue.;;;","15/Mar/14 14:19;rusanu;+1;;;","15/Mar/14 17:32;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634906/HIVE-6680.1.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1835/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1835/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.585s]
[INFO] Hive Ant Utilities ................................ SUCCESS [5.340s]
[INFO] Hive Shims Common ................................. SUCCESS [3.698s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.469s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.373s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.644s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [8.083s]
[INFO] Hive Shims ........................................ SUCCESS [1.302s]
[INFO] Hive Common ....................................... SUCCESS [6.758s]
[INFO] Hive Serde ........................................ SUCCESS [10.735s]
[INFO] Hive Metastore .................................... SUCCESS [32.370s]
[INFO] Hive Query Language ............................... SUCCESS [1:17.707s]
[INFO] Hive Service ...................................... SUCCESS [8.422s]
[INFO] Hive JDBC ......................................... SUCCESS [2.987s]
[INFO] Hive Beeline ...................................... SUCCESS [2.791s]
[INFO] Hive CLI .......................................... SUCCESS [1.907s]
[INFO] Hive Contrib ...................................... SUCCESS [2.379s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.725s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.502s]
[INFO] Hive HCatalog Core ................................ SUCCESS [1.891s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.392s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.258s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [2.197s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [9.940s]
[INFO] Hive HWI .......................................... SUCCESS [1.138s]
[INFO] Hive ODBC ......................................... SUCCESS [0.876s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.201s]
[INFO] Hive TestUtils .................................... SUCCESS [0.658s]
[INFO] Hive Packaging .................................... FAILURE [1.747s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:32.626s
[INFO] Finished at: Sat Mar 15 13:32:01 EDT 2014
[INFO] Final Memory: 74M/422M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634906;;;","15/Mar/14 22:03;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634939/HIVE-6680.1.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1844/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1844/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.907s]
[INFO] Hive Ant Utilities ................................ SUCCESS [6.009s]
[INFO] Hive Shims Common ................................. SUCCESS [3.679s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.466s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.233s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [3.037s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [7.596s]
[INFO] Hive Shims ........................................ SUCCESS [0.824s]
[INFO] Hive Common ....................................... SUCCESS [6.801s]
[INFO] Hive Serde ........................................ SUCCESS [9.826s]
[INFO] Hive Metastore .................................... SUCCESS [32.381s]
[INFO] Hive Query Language ............................... SUCCESS [1:21.168s]
[INFO] Hive Service ...................................... SUCCESS [7.410s]
[INFO] Hive JDBC ......................................... SUCCESS [3.068s]
[INFO] Hive Beeline ...................................... SUCCESS [2.708s]
[INFO] Hive CLI .......................................... SUCCESS [1.791s]
[INFO] Hive Contrib ...................................... SUCCESS [2.449s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.687s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.536s]
[INFO] Hive HCatalog Core ................................ SUCCESS [2.828s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.481s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.257s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [2.110s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [9.975s]
[INFO] Hive HWI .......................................... SUCCESS [1.212s]
[INFO] Hive ODBC ......................................... SUCCESS [0.805s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.194s]
[INFO] Hive TestUtils .................................... SUCCESS [0.651s]
[INFO] Hive Packaging .................................... FAILURE [1.184s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:34.922s
[INFO] Finished at: Sat Mar 15 18:03:49 EDT 2014
[INFO] Final Memory: 74M/553M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634939;;;","17/Mar/14 18:49;jnp;Ran tests locally. Only failures were show_create_table_serde.q and metadata_only_queries_with_filters.q which are unrelated to this patch.;;;","17/Mar/14 21:28;jnp;Committed to trunk. It is a correctness bug affecting hive-0.13, therefore I will port it to hive-13 branch as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hcat cli fails to run when running with hive on tez,HIVE-6676,12701609,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,ekoifman,ekoifman,ekoifman,14/Mar/14 22:03,28/Mar/14 23:24,14/Jul/23 06:14,28/Mar/14 23:24,0.13.0,,,,,,,,,0.13.0,,HCatalog,,,,0,,,HIVE_CLASSPATH should be added to HADOOP_CLASSPATH before launching hcat CLI,,ekoifman,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6587,,,,,,,,,,,,,,,,,,,,,"15/Mar/14 00:40;ekoifman;HIVE-6676.patch;https://issues.apache.org/jira/secure/attachment/12634877/HIVE-6676.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379955,,,,Fri Mar 28 23:24:07 UTC 2014,,,,,,,,,,"0|i1tgtz:",380240,,,,,,,,,,,,,,,,,,,,,"15/Mar/14 00:40;ekoifman;no pre commit tests;;;","15/Mar/14 10:34;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634877/HIVE-6676.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1824/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1824/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.292s]
[INFO] Hive Ant Utilities ................................ SUCCESS [5.863s]
[INFO] Hive Shims Common ................................. SUCCESS [3.714s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.477s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.380s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [3.030s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [7.876s]
[INFO] Hive Shims ........................................ SUCCESS [1.009s]
[INFO] Hive Common ....................................... SUCCESS [6.668s]
[INFO] Hive Serde ........................................ SUCCESS [10.735s]
[INFO] Hive Metastore .................................... SUCCESS [34.216s]
[INFO] Hive Query Language ............................... SUCCESS [1:24.645s]
[INFO] Hive Service ...................................... SUCCESS [7.790s]
[INFO] Hive JDBC ......................................... SUCCESS [3.187s]
[INFO] Hive Beeline ...................................... SUCCESS [2.733s]
[INFO] Hive CLI .......................................... SUCCESS [1.830s]
[INFO] Hive Contrib ...................................... SUCCESS [2.456s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.601s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.577s]
[INFO] Hive HCatalog Core ................................ SUCCESS [1.882s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.387s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.267s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [2.051s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [10.277s]
[INFO] Hive HWI .......................................... SUCCESS [1.311s]
[INFO] Hive ODBC ......................................... SUCCESS [0.721s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.140s]
[INFO] Hive TestUtils .................................... SUCCESS [0.621s]
[INFO] Hive Packaging .................................... FAILURE [1.751s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:41.215s
[INFO] Finished at: Sat Mar 15 06:34:55 EDT 2014
[INFO] Final Memory: 75M/601M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634877;;;","27/Mar/14 20:17;thejas;+1;;;","28/Mar/14 22:43;rhbutani;+1 for 0.13;;;","28/Mar/14 23:24;thejas;Patch committed to 0.13 branch and trunk.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""show grant on all"" throws NPE",HIVE-6674,12701594,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,navis,thejas,thejas,14/Mar/14 21:12,23/Oct/15 09:35,14/Jul/23 06:14,27/Mar/14 00:13,,,,,,,,,,0.13.0,,,,,,0,,,"""show grant on all"" is supposed to show all the grants in the system on all the objects. But it fails with NPE with both SQL standard auth, and legacy auth.
{code}
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalAllDBGrant(ObjectStore.java:4206)
        at org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalDBGrantsAll(ObjectStore.java:4169)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)
        at com.sun.proxy.$Proxy6.listPrincipalDBGrantsAll(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.list_db_privileges(HiveMetaStore.java:4295)
        ... 36 more
{code}",,gates,navis,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6673,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/14 10:47;navis;HIVE-6674.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12634911/HIVE-6674.1.patch.txt","25/Mar/14 02:05;navis;HIVE-6674.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12636499/HIVE-6674.2.patch.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379940,,,,Thu Mar 27 00:13:05 UTC 2014,,,,,,,,,,"0|i1tgqn:",380225,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 21:14;thejas;I am not going to be working on this soon, so anyone can feel free to take this on.
;;;","15/Mar/14 10:47;navis;I'll take this.;;;","15/Mar/14 11:05;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634911/HIVE-6674.1.patch.txt

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1832/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1832/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.640s]
[INFO] Hive Ant Utilities ................................ SUCCESS [5.887s]
[INFO] Hive Shims Common ................................. SUCCESS [3.628s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.420s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.354s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.601s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [8.012s]
[INFO] Hive Shims ........................................ SUCCESS [1.291s]
[INFO] Hive Common ....................................... SUCCESS [6.829s]
[INFO] Hive Serde ........................................ SUCCESS [10.910s]
[INFO] Hive Metastore .................................... SUCCESS [32.234s]
[INFO] Hive Query Language ............................... SUCCESS [1:16.853s]
[INFO] Hive Service ...................................... SUCCESS [7.446s]
[INFO] Hive JDBC ......................................... SUCCESS [2.858s]
[INFO] Hive Beeline ...................................... SUCCESS [2.751s]
[INFO] Hive CLI .......................................... SUCCESS [1.964s]
[INFO] Hive Contrib ...................................... SUCCESS [2.611s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.566s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.603s]
[INFO] Hive HCatalog Core ................................ SUCCESS [1.952s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.412s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.799s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [1.767s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [10.180s]
[INFO] Hive HWI .......................................... SUCCESS [1.189s]
[INFO] Hive ODBC ......................................... SUCCESS [0.816s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.208s]
[INFO] Hive TestUtils .................................... SUCCESS [0.589s]
[INFO] Hive Packaging .................................... FAILURE [1.656s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:31.572s
[INFO] Finished at: Sat Mar 15 07:05:54 EDT 2014
[INFO] Final Memory: 74M/424M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634911;;;","24/Mar/14 17:49;ashutoshc;[~navis] Looks like this one needs rebase.;;;","25/Mar/14 02:15;ashutoshc;+1 LGTM;;;","26/Mar/14 23:48;gates;Ran the tests, all looks good.;;;","27/Mar/14 00:13;ashutoshc;Committed to 0.13 & trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JARs loaded by permanent functions don't work properly with HiveServer2,HIVE-6672,12701566,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,14/Mar/14 19:31,28/Apr/15 17:31,14/Jul/23 06:14,23/Mar/14 17:01,,,,,,,,,,0.13.0,,HiveServer2,UDF,,,0,,,"Permanent functions which specify JAR files to load do not seem to work properly with HiveServer2. While the function can be created and see with SHOW FUNCTIONS, doing DESCRIBE FUNCTION or using the UDF in a query results in java.lang.ClassNotFoundException.

It looks like the JAR is only added to the classloader of the very first thread which references the UDF.  Subsequent threads will have a different class loader which may not have loaded the JAR, but because the UDF is already in the FunctionRegistry these threads do not attempt to load the resources for the UDF.
",,jdere,qwertymaniac,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-10453,,,HIVE-9095,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 00:07;jdere;HIVE-6672.1.patch;https://issues.apache.org/jira/secure/attachment/12635692/HIVE-6672.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379912,,,,Sun Mar 23 17:01:48 UTC 2014,,,,,,,,,,"0|i1tgkf:",380197,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 00:07;jdere;Patch v1 which addresses this issue.  When lookup up the FunctionInfo for a permanent UDF from the FunctionRegistry, check if the classloader can resolve the UDF class, and if that fails then load the JARs necessary for that permanent UDF.;;;","20/Mar/14 00:14;jdere;RB at https://reviews.apache.org/r/19435/;;;","20/Mar/14 00:16;jdere;[~rhbutani] Is this a bug that would be able to get into Hive-0.13?;;;","20/Mar/14 20:45;ashutoshc;Patch looks good. I wonder if we can write a test case for this, that will be good.;;;","22/Mar/14 22:55;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635692/HIVE-6672.1.patch

{color:green}SUCCESS:{color} +1 5440 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1912/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1912/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635692;;;","23/Mar/14 17:01;ashutoshc;Committed to trunk & 0.13. Thanks, Jason!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WebHCat Job Submission API 'enablelog' parameter is only supported with Hadoop 1,HIVE-6671,12701560,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,14/Mar/14 19:13,26/Mar/14 19:10,14/Jul/23 06:14,25/Mar/14 21:48,0.12.0,,,,,,,,,0.13.0,,WebHCat,,,,0,,,We should throw a consistent exception if enablelog=true and WebHCat is talking to H2,,ekoifman,gates,leftyl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-4531,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 19:51;ekoifman;HIVE-6671.patch;https://issues.apache.org/jira/secure/attachment/12634811/HIVE-6671.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379906,,,,Wed Mar 26 19:10:30 UTC 2014,,,,,,,,,,"0|i1tgj3:",380191,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 19:51;ekoifman;no pre commit tests;;;","15/Mar/14 10:24;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634811/HIVE-6671.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1813/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1813/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [9.824s]
[INFO] Hive Ant Utilities ................................ SUCCESS [12.808s]
[INFO] Hive Shims Common ................................. SUCCESS [4.310s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [3.196s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.255s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.648s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [10.116s]
[INFO] Hive Shims ........................................ SUCCESS [0.783s]
[INFO] Hive Common ....................................... SUCCESS [11.327s]
[INFO] Hive Serde ........................................ SUCCESS [9.990s]
[INFO] Hive Metastore .................................... SUCCESS [32.520s]
[INFO] Hive Query Language ............................... SUCCESS [1:28.884s]
[INFO] Hive Service ...................................... SUCCESS [8.738s]
[INFO] Hive JDBC ......................................... SUCCESS [3.005s]
[INFO] Hive Beeline ...................................... SUCCESS [3.161s]
[INFO] Hive CLI .......................................... SUCCESS [2.718s]
[INFO] Hive Contrib ...................................... SUCCESS [2.806s]
[INFO] Hive HBase Handler ................................ SUCCESS [4.920s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.412s]
[INFO] Hive HCatalog Core ................................ SUCCESS [2.213s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.549s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.579s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [1.356s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [12.001s]
[INFO] Hive HWI .......................................... SUCCESS [1.256s]
[INFO] Hive ODBC ......................................... SUCCESS [0.681s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.208s]
[INFO] Hive TestUtils .................................... SUCCESS [0.377s]
[INFO] Hive Packaging .................................... FAILURE [0.735s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 4:04.168s
[INFO] Finished at: Sat Mar 15 06:24:00 EDT 2014
[INFO] Final Memory: 74M/515M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634811;;;","25/Mar/14 21:48;gates;Patch checked into trunk and 0.13;;;","26/Mar/14 05:55;leftyl;This could be documented in the wiki here:

* [MapReduce Job Parameters |https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference+MapReduceJar#WebHCatReferenceMapReduceJar-Parameters]
* [MapReduce Streaming Job Parameters |https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference+MapReduceStream#WebHCatReferenceMapReduceStream-Parameters]
* [Hive Job Parameters  |https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference+Hive#WebHCatReferenceHive-Parameters]
* [Pig Job Parameters |https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference+Pig#WebHCatReferencePig-Parameters]

And perhaps here too:

* [Using WebHCat:  Log Files |https://cwiki.apache.org/confluence/display/Hive/WebHCat+UsingWebHCat#WebHCatUsingWebHCat-LogFiles];;;","26/Mar/14 17:26;ekoifman;this probably does not need be documented at all.  Long term this should be supported on hadoop 2.  Short term the error produced will make it clear to the user it's not supported.;;;","26/Mar/14 19:10;leftyl;Yay, less work to do.  :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassNotFound with Serde,HIVE-6670,12701530,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashahab,ashahab,ashahab,14/Mar/14 17:37,27/Mar/14 22:38,14/Jul/23 06:14,27/Mar/14 22:38,0.12.0,,,,,,,,,0.13.0,,,,,,0,,,"We are finding a ClassNotFound exception when we use CSVSerde(https://github.com/ogrodnek/csv-serde) to create a table.
This is happening because MapredLocalTask does not pass the local added jars to ExecDriver when that is launched.
ExecDriver's classpath does not include the added jars. Therefore, when the plan is deserialized, it throws a ClassNotFoundException in the deserialization code, and results in a TableDesc object with a Null DeserializerClass.
This results in an NPE during Fetch.
Steps to reproduce:
wget https://drone.io/github.com/ogrodnek/csv-serde/files/target/csv-serde-1.1.2-0.11.0-all.jar into somewhere local eg. /home/soam/HiveSerdeIssue/csv-serde-1.1.2-0.11.0-all.jar.
Place some sample SCV files in HDFS as follows:
hdfs dfs -mkdir /user/soam/HiveSerdeIssue/sampleCSV/
hdfs dfs -put /home/soam/sampleCSV.csv /user/soam/HiveSerdeIssue/sampleCSV/
hdfs dfs -mkdir /user/soam/HiveSerdeIssue/sampleJoinTarget/
hdfs dfs -put /home/soam/sampleJoinTarget.csv /user/soam/HiveSerdeIssue/sampleJoinTarget/
====
create the tables in hive:
ADD JAR /home/soam/HiveSerdeIssue/csv-serde-1.1.2-0.11.0-all.jar;
create external table sampleCSV (md5hash string, filepath string)
row format serde 'com.bizo.hive.serde.csv.CSVSerde'
stored as textfile
location '/user/soam/HiveSerdeIssue/sampleCSV/'
;
create external table sampleJoinTarget (md5hash string, filepath string, datestamp string, nblines string, nberrors string)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ',' 
LINES TERMINATED BY '\n'
STORED AS TEXTFILE
LOCATION '/user/soam/HiveSerdeIssue/sampleJoinTarget/'
;
===============
Now, try the following JOIN:
ADD JAR /home/soam/HiveSerdeIssue/csv-serde-1.1.2-0.11.0-all.jar;
SELECT 
sampleCSV.md5hash, 
sampleCSV.filepath 
FROM sampleCSV
JOIN sampleJoinTarget
ON (sampleCSV.md5hash = sampleJoinTarget.md5hash) 
;
—
This will fail with the error:
Execution log at: /tmp/soam/.log
java.lang.ClassNotFoundException: com/bizo/hive/serde/csv/CSVSerde
Continuing ...
2014-03-11 10:35:03 Starting to launch local task to process map join; maximum memory = 238551040
Execution failed with exit status: 2
Obtaining error information
Task failed!
Task ID:
Stage-4
Logs:
/var/log/hive/soam/hive.log
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
Try the following LEFT JOIN. This will work:
SELECT 
sampleCSV.md5hash, 
sampleCSV.filepath 
FROM sampleCSV
LEFT JOIN sampleJoinTarget
ON (sampleCSV.md5hash = sampleJoinTarget.md5hash) 
;
==",,ashahab,gates,jdere,lukas.nalezenec,mdominguez@cloudera.com,qwertymaniac,rstokes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/14 19:25;ashahab;HIVE-6670-branch-0.12.patch;https://issues.apache.org/jira/secure/attachment/12636759/HIVE-6670-branch-0.12.patch","26/Mar/14 23:05;ashutoshc;HIVE-6670.1.patch;https://issues.apache.org/jira/secure/attachment/12637036/HIVE-6670.1.patch","25/Mar/14 19:25;ashahab;HIVE-6670.patch;https://issues.apache.org/jira/secure/attachment/12636760/HIVE-6670.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379876,,,,Thu Mar 27 22:38:53 UTC 2014,,,,,,,,,,"0|i1tgcf:",380161,,,,,,,,,,,,,,,,,,,,,"25/Mar/14 19:26;ashahab;Added patches for both trunk and branch.;;;","25/Mar/14 19:39;ashutoshc;[~ashahab] Can you add a testcase with your patch? You can use JsonSerDe (in hcatalog jar) to repro this issue in .q file
Also, if you can create ReviewBoard entry for this, that will be great.;;;","25/Mar/14 20:03;ashahab;[~hashutosh] I can write a test case. Is there a similar testcase that I can look at?
I'm not sure how to create a ReviewBoard entry. It'd be great if you can do that once I upload the test.
;;;","25/Mar/14 22:37;ashutoshc;Query you posted in description is a good testcase. Just add it in as .q file in ql/src/test/queries/clientpositive/ where all other test queries are. More info at [wiki site | https://cwiki.apache.org/confluence/display/Hive/HiveDeveloperFAQ#HiveDeveloperFAQ] You can create review request on [review board | https://reviews.apache.org/r/new/] ;;;","25/Mar/14 23:58;ashutoshc;I tested manually and I am able to repro. Also, with patch it succeeds. Thats, good. 
However, I think instead of passing on cmd line, better to pass it via Conf object using {{hive.added.jars.path}} variable , the way its done in MapRedTask. That way its consistent across two types of task.;;;","26/Mar/14 01:21;ashahab;But I don't want to overwrite existing added jars.

;;;","26/Mar/14 01:27;ashutoshc;You need not to. You can append.;;;","26/Mar/14 14:47;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12636760/HIVE-6670.patch

{color:green}SUCCESS:{color} +1 5457 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1964/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1964/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12636760;;;","26/Mar/14 23:05;ashutoshc;Patch with my suggested fix and test case.;;;","26/Mar/14 23:09;ashahab;Thanks for rolling it forward!

;;;","26/Mar/14 23:09;ashutoshc;https://reviews.apache.org/r/19705/;;;","26/Mar/14 23:53;jdere;+1;;;","27/Mar/14 16:41;gates;Ran tests locally, all looks good.;;;","27/Mar/14 21:37;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637036/HIVE-6670.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5492 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1987/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1987/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637036;;;","27/Mar/14 22:38;ashutoshc;Committed to trunk & 0.13. Thanks, Abin!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sourcing txn-script from schema script results in failure for mysql & oracle,HIVE-6669,12701528,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,gates,prasadm,prasadm,14/Mar/14 17:31,01/Oct/19 22:07,14/Jul/23 06:14,28/Oct/14 17:02,0.14.0,,,,,,,,,0.14.0,,Metastore,,,,0,,,"This issues is addressed in 0.13 by in-lining the the transaction schema statements in the schema initialization script (HIVE-6559)
The 0.14 schema initialization is not fixed. This is the followup ticket for to address the problem in 0.14. ",,brocknoland,damien.carol,gates,hagleitn,prasadm,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6559,HIVE-8668,,,,,,,,,,,,,,,,,,,,"08/Oct/14 23:37;gates;HIVE-6669.2.patch;https://issues.apache.org/jira/secure/attachment/12673760/HIVE-6669.2.patch","03/Oct/14 22:50;gates;HIVE-6669.patch;https://issues.apache.org/jira/secure/attachment/12672859/HIVE-6669.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379874,,,,Thu Nov 13 19:44:30 UTC 2014,,,,,,,,,,"0|i1tgbz:",380159,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 17:33;prasadm;[~ashutoshc] and [~gates], this is the followup ticket to track the oracle and mysql schema initialization problem for trunk (0.14). Thanks!;;;","03/Oct/14 22:50;gates;This patch adds hive-txn-schema-0.14 scripts, which are all identical to hive-txn-schema-0.13 scripts, but they are added for completeness.  The transaction tables are also added to the hive-schema-0.14 scripts.

[~damien.carol], please review this patch as I changed the transaction tables for postgres to lower case so that they would work without requiring quotes in TxnHandler/CompactionTxnHandler.  I can't reproduce your errors (my version of postgres doesn't seem to care about upper/lower case), so I wanted to have you check it before I commit this.

;;;","04/Oct/14 08:57;damien.carol;[~gates] I will test it on Monday 2014-10-13 but at first sight LGTM.;;;","07/Oct/14 16:01;gates;NO PRECOMMIT TESTS;;;","07/Oct/14 16:20;damien.carol;[~gates] Could you add quotes to identifiers in the postgres scripts. This way, the lowercase identifiers are explicit?;;;","07/Oct/14 16:25;gates;So what is now:
{code}
CREATE TABLE txns (
  txn_id bigint PRIMARY KEY,
  txn_state char(1) NOT NULL,
  txn_started bigint NOT NULL,
  txn_last_heartbeat bigint NOT NULL,
  txn_user varchar(128) NOT NULL,
  txn_host varchar(128) NOT NULL
);
{code}
would instead look like
{code}
CREATE TABLE 'txns' (
  'txn_id' bigint PRIMARY KEY,
  'txn_state' char(1) NOT NULL,
  'txn_started' bigint NOT NULL,
  'txn_last_heartbeat' bigint NOT NULL,
  'txn_user' varchar(128) NOT NULL,
  'txn_host' varchar(128) NOT NULL
);
{code}

Is that right?;;;","07/Oct/14 20:18;damien.carol;[~gates] I think we should use char {{""}} like in the others upgrade scripts of the metastore.
{code:sql}
CREATE TABLE ""BUCKETING_COLS"" (
""SD_ID"" bigint NOT NULL,
""BUCKET_COL_NAME"" character varying(256) DEFAULT NULL::character varying,
""INTEGER_IDX"" bigint NOT NULL
);
{code}

Link here https://github.com/apache/hive/blob/trunk/metastore/scripts/upgrade/postgres/hive-schema-0.14.0.postgres.sql#L22;;;","08/Oct/14 23:37;gates;A new version of the patch that quotes postgres table and field names.;;;","27/Oct/14 18:12;ashutoshc;For all DBs (except for MS-SQL) there are changes in schema file as well new txn-schema file. But for, MS-SQL there are no additions in existing schema file. Was it intentional ? 
Looks good otherwise.;;;","27/Oct/14 18:46;gates;All the txn tables are already in the hive-schema-0.14.0.mssql.sql.  I don't know why.  But that's why I didn't add them.;;;","27/Oct/14 18:53;ashutoshc;oh, ok. +1;;;","28/Oct/14 04:08;hagleitn;+1 for .14. This looks like it should go into that branch too.;;;","28/Oct/14 17:02;gates;Patch 2 committed to trunk and branch 0.14.  Thanks Ashutosh for the review and Damien for your help on the patch.;;;","30/Oct/14 03:22;brocknoland;FYI this breaks svn merge patches. Can you look at HIVE-8668?;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When auto join convert is on and noconditionaltask is off, ConditionalResolverCommonJoin fails to resolve map joins.",HIVE-6668,12701525,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,navis,yhuai,yhuai,14/Mar/14 17:22,20/Mar/14 22:20,14/Jul/23 06:14,20/Mar/14 22:20,0.13.0,0.14.0,,,,,,,,0.13.0,,,,,,0,,,"I tried the following query today ...
{code:sql}
set mapred.job.map.memory.mb=2048;
set mapred.job.reduce.memory.mb=2048;
set mapred.map.child.java.opts=-server -Xmx3072m -Djava.net.preferIPv4Stack=true;
set mapred.reduce.child.java.opts=-server -Xmx3072m -Djava.net.preferIPv4Stack=true;

set mapred.reduce.tasks=60;

set hive.stats.autogather=false;
set hive.exec.parallel=false;
set hive.enforce.bucketing=true;
set hive.enforce.sorting=true;
set hive.map.aggr=true;
set hive.optimize.bucketmapjoin=true;
set hive.optimize.bucketmapjoin.sortedmerge=true;
set hive.mapred.reduce.tasks.speculative.execution=false;
set hive.auto.convert.join=true;
set hive.auto.convert.sortmerge.join=true;
set hive.auto.convert.sortmerge.join.noconditionaltask=false;
set hive.auto.convert.join.noconditionaltask=false;
set hive.auto.convert.join.noconditionaltask.size=100000000;
set hive.optimize.reducededuplication=true;
set hive.optimize.reducededuplication.min.reducer=1;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.mapjoin.smalltable.filesize=45000000;

set hive.optimize.index.filter=false;
set hive.vectorized.execution.enabled=false;
set hive.optimize.correlation=false;
select
   i_item_id,
   s_state,
   avg(ss_quantity) agg1,
   avg(ss_list_price) agg2,
   avg(ss_coupon_amt) agg3,
   avg(ss_sales_price) agg4
FROM store_sales
JOIN date_dim on (store_sales.ss_sold_date_sk = date_dim.d_date_sk)
JOIN item on (store_sales.ss_item_sk = item.i_item_sk)
JOIN customer_demographics on (store_sales.ss_cdemo_sk = customer_demographics.cd_demo_sk)
JOIN store on (store_sales.ss_store_sk = store.s_store_sk)
where
   cd_gender = 'F' and
   cd_marital_status = 'U' and
   cd_education_status = 'Primary' and
   d_year = 2002 and
   s_state in ('GA','PA', 'LA', 'SC', 'MI', 'AL')
group by i_item_id, s_state with rollup
order by
   i_item_id,
   s_state
limit 100;
{code}

The log shows ...
{code}
14/03/14 17:05:02 INFO plan.ConditionalResolverCommonJoin: Failed to resolve driver alias (threshold : 45000000, length mapping : {store=94175, store_sales=48713909726, item=39798667, customer_demographics=1660831, date_dim=2275902})
Stage-27 is filtered out by condition resolver.
14/03/14 17:05:02 INFO exec.Task: Stage-27 is filtered out by condition resolver.
Stage-28 is filtered out by condition resolver.
14/03/14 17:05:02 INFO exec.Task: Stage-28 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
{code}
Stage-3 is a reduce join. Actually, the resolver should pick the map join",,jimhuang,navis,rhbutani,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/14 10:07;navis;HIVE-6668.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12634908/HIVE-6668.1.patch.txt","16/Mar/14 12:17;navis;HIVE-6668.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12634971/HIVE-6668.2.patch.txt","18/Mar/14 12:12;navis;HIVE-6668.3.patch.txt;https://issues.apache.org/jira/secure/attachment/12635291/HIVE-6668.3.patch.txt",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379871,,,,Thu Mar 20 22:20:14 UTC 2014,,,,,,,,,,"0|i1tgbb:",380156,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 17:26;yhuai;I guess it was broken by HIVE-6403 or HIVE-6144.;;;","14/Mar/14 18:23;yhuai;Seems aliases returned from this line (https://github.com/apache/hive/blob/trunk/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java#L178) is an empty set.;;;","14/Mar/14 19:00;yhuai;TestConditionalResolverCommonJoin cannot catch this bug.;;;","15/Mar/14 10:09;navis;Could you try with this patch? Needs some more time for making proper test cases.;;;","15/Mar/14 17:04;rhbutani;Looks right to me, you have to use the original work object because the clone creates new Operator objects.
Ran the test with the patch, confirmed that the ConditionalResolver resolves to a mapJoin. 
Navis thanks for jumping on this.;;;","16/Mar/14 12:17;navis;kick test;;;","17/Mar/14 02:47;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634971/HIVE-6668.2.patch.txt

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5406 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_hook
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1855/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1855/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634971;;;","20/Mar/14 22:07;rhbutani;[~navis]  the PrintCompletedTasksHook is nice
+1;;;","20/Mar/14 22:20;rhbutani;Committed to trunk and 0.13
thanks [~navis];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metastore init scripts should always populate the version information at the end,HIVE-6666,12701427,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,prasadm,prasadm,prasadm,14/Mar/14 07:33,17/Mar/14 23:38,14/Jul/23 06:14,17/Mar/14 23:38,0.13.0,,,,,,,,,0.13.0,,Metastore,,,,0,,,"The metastore schema create scripts for 0.13 and 0.14 (current trunk) has  multiple other operations after setting the schema version. This is problematic   as any failure in those later operations would leave metastore in inconsistent state, and yet with valid version information. The schemaTool depends on the schema version details.

Recording the schema version should be the last step in schema initialization script.
",,prasadm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6555,,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/14 22:11;prasadm;HIVE-6666.1.patch;https://issues.apache.org/jira/secure/attachment/12635016/HIVE-6666.1.patch","14/Mar/14 08:21;prasadm;HIVE-6666.1.patch;https://issues.apache.org/jira/secure/attachment/12634675/HIVE-6666.1.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379773,,,,Mon Mar 17 23:38:13 UTC 2014,,,,,,,,,,"0|i1tfpj:",380058,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 08:23;prasadm;[~ashutoshc] This is the followup patch as discussed in HIVE-6555.;;;","14/Mar/14 15:08;ashutoshc;+1;;;","15/Mar/14 07:56;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634675/HIVE-6666.1.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1805/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1805/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.733s]
[INFO] Hive Ant Utilities ................................ SUCCESS [5.885s]
[INFO] Hive Shims Common ................................. SUCCESS [3.630s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.534s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.094s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.600s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [7.987s]
[INFO] Hive Shims ........................................ SUCCESS [1.210s]
[INFO] Hive Common ....................................... SUCCESS [6.821s]
[INFO] Hive Serde ........................................ SUCCESS [11.326s]
[INFO] Hive Metastore .................................... SUCCESS [30.205s]
[INFO] Hive Query Language ............................... SUCCESS [1:19.900s]
[INFO] Hive Service ...................................... SUCCESS [7.474s]
[INFO] Hive JDBC ......................................... SUCCESS [2.816s]
[INFO] Hive Beeline ...................................... SUCCESS [2.552s]
[INFO] Hive CLI .......................................... SUCCESS [1.905s]
[INFO] Hive Contrib ...................................... SUCCESS [1.675s]
[INFO] Hive HBase Handler ................................ SUCCESS [3.452s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.566s]
[INFO] Hive HCatalog Core ................................ SUCCESS [1.978s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [1.610s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.220s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [2.142s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [9.966s]
[INFO] Hive HWI .......................................... SUCCESS [1.215s]
[INFO] Hive ODBC ......................................... SUCCESS [0.781s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.146s]
[INFO] Hive TestUtils .................................... SUCCESS [0.706s]
[INFO] Hive Packaging .................................... FAILURE [2.469s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:32.186s
[INFO] Finished at: Sat Mar 15 03:56:46 EDT 2014
[INFO] Final Memory: 75M/621M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634675;;;","17/Mar/14 23:23;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635016/HIVE-6666.1.patch

{color:green}SUCCESS:{color} +1 5397 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1862/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1862/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635016;;;","17/Mar/14 23:38;ashutoshc;Committed to 0.13 & branch. Thanks, Prasad!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vectorized variance computation differs from row mode computation.,HIVE-6664,12701423,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jnp,jnp,jnp,14/Mar/14 07:07,17/Mar/14 22:14,14/Jul/23 06:14,17/Mar/14 22:14,,,,,,,,,,0.13.0,,,,,,0,,,"Following query can show the difference:
select  var_samp(ss_sales_price), var_pop(ss_sales_price), stddev_pop(ss_sales_price), stddev_samp(ss_sales_price) from store_sales.

The reason for the difference is that row mode converts the decimal value to double upfront to calculate sum of values, when computing variance. But the vector mode performs local aggregate sum as decimal and converts into double only at flush.",,ehans,jnp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/14 21:41;jnp;HIVE-6664.1.patch;https://issues.apache.org/jira/secure/attachment/12634938/HIVE-6664.1.patch","15/Mar/14 17:28;jnp;HIVE-6664.1.patch;https://issues.apache.org/jira/secure/attachment/12634922/HIVE-6664.1.patch","14/Mar/14 07:23;jnp;HIVE-6664.1.patch;https://issues.apache.org/jira/secure/attachment/12634667/HIVE-6664.1.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379769,,,,Mon Mar 17 21:24:36 UTC 2014,,,,,,,,,,"0|i1tfon:",380054,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 07:23;jnp;Attached patch fixes the issue.;;;","14/Mar/14 09:03;jnp;Review board : https://reviews.apache.org/r/19216/;;;","15/Mar/14 00:07;ehans;+1;;;","15/Mar/14 00:15;ehans;In general, sum/avg/variance aggregate results that involve floating point arithmetic in the sum calculation will return different answers depending on execution order. This is due the nature of floating point arithmetic, where it is easy to show examples where (a + b) + c <> a + (b + c). So it is probably not critical that row-mode and vector mode have results that are compatible to the last decimal place. However, the change here is simple enough and it makes for better compatibility without any serious drawbacks for performance, so I think this is fine.;;;","15/Mar/14 07:52;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634667/HIVE-6664.1.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1803/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1803/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.420s]
[INFO] Hive Ant Utilities ................................ SUCCESS [5.330s]
[INFO] Hive Shims Common ................................. SUCCESS [3.717s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.537s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.268s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [3.015s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [7.921s]
[INFO] Hive Shims ........................................ SUCCESS [1.237s]
[INFO] Hive Common ....................................... SUCCESS [6.619s]
[INFO] Hive Serde ........................................ SUCCESS [10.022s]
[INFO] Hive Metastore .................................... SUCCESS [33.197s]
[INFO] Hive Query Language ............................... SUCCESS [1:18.498s]
[INFO] Hive Service ...................................... SUCCESS [7.763s]
[INFO] Hive JDBC ......................................... SUCCESS [3.215s]
[INFO] Hive Beeline ...................................... SUCCESS [2.868s]
[INFO] Hive CLI .......................................... SUCCESS [1.709s]
[INFO] Hive Contrib ...................................... SUCCESS [2.493s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.804s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.532s]
[INFO] Hive HCatalog Core ................................ SUCCESS [2.415s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.391s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.367s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [2.019s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [10.092s]
[INFO] Hive HWI .......................................... SUCCESS [1.077s]
[INFO] Hive ODBC ......................................... SUCCESS [0.815s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.198s]
[INFO] Hive TestUtils .................................... SUCCESS [0.649s]
[INFO] Hive Packaging .................................... FAILURE [1.704s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:33.544s
[INFO] Finished at: Sat Mar 15 03:52:23 EDT 2014
[INFO] Final Memory: 74M/421M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634667;;;","15/Mar/14 17:28;jnp;Uploading same patch to trigger pre-commit build.;;;","15/Mar/14 21:59;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634938/HIVE-6664.1.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1843/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1843/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.856s]
[INFO] Hive Ant Utilities ................................ SUCCESS [5.697s]
[INFO] Hive Shims Common ................................. SUCCESS [3.693s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.554s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.149s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.681s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [7.913s]
[INFO] Hive Shims ........................................ SUCCESS [1.266s]
[INFO] Hive Common ....................................... SUCCESS [7.722s]
[INFO] Hive Serde ........................................ SUCCESS [10.862s]
[INFO] Hive Metastore .................................... SUCCESS [33.162s]
[INFO] Hive Query Language ............................... SUCCESS [1:16.653s]
[INFO] Hive Service ...................................... SUCCESS [7.245s]
[INFO] Hive JDBC ......................................... SUCCESS [2.926s]
[INFO] Hive Beeline ...................................... SUCCESS [2.834s]
[INFO] Hive CLI .......................................... SUCCESS [1.878s]
[INFO] Hive Contrib ...................................... SUCCESS [2.489s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.801s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.581s]
[INFO] Hive HCatalog Core ................................ SUCCESS [2.012s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.680s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.902s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [1.596s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [10.201s]
[INFO] Hive HWI .......................................... SUCCESS [1.184s]
[INFO] Hive ODBC ......................................... SUCCESS [0.662s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.300s]
[INFO] Hive TestUtils .................................... SUCCESS [0.744s]
[INFO] Hive Packaging .................................... FAILURE [1.690s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:33.671s
[INFO] Finished at: Sat Mar 15 17:59:32 EDT 2014
[INFO] Final Memory: 74M/446M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634938;;;","17/Mar/14 18:49;jnp;Ran tests locally. Only failures were show_create_table_serde.q and metadata_only_queries_with_filters.q which are unrelated to this patch.;;;","17/Mar/14 21:24;jnp;I have committed this to trunk.

[~rhbutani] This bug affects hive-0.13 and causes different results than row-mode execution. This should be fixed in branch-0.13 as well.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vector Join operations with DATE columns fail,HIVE-6662,12701416,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,gopalv,gopalv,gopalv,14/Mar/14 05:58,29/Mar/14 05:32,14/Jul/23 06:14,29/Mar/14 05:32,0.13.0,,,,,,,,,0.13.0,,,,,,0,,,"Trying to generate a DATE column as part of a JOIN's output throws an exception

{code}
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Incompatible Long vector column and primitive category DATE
        at org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.buildObjectAssign(VectorColumnAssignFactory.java:306)
        at org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.buildAssigners(VectorColumnAssignFactory.java:414)
        at org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.internalForward(VectorMapJoinOperator.java:235)
        at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genAllOneUniqueJoinObject(CommonJoinOperator.java:670)
        at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:754)
        at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:229)
        at org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.processOp(VectorMapJoinOperator.java:292)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)
{code}

",,gopalv,jnp,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 05:59;gopalv;HIVE-6662.1.patch;https://issues.apache.org/jira/secure/attachment/12634658/HIVE-6662.1.patch","28/Mar/14 21:27;jnp;HIVE-6662.2.patch;https://issues.apache.org/jira/secure/attachment/12637550/HIVE-6662.2.patch","14/Mar/14 21:27;gopalv;HIVE-6662.2.patch;https://issues.apache.org/jira/secure/attachment/12634841/HIVE-6662.2.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379762,,,,Sat Mar 29 05:32:19 UTC 2014,,,,,,,,,,"0|i1tfn3:",380047,Add VectorColumnAssignFactory methods to assign DATE into LongColumn vectors,,,,,,,,,,,,,,,,,,,,"14/Mar/14 18:25;jnp;Please use DateWritable#getDays, the date representation is number of days since epoch.;;;","14/Mar/14 21:27;gopalv;Update patch as per Jitendra's comments (use days since unix epoch instead of seconds).;;;","27/Mar/14 23:54;jnp;+1.;;;","28/Mar/14 21:28;jnp;Submitting same patch again for pre-commit tests.;;;","29/Mar/14 01:56;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637550/HIVE-6662.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5499 tests executed
*Failed tests:*
{noformat}
org.apache.hcatalog.hbase.snapshot.lock.TestWriteLock.testRun
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2023/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2023/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637550;;;","29/Mar/14 04:49;jnp;The failed test has no relation to the patch. I will commit it shortly.
[~rhbutani] This should be committed to branch-0.13 as well, otherwise vector join on DATE column in hive-0.13 will not work.;;;","29/Mar/14 05:10;rhbutani;+1 for 0.13;;;","29/Mar/14 05:32;jnp;I have committed this to trunk and branch-0.13. Thanks to [~gopalv] !;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WebHCat E2E test TestPig_10 fails (Hadoop 2),HIVE-6661,12701393,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,14/Mar/14 00:49,25/Mar/14 21:24,14/Jul/23 06:14,25/Mar/14 21:24,0.13.0,,,,,,,,,0.13.0,,WebHCat,,,,0,,,enablelog=true is only supported with Hadoop 1.  Need to add a flag to skip the test with Hadoop 2,,ekoifman,gates,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 01:06;ekoifman;HIVE-6661.patch;https://issues.apache.org/jira/secure/attachment/12634602/HIVE-6661.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379739,,,,Tue Mar 25 21:24:50 UTC 2014,,,,,,,,,,"0|i1tfhz:",380024,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 01:06;ekoifman;no pre commit tests;;;","15/Mar/14 07:30;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634602/HIVE-6661.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1796/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1796/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.932s]
[INFO] Hive Ant Utilities ................................ SUCCESS [5.886s]
[INFO] Hive Shims Common ................................. SUCCESS [3.690s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.453s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.242s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [3.063s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [8.328s]
[INFO] Hive Shims ........................................ SUCCESS [1.342s]
[INFO] Hive Common ....................................... SUCCESS [6.680s]
[INFO] Hive Serde ........................................ SUCCESS [10.479s]
[INFO] Hive Metastore .................................... SUCCESS [32.481s]
[INFO] Hive Query Language ............................... SUCCESS [1:15.017s]
[INFO] Hive Service ...................................... SUCCESS [7.991s]
[INFO] Hive JDBC ......................................... SUCCESS [2.902s]
[INFO] Hive Beeline ...................................... SUCCESS [3.216s]
[INFO] Hive CLI .......................................... SUCCESS [1.800s]
[INFO] Hive Contrib ...................................... SUCCESS [2.516s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.632s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.528s]
[INFO] Hive HCatalog Core ................................ SUCCESS [2.101s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.616s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.226s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [2.133s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [9.812s]
[INFO] Hive HWI .......................................... SUCCESS [1.167s]
[INFO] Hive ODBC ......................................... SUCCESS [0.888s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.191s]
[INFO] Hive TestUtils .................................... SUCCESS [0.670s]
[INFO] Hive Packaging .................................... FAILURE [1.255s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:30.787s
[INFO] Finished at: Sat Mar 15 03:29:59 EDT 2014
[INFO] Final Memory: 74M/546M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634602;;;","25/Mar/14 21:24;gates;Patch committed to trunk and 0.13.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2 running in non-http mode closes server socket for an SSL connection after the 1st request,HIVE-6660,12701390,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,prasadm,vgumashta,vgumashta,14/Mar/14 00:32,19/Mar/14 00:13,14/Jul/23 06:14,19/Mar/14 00:13,0.13.0,,,,,,,,,0.13.0,,HiveServer2,JDBC,,,0,,,"*Beeline connection string:*
{code}
!connect jdbc:hive2://<host>:10000/;ssl=true;sslTrustStore=/usr/share/doc/hive-0.13.0.2.1.1.0/examples/files/truststore.jks;trustStorePassword=HiveJdbc vgumashta vgumashta org.apache.hive.jdbc.HiveDriver 
{code}

*Error:*
{code}
pool-7-thread-1, handling exception: java.net.SocketTimeoutException: Read timed out
pool-7-thread-1, called close()
pool-7-thread-1, called closeInternal(true)
pool-7-thread-1, SEND TLSv1 ALERT:  warning, description = close_notify
Padded plaintext before ENCRYPTION:  len = 32
0000: 01 00 BE 72 AC 10 3B FA   4E 01 A5 DE 9B 14 16 AF  ...r..;.N.......
0010: 4E DD 7A 29 AD B4 09 09   09 09 09 09 09 09 09 09  N.z)............
pool-7-thread-1, WRITE: TLSv1 Alert, length = 32
[Raw write]: length = 37
0000: 15 03 01 00 20 6C 37 82   A8 52 40 DA FB 83 2D CD  .... l7..R@...-.
0010: 96 9F F0 B7 22 17 E1 04   C1 D1 93 1B C4 39 5A B0  ....""........9Z.
0020: A2 3F 5D 7D 2D                                     .?].-
pool-7-thread-1, called closeSocket(selfInitiated)
pool-7-thread-1, called close()
pool-7-thread-1, called closeInternal(true)
pool-7-thread-1, called close()
pool-7-thread-1, called closeInternal(true)
{code}

*Subsequent queries fail:*
{code}
main, WRITE: TLSv1 Application Data, length = 144
main, handling exception: java.net.SocketException: Broken pipe
%% Invalidated:  [Session-1, TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]
main, SEND TLSv1 ALERT:  fatal, description = unexpected_message
Padded plaintext before ENCRYPTION:  len = 32
0000: 02 0A 52 C3 18 B1 C1 38   DB 3F B6 D1 C5 CA 14 9C  ..R....8.?......
0010: A5 38 4C 01 31 69 09 09   09 09 09 09 09 09 09 09  .8L.1i..........
main, WRITE: TLSv1 Alert, length = 32
main, Exception sending alert: java.net.SocketException: Broken pipe
main, called closeSocket()
Error: org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe (state=08S01,code=0)
java.sql.SQLException: org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe
	at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:226)
	at org.apache.hive.beeline.Commands.execute(Commands.java:736)
	at org.apache.hive.beeline.Commands.sql(Commands.java:657)
	at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:796)
	at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:659)
	at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:368)
	at org.apache.hive.beeline.BeeLine.main(BeeLine.java:351)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
Caused by: org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe
	at org.apache.thrift.transport.TIOStreamTransport.flush(TIOStreamTransport.java:161)
	at org.apache.thrift.transport.TSaslTransport.flush(TSaslTransport.java:471)
	at org.apache.thrift.transport.TSaslClientTransport.flush(TSaslClientTransport.java:37)
	at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:65)
	at org.apache.hive.service.cli.thrift.TCLIService$Client.send_ExecuteStatement(TCLIService.java:219)
	at org.apache.hive.service.cli.thrift.TCLIService$Client.ExecuteStatement(TCLIService.java:211)
	at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:220)
	... 11 more
Caused by: java.net.SocketException: Broken pipe
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:153)
	at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:377)
	at sun.security.ssl.OutputRecord.write(OutputRecord.java:363)
	at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:830)
	at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:801)
	at sun.security.ssl.AppOutputStream.write(AppOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at org.apache.thrift.transport.TIOStreamTransport.flush(TIOStreamTransport.java:159)
	... 17 more
{code}

Works fine however in http mode using ssl.",,prasadm,rhbutani,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6679,,,,,,,,,,,,,,,,,,,,,"16/Mar/14 16:50;prasadm;HIVE-6660.1.patch;https://issues.apache.org/jira/secure/attachment/12634989/HIVE-6660.1.patch","15/Mar/14 01:43;prasadm;HIVE-6660.1.patch;https://issues.apache.org/jira/secure/attachment/12634882/HIVE-6660.1.patch","14/Mar/14 20:34;vgumashta;hive-site.xml;https://issues.apache.org/jira/secure/attachment/12634827/hive-site.xml",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379736,,,,Wed Mar 19 00:13:18 UTC 2014,,,,,,,,,,"0|i1tfhb:",380021,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 05:46;prasadm;[~vaibhavgumashta] I am not seeing this on the trunk. Repeated SSL connections work fine for me.
Are you seeing this on 0.13 branch ? Would you mind sharing your config.;;;","14/Mar/14 20:33;vgumashta;[~prasadm] Tried on 13. Attached is my hive-site. Let me try and see if trunk works fine.;;;","14/Mar/14 22:13;vgumashta;[~prasadm] I am seeing this on trunk as well.;;;","15/Mar/14 00:03;prasadm;hmm .. I can reproduce this when the session is idle for a short while. The SSL server transport is getting created with a hardcoded client read timeout which I guess is causing this. 
Ideally this should be configurable. I will test it and submit a patch. Thanks!;;;","15/Mar/14 00:16;vgumashta;[~prasadm] Actually we're setting the loginTimeout to 0. TSSLTransportFactory#createClient internally calls socket.setSoTimeout(0). Per the docs, a timeout of 0 is interpreted as infinite timeout. Let me also investigate more...;;;","15/Mar/14 00:17;vgumashta;[~prasadm] Let me check the server transport too. Client looks fine.;;;","15/Mar/14 00:19;prasadm; no, It's the timeout on [server socket| https://github.com/apache/hive/blob/trunk/service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java#L220];;;","15/Mar/14 00:23;vgumashta;[~prasadm] Oops, clicked on assign to me by mistake.;;;","15/Mar/14 00:25;vgumashta;[~prasadm] If you set it to 0, should fix the issue. ;;;","15/Mar/14 00:29;vgumashta;[~prasadm] Just tested, that fixes it. I think we should have this in 13. Will make this blocker. Thanks!;;;","15/Mar/14 00:31;prasadm;yes, That's what I tested.
As I mentioned before, it would be good to make it configurable. We did support the server side read timeout as well as keep-alive in the old hiveserver. For HiveServer2, now we have multiple connection options, Kerberos, Delegation Token (Digest-MD5), Plain Sasl with and without SSL and raw sockets. The configuration should be applicable to all flavors.

I will submit the short term fix to the server timeout to 0 which can go into 0.13 and log a followup ticket to make it configurable of 0.14.;;;","15/Mar/14 00:41;vgumashta;[~prasadm] I agree. Yeah, short term for 13 sounds good! ;;;","15/Mar/14 01:45;prasadm;The SSL Server socket is created with a hardcoded timeout. This is inconsistent with rest of the server transport types.
The patch sets it to infinite timeout.

I will create a followup ticket to make the timeout  configurable for all transport types.
;;;","15/Mar/14 01:53;vgumashta;+1 (non-binding).;;;","15/Mar/14 10:48;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634882/HIVE-6660.1.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1827/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1827/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.386s]
[INFO] Hive Ant Utilities ................................ SUCCESS [6.096s]
[INFO] Hive Shims Common ................................. SUCCESS [3.638s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.442s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.415s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.639s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [8.021s]
[INFO] Hive Shims ........................................ SUCCESS [1.249s]
[INFO] Hive Common ....................................... SUCCESS [6.847s]
[INFO] Hive Serde ........................................ SUCCESS [10.164s]
[INFO] Hive Metastore .................................... SUCCESS [32.077s]
[INFO] Hive Query Language ............................... SUCCESS [1:15.619s]
[INFO] Hive Service ...................................... SUCCESS [7.847s]
[INFO] Hive JDBC ......................................... SUCCESS [2.907s]
[INFO] Hive Beeline ...................................... SUCCESS [2.583s]
[INFO] Hive CLI .......................................... SUCCESS [1.955s]
[INFO] Hive Contrib ...................................... SUCCESS [2.457s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.609s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.538s]
[INFO] Hive HCatalog Core ................................ SUCCESS [1.921s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.534s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.283s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [2.122s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [9.937s]
[INFO] Hive HWI .......................................... SUCCESS [1.058s]
[INFO] Hive ODBC ......................................... SUCCESS [0.811s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.217s]
[INFO] Hive TestUtils .................................... SUCCESS [0.661s]
[INFO] Hive Packaging .................................... FAILURE [1.885s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:29.422s
[INFO] Finished at: Sat Mar 15 06:48:00 EDT 2014
[INFO] Final Memory: 74M/381M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634882;;;","16/Mar/14 16:50;prasadm;re-attaching for pre-commit run;;;","17/Mar/14 04:39;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634989/HIVE-6660.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5406 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority2
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1856/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1856/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634989;;;","17/Mar/14 06:01;prasadm;The failures look unrelated.;;;","17/Mar/14 13:13;thejas;+1;;;","18/Mar/14 06:30;prasadm;Patch committed to trunk.

[~rhbutani] This should be a blocker for hive 0.13. Requesting approval to push the patch to 0.13 release branch.;;;","18/Mar/14 17:50;rhbutani;+1 for porting to 0.13 branch;;;","19/Mar/14 00:13;prasadm;Patch committed to trunk and 0.13 release branch.
Thanks Thejas and Vaibhav for review;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update log for list_bucket_* to add pre/post DB,HIVE-6659,12701372,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jpullokkaran,jpullokkaran,jpullokkaran,13/Mar/14 22:34,15/Mar/14 05:28,14/Jul/23 06:14,15/Mar/14 05:28,,,,,,,,,,0.13.0,,,,,,0,,,On Hadoop2 we now print out Database Name using pre/post hooks.,,jpullokkaran,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 05:23;jpullokkaran;HIVE-6659.patch;https://issues.apache.org/jira/secure/attachment/12634653/HIVE-6659.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379718,,,,Sat Mar 15 05:28:13 UTC 2014,,,,,,,,,,"0|i1tfdb:",380003,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 15:13;ashutoshc;+1;;;","15/Mar/14 05:28;ashutoshc;Committed to trunk & 0.13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Modify Alter_numbuckets* test to reflect hadoop2 changes,HIVE-6658,12701370,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jpullokkaran,jpullokkaran,jpullokkaran,13/Mar/14 22:30,19/Mar/14 16:04,14/Jul/23 06:14,19/Mar/14 16:04,0.12.0,,,,,,,,,0.13.0,,,,,,0,,,Hadoop2 now honors number of reducers config while running in local mode. This affects bucketing tests as the data gets properly bucketed in Hadoop2 (In hadoop1 all data ended up in same bucket while in local mode).,,jdere,jpullokkaran,szehon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/14 00:10;jpullokkaran;HIVE-6658.2.patch;https://issues.apache.org/jira/secure/attachment/12635205/HIVE-6658.2.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379716,,,,Wed Mar 19 16:04:02 UTC 2014,,,,,,,,,,"0|i1tfcv:",380001,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 18:08;szehon;Yea I saw this too.   Thanks for fixing this, one comment for consideration- Looking at other q-tests with two versions, original includes 0.20,0.20S, and new version exclude 0.20,20S.  Do we want to do that instead for consistency, as future versions of hadoop should probably adhere to 'new' behavior (which is proper #buckets in this case).;;;","15/Mar/14 00:00;jpullokkaran;Szehon,
Shouldn't it depend on de-supporting time line for HADOOP 0.20 ?
In my opinion till HADOOP 0.20 is de-supported we would want to test the behavior (alter bucket) for HADOOP 0.20 and hence require two different set of tests.

;;;","15/Mar/14 00:09;szehon;Yea we should have two tests and keep testing 0.20.  

Sorry for not being clear, my suggestion was :  the hadoop1 version can have --include 0.20,0.20S and the hadoop2 version can have --exclude 0.20,0.20S.  This is how most tests are split right now.   In this patch, hadoop1 version says --exclude hadoop0.23 and hadoop2 version says  --include 0.23.  But then, any later version of hadoop beyond 0.23 will test using hadoop1 version, which would be the 'wrong' result in this case.  Just a thought.;;;","15/Mar/14 07:47;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634652/HIVE-6658.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1802/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1802/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.611s]
[INFO] Hive Ant Utilities ................................ SUCCESS [5.871s]
[INFO] Hive Shims Common ................................. SUCCESS [3.777s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.924s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [3.984s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.640s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [8.053s]
[INFO] Hive Shims ........................................ SUCCESS [1.289s]
[INFO] Hive Common ....................................... SUCCESS [6.992s]
[INFO] Hive Serde ........................................ SUCCESS [10.311s]
[INFO] Hive Metastore .................................... SUCCESS [33.157s]
[INFO] Hive Query Language ............................... SUCCESS [1:17.583s]
[INFO] Hive Service ...................................... SUCCESS [7.018s]
[INFO] Hive JDBC ......................................... SUCCESS [3.056s]
[INFO] Hive Beeline ...................................... SUCCESS [2.833s]
[INFO] Hive CLI .......................................... SUCCESS [1.807s]
[INFO] Hive Contrib ...................................... SUCCESS [2.494s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.796s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.514s]
[INFO] Hive HCatalog Core ................................ SUCCESS [2.040s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.469s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.847s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [1.590s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [9.924s]
[INFO] Hive HWI .......................................... SUCCESS [1.160s]
[INFO] Hive ODBC ......................................... SUCCESS [0.761s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.291s]
[INFO] Hive TestUtils .................................... SUCCESS [0.651s]
[INFO] Hive Packaging .................................... FAILURE [1.743s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:32.916s
[INFO] Finished at: Sat Mar 15 03:47:51 EDT 2014
[INFO] Final Memory: 75M/608M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634652;;;","17/Mar/14 07:16;ashutoshc;[~szehon] has valid point. [~jpullokkaran] Would you like to redo in/exclude version numbers.;;;","18/Mar/14 00:11;jpullokkaran;I assumed there were older versions of hadoop that Hive supported (< 0.20).
I have reworked the patch.;;;","18/Mar/14 07:25;szehon;Yea, looks good, thanks;;;","19/Mar/14 07:01;jdere;+1;;;","19/Mar/14 16:04;ashutoshc;Committed to trunk & 0.13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in ORC Timestamp reader returns wrong nanoseconds,HIVE-6656,12701336,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,prasanth_j,prasanth_j,prasanth_j,13/Mar/14 19:54,18/Mar/14 00:59,14/Jul/23 06:14,18/Mar/14 00:59,0.13.0,0.14.0,,,,,,,,0.13.0,,,,,,0,orcfile,,"ORC timestamp writer stores the number of trailing zeros in 3 LSB bits. There is a bug in parsing nanosecond logic that returns incorrect value.

Input:
1999-01-01 00:00:00.999999999

Output: 
1999-01-01 00:00:00.463129087

The fix for this is parseNanos() should first right shift by 3 and then typecast to int.",,omalley,prasanth_j,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 19:56;prasanth_j;HIVE-6656.1.patch;https://issues.apache.org/jira/secure/attachment/12634529/HIVE-6656.1.patch","16/Mar/14 06:31;prasanth_j;HIVE-6656.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12634959/HIVE-6656.1.patch.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379682,,,,Tue Mar 18 00:59:19 UTC 2014,,,,,,,,,,"0|i1tf5b:",379967,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 22:05;omalley;+1 LGTM;;;","15/Mar/14 06:50;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634529/HIVE-6656.1.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1787/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1787/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.754s]
[INFO] Hive Ant Utilities ................................ SUCCESS [5.856s]
[INFO] Hive Shims Common ................................. SUCCESS [3.806s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.513s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.451s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.825s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [8.067s]
[INFO] Hive Shims ........................................ SUCCESS [1.124s]
[INFO] Hive Common ....................................... SUCCESS [6.845s]
[INFO] Hive Serde ........................................ SUCCESS [10.153s]
[INFO] Hive Metastore .................................... SUCCESS [33.764s]
[INFO] Hive Query Language ............................... SUCCESS [1:16.983s]
[INFO] Hive Service ...................................... SUCCESS [9.917s]
[INFO] Hive JDBC ......................................... SUCCESS [2.790s]
[INFO] Hive Beeline ...................................... SUCCESS [2.772s]
[INFO] Hive CLI .......................................... SUCCESS [2.395s]
[INFO] Hive Contrib ...................................... SUCCESS [2.484s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.527s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.583s]
[INFO] Hive HCatalog Core ................................ SUCCESS [2.447s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.464s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.270s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [2.178s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [10.646s]
[INFO] Hive HWI .......................................... SUCCESS [1.107s]
[INFO] Hive ODBC ......................................... SUCCESS [0.787s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.239s]
[INFO] Hive TestUtils .................................... SUCCESS [0.658s]
[INFO] Hive Packaging .................................... FAILURE [1.967s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:36.918s
[INFO] Finished at: Sat Mar 15 02:50:09 EDT 2014
[INFO] Final Memory: 74M/426M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634529;;;","16/Mar/14 06:31;prasanth_j;reuploading for jenkins;;;","16/Mar/14 21:32;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634959/HIVE-6656.1.patch.txt

{color:green}SUCCESS:{color} +1 5408 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1853/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1853/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634959;;;","18/Mar/14 00:59;sershe;in trunk and 13. Didn't review, based on other +1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WebHCat E2E test JOBS_7 and JOBS_9 fail as profile.url in job details is being returned as null,HIVE-6653,12701299,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,13/Mar/14 18:13,25/Mar/14 21:18,14/Jul/23 06:14,25/Mar/14 21:18,0.13.0,,,,,,,,,0.13.0,,WebHCat,,,,0,,,the 2 tests should not be checking profile.url property in the returned JSON since 'url' comes from org.apache.hadoop.mapred.JobProfile class which is marked LimitedPrivate,,ekoifman,gates,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 20:10;ekoifman;HIVE-6653.patch;https://issues.apache.org/jira/secure/attachment/12634533/HIVE-6653.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379645,,,,Tue Mar 25 21:18:56 UTC 2014,,,,,,,,,,"0|i1tex3:",379930,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 20:10;ekoifman;removed the check for url and fixed minor problems in JOBS_9;;;","13/Mar/14 20:10;ekoifman;no pre commit tests;;;","15/Mar/14 06:54;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634533/HIVE-6653.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1788/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1788/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.814s]
[INFO] Hive Ant Utilities ................................ SUCCESS [5.403s]
[INFO] Hive Shims Common ................................. SUCCESS [3.772s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.579s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.223s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.616s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [7.913s]
[INFO] Hive Shims ........................................ SUCCESS [1.246s]
[INFO] Hive Common ....................................... SUCCESS [6.976s]
[INFO] Hive Serde ........................................ SUCCESS [10.484s]
[INFO] Hive Metastore .................................... SUCCESS [33.682s]
[INFO] Hive Query Language ............................... SUCCESS [1:18.968s]
[INFO] Hive Service ...................................... SUCCESS [7.958s]
[INFO] Hive JDBC ......................................... SUCCESS [2.991s]
[INFO] Hive Beeline ...................................... SUCCESS [2.721s]
[INFO] Hive CLI .......................................... SUCCESS [1.715s]
[INFO] Hive Contrib ...................................... SUCCESS [2.635s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.826s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.511s]
[INFO] Hive HCatalog Core ................................ SUCCESS [2.405s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.471s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.217s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [2.116s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [10.087s]
[INFO] Hive HWI .......................................... SUCCESS [0.968s]
[INFO] Hive ODBC ......................................... SUCCESS [0.899s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.210s]
[INFO] Hive TestUtils .................................... SUCCESS [0.651s]
[INFO] Hive Packaging .................................... FAILURE [1.702s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:35.345s
[INFO] Finished at: Sat Mar 15 02:54:35 EDT 2014
[INFO] Final Memory: 74M/465M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634533;;;","25/Mar/14 21:18;gates;Patch committed to trunk and 0.13.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Beeline gives evasive error message for any unrecognized command line arguement,HIVE-6652,12701296,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,xuefuz,xuefuz,xuefuz,13/Mar/14 18:09,13/Nov/14 19:42,14/Jul/23 06:14,20/Mar/14 22:52,0.11.0,0.12.0,0.13.0,,,,,,,0.14.0,,,,,,0,,,"For any unrecognized command line argument, Beeline emits a warning message that's evasive and meaningless. For instance:
{code}
beeline abc
abc (No such file or directory)
Beeline version 0.14.0-SNAPSHOT by Apache Hive
...
beeline -hh
-hh (No such file or directory)
{code}

The error seeming suggests that Beeline accepts an argument as a file name. However, neither Beeline doc nor command line help indicates there is such an option. 
{code}
beeline --help
Usage: java org.apache.hive.cli.beeline.BeeLine 
   -u <database url>               the JDBC URL to connect to
   -n <username>                   the username to connect as
   -p <password>                   the password to connect as
   -d <driver class>               the driver class to use
   -e <query>                      query that should be executed
   -f <file>                       script file that should be executed
   --hiveconf property=value       Use value for given property
   --hivevar name=value            hive variable name and value
                                   This is Hive specific settings in which variables
                                   can be set at session level and referenced in Hive
                                   commands or queries.
   --color=[true/false]            control whether color is used for display
   --showHeader=[true/false]       show column names in query results
   --headerInterval=ROWS;          the interval between which heades are displayed
   --fastConnect=[true/false]      skip building table/column list for tab-completion
   --autoCommit=[true/false]       enable/disable automatic transaction commit
   --verbose=[true/false]          show verbose error messages and debug info
   --showWarnings=[true/false]     display connection warnings
   --showNestedErrs=[true/false]   display nested errors
   --numberFormat=[pattern]        format numbers using DecimalFormat pattern
   --force=[true/false]            continue running script even after errors
   --maxWidth=MAXWIDTH             the maximum width of the terminal
   --maxColumnWidth=MAXCOLWIDTH    the maximum width to use when displaying columns
   --silent=[true/false]           be more silent
   --autosave=[true/false]         automatically save preferences
   --outputformat=[table/vertical/csv/tsv]   format mode for result display
   --isolation=LEVEL               set the transaction isolation level
   --nullemptystring=[true/false]  set to true to get historic behavior of printing null as empty string
   --help                          display this message
{code}

Further research shows that this is a residual from SQLLine from which Beeline is derived, which allows user to specify a property file based on which SQLLine can make a DB connection.

While this might be useful, this isn't documented and has caused a lot of confusions. And it's the root cause for quite a few problems such as those described in HIVE-5677. HIVE-6173 had the same symptom, which uncovered another problem.

Thus, I'd suggest we disable this option. If it's desirable to have this option, then we need at least corresponding documentation plus better error message.
",,leftyl,mdominguez@cloudera.com,szehon,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6176,,,,,,,,,,HIVE-5677,HIVE-6173,,,,,,,,,,,,,,,,,,,,"13/Mar/14 18:29;xuefuz;HIVE-6652.patch;https://issues.apache.org/jira/secure/attachment/12634505/HIVE-6652.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379642,,,,Thu Nov 13 19:42:14 UTC 2014,,,,,,,,,,"0|i1tewf:",379927,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 18:29;xuefuz;With the attached patch, Beeline will emit:
{code}
beeline abc
Unrecognized argument: abc
Usage: java org.apache.hive.cli.beeline.BeeLine 
   -u <database url>               the JDBC URL to connect to
   -n <username>                   the username to connect as
   -p <password>                   the password to connect as
   -d <driver class>               the driver class to use
   -e <query>                      query that should be executed
   -f <file>                       script file that should be executed
   --hiveconf property=value       Use value for given property
   --hivevar name=value            hive variable name and value
                                   This is Hive specific settings in which variables
                                   can be set at session level and referenced in Hive
                                   commands or queries.
   --color=[true/false]            control whether color is used for display
   --showHeader=[true/false]       show column names in query results
   --headerInterval=ROWS;          the interval between which heades are displayed
   --fastConnect=[true/false]      skip building table/column list for tab-completion
   --autoCommit=[true/false]       enable/disable automatic transaction commit
   --verbose=[true/false]          show verbose error messages and debug info
   --showWarnings=[true/false]     display connection warnings
   --showNestedErrs=[true/false]   display nested errors
   --numberFormat=[pattern]        format numbers using DecimalFormat pattern
   --force=[true/false]            continue running script even after errors
   --maxWidth=MAXWIDTH             the maximum width of the terminal
   --maxColumnWidth=MAXCOLWIDTH    the maximum width to use when displaying columns
   --silent=[true/false]           be more silent
   --autosave=[true/false]         automatically save preferences
   --outputformat=[table/vertical/csv/tsv]   format mode for result display
   --isolation=LEVEL               set the transaction isolation level
   --nullemptystring=[true/false]  set to true to get historic behavior of printing null as empty string
   --help                          display this message
{code};;;","13/Mar/14 19:18;szehon;It also doesn't make sense to me, that its treating all un-recognized argument as db-property file and connecting one after another.  Looks like a good usability change to me, if not being used now.

One minor thought, as the property file does seem useful, we can consider supporting property file with another -propertyfile argument if needed;;;","15/Mar/14 02:54;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634505/HIVE-6652.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5406 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1784/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1784/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634505;;;","15/Mar/14 02:58;xuefuz;The above test failure is unrelated. The patch is ready for review.;;;","15/Mar/14 03:02;xuefuz;RB: https://reviews.apache.org/r/19251/;;;","20/Mar/14 22:12;ashutoshc;+1;;;","20/Mar/14 22:52;ashutoshc;Committted to trunk. Thanks, Xuefu!;;;","21/Mar/14 00:25;leftyl;{quote}
this is a residual from SQLLine from which Beeline is derived, which allows user to specify a property file based on which SQLLine can make a DB connection.  . . .  I'd suggest we disable this option. If it's desirable to have this option, then we need at least corresponding documentation plus better error message.
{quote}

Should the wiki mention this change from SQLLine behavior (with a version 0.14 note)?

* [Beeline – New Command Line Shell |https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-Beeline–NewCommandLineShell];;;","21/Mar/14 01:00;xuefuz;I don't think we need to document this, as it's never documented or officially supported. ;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
broken link in WebHCat doc: Job Information — GET queue/:jobid,HIVE-6651,12701293,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,leftyl,ekoifman,ekoifman,13/Mar/14 18:08,21/May/14 04:12,14/Jul/23 06:14,21/May/14 04:11,,,,,,,,,,,,Documentation,WebHCat,,,0,,,"https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference+JobInfo#WebHCatReferenceJobInfo-Results

the link in the table to ""Class JobProfile"" is broken
",,ekoifman,leftyl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379639,,,,Wed May 21 04:11:33 UTC 2014,,,,,,,,,,"0|i1tevr:",379924,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 09:10;leftyl;The link is broken in four docs:  GET queue/:jobid, DELETE queue/:jobid, GET jobs/:jobid, and DELETE jobs/:jobid.

The link tries to go to the ""stable"" version of Hadoop API docs -- http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/JobProfile.html -- but there are two stable sets of docs, stable1 for Hadoop 1 and stable2 for Hadoop 2, with ""stable"" currently set to stable2.  Class JobProfile is not in stable2, but it is in stable1:  [http://hadoop.apache.org/docs/stable1/api/org/apache/hadoop/mapred/JobProfile.html].

It's easy enough to change all the links to stable1 (and for consistency also change the JobStatus links) but does the Hadoop 2 vs. Hadoop 1 distinction matter in the WebHCat documentation?

Hadoop doc sets are here:  [http://hadoop.apache.org/docs/].  Stable, stable1, and stable2 are listed at the bottom.

* stable (Hadoop 2.2.0) API docs:  [http://hadoop.apache.org/docs/stable/api/index.html]
* stable1 (Hadoop 1.2.1) API docs:  [http://hadoop.apache.org/docs/stable1/api/index.html]
* stable2 (Hadoop 2.2.0) API docs:  [http://hadoop.apache.org/docs/stable2/api/index.html]

WebHCat docs with broken links:

* [Get Queue JobID |https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference+JobInfo#WebHCatReferenceJobInfo-Results]
* [Delete Queue JobID |https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference+DeleteJob#WebHCatReferenceDeleteJob-Results]
* [Get Jobs JobID |https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference+Job#WebHCatReferenceJob-Results]
* [Delete Jobs JobID |https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference+DeleteJobID#WebHCatReferenceDeleteJobID-Results]
;;;","14/Mar/14 18:35;ekoifman;1. WebHCat just passes along to the user whatever information it finds in JobProfile object.
2. In H2, JobProfile was annotated with InterfaceAudience.LimitedPrivate({""MapReduce""}) and InterfaceStability.Unstable.  In H1 it's not annotated with anything.
3. JobStatus is a public (via annotations) class so we can assume that it will be stable.


So I think we should remove the link for JobProfile and just put org.apache.hadoop.mapred.JobProfile class name there with a note that WebHCat just passes along the info in this object which is subject to change from one Hadoop version to another.
;;;","20/May/14 08:27;leftyl;Done, please review so we can resolve this jira:

* [Get Queue JobID | https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference+JobInfo#WebHCatReferenceJobInfo-Results]
* [Delete Queue JobID | https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference+DeleteJob#WebHCatReferenceDeleteJob-Results]
* [Get Jobs JobID | https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference+Job#WebHCatReferenceJob-Results]
* [Delete Jobs JobID | https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference+DeleteJobID#WebHCatReferenceDeleteJobID-Results];;;","20/May/14 17:44;ekoifman;+1;;;","21/May/14 04:11;leftyl;Fixed in the wiki.  Thanks, Eugene!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hive.optimize.index.filter breaks non-index where with HBaseStorageHandler,HIVE-6650,12701243,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ndimiduk,ndimiduk,ndimiduk,13/Mar/14 14:29,22/Mar/14 06:05,14/Jul/23 06:14,22/Mar/14 06:05,0.12.0,,,,,,,,,0.13.0,,HBase Handler,,,,0,,,"With the above enabled, where clauses including non-rowkey columns cannot be used with the HBaseStorageHandler. Job fails to launch with the following exception.

{noformat}
java.lang.RuntimeException: Unexpected residual predicate (s_address = '200 WEST 56TH STREET')
at org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.convertFilter(HiveHBaseTableInputFormat.java:292)
at org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getSplits(HiveHBaseTableInputFormat.java:495)
at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:294)
at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:303)
at org.apache.hadoop.mapreduce.JobSubmitter.writeOldSplits(JobSubmitter.java:518)
at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:510)
at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:392)
at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1268)
at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1265)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
at org.apache.hadoop.mapreduce.Job.submit(Job.java:1265)
at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:425)
at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:136)
at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)
at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)
at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1437)
at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1215)
at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1043)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)
at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:781)
at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
Job Submission failed with exception 'java.lang.RuntimeException(Unexpected residual predicate (s_address = '200 WEST 56TH STREET'))'
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
{noformat}

I believe this bug was introduced in HIVE-2036, see change to OpProcFactory.java that always includes full predicate, even after storage handler negotiates the predicates it can pushdown. Since this behavior is divergent from input formats (they cannot negotiate), there's no harm in the SH ignoring non-indexed predicates -- Hive respects all of them at a layer above anyway. Might as well remove the check/exception.",,brocknoland,ndimiduk,qwertymaniac,stack,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 22:17;ndimiduk;HIVE-6650.0.patch;https://issues.apache.org/jira/secure/attachment/12634566/HIVE-6650.0.patch","14/Mar/14 22:48;ndimiduk;HIVE-6650.1.patch;https://issues.apache.org/jira/secure/attachment/12634857/HIVE-6650.1.patch","18/Mar/14 16:40;ndimiduk;HIVE-6650.2.patch;https://issues.apache.org/jira/secure/attachment/12635341/HIVE-6650.2.patch","20/Mar/14 19:16;ndimiduk;HIVE-6650.3.patch;https://issues.apache.org/jira/secure/attachment/12635847/HIVE-6650.3.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379589,,,,Sat Mar 22 06:05:06 UTC 2014,,,,,,,,,,"0|i1tekv:",379875,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 22:17;ndimiduk;This patch removes the assumption from createFilterScan that any decomposition has been performed, so as to be compatible with the behavior of hive.optimize.index.filter that is expected by non-StorageHandler formats. Someone please refactor Input/OutputFormat so that it's is not interacted with directly, instead make StorageHandlers the omnipresent interface.

Take the patch for a spin in the build bot, see what I broke.;;;","14/Mar/14 22:48;ndimiduk;I finally got the qtest to run locally, updating patch with corrected out file.;;;","14/Mar/14 23:38;brocknoland;Looks like a simple enough, patch. Can you post a RB item for it?;;;","14/Mar/14 23:53;ndimiduk;As you like: https://reviews.apache.org/r/19247/;;;","15/Mar/14 07:07;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634857/HIVE-6650.1.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1791/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1791/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.697s]
[INFO] Hive Ant Utilities ................................ SUCCESS [6.186s]
[INFO] Hive Shims Common ................................. SUCCESS [3.661s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.561s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.259s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.625s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [8.045s]
[INFO] Hive Shims ........................................ SUCCESS [1.247s]
[INFO] Hive Common ....................................... SUCCESS [6.944s]
[INFO] Hive Serde ........................................ SUCCESS [12.358s]
[INFO] Hive Metastore .................................... SUCCESS [33.139s]
[INFO] Hive Query Language ............................... SUCCESS [1:16.623s]
[INFO] Hive Service ...................................... SUCCESS [6.754s]
[INFO] Hive JDBC ......................................... SUCCESS [3.161s]
[INFO] Hive Beeline ...................................... SUCCESS [2.855s]
[INFO] Hive CLI .......................................... SUCCESS [2.599s]
[INFO] Hive Contrib ...................................... SUCCESS [2.602s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.918s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.372s]
[INFO] Hive HCatalog Core ................................ SUCCESS [2.439s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.457s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.348s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [1.954s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [10.181s]
[INFO] Hive HWI .......................................... SUCCESS [1.222s]
[INFO] Hive ODBC ......................................... SUCCESS [0.736s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.402s]
[INFO] Hive TestUtils .................................... SUCCESS [0.538s]
[INFO] Hive Packaging .................................... FAILURE [2.427s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:36.050s
[INFO] Finished at: Sat Mar 15 03:07:37 EDT 2014
[INFO] Final Memory: 74M/446M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634857;;;","18/Mar/14 16:02;ndimiduk;Can someone give me some context for this build error?

(cc [~sushanth], [~ashutoshc], [~brocknoland]);;;","18/Mar/14 16:30;ashutoshc;It was not because of patch. Trunk was broken in interim. Its fixed now. Just reupload your patch.;;;","18/Mar/14 16:40;ndimiduk;Same as patch v1.;;;","20/Mar/14 19:16;ndimiduk;I spoke with [~ashutoshc] offline for a review, new patch addresses his concern. The interdiff is short

{noformat}
diff -u b/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java b/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java
--- b/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java
+++ b/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java
@@ -319,7 +319,7 @@
           .equals(comparisonOp)){
         stopRow = getNextBA(constantVal);
       } else {
-        LOG.debug(comparisonOp + "" is not a supported comparison operator"");
+        throw new IOException(comparisonOp + "" is not a supported comparison operator"");
       }
     }
     scan.setStartRow(startRow);
{noformat};;;","20/Mar/14 19:36;ashutoshc;+1;;;","22/Mar/14 01:07;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635847/HIVE-6650.3.patch

{color:green}SUCCESS:{color} +1 5437 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1896/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1896/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635847;;;","22/Mar/14 06:05;ashutoshc;Committed to 0.13 & trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vectorization: some date expressions throw exception.,HIVE-6649,12701182,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jnp,jnp,jnp,13/Mar/14 08:01,17/Mar/14 22:11,14/Jul/23 06:14,17/Mar/14 22:11,,,,,,,,,,0.13.0,,,,,,0,,,"Query ran with hive.vectorized.execution.enabled=true:
{code}
select dt, to_date(date_add(dt, 2)), to_date(date_sub(dt, 2)),
       datediff(dt, date_add(dt, 2)), datediff(dt, date_sub(dt, 2)),
       datediff(date_add(dt, 2), date_sub(dt, 2))
from vectortab10korc limit 1;
{code}
fails with the following error:
{noformat}
Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:195)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:45)
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)
	... 8 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating datediff(date_add(dt, 2), date_sub(dt, 2))
	at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.processOp(VectorSelectOperator.java:117)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:791)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:791)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:43)
	... 9 more
Caused by: java.lang.NullPointerException
	at java.lang.String.checkBounds(String.java:400)
	at java.lang.String.<init>(String.java:569)
	at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.setDays(VectorUDFDateDiffColCol.java:254)
	at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.copySelected(VectorUDFDateDiffColCol.java:231)
	at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.toDateArray(VectorUDFDateDiffColCol.java:190)
	at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.evaluate(VectorUDFDateDiffColCol.java:72)
	at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.processOp(VectorSelectOperator.java:115)
	... 13 more
{noformat}",,ehans,jithendhir92,jnp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 08:38;jnp;HIVE-6649.1.patch;https://issues.apache.org/jira/secure/attachment/12634396/HIVE-6649.1.patch","15/Mar/14 21:40;jnp;HIVE-6649.2.patch;https://issues.apache.org/jira/secure/attachment/12634937/HIVE-6649.2.patch","15/Mar/14 16:56;jnp;HIVE-6649.2.patch;https://issues.apache.org/jira/secure/attachment/12634921/HIVE-6649.2.patch","14/Mar/14 09:07;jnp;HIVE-6649.2.patch;https://issues.apache.org/jira/secure/attachment/12634677/HIVE-6649.2.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379528,,,,Mon Mar 17 19:05:22 UTC 2014,,,,,,,,,,"0|i1te8f:",379819,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 16:37;ehans;Can you put this up on ReviewBoard if you're ready for a review?;;;","14/Mar/14 09:07;jnp;Review board: https://reviews.apache.org/r/19218/;;;","14/Mar/14 09:49;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634396/HIVE-6649.1.patch

{color:green}SUCCESS:{color} +1 5394 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1770/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1770/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634396;;;","14/Mar/14 20:49;ehans;+1

Please see my minor comments on ReviewBoard;;;","15/Mar/14 08:01;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634677/HIVE-6649.2.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1806/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1806/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.411s]
[INFO] Hive Ant Utilities ................................ SUCCESS [5.908s]
[INFO] Hive Shims Common ................................. SUCCESS [3.685s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.434s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.213s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [3.024s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [8.086s]
[INFO] Hive Shims ........................................ SUCCESS [1.024s]
[INFO] Hive Common ....................................... SUCCESS [6.569s]
[INFO] Hive Serde ........................................ SUCCESS [10.029s]
[INFO] Hive Metastore .................................... SUCCESS [31.428s]
[INFO] Hive Query Language ............................... SUCCESS [1:20.187s]
[INFO] Hive Service ...................................... SUCCESS [8.150s]
[INFO] Hive JDBC ......................................... SUCCESS [3.087s]
[INFO] Hive Beeline ...................................... SUCCESS [2.844s]
[INFO] Hive CLI .......................................... SUCCESS [1.971s]
[INFO] Hive Contrib ...................................... SUCCESS [2.519s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.764s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.589s]
[INFO] Hive HCatalog Core ................................ SUCCESS [2.525s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.388s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.297s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [2.058s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [9.982s]
[INFO] Hive HWI .......................................... SUCCESS [1.091s]
[INFO] Hive ODBC ......................................... SUCCESS [0.819s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.265s]
[INFO] Hive TestUtils .................................... SUCCESS [0.678s]
[INFO] Hive Packaging .................................... FAILURE [1.375s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:33.939s
[INFO] Finished at: Sat Mar 15 04:01:03 EDT 2014
[INFO] Final Memory: 74M/553M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634677;;;","15/Mar/14 16:56;jnp;Uploading same patch to trigger pre-commit build.;;;","15/Mar/14 17:27;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634921/HIVE-6649.2.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1834/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1834/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.750s]
[INFO] Hive Ant Utilities ................................ SUCCESS [5.916s]
[INFO] Hive Shims Common ................................. SUCCESS [3.781s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.504s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.415s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.806s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [7.882s]
[INFO] Hive Shims ........................................ SUCCESS [1.003s]
[INFO] Hive Common ....................................... SUCCESS [11.949s]
[INFO] Hive Serde ........................................ SUCCESS [9.976s]
[INFO] Hive Metastore .................................... SUCCESS [33.143s]
[INFO] Hive Query Language ............................... SUCCESS [1:18.256s]
[INFO] Hive Service ...................................... SUCCESS [6.850s]
[INFO] Hive JDBC ......................................... SUCCESS [2.696s]
[INFO] Hive Beeline ...................................... SUCCESS [2.719s]
[INFO] Hive CLI .......................................... SUCCESS [1.934s]
[INFO] Hive Contrib ...................................... SUCCESS [1.753s]
[INFO] Hive HBase Handler ................................ SUCCESS [3.610s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.533s]
[INFO] Hive HCatalog Core ................................ SUCCESS [3.306s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [1.769s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.824s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [1.836s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [10.207s]
[INFO] Hive HWI .......................................... SUCCESS [1.268s]
[INFO] Hive ODBC ......................................... SUCCESS [0.864s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.331s]
[INFO] Hive TestUtils .................................... SUCCESS [0.585s]
[INFO] Hive Packaging .................................... FAILURE [2.367s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:38.124s
[INFO] Finished at: Sat Mar 15 13:27:28 EDT 2014
[INFO] Final Memory: 74M/419M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634921;;;","15/Mar/14 21:55;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634937/HIVE-6649.2.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1842/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1842/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.663s]
[INFO] Hive Ant Utilities ................................ SUCCESS [5.475s]
[INFO] Hive Shims Common ................................. SUCCESS [3.741s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.610s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.402s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.589s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [8.047s]
[INFO] Hive Shims ........................................ SUCCESS [1.270s]
[INFO] Hive Common ....................................... SUCCESS [7.163s]
[INFO] Hive Serde ........................................ SUCCESS [10.347s]
[INFO] Hive Metastore .................................... SUCCESS [33.518s]
[INFO] Hive Query Language ............................... SUCCESS [1:19.821s]
[INFO] Hive Service ...................................... SUCCESS [7.875s]
[INFO] Hive JDBC ......................................... SUCCESS [3.121s]
[INFO] Hive Beeline ...................................... SUCCESS [2.820s]
[INFO] Hive CLI .......................................... SUCCESS [1.737s]
[INFO] Hive Contrib ...................................... SUCCESS [2.468s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.813s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.521s]
[INFO] Hive HCatalog Core ................................ SUCCESS [2.476s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.471s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.221s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [2.116s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [9.947s]
[INFO] Hive HWI .......................................... SUCCESS [1.004s]
[INFO] Hive ODBC ......................................... SUCCESS [0.915s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.165s]
[INFO] Hive TestUtils .................................... SUCCESS [0.676s]
[INFO] Hive Packaging .................................... FAILURE [1.754s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:36.505s
[INFO] Finished at: Sat Mar 15 17:55:19 EDT 2014
[INFO] Final Memory: 74M/467M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634937;;;","17/Mar/14 18:48;jnp;Ran tests locally. Only failures were show_create_table_serde.q and metadata_only_queries_with_filters.q which are unrelated to this patch.;;;","17/Mar/14 19:05;jnp;I have committed this to trunk.

[~rhbutani] This bug affects hive-0.13 and fails several vectorized queries on DATE. This should be fixed in branch-0.13 as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Permissions are not inherited correctly when tables have multiple partition columns,HIVE-6648,12701165,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,szehon,henryr,henryr,13/Mar/14 05:53,13/Nov/14 19:44,14/Jul/23 06:14,10/Apr/14 14:40,0.12.0,0.13.0,,,,,,,,0.14.0,,,,,,0,,,"{{Warehouse.mkdirs()}} always looks at the immediate parent of the path that it creates when determining what permissions to inherit. However, it may have created that parent directory as well, in which case it will have the default permissions and will not have inherited them.

This is a problem when performing an {{INSERT}} into a table with more than one partition column. E.g., in an empty table:

{code}INSERT INTO TABLE tbl PARTITION(p1=1, p2=2) ... {code}

A new subdirectory /p1=1/p2=2  will be created, and with permission inheritance (per HIVE-2504) enabled, the intention is presumably for both new directories to inherit the root table dir's permissions. However, {{mkdirs()}} will only set the permission of the leaf directory (i.e. /p2=2/), and then only to the permissions of /p1=1/, which was just created.

{code}
public boolean mkdirs(Path f) throws MetaException {
    FileSystem fs = null;
    try {
      fs = getFs(f);
      LOG.debug(""Creating directory if it doesn't exist: "" + f);
      //Check if the directory already exists. We want to change the permission
      //to that of the parent directory only for newly created directories.
      if (this.inheritPerms) {
        try {
          return fs.getFileStatus(f).isDir();
        } catch (FileNotFoundException ignore) {
        }
      }
      boolean success = fs.mkdirs(f);
      if (this.inheritPerms && success) {
        // Set the permission of parent directory.
        // HNR: This is the bug - getParent() may refer to a just-created directory.
        fs.setPermission(f, fs.getFileStatus(f.getParent()).getPermission());
      }
      return success;
    } catch (IOException e) {
      closeFs(fs);
      MetaStoreUtils.logAndThrowMetaException(e);
    }
    return false;
  }
{code}",,brocknoland,cdrome,ctang,henryr,mdominguez@cloudera.com,sushanth,szehon,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6892,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/14 01:48;szehon;HIVE-6648.patch;https://issues.apache.org/jira/secure/attachment/12639322/HIVE-6648.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379511,,,,Thu Nov 13 19:44:10 UTC 2014,,,,,,,,,,"0|i1te4n:",379802,,,,,,,,,,,,,,,,,,,,,"09/Apr/14 01:48;szehon;This patch fixes the issue described.  After creating static partitions (insert into ... partition...), the partition directory and its intermediate-created directories now have the parent's permission.  See newly-added test case.

One note- the fix is in Hive.copyFile().  This JIRA also describes another problematic method (Warehouse.mkdirs()).  It is actually not invoked in this particular code path, and probably I can take a look in a follow-up JIRA about it to fix cases where it is invoked.;;;","09/Apr/14 15:38;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12639322/HIVE-6648.patch

{color:green}SUCCESS:{color} +1 5556 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2190/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2190/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12639322;;;","09/Apr/14 20:39;brocknoland;+1;;;","10/Apr/14 14:40;brocknoland;Thank you Szehon! I have committed this to trunk!;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump the thrift api version to V7 for HiveServer2,HIVE-6647,12701162,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,prasadm,vgumashta,vgumashta,13/Mar/14 05:27,19/Dec/14 06:54,14/Jul/23 06:14,14/Mar/14 21:18,0.13.0,,,,,,,,,0.13.0,,HiveServer2,JDBC,,,0,,,"HIVE-5155 added new api for delegation token support. Per the convention followed till now, we should update the version to 7. 

Marking it as blocker for 13. cc [~prasadm] [~thejas]",,prasadm,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-9006,,,,,,,,"13/Mar/14 08:17;prasadm;HIVE-6647.1.patch;https://issues.apache.org/jira/secure/attachment/12634391/HIVE-6647.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379508,,,,Fri Mar 14 21:18:01 UTC 2014,,,,,,,,,,"0|i1te3z:",379799,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 05:34;thejas;Hi [~prasadm], Will you able to contribute a patch for this change ?
;;;","13/Mar/14 08:17;prasadm;Attached trunk patch, includes generated code.;;;","13/Mar/14 08:27;vgumashta;+1 (non-binding). Thanks a lot [~prasadm]!;;;","13/Mar/14 09:04;thejas;+1;;;","14/Mar/14 13:51;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634391/HIVE-6647.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5394 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1771/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1771/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634391;;;","14/Mar/14 15:43;thejas;testExecuteStatementAsync is a flaky test, documented in HIVE-6543 .
I will commit this shortly.
;;;","14/Mar/14 21:18;thejas;Patch committed to trunk and 0.13 branch (this was a 0.13 blocker). 
Thanks for the contribution Prasad! Thanks for the review Vaibhav!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error in txn handler SQL,HIVE-6646,12701144,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,gates,gates,gates,13/Mar/14 02:00,25/Mar/14 17:21,14/Jul/23 06:14,25/Mar/14 17:21,0.13.0,,,,,,,,,0.13.0,,Metastore,,,,0,,,There are a few places where the SQL in TxnHandler.java works in Derby but not in MySQL due to differences in the way they parse tokens.  Adding spaces to all such places addresses the issue.,,gates,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 03:26;gates;HIVE-6646.patch;https://issues.apache.org/jira/secure/attachment/12634338/HIVE-6646.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379490,,,,Tue Mar 25 17:21:47 UTC 2014,,,,,,,,,,"0|i1tdzz:",379781,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 05:59;ashutoshc;+1;;;","13/Mar/14 18:01;gates;Resubmitting patch to kick off tests.;;;","25/Mar/14 17:13;gates;I ran the tests locally and saw only one failure, in union_top_level.q, which is just an ordering issue of the results and has nothing to do with my changes.;;;","25/Mar/14 17:18;ashutoshc;Sounds good. Will check it in shortly.;;;","25/Mar/14 17:21;ashutoshc;Committed to 0.13 & trunk. Thanks, Alan!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
to_date()/to_unix_timestamp() fail with NPE if input is null,HIVE-6645,12701143,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,13/Mar/14 01:57,20/Mar/14 01:41,14/Jul/23 06:14,20/Mar/14 01:41,,,,,,,,,,0.13.0,,UDF,,,,0,,,"{noformat}
hive> describe tab2;
Query ID = jdere_20140312185454_e3ed213e-8b3a-4963-b815-19965edad587
OK
c1                  	timestamp           	None                
Time taken: 0.155 seconds, Fetched: 1 row(s)
hive> select * from tab2;
Query ID = jdere_20140312185454_8a009070-df79-45de-8642-e85668a378d7
OK
NULL
NULL
NULL
NULL
NULL
Time taken: 0.067 seconds, Fetched: 5 row(s)

hive> select to_unix_timestamp(c1) from tab2;           
hive> select to_date(c1) from tab2;          
{noformat}

Fails with errors like:

{noformat}
java.lang.Exception: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""c1"":null}
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:401)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""c1"":null}
        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:195)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:233)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        at java.lang.Thread.run(Thread.java:680)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""c1"":null}
        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:534)
        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)
        ... 10 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating to_date(c1)
        at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)
        at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)
        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:524)
        ... 11 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFDate.evaluate(GenericUDFDate.java:106)
        at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:166)
        at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:77)
        at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:65)
        at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:79)
        ... 15 more
{noformat}",,jdere,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 02:37;jdere;HIVE-6645.1.patch;https://issues.apache.org/jira/secure/attachment/12634624/HIVE-6645.1.patch","17/Mar/14 17:49;jdere;HIVE-6645.2.patch;https://issues.apache.org/jira/secure/attachment/12635126/HIVE-6645.2.patch","14/Mar/14 20:45;jdere;HIVE-6645.2.patch;https://issues.apache.org/jira/secure/attachment/12634832/HIVE-6645.2.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379489,,,,Thu Mar 20 01:41:27 UTC 2014,,,,,,,,,,"0|i1tdzr:",379780,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 02:37;jdere;patch v1:
- Avoid NPE when there are null inputs
- Also allow char/varchar args to be used in place of string;;;","14/Mar/14 20:45;jdere;Updating patch based on feedback from [~kamrul];;;","15/Mar/14 07:34;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634832/HIVE-6645.2.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1797/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1797/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.727s]
[INFO] Hive Ant Utilities ................................ SUCCESS [5.439s]
[INFO] Hive Shims Common ................................. SUCCESS [3.696s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.526s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.272s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.592s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [8.037s]
[INFO] Hive Shims ........................................ SUCCESS [1.301s]
[INFO] Hive Common ....................................... SUCCESS [6.701s]
[INFO] Hive Serde ........................................ SUCCESS [10.261s]
[INFO] Hive Metastore .................................... SUCCESS [33.503s]
[INFO] Hive Query Language ............................... SUCCESS [1:17.752s]
[INFO] Hive Service ...................................... SUCCESS [7.751s]
[INFO] Hive JDBC ......................................... SUCCESS [3.085s]
[INFO] Hive Beeline ...................................... SUCCESS [2.558s]
[INFO] Hive CLI .......................................... SUCCESS [1.998s]
[INFO] Hive Contrib ...................................... SUCCESS [2.546s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.602s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.508s]
[INFO] Hive HCatalog Core ................................ SUCCESS [1.888s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.481s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.216s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [2.157s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [9.881s]
[INFO] Hive HWI .......................................... SUCCESS [1.179s]
[INFO] Hive ODBC ......................................... SUCCESS [0.802s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.196s]
[INFO] Hive TestUtils .................................... SUCCESS [0.659s]
[INFO] Hive Packaging .................................... FAILURE [1.684s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:32.740s
[INFO] Finished at: Sat Mar 15 03:34:12 EDT 2014
[INFO] Final Memory: 74M/467M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634832;;;","17/Mar/14 17:49;jdere;re-uploading patch for precommit tests;;;","18/Mar/14 23:50;ashutoshc;+1;;;","20/Mar/14 01:04;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635126/HIVE-6645.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5416 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1872/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1872/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635126;;;","20/Mar/14 01:41;ashutoshc;Committed to trunk & 0.13. Thanks, Jason!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
document TestStreaming_2 e2e test case for webhcat,HIVE-6644,12701141,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,13/Mar/14 01:41,25/Mar/14 20:31,14/Jul/23 06:14,25/Mar/14 20:31,0.12.0,,,,,,,,,0.13.0,,WebHCat,,,,0,,,,,ekoifman,gates,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 02:03;ekoifman;HIVE-6644.patch;https://issues.apache.org/jira/secure/attachment/12634328/HIVE-6644.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379487,,,,Tue Mar 25 20:31:20 UTC 2014,,,,,,,,,,"0|i1tdzb:",379778,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 02:03;ekoifman;no pre commit test;;;","25/Mar/14 20:31;gates;Patch committed to trunk and 0.13.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a check for cross products in plans and output a warning,HIVE-6643,12701130,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,rhbutani,rhbutani,rhbutani,12/Mar/14 23:50,08/Oct/15 02:11,14/Jul/23 06:14,29/Mar/14 00:03,,,,,,,,,,0.13.0,,,,,,0,,,"Now that we support old style join syntax, it is easy to write queries that generate a plan with a cross product.
For e.g. say you have A join B join C join D on A.x = B.x and A.y = D.y and C.z = D.z
So the JoinTree is:

A — B
\|__  D — C

Since we don't reorder join graphs, we will end up with a cross product between (A join B) and C",,erwaman,gates,hagleitn,leftyl,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/14 23:51;rhbutani;HIVE-6643.1.patch;https://issues.apache.org/jira/secure/attachment/12634302/HIVE-6643.1.patch","17/Mar/14 00:43;rhbutani;HIVE-6643.2.patch;https://issues.apache.org/jira/secure/attachment/12635022/HIVE-6643.2.patch","24/Mar/14 20:16;rhbutani;HIVE-6643.3.patch;https://issues.apache.org/jira/secure/attachment/12636417/HIVE-6643.3.patch","26/Mar/14 16:39;rhbutani;HIVE-6643.4.patch;https://issues.apache.org/jira/secure/attachment/12636943/HIVE-6643.4.patch","26/Mar/14 20:48;rhbutani;HIVE-6643.5.patch;https://issues.apache.org/jira/secure/attachment/12637001/HIVE-6643.5.patch","28/Mar/14 02:23;rhbutani;HIVE-6643.6.patch;https://issues.apache.org/jira/secure/attachment/12637315/HIVE-6643.6.patch","28/Mar/14 23:57;rhbutani;HIVE-6643.7.patch;https://issues.apache.org/jira/secure/attachment/12637578/HIVE-6643.7.patch",,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379476,,,,Thu Jul 03 09:04:31 UTC 2014,,,,,,,,,,"0|i1tdwv:",379767,Added new config parameter 'hive.exec.check.crossproducts' to warn about Cross Products. By default this check is on.,,,,,,,,,,,,,,,,,,,,"12/Mar/14 23:55;rhbutani;preliminary patch attached. There are some issues with this:
- for MapJoins: the warning only shows the bigTable (even this is not available). Hopefully the Vertex/Stage and Operator information  is enough for the user to see where the cross product is happening.
- for Shuffle Joins: we need the HIVE-4293 changes to show the tables in a Reducer that has more than 1 input.
- the .q.out files have the warnings, but these are appearing before the preehooks for the queries. Is this ok? ;;;","12/Mar/14 23:59;rhbutani;review at: https://reviews.apache.org/r/19165/;;;","14/Mar/14 02:36;hagleitn;Minor comments on rb. This is neat: +1;;;","14/Mar/14 05:19;rhbutani;thanks gunther for the review.
Will make the changes you have suggested.
Submitting patch to check if any existing tests are affected by this check.;;;","15/Mar/14 07:43;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634302/HIVE-6643.1.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1800/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1800/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.918s]
[INFO] Hive Ant Utilities ................................ SUCCESS [5.815s]
[INFO] Hive Shims Common ................................. SUCCESS [3.693s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.572s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.309s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.582s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [7.943s]
[INFO] Hive Shims ........................................ SUCCESS [1.239s]
[INFO] Hive Common ....................................... SUCCESS [6.691s]
[INFO] Hive Serde ........................................ SUCCESS [9.978s]
[INFO] Hive Metastore .................................... SUCCESS [33.677s]
[INFO] Hive Query Language ............................... SUCCESS [1:16.466s]
[INFO] Hive Service ...................................... SUCCESS [7.008s]
[INFO] Hive JDBC ......................................... SUCCESS [3.162s]
[INFO] Hive Beeline ...................................... SUCCESS [2.818s]
[INFO] Hive CLI .......................................... SUCCESS [1.826s]
[INFO] Hive Contrib ...................................... SUCCESS [1.822s]
[INFO] Hive HBase Handler ................................ SUCCESS [3.739s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.514s]
[INFO] Hive HCatalog Core ................................ SUCCESS [2.778s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [1.586s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [2.044s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [1.540s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [10.170s]
[INFO] Hive HWI .......................................... SUCCESS [1.263s]
[INFO] Hive ODBC ......................................... SUCCESS [0.759s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.299s]
[INFO] Hive TestUtils .................................... SUCCESS [0.639s]
[INFO] Hive Packaging .................................... FAILURE [1.617s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:32.251s
[INFO] Finished at: Sat Mar 15 03:43:26 EDT 2014
[INFO] Final Memory: 74M/553M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634302;;;","17/Mar/14 23:24;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635022/HIVE-6643.2.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1864/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1864/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-1864/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'metastore/scripts/upgrade/derby/hive-schema-0.13.0.derby.sql'
Reverted 'metastore/scripts/upgrade/derby/hive-schema-0.14.0.derby.sql'
Reverted 'metastore/scripts/upgrade/mysql/hive-schema-0.13.0.mysql.sql'
Reverted 'metastore/scripts/upgrade/mysql/hive-schema-0.14.0.mysql.sql'
Reverted 'metastore/scripts/upgrade/oracle/hive-schema-0.13.0.oracle.sql'
Reverted 'metastore/scripts/upgrade/oracle/hive-schema-0.14.0.oracle.sql'
Reverted 'metastore/scripts/upgrade/postgres/hive-schema-0.14.0.postgres.sql'
Reverted 'metastore/scripts/upgrade/postgres/hive-schema-0.13.0.postgres.sql'
++ awk '{print $2}'
++ egrep -v '^X|^Performing status on external'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/storage-handlers/hbase/target hcatalog/server-extensions/target hcatalog/core/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target hwi/target common/target common/src/gen contrib/target service/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target
+ svn update
U    itests/qtest/pom.xml
U    common/src/test/org/apache/hadoop/hive/common/type/TestDecimal128.java
U    common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
U    common/src/java/org/apache/hadoop/hive/common/type/Decimal128.java
U    ql/src/test/results/clientpositive/vector_decimal_aggregate.q.out
A    ql/src/test/results/clientpositive/tez/orc_analyze.q.out
A    ql/src/test/results/clientpositive/orc_analyze.q.out
A    ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFPrintf.java
A    ql/src/test/queries/clientpositive/orc_analyze.q
A    ql/src/java/org/apache/hadoop/hive/ql/plan/StatsNoJobWork.java
U    ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java
U    ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java
A    ql/src/java/org/apache/hadoop/hive/ql/exec/StatsNoJobTask.java
U    ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/MathExpr.java
U    ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java
U    ql/src/java/org/apache/hadoop/hive/ql/exec/TaskFactory.java
A    ql/src/java/org/apache/hadoop/hive/ql/io/StatsProvidingRecordReader.java
U    ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java
U    ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
U    ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java
U    ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
U    ql/src/gen/vectorization/UDAFTemplates/VectorUDAFVarDecimal.txt
U    conf/hive-default.xml.template

Fetching external item into 'hcatalog/src/test/e2e/harness'
Updated external to revision 1578659.

Updated to revision 1578659.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635022;;;","26/Mar/14 18:39;gates;I see the following errors when I run the tests:
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join0
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join23
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_filters
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_nulls
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_12
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_view
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cross_join
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input23
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join0
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join23
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_alt_syntax
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_1
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_3
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_unqual1
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_unqual3
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_filters
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_nulls
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_no_hooks
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonblock_op_deduplicate
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_repeated_alias
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_print_header
2014-03-26 17:33:15 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample8;;;","26/Mar/14 20:48;rhbutani;fix .q.out diffs;;;","27/Mar/14 04:29;gates;I still see the following failures when running the tests on the latest patch:
2014-03-27 01:20:45 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input23
2014-03-27 01:20:45 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join0
2014-03-27 01:20:45 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join23;;;","28/Mar/14 02:24;rhbutani;fix more .q.out files;;;","28/Mar/14 22:54;thejas;Ran tests locally and they passed.
;;;","28/Mar/14 23:57;rhbutani;fix merge conflicts;;;","29/Mar/14 00:03;rhbutani;Committed to trunk and 0.13
Gunther thanks for the review
Thejas thanks for running the tests.;;;","29/Mar/14 02:41;leftyl;This adds *hive.exec.check.crossproducts* to HiveConf.java and hive-default.xml.template, so I'll add it to the wiki and include it in HIVE-6586 so the description can be put in the new HiveConf.java after HIVE-6037 gets committed.;;;","29/Mar/14 02:44;leftyl;A release note for *hive.exec.check.crossproducts* would also be good.;;;","30/Mar/14 16:36;rhbutani;Thanks for updating the wiki.
Added the Release Note.;;;","03/Jul/14 09:04;leftyl;*hive.exec.check.crossproducts* is documented in the wiki here:

* [Configuration Properties -- hive.exec.check.crossproducts | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.check.crossproducts]

I added a comment to HIVE-6586 so it won't get lost in the shuffle when HIVE-6037 changes HiveConf.java.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Query fails to vectorize when a non string partition column is part of the query expression,HIVE-6642,12701113,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hsubramaniyan,hsubramaniyan,hsubramaniyan,12/Mar/14 22:15,05/Nov/14 18:55,14/Jul/23 06:14,31/Mar/14 21:04,,,,,,,,,,0.13.0,,,,,,0,,,"drop table if exists alltypesorc_part;

CREATE TABLE alltypesorc_part (
ctinyint tinyint,
csmallint smallint,
cint int,
cbigint bigint,
cfloat float,
cdouble double,
cstring1 string,
cstring2 string,
ctimestamp1 timestamp,
ctimestamp2 timestamp,
cboolean1 boolean,
cboolean2 boolean) partitioned by (ds int) STORED AS ORC;

insert overwrite table alltypesorc_part partition (ds=2011) select * from alltypesorc limit 100;
insert overwrite table alltypesorc_part partition (ds=2012) select * from alltypesorc limit 200;

explain select *
from (select ds from alltypesorc_part) t1,
     alltypesorc t2
where t1.ds = t2.cint
order by t2.ctimestamp1
limit 100;

The above query fails to vectorize because (select ds from alltypesorc_part) t1 returns a string column and the join equality on t2 is performed on an int column. The correct output when vectorization is turned on should be:
STAGE DEPENDENCIES:
  Stage-5 is a root stage
  Stage-2 depends on stages: Stage-5
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-5
    Map Reduce Local Work
      Alias -> Map Local Tables:
        t1:alltypesorc_part
          Fetch Operator
            limit: -1
      Alias -> Map Local Operator Tree:
        t1:alltypesorc_part
          TableScan
            alias: alltypesorc_part
            Statistics: Num rows: 300 Data size: 62328 Basic stats: COMPLETE Column stats: COMPLETE
            Select Operator
              expressions: ds (type: int)
              outputColumnNames: _col0
              Statistics: Num rows: 300 Data size: 1200 Basic stats: COMPLETE Column stats: COMPLETE
              HashTable Sink Operator
                condition expressions:
                  0 {_col0}
                  1 {ctinyint} {csmallint} {cint} {cbigint} {cfloat} {cdouble} {cstring1} {cstring2} {ctimestamp1} {ctimestamp2} {cboolean1} {cboolean2}
                keys:
                  0 _col0 (type: int)
                  1 cint (type: int)

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: t2
            Statistics: Num rows: 3536 Data size: 1131711 Basic stats: COMPLETE Column stats: NONE
            Map Join Operator
              condition map:
                   Inner Join 0 to 1
              condition expressions:
                0 {_col0}
                1 {ctinyint} {csmallint} {cint} {cbigint} {cfloat} {cdouble} {cstring1} {cstring2} {ctimestamp1} {ctimestamp2} {cboolean1} {cboolean2}
              keys:
                0 _col0 (type: int)
                1 cint (type: int)
              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
              Statistics: Num rows: 3889 Data size: 1244882 Basic stats: COMPLETE Column stats: NONE
              Filter Operator
                predicate: (_col0 = _col3) (type: boolean)
                Statistics: Num rows: 1944 Data size: 622280 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col0 (type: int), _col1 (type: tinyint), _col2 (type: smallint), _col3 (type: int), _col4 (type: bigint), _col5 (type: float), _col6 (type: double), _col7 (type: string), _col8 (type: string), _col\
9 (type: timestamp), _col10 (type: timestamp), _col11 (type: boolean), _col12 (type: boolean)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                  Statistics: Num rows: 1944 Data size: 622280 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col9 (type: timestamp)
                    sort order: +
                    Statistics: Num rows: 1944 Data size: 622280 Basic stats: COMPLETE Column stats: NONE
                    value expressions: _col0 (type: int), _col1 (type: tinyint), _col2 (type: smallint), _col3 (type: int), _col4 (type: bigint), _col5 (type: float), _col6 (type: double), _col7 (type: string), _col8 (type: strin\
g), _col9 (type: timestamp), _col10 (type: timestamp), _col11 (type: boolean), _col12 (type: boolean)
      Local Work:
        Map Reduce Local Work
      Execution mode: vectorized
      Reduce Operator Tree:
        Extract
          Statistics: Num rows: 1944 Data size: 622280 Basic stats: COMPLETE Column stats: NONE
          Limit
            Number of rows: 100
            Statistics: Num rows: 100 Data size: 32000 Basic stats: COMPLETE Column stats: NONE
            File Output Operator
              compressed: false
              Statistics: Num rows: 100 Data size: 32000 Basic stats: COMPLETE Column stats: NONE
              table:
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 100
where as with the current code, vectorization fails to take place because of the following exception
14/03/12 14:43:19 DEBUG vector.VectorizationContext: No vector udf found for GenericUDFOPEqual, descriptor: Argument Count = 2, mode = FILTER, Argument Types = {STRING,LONG}, Input Expression Types = {COLUMN,COLUMN}
14/03/12 14:43:19 DEBUG physical.Vectorizer: Failed to vectorize
org.apache.hadoop.hive.ql.metadata.HiveException: Udf: GenericUDFOPEqual, is not supported
	at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getGenericUdfVectorExpression(VectorizationContext.java:854)
	at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpression(VectorizationContext.java:300)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(Vectorizer.java:682)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateFilterOperator(Vectorizer.java:606)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateOperator(Vectorizer.java:537)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$ValidationNodeProcessor.process(Vectorizer.java:367)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:78)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:132)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:109)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.validateMapWork(Vectorizer.java:314)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.convertMapWork(Vectorizer.java:283)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.dispatch(Vectorizer.java:270)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:194)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:139)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.resolve(Vectorizer.java:519)
	at org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.optimize(PhysicalOptimizer.java:100)
	at org.apache.hadoop.hive.ql.parse.MapReduceCompiler.optimizeTaskPlan(MapReduceCompiler.java:290)
	at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:216)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9286)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
	at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:64)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:398)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:294)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:948)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:996)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:884)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:874)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:424)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:359)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:457)
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:467)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:125)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:424)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:793)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:687)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:626)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:160)
",,gates,hsubramaniyan,qwertymaniac,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6775,HIVE-6778,,,,,,,,,,,,HIVE-8099,,,,,,,,"21/Mar/14 22:48;hsubramaniyan;HIVE-6642-2.patch;https://issues.apache.org/jira/secure/attachment/12636135/HIVE-6642-2.patch","24/Mar/14 19:15;hsubramaniyan;HIVE-6642-3.patch;https://issues.apache.org/jira/secure/attachment/12636407/HIVE-6642-3.patch","26/Mar/14 23:26;hsubramaniyan;HIVE-6642-4.patch;https://issues.apache.org/jira/secure/attachment/12637048/HIVE-6642-4.patch","20/Mar/14 20:17;hsubramaniyan;HIVE-6642.1.patch;https://issues.apache.org/jira/secure/attachment/12635859/HIVE-6642.1.patch","28/Mar/14 22:53;hsubramaniyan;HIVE-6642.5.patch;https://issues.apache.org/jira/secure/attachment/12637563/HIVE-6642.5.patch","30/Mar/14 06:33;hsubramaniyan;HIVE-6642.6.patch;https://issues.apache.org/jira/secure/attachment/12637695/HIVE-6642.6.patch","31/Mar/14 18:23;hsubramaniyan;HIVE-6642.7.patch;https://issues.apache.org/jira/secure/attachment/12637892/HIVE-6642.7.patch",,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379459,,,,Mon Mar 31 21:04:36 UTC 2014,,,,,,,,,,"0|i1tdt3:",379750,,,,,,,,,,,,,,,,,,,,,"20/Mar/14 20:17;hsubramaniyan;cc-ing [~rhbutani] [~jnp] for review;;;","20/Mar/14 20:22;hsubramaniyan;https://reviews.apache.org/r/19492/;;;","21/Mar/14 22:48;hsubramaniyan;cc-ing [~rhbutani] for reviewing the new patch;;;","24/Mar/14 20:06;rhbutani; lgtm; +1 subject to adding new tests ;;;","26/Mar/14 23:26;hsubramaniyan;added test cases as well.;;;","27/Mar/14 04:45;gates;Saw the following failures:
{code}
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_coltype
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_1
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_12
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_2
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_3
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_4
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_7
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_8
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket3
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_1
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_2
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_3
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_4
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_6
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_7
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_8
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin1
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin10
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin11
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin12
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin13
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin2
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin3
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin5
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin8
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin9
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin_negative
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin_negative2
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_columnstats_partlvl
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_combine2_hadoop20
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynamic_partition_skip_default
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_map_ppr
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_map_ppr_multi_distinct
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_ppr
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_ppr_multi_distinct
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_6
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input23
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input42
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part1
014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part2
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part7
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part9
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join26
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32_lessSize
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join33
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join9
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_map_ppr
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part8
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_louter_join_ppr
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge3
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition2
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadataonly1
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_outer_join_ppr
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_varchar2
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pcr
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_union_view
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_vc
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_allchildsarenull
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_push_or
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner2
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner3
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_router_join_ppr
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample1
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample10
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample8
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_11
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_12
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_5
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_6
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_7
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats11
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats12
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats13
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr1
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr2
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union22
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin7
2014-03-27 02:49:33 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_reduce_deduplicate
{code};;;","28/Mar/14 22:53;hsubramaniyan;Modified the out files. There is an existing issue associated with the introduction of the changes:
1. altering a partition column type and then trying to query on the previous column type which is covered by HIVE-6775. As of now,  I have invalidated the tests in alter_partition_coltype.q;;;","29/Mar/14 03:53;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637563/HIVE-6642.5.patch

{color:red}ERROR:{color} -1 due to 35 failed/errored test(s), 5503 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_ppr_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_allchildsarenull
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin7
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_list_bucket_dml_10
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_reduce_deduplicate
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input2
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input3
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input_part1
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample1
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2024/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2024/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 35 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637563;;;","29/Mar/14 20:18;hsubramaniyan;more .q.out file changes;;;","30/Mar/14 01:34;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637667/HIVE-6642.6.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2038/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2038/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-2038/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestHiveSchemaConverter.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveSchemaConverter.java'
++ awk '{print $2}'
++ egrep -v '^X|^Performing status on external'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/storage-handlers/hbase/target hcatalog/server-extensions/target hcatalog/core/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target hwi/target common/target common/src/gen service/target contrib/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target
+ svn update
U    jdbc/src/java/org/apache/hive/jdbc/HttpKerberosRequestInterceptor.java

Fetching external item into 'hcatalog/src/test/e2e/harness'
Updated external to revision 1583097.

Updated to revision 1583097.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637667;;;","30/Mar/14 11:08;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637695/HIVE-6642.6.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5502 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_8
org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.testRevokeTimedOutWorkers
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input2
org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.testSimplePrivileges
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2043/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2043/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637695;;;","31/Mar/14 20:45;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637892/HIVE-6642.7.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5513 tests executed
*Failed tests:*
{noformat}
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2051/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2051/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637892;;;","31/Mar/14 20:49;hsubramaniyan;The above failure is unrelated to my change.;;;","31/Mar/14 21:04;rhbutani;committed to trunk and 0.13
thanks Hari.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
optimized HashMap keys won't work correctly with decimals,HIVE-6641,12701106,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sershe,sershe,sershe,12/Mar/14 21:50,18/Mar/14 18:39,14/Jul/23 06:14,18/Mar/14 18:39,,,,,,,,,,0.13.0,,,,,,0,,,"Decimal values with can be equal while having different byte representations (different precision/scale), so comparing bytes is not enough. For a quick fix, we can disable this for decimals",,gopalv,hagleitn,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6691,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 16:16;sershe;HIVE-6641.patch;https://issues.apache.org/jira/secure/attachment/12634457/HIVE-6641.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379452,,,,Tue Mar 18 18:39:44 UTC 2014,,,,,,,,,,"0|i1tdrj:",379743,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 16:16;sershe;Disable the key usage for decimal, add test. Alternativelty, since the keys are only used for equality compares in binary form, and never deserialized, we could discard trailing zeroes (we can disregard precision/scale). We can consider it later...;;;","14/Mar/14 17:26;sershe;[~gopalv] [~hagleitn] can you guys review? it's a small patch, most of it is a new .q.out;;;","14/Mar/14 22:53;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634457/HIVE-6641.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5397 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_map_operators
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1777/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1777/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634457;;;","17/Mar/14 19:53;hagleitn;+1 LGTM. However, can you open a follow up jira to properly fix?

Also, are you sure this is the only datatype suffering from this?;;;","18/Mar/14 00:42;sershe;pretty sure...;;;","18/Mar/14 18:39;sershe;in trunk and 13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change hive.version.shortname in hive 0.13 branch to '0.13.0',HIVE-6640,12701085,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,Fixed,rhbutani,rhbutani,rhbutani,12/Mar/14 20:59,14/Mar/14 22:12,14/Jul/23 06:14,14/Mar/14 22:12,,,,,,,,,,0.13.0,,,,,,0,,,[~jdere] thanks for pointing this out.,,jdere,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 21:59;rhbutani;HIVE-6640.1.patch;https://issues.apache.org/jira/secure/attachment/12634851/HIVE-6640.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379431,,,,Fri Mar 14 22:12:15 UTC 2014,,,,,,,,,,"0|i1tdmv:",379722,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 22:03;jdere;simple enough, +1;;;","14/Mar/14 22:12;rhbutani;Committed to 0.13 branch
thanks Jason;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vectorization: Partition column names are not picked up.,HIVE-6639,12701072,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jnp,jnp,jnp,12/Mar/14 20:06,18/Mar/14 20:18,14/Jul/23 06:14,18/Mar/14 20:18,0.13.0,,,,,,,,,0.13.0,,,,,,0,,,"The vectorized plan generation finds the list of partitioning columns from pruned-partition-list using table scan operator. In some cases the list is coming as null. TPCDS query 27 can reproduce this issue if the store_sales table is partitioned on ss_store_sk. The exception stacktrace is :

{code}
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getInputColumnIndex(VectorizationContext.java:166)
	at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getColumnVectorExpression(VectorizationContext.java:240)
	at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpression(VectorizationContext.java:287)
	at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpressions(VectorizationContext.java:267)
	at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpressions(VectorizationContext.java:255)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.<init>(VectorMapJoinOperator.java:116)
	... 42 more

{code}
",,jnp,rhbutani,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 00:34;jnp;HIVE-6639.2.patch;https://issues.apache.org/jira/secure/attachment/12634312/HIVE-6639.2.patch","13/Mar/14 22:49;jnp;HIVE-6639.3.patch;https://issues.apache.org/jira/secure/attachment/12634572/HIVE-6639.3.patch","13/Mar/14 23:24;jnp;HIVE-6639.4.patch;https://issues.apache.org/jira/secure/attachment/12634585/HIVE-6639.4.patch","15/Mar/14 17:29;jnp;HIVE-6639.5.patch;https://issues.apache.org/jira/secure/attachment/12634923/HIVE-6639.5.patch","14/Mar/14 09:20;jnp;HIVE-6639.5.patch;https://issues.apache.org/jira/secure/attachment/12634679/HIVE-6639.5.patch","15/Mar/14 18:13;jnp;HIVE-6639.6.patch;https://issues.apache.org/jira/secure/attachment/12634926/HIVE-6639.6.patch",,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379418,,,,Tue Mar 18 20:18:30 UTC 2014,,,,,,,,,,"0|i1tdjz:",379709,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 23:27;jnp;The patch gets all the columns from the row schema eliminating the need to look at pruned partition list. ;;;","14/Mar/14 09:24;jnp;Review board: https://reviews.apache.org/r/19219/;;;","15/Mar/14 07:03;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634679/HIVE-6639.5.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1790/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1790/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.575s]
[INFO] Hive Ant Utilities ................................ SUCCESS [6.352s]
[INFO] Hive Shims Common ................................. SUCCESS [3.768s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.518s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.415s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.634s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [8.160s]
[INFO] Hive Shims ........................................ SUCCESS [1.274s]
[INFO] Hive Common ....................................... SUCCESS [6.899s]
[INFO] Hive Serde ........................................ SUCCESS [10.002s]
[INFO] Hive Metastore .................................... SUCCESS [32.773s]
[INFO] Hive Query Language ............................... SUCCESS [1:13.532s]
[INFO] Hive Service ...................................... SUCCESS [7.827s]
[INFO] Hive JDBC ......................................... SUCCESS [2.858s]
[INFO] Hive Beeline ...................................... SUCCESS [2.582s]
[INFO] Hive CLI .......................................... SUCCESS [1.884s]
[INFO] Hive Contrib ...................................... SUCCESS [1.634s]
[INFO] Hive HBase Handler ................................ SUCCESS [3.634s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.551s]
[INFO] Hive HCatalog Core ................................ SUCCESS [2.021s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [1.631s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.443s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [2.084s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [9.903s]
[INFO] Hive HWI .......................................... SUCCESS [1.275s]
[INFO] Hive ODBC ......................................... SUCCESS [0.760s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.195s]
[INFO] Hive TestUtils .................................... SUCCESS [0.602s]
[INFO] Hive Packaging .................................... FAILURE [2.214s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:28.664s
[INFO] Finished at: Sat Mar 15 03:03:15 EDT 2014
[INFO] Final Memory: 74M/424M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634679;;;","15/Mar/14 17:29;jnp;Uploading same patch to trigger pre-commit build.;;;","15/Mar/14 18:47;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634926/HIVE-6639.6.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1837/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1837/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.889s]
[INFO] Hive Ant Utilities ................................ SUCCESS [5.465s]
[INFO] Hive Shims Common ................................. SUCCESS [3.687s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.541s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.216s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.585s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [7.907s]
[INFO] Hive Shims ........................................ SUCCESS [1.293s]
[INFO] Hive Common ....................................... SUCCESS [6.898s]
[INFO] Hive Serde ........................................ SUCCESS [9.743s]
[INFO] Hive Metastore .................................... SUCCESS [32.047s]
[INFO] Hive Query Language ............................... SUCCESS [1:19.040s]
[INFO] Hive Service ...................................... SUCCESS [8.117s]
[INFO] Hive JDBC ......................................... SUCCESS [3.094s]
[INFO] Hive Beeline ...................................... SUCCESS [2.638s]
[INFO] Hive CLI .......................................... SUCCESS [1.968s]
[INFO] Hive Contrib ...................................... SUCCESS [2.506s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.647s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.560s]
[INFO] Hive HCatalog Core ................................ SUCCESS [1.885s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.583s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.558s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [1.237s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [9.843s]
[INFO] Hive HWI .......................................... SUCCESS [1.228s]
[INFO] Hive ODBC ......................................... SUCCESS [0.914s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.217s]
[INFO] Hive TestUtils .................................... SUCCESS [0.625s]
[INFO] Hive Packaging .................................... FAILURE [1.855s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:32.390s
[INFO] Finished at: Sat Mar 15 14:47:25 EDT 2014
[INFO] Final Memory: 74M/467M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634926;;;","17/Mar/14 18:00;vikram.dixit;LGTM +1.;;;","17/Mar/14 18:49;jnp;Ran tests locally. Only failures were show_create_table_serde.q and metadata_only_queries_with_filters.q which are unrelated to this patch.;;;","18/Mar/14 18:31;jnp;I have committed this to trunk.

[~rhbutani] This bug affects hive-0.13 and fails queries having partitioned columns but no filters. This should be fixed in branch-0.13 as well.
;;;","18/Mar/14 18:34;rhbutani;+1 for 0.13;;;","18/Mar/14 20:18;jnp;I have committed this to trunk and branch-0.13.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDF in_file() doesn't take CHAR or VARCHAR as input,HIVE-6637,12701046,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,singhashish,xuefuz,xuefuz,12/Mar/14 18:00,13/Nov/14 19:43,14/Jul/23 06:14,14/Jul/14 23:08,0.14.0,,,,,,,,,0.14.0,,Types,UDF,,,0,,,"{code}
hive> desc alter_varchar_1;
key                 	string              	None                
value               	varchar(3)          	None                
key2                	int                 	None                
value2              	varchar(10)         	None                
hive> select in_file(value, value2) from alter_varchar_1;
FAILED: SemanticException [Error 10016]: Line 1:15 Argument type mismatch 'value': The 1st argument of function IN_FILE must be a string but org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveVarcharObjectInspector@10f1f34a was given.
{code}",,leftyl,singhashish,szehon,thejas,wilbur.yang,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/14 18:57;singhashish;HIVE-6637.1.patch;https://issues.apache.org/jira/secure/attachment/12651470/HIVE-6637.1.patch","21/Jun/14 01:29;singhashish;HIVE-6637.2.patch;https://issues.apache.org/jira/secure/attachment/12651789/HIVE-6637.2.patch","26/Jun/14 17:10;singhashish;HIVE-6637.3.patch;https://issues.apache.org/jira/secure/attachment/12652648/HIVE-6637.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379392,,,,Thu Nov 13 19:43:46 UTC 2014,,,,,,,,,,"0|i1tde7:",379683,,,,,,,,,,,,TODOC14,,,,,,,,,"19/Jun/14 00:24;wilbur.yang;Since in_file() apparently only accepts a constant string as the second argument, I think the description as specified shouldn't work because the second argument is not constant. Is this correct?;;;","19/Jun/14 04:13;singhashish;[~wilbur.yang] Yes, it is correct. However, there is no reason to not allow varchar or char as first argument.;;;","19/Jun/14 04:14;singhashish;RB: https://reviews.apache.org/r/22772/

;;;","19/Jun/14 17:16;wilbur.yang;Regarding the currently submitted patch: I agree with the type checking for both arguments, and I think that this code will work in terms of functionality. However, I believe that there's a slight problem -- if a user runs a query with, say, an INT as the second argument, then the error message will be ""The 2nd argument of function IN_FILE must be a string, char or varchar..."" when it really must be a constant string.;;;","19/Jun/14 18:58;singhashish;[~wilbur.yang] good point. Updated the patch and RB.;;;","20/Jun/14 21:33;singhashish;Updated based on review.;;;","21/Jun/14 03:26;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12651738/HIVE-6637.2.patch

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 5668 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_in_file
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_binary_storage_queries
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/535/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/535/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-535/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12651738;;;","21/Jun/14 05:08;singhashish;Test errors do not look related.;;;","22/Jun/14 04:28;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12651789/HIVE-6637.2.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5653 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_multi_insert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_in_file
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/546/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/546/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-546/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12651789;;;","22/Jun/14 05:36;singhashish;[~xuefuz] could you take a look when you get a chance.;;;","22/Jun/14 17:29;xuefuz;+1;;;","26/Jun/14 02:14;singhashish;Thanks [~xuefuz] and [~wilbur.yang] for reviewing.;;;","26/Jun/14 06:17;szehon;Udf_infile test is failing, which looks related..  Can you make sure it passes first before commit?;;;","26/Jun/14 17:10;singhashish;Add missing data file.;;;","26/Jun/14 17:13;singhashish;[~szehon] sorry about not catching this earlier. The patch was missing a data file. Updated patch and rb.;;;","26/Jun/14 22:45;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12652648/HIVE-6637.3.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5669 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/605/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/605/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-605/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12652648;;;","26/Jun/14 22:50;xuefuz;I left some minor comment on RB.;;;","27/Jun/14 19:16;xuefuz;+1;;;","27/Jun/14 20:33;singhashish;[~xuefuz], thanks again for reviewing :);;;","08/Jul/14 23:05;singhashish;[~xuefuz], if this looks good could this be pushed into trunk?;;;","14/Jul/14 23:08;xuefuz;Patch committed to trunk. Thanks to Ashish for the contribution.;;;","15/Jul/14 07:25;leftyl;This should be documented here, with a release note:

* [LanguageManual -- Operators and UDFs -- String Functions | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-StringFunctions];;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/user/hive is a bad default for HDFS jars path for Tez,HIVE-6636,12701040,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sershe,sershe,sershe,12/Mar/14 17:53,18/May/14 01:40,14/Jul/23 06:14,14/Mar/14 18:01,,,,,,,,,,0.13.0,,,,,,0,,,"If user runs hive under the user name that is not ""hive"", jobs will fail until everyone is granted write access to /user/hive, which is not nice.",,hagleitn,leftyl,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-5003,HIVE-6172,HIVE-6098,,,,,,,,,,,,,,,,,,,"12/Mar/14 18:25;sershe;HIVE-6636.01.patch;https://issues.apache.org/jira/secure/attachment/12634219/HIVE-6636.01.patch","12/Mar/14 20:26;sershe;HIVE-6636.02.patch;https://issues.apache.org/jira/secure/attachment/12634245/HIVE-6636.02.patch","12/Mar/14 18:02;sershe;HIVE-6636.patch;https://issues.apache.org/jira/secure/attachment/12634212/HIVE-6636.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379386,,,,Sun May 18 01:40:42 UTC 2014,,,,,,,,,,"0|i1tdcv:",379677,"Hive on Tez will now use /user/<current Hadoop user name>, rather than just /user/hive, as default HDFS directory for jars. {{hive.jar.directory}} can still be used to set the path explicitly.",,,,,,,,,,,,,,,,,,,,"12/Mar/14 18:14;hagleitn;Looks good except:

- You're setting the var to """", I think that means that your null check won't catch that the var wasn't set and you really catch it only in the fs.status call. Why not set it to null in the first place?
- You should update the hive-default.xml.template too;;;","12/Mar/14 18:19;sershe;1 - last-minute change, will fix; 2 - will do;;;","12/Mar/14 18:21;sershe;1 actually I'm passing the default explicitly:
+    String hdfsDirPathStr = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_JAR_DIRECTORY, null);
;;;","12/Mar/14 18:58;hagleitn;Passing the default means that if the conf doesn't have it return null. But you set it explicitely in HiveConf. So that will never trigger. At least that's my understanding.;;;","12/Mar/14 20:26;sershe;i think it works the other way around, overriding the default default; anyway I can change it;;;","13/Mar/14 23:19;hagleitn;ugh. you're right. it does replace the default value. +1;;;","14/Mar/14 18:01;sershe;committed to trunk and 0.13 branch;;;","18/May/14 01:40;leftyl;Documented in the wiki:

* [hive.jar.directory | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.jar.directory];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Heartbeats are not being sent when DbLockMgr is used and an operation holds locks,HIVE-6635,12701036,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,gates,gates,gates,12/Mar/14 17:47,09/Nov/15 18:52,14/Jul/23 06:14,24/Mar/14 17:57,0.13.0,,,,,,,,,0.13.0,,Locking,,,,0,,,The new DbLockManager depends on heartbeats from the client in order to determine that a lock has not timed out.  The client is not currently sending those heartbeats.,,gates,wzheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-12366,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 04:51;gates;HIVE-6635.2.patch;https://issues.apache.org/jira/secure/attachment/12634643/HIVE-6635.2.patch","22/Mar/14 18:06;gates;HIVE-6635.3.patch;https://issues.apache.org/jira/secure/attachment/12636208/HIVE-6635.3.patch","12/Mar/14 23:12;gates;HIVE-6635.patch;https://issues.apache.org/jira/secure/attachment/12634298/HIVE-6635.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379382,,,,Mon Mar 24 17:57:17 UTC 2014,,,,,,,,,,"0|i1tdbz:",379673,,,,,,,,,,,,,,,,,,,,,"12/Mar/14 23:12;gates;Added a thread to Driver to send heartbeats.  This thread only runs during the main loop in Driver.execute.  I added this in a separate thread because otherwise I would have needed to add threads in every task to see if heartbeats needed to be sent.  This would be very invasive, and also it's not clear it would be possible to cover all cases as there are actions that may simply take a long time (like certain metastore operations).  The downside is that a query will keep running even after it's found out it's locks were aborted and only be terminated at the end.;;;","12/Mar/14 23:14;gates;Review board: https://reviews.apache.org/r/19161/;;;","13/Mar/14 00:09;gates;[~rhbutani] This patch should go into the 0.13 branch as well.;;;","13/Mar/14 00:30;ashutoshc;This will double # of threads for HS2 and worse these threads are not coming from shared threadpool of HS2. How about putting heartbeat while we are waiting for MR job to finish. There is a while loop waiting polling for MR job status. I think we can put heartbeating there. This is in HadoopJobExecHelper::progress();;;","13/Mar/14 03:29;gates;Have we solved all the places that DDL operations can take an hour plus?  And is this same job helper used by Tez or do we need to do it separately in that case?;;;","13/Mar/14 15:33;ashutoshc;I don't remember about any DDL operation taking that long. I don't think this job helper is used by Tez. For, Tez we need to put this somewhere else, need to dig into to find where.;;;","14/Mar/14 04:51;gates;New version of the patch reworked according the approach indicated by Ashutosh.;;;","14/Mar/14 15:01;ashutoshc;+1;;;","15/Mar/14 07:39;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634643/HIVE-6635.2.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1799/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1799/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-1799/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'conf/hive-default.xml.template'
Reverted 'itests/qtest/pom.xml'
Reverted 'common/src/java/org/apache/hadoop/hive/conf/HiveConf.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/TaskFactory.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java'
++ awk '{print $2}'
++ egrep -v '^X|^Performing status on external'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target hbase-handler/target testutils/target jdbc/target metastore/target hcatalog/target hcatalog/server-extensions/target hcatalog/core/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target hwi/target common/target common/src/gen common/src/java/org/apache/hadoop/hive/conf/HiveConf.java.orig contrib/target service/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target ql/src/test/results/clientpositive/orc_analyze.q.out ql/src/test/results/clientpositive/tez/orc_analyze.q.out ql/src/test/queries/clientpositive/orc_analyze.q ql/src/java/org/apache/hadoop/hive/ql/plan/StatsNoJobWork.java ql/src/java/org/apache/hadoop/hive/ql/exec/StatsNoJobTask.java ql/src/java/org/apache/hadoop/hive/ql/io/StatsProvidingRecordReader.java
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1577797.

At revision 1577797.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634643;;;","15/Mar/14 22:29;gates;Resubmitting patch after build break to get tests run.;;;","22/Mar/14 18:05;gates;Patch no longer applies.  I'll upload a new version.;;;","24/Mar/14 16:54;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12636208/HIVE-6635.3.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5443 tests executed
*Failed tests:*
{noformat}
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1950/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1950/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12636208;;;","24/Mar/14 17:57;ashutoshc;Committed to trunk & 0.13. Thanks, Alan!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig -useHCatalog with embedded metastore fails to pass command line args to metastore,HIVE-6633,12701030,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ehans,ehans,ehans,12/Mar/14 17:26,28/Aug/14 21:21,14/Jul/23 06:14,28/Aug/14 20:19,0.11.0,0.12.0,0.13.0,0.14.0,,,,,,0.13.0,,HCatalog,,,,0,,,"This fails because the embedded metastore can't connect to the database because the command line -D arguments passed to pig are not getting passed to the metastore when the embedded metastore is created. Using hive.metastore.uris set to the empty string causes creation of an embedded metastore.

pig -useHCatalog ""-Dhive.metastore.uris="" ""-Djavax.jdo.option.ConnectionPassword=AzureSQLDBXYZ""

The goal is to allow a pig job submitted via WebHCat to specify a metastore to use via job arguments. That is not working because it is not possible to pass Djavax.jdo.option.ConnectionPassword and other necessary arguments to the embedded metastore.
",,daijy,ehans,ekoifman,rhbutani,sushanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7901,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/14 18:32;ehans;HIVE-6633.01.patch;https://issues.apache.org/jira/secure/attachment/12634222/HIVE-6633.01.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379376,,,,Thu Aug 28 21:21:11 UTC 2014,,,,,,,,,,"0|i1tdan:",379667,,,,,,,,,,,,,,,,,,,,,"12/Mar/14 18:38;ehans;Code review at https://reviews.apache.org/r/19140/;;;","18/Mar/14 00:24;sushanth;Looks good to me. Once the precommit tests on this pass, I can commit it.;;;","18/Mar/14 00:24;sushanth;(to be explicit) +1;;;","20/Mar/14 22:26;sushanth;Committed to trunk after local testing, since the precommit queue is pretty long.;;;","20/Mar/14 22:26;sushanth;Thanks for the fix, Eric!;;;","28/Mar/14 17:09;ehans;[~thejas] Can you commit this to 0.13 please?;;;","28/Mar/14 18:15;ehans;[~rhbutani] Can you approve this to go into 0.13 please?;;;","28/Mar/14 18:30;rhbutani;+1 for 0.13;;;","28/Mar/14 21:45;sushanth;Committed to 0.13. Thanks Eric and Harish!;;;","28/Mar/14 23:50;ehans;Sushanth, thanks for getting this in to 0.13!;;;","26/Aug/14 21:26;ekoifman;This patch is missing from trunk;;;","28/Aug/14 20:16;sushanth;Hi,

I'm afraid this patch modified only org.apache.hcatalog.* files, and those were pruned out with the removal of deprecated classes for 0.14. So this patch was committed to trunk, but the changes effectively blown away.

To make commit log tracking easier in this scenario, I'm going to clone this jira to track addition of this same patch to org.apache.hive.hcatalog as well. Eric, could you please regenerate your patch for that?;;;","28/Aug/14 20:19;sushanth;Re-marking as resolved, and tracking the org.apache.hive.hcatalog issue on HIVE-7901;;;","28/Aug/14 21:21;ehans;Thanks Sushanth for tracking down the problem. I'll regenerate the patch and track that on HIVE-7901.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FS based stats collection have issues for list bucketing case,HIVE-6630,12700995,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,12/Mar/14 15:55,13/Mar/14 22:51,14/Jul/23 06:14,13/Mar/14 22:51,0.13.0,,,,,,,,,0.13.0,,Statistics,,,,0,,,"We need not to track per directory stats in FS based stats collection mechanism, which other stats collection mechanism need to do.",,hagleitn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 07:29;ashutoshc;HIVE-6630.2.patch;https://issues.apache.org/jira/secure/attachment/12634380/HIVE-6630.2.patch","12/Mar/14 16:00;ashutoshc;HIVE-6630.patch;https://issues.apache.org/jira/secure/attachment/12634191/HIVE-6630.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379341,,,,Thu Mar 13 22:51:33 UTC 2014,,,,,,,,,,"0|i1td33:",379633,,,,,,,,,,,,,,,,,,,,,"12/Mar/14 16:00;ashutoshc;Simple fix.;;;","12/Mar/14 16:00;ashutoshc;https://reviews.apache.org/r/19127/;;;","13/Mar/14 07:29;ashutoshc;Reupload for Hive QA to pick up;;;","13/Mar/14 14:52;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634380/HIVE-6630.2.patch

{color:green}SUCCESS:{color} +1 5388 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1763/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1763/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634380;;;","13/Mar/14 18:03;hagleitn;LGTM +1;;;","13/Mar/14 22:51;ashutoshc;Committed to trunk & 0.13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add ""owner"" tag to ptest2 created instances",HIVE-6623,12700831,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,brocknoland,brocknoland,brocknoland,11/Mar/14 21:37,12/Feb/15 23:40,14/Jul/23 06:14,12/Dec/14 20:01,,,,,,,,,,1.1.0,,,,,,0,,,"We have a new requirement to have an ""owner"" tag on instances. We need to change ptest2 to support this.",,brocknoland,szehon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Dec/14 23:26;brocknoland;HIVE-6623.patch;https://issues.apache.org/jira/secure/attachment/12686708/HIVE-6623.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379174,,,,Fri Dec 12 20:01:20 UTC 2014,,,,,,,,,,"0|i1tc2f:",379466,,,,,,,,,,,,,,,,,,,,,"11/Mar/14 21:37;brocknoland;FYI [~szehon];;;","15/Mar/14 01:42;szehon;I took a look, unfortunately AWS-EC2 jcloud provider 1.7.0 doesnt seem to support adding name/value to tag, they support adding name only to tag.   

Like (name = 'owner=szehon' , value=null).  That is the way the group tag is right now as well (name = 'group=hive-ptest-slave', value = null).  That might have to do until they support it;;;","11/Dec/14 23:33;szehon;+1 thanks for figuring this out;;;","12/Dec/14 20:01;brocknoland;Thank you for the review! I have committed this to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDF translate doesn't take either CHAR or VARCHAR as any of its arguments,HIVE-6622,12700816,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,wilbur.yang,xuefuz,xuefuz,11/Mar/14 20:56,13/Nov/14 19:41,14/Jul/23 06:14,20/Jun/14 21:14,0.12.0,,,,,,,,,0.14.0,,UDF,,,,0,,,"UDF translate(input, from, to) doesn't accept any CHAR or VARCHAR typed argument.
{code}
hive> desc vc;
c                   	char(5)             	None                
vc                  	varchar(7)          	None                
s                   	string              	None                
hive> select translate(""my input"", c, ""ll"") from vc;
FAILED: SemanticException [Error 10016]: Line 1:29 Argument type mismatch 'c': A string argument was expected but an argument of type char(5) was given.
{code}

However, if the type is String, then the UDF works fine.",,leftyl,szehon,thejas,wilbur.yang,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/14 01:01;wilbur.yang;HIVE-6622.1.patch;https://issues.apache.org/jira/secure/attachment/12651332/HIVE-6622.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379159,,,,Thu Nov 13 19:41:13 UTC 2014,,,,,,,,,,"0|i1tbz3:",379451,,,,,,,,,,,,,,,,,,,,,"19/Jun/14 03:53;xuefuz;[~wilbur.yang] Thanks for submitting the patch. However, you need to send an email to dev@hive.apache.org requesting to be included as a contributor. After that, the JIRA can be assigned to you.

+1 pending on the test run.;;;","20/Jun/14 10:44;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12651332/HIVE-6622.1.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 5653 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/526/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/526/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-526/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12651332;;;","20/Jun/14 21:14;xuefuz;Patch committed to trunk. Thanks Wilbur for the contribution.;;;","22/Jun/14 03:27;leftyl;User doc for translate(input, from, to) is here:

* [Hive Operators and UDFs -- String Functions | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-StringFunctions]

so a note should be added to the description, saying CHAR and VARCHAR are supported as of Hive 0.14.0 and linking to this jira.  Also, the name/signature column should show CHAR and VARCHAR alternatives.;;;","12/Nov/14 08:00;szehon;Added doc as described.;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDF printf doesn't take either CHAR or VARCHAR as the first argument,HIVE-6620,12700806,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,xuefuz,xuefuz,xuefuz,11/Mar/14 20:38,13/Nov/14 19:42,14/Jul/23 06:14,17/Mar/14 20:43,0.12.0,,,,,,,,,0.14.0,,UDF,,,,0,,,"{code}
hive> desc vc;
OK
c                   	char(5)             	None                
vc                  	varchar(7)          	None                
s                   	string              	None                

hive> select printf(c) from vc;
FAILED: SemanticException [Error 10016]: Line 1:14 Argument type mismatch 'c': Argument 1 of function PRINTF must be ""string"", but ""char(5)"" was found.
{code}

However, if the argument is string type, the query runs successfully.",,jdere,qwertymaniac,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/14 20:57;xuefuz;HIVE-6620.1.patch;https://issues.apache.org/jira/secure/attachment/12635011/HIVE-6620.1.patch","15/Mar/14 23:20;xuefuz;HIVE-6620.patch;https://issues.apache.org/jira/secure/attachment/12634942/HIVE-6620.patch","15/Mar/14 21:14;xuefuz;HIVE-6620.patch;https://issues.apache.org/jira/secure/attachment/12634932/HIVE-6620.patch","12/Mar/14 21:07;xuefuz;HIVE-6620.patch;https://issues.apache.org/jira/secure/attachment/12634259/HIVE-6620.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379149,,,,Thu Nov 13 19:42:13 UTC 2014,,,,,,,,,,"0|i1tbwv:",379441,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 01:22;jdere;+1 if tests pass;;;","15/Mar/14 10:57;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634259/HIVE-6620.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1829/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1829/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [9.003s]
[INFO] Hive Ant Utilities ................................ SUCCESS [5.923s]
[INFO] Hive Shims Common ................................. SUCCESS [3.715s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.554s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.333s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [3.066s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [7.812s]
[INFO] Hive Shims ........................................ SUCCESS [1.240s]
[INFO] Hive Common ....................................... SUCCESS [6.309s]
[INFO] Hive Serde ........................................ SUCCESS [10.150s]
[INFO] Hive Metastore .................................... SUCCESS [32.375s]
[INFO] Hive Query Language ............................... SUCCESS [1:15.744s]
[INFO] Hive Service ...................................... SUCCESS [7.098s]
[INFO] Hive JDBC ......................................... SUCCESS [2.879s]
[INFO] Hive Beeline ...................................... SUCCESS [2.872s]
[INFO] Hive CLI .......................................... SUCCESS [1.765s]
[INFO] Hive Contrib ...................................... SUCCESS [2.444s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.724s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.536s]
[INFO] Hive HCatalog Core ................................ SUCCESS [1.929s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.335s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.975s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [1.614s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [10.084s]
[INFO] Hive HWI .......................................... SUCCESS [1.098s]
[INFO] Hive ODBC ......................................... SUCCESS [0.904s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.173s]
[INFO] Hive TestUtils .................................... SUCCESS [0.617s]
[INFO] Hive Packaging .................................... FAILURE [1.844s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:29.675s
[INFO] Finished at: Sat Mar 15 06:57:08 EDT 2014
[INFO] Final Memory: 74M/457M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634259;;;","15/Mar/14 21:14;xuefuz;Attache the same patch to trigger test.;;;","15/Mar/14 21:40;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634932/HIVE-6620.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1840/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1840/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [9.094s]
[INFO] Hive Ant Utilities ................................ SUCCESS [5.935s]
[INFO] Hive Shims Common ................................. SUCCESS [3.656s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.545s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.222s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [3.064s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [7.804s]
[INFO] Hive Shims ........................................ SUCCESS [1.261s]
[INFO] Hive Common ....................................... SUCCESS [6.583s]
[INFO] Hive Serde ........................................ SUCCESS [9.922s]
[INFO] Hive Metastore .................................... SUCCESS [32.837s]
[INFO] Hive Query Language ............................... SUCCESS [1:18.364s]
[INFO] Hive Service ...................................... SUCCESS [7.665s]
[INFO] Hive JDBC ......................................... SUCCESS [2.852s]
[INFO] Hive Beeline ...................................... SUCCESS [2.845s]
[INFO] Hive CLI .......................................... SUCCESS [1.717s]
[INFO] Hive Contrib ...................................... SUCCESS [2.421s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.688s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.533s]
[INFO] Hive HCatalog Core ................................ SUCCESS [2.376s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.548s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.235s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [2.169s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [10.033s]
[INFO] Hive HWI .......................................... SUCCESS [1.055s]
[INFO] Hive ODBC ......................................... SUCCESS [0.864s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.157s]
[INFO] Hive TestUtils .................................... SUCCESS [0.693s]
[INFO] Hive Packaging .................................... FAILURE [1.734s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:33.557s
[INFO] Finished at: Sat Mar 15 17:40:48 EDT 2014
[INFO] Final Memory: 74M/419M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634932;;;","16/Mar/14 01:25;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634942/HIVE-6620.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5406 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1845/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1845/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634942;;;","16/Mar/14 20:57;xuefuz;Patch #1 included the new tests that were added but missed when the previous patch was generated.;;;","17/Mar/14 19:57;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635011/HIVE-6620.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5401 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1860/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1860/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635011;;;","17/Mar/14 20:41;xuefuz;The above test failure is transient and unrelated.;;;","17/Mar/14 20:43;xuefuz;Patch committed to trunk. Thanks to Jason for the review.;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stats inaccurate for auto_join32.q,HIVE-6619,12700784,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,prasanth_j,jpullokkaran,jpullokkaran,11/Mar/14 19:02,02/Jun/14 06:35,14/Jul/23 06:14,02/Jun/14 06:35,,,,,,,,,,,,Statistics,,,,0,,,"auto_join32.q unit test fails for hadoop2.
Seems like stats have changed.",,jpullokkaran,prasanth_j,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6588,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379127,,,,Mon Jun 02 06:35:34 UTC 2014,,,,,,,,,,"0|i1tbrz:",379419,,,,,,,,,,,,,,,,,,,,,"25/Mar/14 06:21;rhbutani;This issue is fixed by HADOOP-10425;;;","02/Jun/14 06:35;prasanth_j;Closing it based on Harish's comment.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
assertion when getting reference key from loader with byte-array mapjoin key,HIVE-6618,12700773,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sershe,sershe,sershe,11/Mar/14 18:28,13/Mar/14 18:30,14/Jul/23 06:14,13/Mar/14 18:30,0.13.0,,,,,,,,,0.13.0,,,,,,0,,,"java.lang.AssertionError: Should be called after loading tables
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.processRow(MapRecordProcessor.java:205)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:171)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:152)

This is because tables may have already been loaded.
",,hagleitn,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/14 18:34;sershe;HIVE-6618.patch;https://issues.apache.org/jira/secure/attachment/12633967/HIVE-6618.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,379116,,,,Thu Mar 13 18:30:36 UTC 2014,,,,,,,,,,"0|i1tbpj:",379408,,,,,,,,,,,,,,,,,,,,,"11/Mar/14 18:35;sershe;Instead of getting key from loader, get from tables;;;","12/Mar/14 17:59;hagleitn;+1 LGTM.;;;","13/Mar/14 18:30;sershe;committed to trunk and 0.13 branch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Misspelling ""schemaTool completeted"" ",HIVE-6612,12700617,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,szehon,szehon,szehon,11/Mar/14 01:03,13/Nov/14 19:42,14/Jul/23 06:14,19/Mar/14 15:49,0.13.0,,,,,,,,,0.14.0,,CLI,,,,0,,,"There is a misspelling of ""completed"" as ""completeted"" in the last message from schematool:

{noformat}
Metastore connection URL:	 jdbc:derby:;databaseName=metastore_db;create=true
Metastore Connection Driver :	 org.apache.derby.jdbc.EmbeddedDriver
Metastore connection User: hiveuser
Starting metastore schema initialization to 0.14.0
Initialization script hive-schema-0.14.0.derby.sql
Initialization script completed
schemaTool completeted
{noformat}

It is this way even in the wiki:  [https://cwiki.apache.org/confluence/display/Hive/Hive+Schema+Tool|https://cwiki.apache.org/confluence/display/Hive/Hive+Schema+Tool]
",,leftyl,szehon,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/14 16:44;szehon;HIVE-6612.2.patch;https://issues.apache.org/jira/secure/attachment/12635114/HIVE-6612.2.patch","11/Mar/14 01:04;szehon;HIVE-6612.patch;https://issues.apache.org/jira/secure/attachment/12633833/HIVE-6612.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,378334,,,,Thu Nov 13 19:42:27 UTC 2014,,,,,,,,,,"0|i1t6vz:",378626,,,,,,,,,,,,,,,,,,,,,"11/Mar/14 01:04;szehon;Simple fix.;;;","11/Mar/14 01:19;ashutoshc;+1;;;","17/Mar/14 16:44;szehon;Attaching to requeue for precommit test.;;;","19/Mar/14 14:27;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635114/HIVE-6612.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5411 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_dyn_part
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1871/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1871/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635114;;;","19/Mar/14 15:49;ashutoshc;Committed to trunk. Thanks, Szehon Ho!;;;","19/Mar/14 23:28;szehon;Thanks Ashutosh!  

[~leftylev] When you get a chance, can you fix the wiki page mentioned in the JIRA with the correct spelling?  ;;;","20/Mar/14 00:27;leftyl;Done:

* [Hive Schema Tool:  Usage Examples |https://cwiki.apache.org/confluence/display/Hive/Hive+Schema+Tool#HiveSchemaTool-UsageExamples]

Thanks for remembering the doc.;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Joining multiple union all outputs fails on Tez,HIVE-6611,12700609,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,hagleitn,hagleitn,hagleitn,11/Mar/14 00:23,12/Mar/14 21:13,14/Jul/23 06:14,12/Mar/14 21:13,,,,,,,,,,0.13.0,,,,,,0,,,"Queries like:

with u as (select * from src union all select * from src)
select * from u join u;

will fail on Tez because only one union flows into the join reduce phase.",,hagleitn,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/14 00:24;hagleitn;HIVE-6611.1.patch;https://issues.apache.org/jira/secure/attachment/12633827/HIVE-6611.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,378326,,,,Wed Mar 12 21:13:44 UTC 2014,,,,,,,,,,"0|i1t6u7:",378618,,,,,,,,,,,,,,,,,,,,,"11/Mar/14 00:24;hagleitn;.1 also fixes some outdated golden files on tez.;;;","11/Mar/14 03:49;vikram.dixit;LGTM +1.;;;","12/Mar/14 19:00;hagleitn;Ran unit tests locally: results are good.;;;","12/Mar/14 21:13;hagleitn;Committed to trunk. Thanks Vikram!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit test log needs to reflect DB Name,HIVE-6610,12700604,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jpullokkaran,jpullokkaran,jpullokkaran,10/Mar/14 23:59,14/Mar/14 23:04,14/Jul/23 06:14,14/Mar/14 23:04,0.12.0,,,,,,,,,0.13.0,,,,,,0,,,"Following Hadoop2 Unit tests are failing because ddl pre/post hooks are printing out database name.
auto_join14.q, join14.q, input12.q, input39.q
Current analysis suggest authentication changes caused it.
These tests are marked as hadoop-2 only.",,jpullokkaran,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6588,,,,,,,,,,,,,,,,,,,,,"11/Mar/14 17:42;jpullokkaran;HIVE-6610.patch;https://issues.apache.org/jira/secure/attachment/12633953/HIVE-6610.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,378253,,,,Fri Mar 14 23:04:48 UTC 2014,,,,,,,,,,"0|i1t6dz:",378545,,,,,,,,,,,,,,,,,,,,,"11/Mar/14 17:41;ashutoshc;Name the patch as per convention.;;;","11/Mar/14 17:43;jpullokkaran;Renamed patch to follow naming convention;;;","11/Mar/14 17:50;ashutoshc;+1;;;","14/Mar/14 23:04;ashutoshc;Committed to trunk & 0.13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Doing Ctrl-C on hive cli doesn't kill running MR jobs on hadoop-2,HIVE-6609,12699898,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,10/Mar/14 23:20,13/Mar/14 22:48,14/Jul/23 06:14,13/Mar/14 22:48,0.11.0,0.12.0,,,,,,,,0.13.0,,Query Processor,,,,0,,,"This is because url based job killing which we use doesn't work on hadoop2. We need to use java api. 
",,jdere,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/14 16:47;ashutoshc;HIVE-6609.2.patch;https://issues.apache.org/jira/secure/attachment/12634204/HIVE-6609.2.patch","13/Mar/14 07:17;ashutoshc;HIVE-6609.3.patch;https://issues.apache.org/jira/secure/attachment/12634374/HIVE-6609.3.patch","10/Mar/14 23:40;ashutoshc;HIVE-6609.patch;https://issues.apache.org/jira/secure/attachment/12633815/HIVE-6609.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,378244,,,,Thu Mar 13 22:48:06 UTC 2014,,,,,,,,,,"0|i1t6bz:",378536,,,,,,,,,,,,,,,,,,,,,"10/Mar/14 23:39;ashutoshc;https://reviews.apache.org/r/18992/;;;","12/Mar/14 16:47;ashutoshc;Addressed Mohammad's comments.;;;","13/Mar/14 07:17;ashutoshc;Re-upload for Hive QA to pick up;;;","13/Mar/14 09:09;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634374/HIVE-6609.3.patch

{color:green}SUCCESS:{color} +1 5387 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1762/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1762/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634374;;;","13/Mar/14 21:01;jdere;+1;;;","13/Mar/14 22:48;ashutoshc;Committed to trunk & 0.13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add apache pom as parent pom,HIVE-6608,12699882,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,Fixed,rhbutani,rhbutani,rhbutani,10/Mar/14 22:04,11/Mar/14 16:57,14/Jul/23 06:14,11/Mar/14 16:57,,,,,,,,,,0.13.0,,,,,,0,,,"From https://www.apache.org/dev/publishing-maven-artifacts.html
So we can use the distribution management targets.

We manually did the prepare your release step.
Will run Step 4 Stage the release for a vote when we are ready to release 0.13.",,brocknoland,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/14 22:05;rhbutani;HIVE-6608.1.patch;https://issues.apache.org/jira/secure/attachment/12633791/HIVE-6608.1.patch","11/Mar/14 16:43;rhbutani;HIVE-6608.2.patch;https://issues.apache.org/jira/secure/attachment/12633933/HIVE-6608.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,378228,,,,Tue Mar 11 16:57:32 UTC 2014,,,,,,,,,,"0|i1t68f:",378520,,,,,,,,,,,,,,,,,,,,,"10/Mar/14 22:07;rhbutani;[~brocknoland] [~ashutoshc] can one of you take a look at this. 
Thank you;;;","10/Mar/14 22:12;brocknoland;Other than the tabs this lgtm.;;;","11/Mar/14 16:45;brocknoland;+1;;;","11/Mar/14 16:57;rhbutani;Brock thanks for reviewing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describe extended on a view fails with NPE,HIVE-6607,12699878,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,ekoifman,ekoifman,ekoifman,10/Mar/14 21:52,14/Mar/14 21:36,14/Jul/23 06:14,12/Mar/14 23:07,0.12.0,0.13.0,,,,,,,,0.13.0,,WebHCat,,,,0,,,"STEPS TO REPRODUCE:
Create a table called 'sample_08'
Create a view of the table. From hive command line, please run:
hive> create view sample_09 as select * from sample_08 ;
ACTUAL BEHAVIOR:
Run the following command in the browser:
http://localhost:50111/templeton/v1/ddl/database/default/table/sample_09?format=extended
It fails with the following exception:
{""errorDetail"":""org.apache.hadoop.hive.ql.metadata.HiveException: Exception while processing show table status\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.showTableStatus(DDLTask.java:2707)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:343)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)\n\tat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1437)\n\tat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1215)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1043)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)\n\tat org.apache.hive.hcatalog.cli.HCatDriver.run(HCatDriver.java:43)\n\tat org.apache.hive.hcatalog.cli.HCatCli.processCmd(HCatCli.java:259)\n\tat org.apache.hive.hcatalog.cli.HCatCli.processLine(HCatCli.java:213)\n\tat org.apache.hive.hcatalog.cli.HCatCli.main(HCatCli.java:172)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.putFileSystemsStats(JsonMetaDataFormatter.java:264)\n\tat org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.makeOneTableStatus(JsonMetaDataFormatter.java:218)\n\tat org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.makeAllTableStatus(JsonMetaDataFormatter.java:170)\n\tat org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.showTableStatus(JsonMetaDataFormatter.java:153)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.showTableStatus(DDLTask.java:2702)\n\t... 16 more\n"",""error"":""FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Exception while processing show table status"",""sqlState"":""08S01"",""errorCode"":40000,""database"":""default"",""table"":""sample_09""}",,aperepel,ekoifman,leftyl,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/14 22:04;ekoifman;HIVE-6607.patch;https://issues.apache.org/jira/secure/attachment/12633789/HIVE-6607.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,378224,,,,Fri Mar 14 21:36:51 UTC 2014,,,,,,,,,,"0|i1t67j:",378516,,,,,,,,,,,,,,,,,,,,,"12/Mar/14 19:00;ashutoshc;+1;;;","12/Mar/14 23:00;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633789/HIVE-6607.patch

{color:green}SUCCESS:{color} +1 5387 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1715/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1715/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633789;;;","12/Mar/14 23:07;ashutoshc;Committed to trunk. Thanks, Eugene!;;;","13/Mar/14 01:30;leftyl;Should the userdoc mention this bug and when it's going to be fixed?

* [Language Manual DDL:  Describe Table/View/Column |https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-DescribeTable/View/Column];;;","13/Mar/14 01:34;ekoifman;it's a WebHCat bug and I don't think it needs a doc;;;","13/Mar/14 01:38;leftyl;(Didn't notice it's WebHCat.)  Should this bug be documented here?

* [WebHCat Reference GetTable:  Parameters |https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference+GetTable#WebHCatReferenceGetTable-Parameters];;;","13/Mar/14 01:39;leftyl;Okay, thanks Eugene.;;;","14/Mar/14 21:36;rhbutani;patch applied to 0.13 branch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stand alone metastore fails to start if new transaction values not defined in config,HIVE-6606,12699866,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,gates,gates,gates,10/Mar/14 21:02,01/Oct/19 22:07,14/Jul/23 06:14,25/Mar/14 19:48,0.13.0,,,,,,,,,0.13.0,,Metastore,,,,0,,,The metastore creates instances of TxnHandler.  The constructor of this class will fail if the config value for the jdbc string it expects is not defined in the config file.,,gates,leftyl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-5843,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 00:08;gates;HIVE-6606.2.patch;https://issues.apache.org/jira/secure/attachment/12634307/HIVE-6606.2.patch","10/Mar/14 23:29;gates;HIVE-6606.patch;https://issues.apache.org/jira/secure/attachment/12633813/HIVE-6606.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,378212,,,,Wed Mar 26 07:06:34 UTC 2014,,,,,,,,,,"0|i1t64v:",378504,,,,,,,,,,,,,,,,,,,,,"10/Mar/14 23:29;gates;Fixed this by changing transaction connection to use the same JDBC connection string as the rest of the metastore.;;;","12/Mar/14 19:03;ashutoshc;[~gates] Can you create a RB entry for this?;;;","12/Mar/14 19:21;gates;https://reviews.apache.org/r/19149/;;;","12/Mar/14 20:21;ashutoshc;Left some comments on RB.;;;","13/Mar/14 00:08;gates;New version of the patch with changes based on Ashutosh's comments.;;;","13/Mar/14 00:08;gates;[~rhbutani] This patch should go into the 0.13 branch as well.;;;","13/Mar/14 00:38;ashutoshc;+1;;;","13/Mar/14 18:01;gates;Resubmitting patch to kick off tests.;;;","25/Mar/14 19:10;gates;Ran this locally and all tests passed except for union_top_level.q which seems to have a different ordering on my machine then the build machine.;;;","25/Mar/14 19:48;ashutoshc;Committed to trunk & 0.13. Thanks, Alan!;;;","26/Mar/14 07:06;leftyl;For the record, this removes *hive.txn.driver* and *hive.txn.connection.string* which were introduced in HIVE-5843.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive does not set the environment correctly when running in Tez mode,HIVE-6605,12699833,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sershe,sershe,sershe,10/Mar/14 18:41,11/Mar/14 18:23,14/Jul/23 06:14,11/Mar/14 18:23,,,,,,,,,,0.13.0,,,,,,0,,,"When running in Tez mode, Hive does not correctly set the java.library.path.",,hagleitn,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/14 21:31;sershe;HIVE-6605.01.patch;https://issues.apache.org/jira/secure/attachment/12633779/HIVE-6605.01.patch","10/Mar/14 18:58;sershe;HIVE-6605.patch;https://issues.apache.org/jira/secure/attachment/12633750/HIVE-6605.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,378179,,,,Tue Mar 11 18:23:43 UTC 2014,,,,,,,,,,"0|i1t5xj:",378471,,,,,,,,,,,,,,,,,,,,,"10/Mar/14 18:42;sershe;I'll attach a trivial patch shortly ;;;","10/Mar/14 21:31;sershe;Updated patch after discussion;;;","10/Mar/14 22:02;hagleitn;Good point. +1;;;","11/Mar/14 18:23;sershe;Committed to trunk and 0.13 branch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multi-user HiveServer2 throws error,HIVE-6602,12699773,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vgumashta,vgumashta,vgumashta,10/Mar/14 12:34,13/Nov/14 19:44,14/Jul/23 06:14,11/Mar/14 13:43,0.13.0,,,,,,,,,0.14.0,,HiveServer2,,,,0,,,"Error thrown:
Error while processing statement: FAILED: RuntimeException org.apache.hadoop.security.AccessControlException: Permission denied: user=user_1, access=WRITE, inode=""/tmp/hive-hive"":hdfs:drwxr-xr-x

For hive query execution, a scratch directory specified by hive.exec.scratchdir is created with default permission 700. In HiveServer2, during the CLIService startup, we check for the presence of scratch directories (local + dfs) and if they don't exist, create them with permission 777. However, we should also change the permission from the default 700 to 777 in case the dfs scratch directory already exists.",,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6847,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/14 12:37;vgumashta;HIVE-6602.1.patch;https://issues.apache.org/jira/secure/attachment/12633685/HIVE-6602.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,378119,,,,Thu Nov 13 19:44:20 UTC 2014,,,,,,,,,,"0|i1t5kf:",378411,,,,,,,,,,,,,,,,,,,,,"10/Mar/14 12:37;vgumashta;cc [~thejas];;;","10/Mar/14 14:46;thejas;Deleting the earlier comment, as I had misread the code.;;;","10/Mar/14 14:48;thejas;+1;;;","11/Mar/14 08:55;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633685/HIVE-6602.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5376 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1699/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1699/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633685;;;","11/Mar/14 13:43;thejas;Patch committed to trunk. Thanks Vaibhav!
;;;","12/Mar/14 09:43;vgumashta;[~rhbutani] I think HIVE-6627 and HIVE-6626 should also get in if we're keeping this for 13. Or else this should be deferred.

[~thejas]: What do you say?;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
alter database commands should support schema synonym keyword,HIVE-6601,12699763,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,navis,thejas,thejas,10/Mar/14 11:29,13/Nov/14 19:41,14/Jul/23 06:14,29/Jul/14 06:56,,,,,,,,,,0.14.0,,,,,,0,,,"It should be possible to use ""alter schema""  as an alternative to ""alter database"".  But the syntax is not currently supported.

{code}
alter schema db1 set owner user x;  
NoViableAltException(215@[])
FAILED: ParseException line 1:6 cannot recognize input near 'schema' 'db1' 'set' in alter statement
{code}

",,forrest_lv,icgrw,leftyl,navis,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7047,,,,,,,HIVE-675,,,HIVE-6440,,,,,,,,,,,,,,,,,,,,,"28/Jul/14 07:44;navis;HIVE-6601.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12658100/HIVE-6601.1.patch.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,378109,,,,Thu Nov 13 19:41:30 UTC 2014,,,,,,,,,,"0|i1t5i7:",378401,,,,,,,,,,,,,,,,,,,,,"12/May/14 18:25;thejas;This is the case with the other alter database command as well -  ALTER DATABASE database_name SET DBPROPERTIES
;;;","28/Jul/14 11:38;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12658100/HIVE-6601.1.patch.txt

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5770 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/76/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/76/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-76/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12658100;;;","28/Jul/14 18:17;thejas;+1;;;","29/Jul/14 06:56;navis;Committed to trunk. Thanks for the review, Thejas.;;;","29/Jul/14 07:58;leftyl;Documentation goes here:

* [DDL -- Alter Database | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-AlterDatabase]

similar to create and drop database here (although it would be better to use new lines of syntax to show the version information, instead of ""|"" syntax):

* [DDL -- Create/Drop/Alter Database | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-Create/Drop/AlterDatabase];;;","10/Nov/14 03:55;leftyl;Documented by [~navis] and revised by the nitpicker (me).  Please review:

* [_just the diffs_ | https://cwiki.apache.org/confluence/pages/diffpages.action?pageId=27362034&originalId=48202507]
* [Overview | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-Overview]
* [Alter Database | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-AlterDatabase]
* [Show Databases | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-ShowDatabases]
* [Show Locks | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-ShowLocks]
* [Describe Database | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-DescribeDatabase]
;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document new jdbc url connection parameters for HiveServer2,HIVE-6599,12699739,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vgumashta,vgumashta,vgumashta,10/Mar/14 08:01,24/Jul/14 18:40,14/Jul/23 06:14,03/Apr/14 02:10,0.13.0,,,,,,,,,0.13.0,,HiveServer2,JDBC,,,0,,,"Need to document the parameters for http mode, ssl mode and secure proxy access.",,kminder,leftyl,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6318,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,378086,,,,Thu Jul 24 18:40:02 UTC 2014,,,,,,,,,,"0|i1t5d3:",378378,,,,,,,,,,,,,,,,,,,,,"10/Mar/14 15:57;kminder;Should there be a different jira filed for setting up the server side or will that be covered by this jira as well?;;;","10/Mar/14 21:15;leftyl;Which jiras introduced these parameters?  (I'm keeping a list for release 0.13.0 because of HIVE-6037.);;;","03/Apr/14 02:09;vgumashta;[~kevin.minder] I've updated both the client ([HiveServer2+Clients|https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients]) and server wikis ([Setting+Up+HiveServer2|https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2])

[~leftylev] These are HIVE-4752, HIVE-5351 and HIVE-6466. 

Thanks! ;;;","24/Jul/14 18:40;leftyl;All these parameters were documented in the wiki back in mid-April.

* [Configuration Properties -- HiveServer2 | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-HiveServer2]
** [doc-done comment on HIVE-5351 | https://issues.apache.org/jira/browse/HIVE-5351?focusedCommentId=13973934&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13973934]
** [doc-done comment on HIVE-6466 | https://issues.apache.org/jira/browse/HIVE-6466?focusedCommentId=13974758&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13974758]
** HIVE-4752 is an umbrella JIRA without any new parameters of its own

Their descriptions or update will be added to the new, improved HiveConf.java with HIVE-6586:

* [fix-it comment for HIVE-5351 | https://issues.apache.org/jira/browse/HIVE-6586?focusedCommentId=14051193&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14051193]
* [fix-it comment for HIVE-6466 | https://issues.apache.org/jira/browse/HIVE-6586?focusedCommentId=14073461&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14073461]

But another value for hive.server2.authentication (NOSASL) needs a description, and I don't know what JIRA ticket added it in 0.13.0:

* [fix-it comment for HIVE-???? -- NOSASL | https://issues.apache.org/jira/browse/HIVE-6586?focusedCommentId=14073468&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14073468];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WebHCat E2E tests doAsTests_6 and doAsTests_7 need to be updated,HIVE-6597,12699674,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,deepesh,deepesh,deepesh,09/Mar/14 05:18,13/Nov/14 19:44,14/Jul/23 06:14,28/Mar/14 23:24,0.13.0,,,,,,,,,0.13.0,0.14.0,Tests,WebHCat,,,0,,,"Currently the following WebHCat doAsTests need to be fixed:
In doAsTests_6 REST request url needs to be updated and corresponding expected output to reflect the correct intent.
doAsTests_7 fails because of the strict error message checking.",,deepesh,ekoifman,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/14 05:25;deepesh;HIVE-6597.patch;https://issues.apache.org/jira/secure/attachment/12633581/HIVE-6597.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,378021,,,,Thu Nov 13 19:44:06 UTC 2014,,,,,,,,,,"0|i1t4yn:",378313,,,,,,,,,,,,,,,,,,,,,"09/Mar/14 05:25;deepesh;Attaching the patch that fixes the two tests.;;;","10/Mar/14 09:30;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633581/HIVE-6597.patch

{color:green}SUCCESS:{color} +1 5374 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1689/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1689/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633581;;;","10/Mar/14 18:34;ekoifman;+1;;;","28/Mar/14 23:21;sushanth;+1, will go ahead and commit. I've also received a verbal confirmation from [~rhbutani] that this is okay to commit to 0.13, will do so there too.;;;","28/Mar/14 23:24;sushanth;Committed to  0.13 and trunk. Thanks Deepesh, Eugene & Harish;;;","13/Nov/14 19:44;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsignedInt128 addition does not increase internal int array count resulting in corrupted values during serialization,HIVE-6594,12699609,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,rusanu,rusanu,rusanu,08/Mar/14 13:22,13/Nov/14 19:42,14/Jul/23 06:14,11/Mar/14 12:51,0.13.0,,,,,,,,,0.13.0,0.14.0,Query Processor,,,,0,,,"Discovered this while investigating why my fix for HIVE-6222 produced diffs. I discovered that Decimal128.addDestructive does not adjust the internal count when an the number of relevant ints increases. Since this count is used in the fast HiveDecimalWriter conversion code, the results are off. 

The root cause is UnsignedDecimal128.differenceInternal does not do an updateCount() on the result.",,jnp,rusanu,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/14 13:32;rusanu;HIVE-6594.1.patch;https://issues.apache.org/jira/secure/attachment/12633541/HIVE-6594.1.patch","08/Mar/14 14:07;rusanu;HIVE-6594.2.patch;https://issues.apache.org/jira/secure/attachment/12633542/HIVE-6594.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377956,,,,Thu Nov 13 19:42:38 UTC 2014,,,,,,,,,,"0|i1t4k7:",378248,,,,,,,,,,,,,,,,,,,,,"08/Mar/14 14:07;rusanu;Patch .2 contains updated expected results (now correct);;;","09/Mar/14 03:07;jnp;+1;;;","10/Mar/14 06:10;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633542/HIVE-6594.2.patch

{color:green}SUCCESS:{color} +1 5375 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1685/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1685/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633542;;;","11/Mar/14 12:51;rusanu;Committed to trunk r1576317;;;","11/Mar/14 16:57;jnp;[~rhbutani] This is a serious bug and can cause incorrect results and affects hive-0.13 as well. I will port the fix to branch-0.13.;;;","11/Mar/14 18:26;jnp;Committed to branch-0.13 as well.;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WebHCat E2E test abort when pointing to https url of webhdfs,HIVE-6592,12699550,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,deepesh,deepesh,deepesh,08/Mar/14 00:41,13/Nov/14 19:43,14/Jul/23 06:14,28/Mar/14 22:18,0.13.0,,,,,,,,,0.13.0,0.14.0,Tests,WebHCat,,,0,,,"WebHCat E2E tests when running against a ssl enabled webhdfs url fails.
NO PRECOMMIT TESTS",,deepesh,rhbutani,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/14 00:43;deepesh;HIVE-6592.patch;https://issues.apache.org/jira/secure/attachment/12633500/HIVE-6592.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377897,,,,Thu Nov 13 19:43:13 UTC 2014,,,,,,,,,,"0|i1t473:",378189,,,,,,,,,,,,,,,,,,,,,"08/Mar/14 00:43;deepesh;The patch appends a ""-k"" to the curl command when making requests to https urls. The option allows curl to make insecure ssl connections and transfers.;;;","27/Mar/14 22:39;thejas;+1
;;;","28/Mar/14 22:11;sushanth;Looks good to me from reading up ""man curl"" :

{noformat}
       -k/--insecure
              (SSL) This option explicitly allows curl to perform ""insecure"" SSL connections and transfers. All SSL connections  are  attempted  to  be
              made  secure  by  using  the  CA  certificate  bundle  installed by default. This makes all connections considered ""insecure"" fail unless
              -k/--insecure is used.

              See this online resource for further details: http://curl.haxx.se/docs/sslcerts.html
{noformat}

+1.;;;","28/Mar/14 22:16;sushanth;Committed to trunk, Thanks, Deepesh! Setting the fix version to 0.14.

[~rhbutani], Deepesh would like to get this included in 0.13 as well. I think it makes sense for inclusion, since it's needed to allow our E2E tests to run in a secure environment. Could we backport this?;;;","28/Mar/14 22:31;rhbutani;+1 for 0.13;;;","28/Mar/14 22:56;sushanth;Thanks Harish and Deepesh, committed to 0.13.;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Importing a table containing hidden dirs fails,HIVE-6591,12699534,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,07/Mar/14 23:38,11/Mar/14 01:26,14/Jul/23 06:14,10/Mar/14 22:44,0.10.0,0.11.0,0.12.0,,,,,,,0.13.0,,Import/Export,,,,0,,,hidden files should be ignored while exporting,,rhbutani,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/14 23:42;ashutoshc;HIVE-6591.patch;https://issues.apache.org/jira/secure/attachment/12633488/HIVE-6591.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377881,,,,Tue Mar 11 01:26:10 UTC 2014,,,,,,,,,,"0|i1t43j:",378173,,,,,,,,,,,,,,,,,,,,,"07/Mar/14 23:48;ashutoshc;https://reviews.apache.org/r/18932/;;;","10/Mar/14 01:00;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633488/HIVE-6591.patch

{color:green}SUCCESS:{color} +1 5375 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1683/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1683/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633488;;;","10/Mar/14 20:06;vikram.dixit;+1;;;","10/Mar/14 22:44;ashutoshc;Committed to trunk. 
[~rhbutani] It will be good to have this long standing bug fixed in 0.13 as well.;;;","11/Mar/14 01:21;rhbutani;+1 for 0.13;;;","11/Mar/14 01:26;ashutoshc;Committed to 0.13 as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive does not work properly with boolean partition columns (wrong results and inserts to incorrect HDFS path),HIVE-6590,12699533,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,kgyrtkirk,lskuff,lskuff,07/Mar/14 23:37,01/Oct/19 22:06,14/Jul/23 06:14,09/Nov/17 17:18,0.10.0,,,,,,,,,3.0.0,,Database/Schema,Metastore,,,1,,,"Hive does not work properly with boolean partition columns. Queries return wrong results and also insert to incorrect HDFS paths.

{code}
create table bool_part(int_col int) partitioned by(bool_col boolean);
# This works, creating 3 unique partitions!
ALTER TABLE bool_table ADD PARTITION (bool_col=FALSE);
ALTER TABLE bool_table ADD PARTITION (bool_col=false);
ALTER TABLE bool_table ADD PARTITION (bool_col=False);
{code}

The first problem is that Hive cannot filter on a bool partition key column. ""select * from bool_part"" returns the correct results, but if you apply a filter on the bool partition key column hive won't return any results.

The second problem is that Hive seems to just call ""toString()"" on the boolean literal value. This means you can end up with multiple partitions (FALSE, false, FaLSE, etc) mapping to the literal value 'FALSE'. For example, if you can add three partition in have for the same logic value ""false"" doing:
ALTER TABLE bool_table ADD PARTITION (bool_col=FALSE) -> /test-warehouse/bool_table/bool_col=FALSE/
ALTER TABLE bool_table ADD PARTITION (bool_col=false) -> /test-warehouse/bool_table/bool_col=false/
ALTER TABLE bool_table ADD PARTITION (bool_col=False) -> /test-warehouse/bool_table/bool_col=False/",,alex.behm,gates,jbapple,jdere,kgyrtkirk,lskuff,vgarg,wchisholm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-15939,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/17 21:43;kgyrtkirk;HIVE-6590.1.patch;https://issues.apache.org/jira/secure/attachment/12852671/HIVE-6590.1.patch","15/Feb/17 07:17;kgyrtkirk;HIVE-6590.2.patch;https://issues.apache.org/jira/secure/attachment/12852755/HIVE-6590.2.patch","15/Feb/17 15:01;kgyrtkirk;HIVE-6590.3.patch;https://issues.apache.org/jira/secure/attachment/12852829/HIVE-6590.3.patch","27/Feb/17 19:59;kgyrtkirk;HIVE-6590.4.patch;https://issues.apache.org/jira/secure/attachment/12854958/HIVE-6590.4.patch","03/Sep/17 06:18;kgyrtkirk;HIVE-6590.5.patch;https://issues.apache.org/jira/secure/attachment/12885121/HIVE-6590.5.patch","16/Jun/17 12:24;kgyrtkirk;HIVE-6590.5.patch;https://issues.apache.org/jira/secure/attachment/12873275/HIVE-6590.5.patch",,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377880,Incompatible change,,,Wed May 23 00:00:50 UTC 2018,,,,,,,,,,"0|i1t43b:",378172,,,,,,,,,,,,,,,,,,,,,"16/Aug/16 06:28;alex.behm;A related and severe issue is that ALTER TABLE DROP PARTITION may drop partitions that were not specified in the ALTER statement. This could lead to accidental data loss.

Reproduction:
{code}
CREATE TABLE broken (c int) PARTITIONED BY (b1 BOOLEAN, s STRING, b2 BOOLEAN, i INT);

# Insert a few variants of 'false' partition-key values.
INSERT INTO TABLE broken PARTITION(b1=false,s='a',b2=false,i=0) VALUES(1);
INSERT INTO TABLE broken PARTITION(b1=FALSE,s='a',b2=false,i=0) VALUES(3);
INSERT INTO TABLE broken PARTITION(b1=false,s='a',b2=False,i=0) VALUES(5);
INSERT INTO TABLE broken PARTITION(b1=false,s='a',b2=FalsE,i=0) VALUES(7);

# Insert a few variants of 'true' partition-key values.
INSERT INTO TABLE broken PARTITION(b1=true,s='a',b2=true,i=0) VALUES(2);
INSERT INTO TABLE broken PARTITION(b1=TRUE,s='a',b2=true,i=0) VALUES(4);
INSERT INTO TABLE broken PARTITION(b1=true,s='a',b2=True,i=0) VALUES(6);
INSERT INTO TABLE broken PARTITION(b1=true,s='a',b2=TruE,i=0) VALUES(8);

# Insert a few variants of mixed 'true'/'false' partition-key values.
INSERT INTO TABLE broken PARTITION(b1=false,s='a',b2=true,i=0) VALUES(100);
INSERT INTO TABLE broken PARTITION(b1=FALSE,s='a',b2=TRUE,i=0) VALUES(1000);
INSERT INTO TABLE broken PARTITION(b1=true,s='a',b2=false,i=0) VALUES(10000);
INSERT INTO TABLE broken PARTITION(b1=tRUe,s='a',b2=fALSe,i=0) VALUES(100000);

# Very broken partition drop.
hive> ALTER TABLE broken DROP PARTITION(b1=true,s='a',b2=true,i=0);
Dropped the partition b1=false/s=a/b2=false/i=0
Dropped the partition b1=false/s=a/b2=False/i=0
Dropped the partition b1=false/s=a/b2=FalsE/i=0
Dropped the partition b1=FALSE/s=a/b2=false/i=0
Dropped the partition b1=false/s=a/b2=true/i=0
Dropped the partition b1=FALSE/s=a/b2=TRUE/i=0
Dropped the partition b1=true/s=a/b2=false/i=0
Dropped the partition b1=tRUe/s=a/b2=fALSe/i=0
Dropped the partition b1=true/s=a/b2=true/i=0
Dropped the partition b1=true/s=a/b2=True/i=0
Dropped the partition b1=true/s=a/b2=TruE/i=0
Dropped the partition b1=TRUE/s=a/b2=true/i=0
OK
Time taken: 1.387 seconds
{code}
;;;","14/Feb/17 21:43;kgyrtkirk;serde treated all non-zero length strings as true.
i've changed this to only consider strings starting with 't' or 'T' to be true.;;;","15/Feb/17 02:45;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12852671/HIVE-6590.1.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 11 failed/errored test(s), 10240 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_rp_auto_join1] (batchId=3)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join31] (batchId=81)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[partition_boolean] (batchId=21)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[multiMapJoin2] (batchId=152)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_nonvec_part_all_primitive] (batchId=148)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vec_part_all_primitive] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vecrow_part_all_primitive] (batchId=150)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join31] (batchId=133)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3553/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3553/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3553/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 11 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12852671 - PreCommit-HIVE-Build;;;","15/Feb/17 07:17;kgyrtkirk;patch.2
* enable sort results for qtest
* schema_evol tests contained a string to boolean conversion ;;;","15/Feb/17 11:38;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12852755/HIVE-6590.2.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3565/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3565/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3565/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2017-02-15 11:38:34.017
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-3565/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2017-02-15 11:38:34.020
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at 1aad3be HIVE-15914: Fix issues with druid-handler pom file (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)
+ git clean -f -d
+ git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at 1aad3be HIVE-15914: Fix issues with druid-handler pom file (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2017-02-15 11:38:35.045
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
error: patch failed: ql/src/test/results/clientpositive/llap/schema_evol_text_vec_part_all_primitive.q.out:323
error: ql/src/test/results/clientpositive/llap/schema_evol_text_vec_part_all_primitive.q.out: patch does not apply
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12852755 - PreCommit-HIVE-Build;;;","15/Feb/17 16:03;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12852829/HIVE-6590.3.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 10226 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_rp_auto_join1] (batchId=3)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join31] (batchId=81)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[multiMapJoin2] (batchId=152)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=223)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=129)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join31] (batchId=133)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3570/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3570/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3570/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12852829 - PreCommit-HIVE-Build;;;","15/Feb/17 16:09;kgyrtkirk;I think these test failures are unrelated....

[~jdere] Could you take a look at these changes?;;;","15/Feb/17 18:28;alex.behm;[~kgyrtkirk], can you explain how your new proposed behavior is better? It seems like there is still a fundamental issue that partition-key values do not have a canonical string representation, so you could still have multiple HDFS paths referring to the same logical partition.;;;","15/Feb/17 18:53;kgyrtkirk;[~alex.behm] I guess the path naming issue have been solved earlier...because the paths seems to be correct to me:

{code}
+POSTHOOK: query: select * from broken
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@broken
+POSTHOOK: Input: default@broken@b1=false/s=a/b2=false/i=0
+POSTHOOK: Input: default@broken@b1=false/s=a/b2=true/i=0
+POSTHOOK: Input: default@broken@b1=true/s=a/b2=false/i=0
+POSTHOOK: Input: default@broken@b1=true/s=a/b2=true/i=0
+#### A masked pattern was here ####
{code};;;","15/Feb/17 19:09;jdere;Wow, I had no idea PrimitiveObjectInpsectorUtils.getBoolean() treated all non-empty strings as TRUE.
The thing about changing this behavior is that it has ramifications beyond just the partition columns, and in fact even UDFToBoolean has the same behavior.
cc [~ashutoshc] [~gates] to see if they have any opinions here.

Where does the partitioning code actually call PrimitiveObjectInpsectorUtils.getBoolean() to convert the string value to boolean? Wondering if it is possible to special case boolean behavior to not use PrimitiveObjectInpsectorUtils.getBoolean() during partitioning.;;;","15/Feb/17 21:10;gates;cc [~owen.omalley] as he's bumped into some issues with this as well I believe.;;;","16/Feb/17 00:18;gates;I agree with Jason that we don't want to change PrimitiveObjectInspectorUtils.getBoolean().  That's just too fundamental of a change and risks breaking unknown things.  I also agree that it is better to see if we can special case this in the partitioning code to do the right thing for partitions.;;;","16/Feb/17 06:28;kgyrtkirk;partition keys are parsed here:
https://github.com/apache/hive/blob/d357f38521ae583007ff96ed7090ac41f56b78b2/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java#L257

I was expecting a whole myriad of qtests to fail when I submitted the patch the first time...but there were only just a few - how much does the fact that something is ""under-tested"" correlates with its being ""under-used"" ? :)

Anyway; of course its possible to apply local changes to the partition key parsing code...but I think there is an alternative path:

* parse empty strings and ""false"" as false (optionally ignoring casing)
* all others as true

this would be a minimal change to the serde code which would be enough to fix the partition parsing.

about cast: I haven't looked into sql2011 specs about this aspect, but i'm pretty confident that it will suggest the following {{cast('false' as boolean)}} should be false...;;;","24/Feb/17 07:41;ashutoshc;I looked the spec. Section 6.13 clearly says only strings are allowed to be cast into boolean. All other types (which Hive supports) if attempted to be cast to boolean need to raise an error. Further 20) in section states for string only valid boolean literals are allowed in strings, which implies string ""true"" should parse as boolean true and string ""false"" parse as boolean false. All other strings need to raise error.

So, if we want to be spec compliant this behavior needs to change. Perhaps, in major version : 3.0?;;;","27/Feb/17 19:59;kgyrtkirk;[~ashutoshc] I'm not sure what to do now :)

I totally aggree that raising errors for this would be problematic...it would cause more problems than it would solve...I would recommend to change the behaviour to a half-way solution - which would even comply to the standard more, by giving the expected results when the case is defined; but instead of errors - retains the old behaviour for all other cases.

Surprisingly in the schema evolution test the string column contained 'true' and 'false' strings, and the change in those q.out files are the result of the fact that now 'false' is parsed as 'false'.

I'll attach patch#4 which covers this change; if this is still too much, I'll do the partition parser only change.;;;","28/Feb/17 00:36;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12854958/HIVE-6590.4.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 10275 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vec_table] (batchId=147)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
org.apache.hive.beeline.TestBeeLineWithArgs.testQueryProgressParallel (batchId=211)
org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark.testSparkQuery (batchId=217)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3821/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3821/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3821/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12854958 - PreCommit-HIVE-Build;;;","13/Mar/17 20:57;kgyrtkirk;the last, #4 patch would only change one thing at the serde level: the parsing outcome of the string 'false' will be false from now on...instead of the current true.

[~ashutoshc] should I be more carefull...and do the partition only fix?
;;;","16/Mar/17 22:28;kgyrtkirk;[~ashutoshc] Could you please take a look at my last two comments?;;;","30/Mar/17 17:30;ashutoshc;Now that master no longer needs to be backward compatible, we can make changes to be standard compliant. Also, it will be worth observing the behavior on other dbs (especially postgres since thats usually closest to standard.);;;","16/Jun/17 10:56;kgyrtkirk;I've tried to fix this with minimal changes for branch-2...by introducing lesser changes - but even thru I was able to fix insert and such...{{alter table drop partition}} still retained the problem - and because its using hive internal function to do the check that would look awkward...so I think it will be safer to go forward...;;;","16/Jun/17 12:05;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12854958/HIVE-6590.4.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 12 failed/errored test(s), 10833 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[create_merge_compressed] (batchId=237)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query16] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query94] (batchId=232)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testBootstrapFunctionReplication (batchId=216)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testCreateFunctionIncrementalReplication (batchId=216)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testCreateFunctionWithFunctionBinaryJarsOnHDFS (batchId=216)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5660/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5660/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5660/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 12 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12854958 - PreCommit-HIVE-Build;;;","16/Jun/17 12:31;kgyrtkirk;patch#5)

* parsed as false, without case sensitivity: {{no,off,0,false}}  (like postgres)
* retained the feature: nonempty strings are true

postgres gives hard errors for anything which is not {{yes/no/on/off/0/1/true/false}}.
It might good to parse unknown values as {{NULL}} ...or keep the old behaviour - nonEmpty strings are true?
[~ashutoshc]
;;;","16/Jun/17 16:41;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12873275/HIVE-6590.5.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 12 failed/errored test(s), 10833 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[columnstats_part_coltype] (batchId=157)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query16] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query94] (batchId=232)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testBootstrapFunctionReplication (batchId=216)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testCreateFunctionIncrementalReplication (batchId=216)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testCreateFunctionWithFunctionBinaryJarsOnHDFS (batchId=216)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5665/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5665/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5665/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 12 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12873275 - PreCommit-HIVE-Build;;;","13/Jul/17 09:16;kgyrtkirk;[~ashutoshc] could you please take a look?;;;","03/Sep/17 06:18;kgyrtkirk;re-attach#5 to trigger a new test execution;;;","03/Sep/17 07:43;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12885121/HIVE-6590.5.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 11033 tests executed
*Failed tests:*
{noformat}
TestNoSaslAuth - did not produce a TEST-*.xml file (likely timed out) (batchId=227)
TestTxnCommandsBase - did not produce a TEST-*.xml file (likely timed out) (batchId=280)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=61)
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver[hbase_binary_storage_queries] (batchId=96)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_vectorized_dynamic_partition_pruning] (batchId=169)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=100)
org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.checkExpectedLocks (batchId=282)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6661/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6661/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6661/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12885121 - PreCommit-HIVE-Build;;;","17/Sep/17 14:52;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12885121/HIVE-6590.5.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 11037 tests executed
*Failed tests:*
{noformat}
TestAccumuloCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=230)
TestDummy - did not produce a TEST-*.xml file (likely timed out) (batchId=230)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_rp_gby2_map_multi_distinct] (batchId=79)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[create_view] (batchId=39)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_mask_hash] (batchId=28)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[union_fast_stats] (batchId=156)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[drop_table_failure2] (batchId=89)
org.apache.hadoop.hive.cli.TestSparkNegativeCliDriver.org.apache.hadoop.hive.cli.TestSparkNegativeCliDriver (batchId=241)
org.apache.hadoop.hive.ql.TestAcidOnTez.testCtasTezUnion (batchId=215)
org.apache.hadoop.hive.ql.TestAcidOnTez.testNonStandardConversion01 (batchId=215)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6851/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6851/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6851/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12885121 - PreCommit-HIVE-Build;;;","08/Nov/17 22:43;ashutoshc;+1;;;","09/Nov/17 09:06;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12885121/HIVE-6590.5.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 11374 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[dbtxnmgr_showlocks] (batchId=77)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=62)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[unionDistinct_1] (batchId=146)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid_fast] (batchId=157)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sysdb] (batchId=156)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=102)
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testCliDriver[ct_noperm_loc] (batchId=94)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_multi] (batchId=111)
org.apache.hadoop.hive.cli.control.TestDanglingQOuts.checkDanglingQOut (batchId=206)
org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.testConstraints (batchId=223)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/7729/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/7729/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-7729/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12885121 - PreCommit-HIVE-Build;;;","09/Nov/17 17:18;ashutoshc;Pushed to master. Thanks, Zoltan!;;;","09/Nov/17 17:22;ashutoshc;[~kgyrtkirk] I think we shall be consistent and change UDFToBoolean also along similar lines. Can you take that up in a follow-up?;;;","09/Nov/17 18:07;kgyrtkirk;[~ashutoshc] I've filed the followup a long time ago; and fortunately, there's already a patch to fix that in HIVE-15939 :);;;","23/May/18 00:00;vgarg;Hive 3.0.0 has been released so closing this jira.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update parameters in HiveConf.java after commit HIVE-6037,HIVE-6586,12699506,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,leftyl,leftyl,leftyl,07/Mar/14 21:10,01/Oct/19 22:07,14/Jul/23 06:14,29/Oct/14 04:35,0.13.0,0.14.0,,,,,,,,0.14.0,,,,,,0,,,"HIVE-6037 puts the definitions of configuration parameters into the HiveConf.java file, but several recent jiras for release 0.13.0 introduce new parameters that aren't in HiveConf.java yet and some parameter definitions need to be altered for 0.13.0.  This jira will patch HiveConf.java after HIVE-6037 gets committed.

Also, four typos patched in HIVE-6582 need to be fixed in the new HiveConf.java.",,gates,hagleitn,leftyl,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6582,,,HIVE-6037,HIVE-7227,,,,,,,,,,,,,,,,,,,,"01/Oct/14 06:52;leftyl;HIVE-6586.2.patch;https://issues.apache.org/jira/secure/attachment/12672268/HIVE-6586.2.patch","27/Oct/14 07:43;leftyl;HIVE-6586.3.patch;https://issues.apache.org/jira/secure/attachment/12677261/HIVE-6586.3.patch","28/Oct/14 05:59;leftyl;HIVE-6586.4.patch;https://issues.apache.org/jira/secure/attachment/12677531/HIVE-6586.4.patch","28/Oct/14 20:30;hagleitn;HIVE-6586.5.patch;https://issues.apache.org/jira/secure/attachment/12677680/HIVE-6586.5.patch","15/Sep/14 07:08;leftyl;HIVE-6586.patch;https://issues.apache.org/jira/secure/attachment/12668718/HIVE-6586.patch","15/Sep/14 07:08;leftyl;hive-default.xml.template;https://issues.apache.org/jira/secure/attachment/12668719/hive-default.xml.template","01/Oct/14 06:52;leftyl;hive-default.xml.template.patch2;https://issues.apache.org/jira/secure/attachment/12672269/hive-default.xml.template.patch2","27/Oct/14 07:43;leftyl;hive-default.xml.template.patch3;https://issues.apache.org/jira/secure/attachment/12677262/hive-default.xml.template.patch3","28/Oct/14 05:59;leftyl;hive-default.xml.template.patch4;https://issues.apache.org/jira/secure/attachment/12677532/hive-default.xml.template.patch4",,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377853,,,,Thu Nov 13 19:42:26 UTC 2014,,,,,,,,,,"0|i1t3xb:",378145,Update the parameter descriptions in HiveConf.java.,,,,,,,,,,,,,,,,,,,,"25/Mar/14 08:04;leftyl;Add hive.resultset.use.unique.column.names (HIVE-6687).;;;","26/Mar/14 07:03;leftyl;Copied from HIVE-6037 (edited for brevity):

HIVE-5843 introduces 13 new config params, but HIVE-6606 removes two of them.

* hive.txn.manager, -hive.txn.driver- (removed), -hive.txn.connection.string- (removed), hive.txn.timeout, hive.txn.max.open.batch, hive.txn.testing, hive.compactor.initiator.on, hive.compactor.worker.threads, hive.compactor.worker.timeout, hive.compactor.check.interval, hive.compactor.delta.num.threshold, hive.compactor.delta.pct.threshold, hive.compactor.abortedtxn.threshold

Note added later:  hive.txn.testing has also been removed.;;;","19/Apr/14 09:15;leftyl;HIVE-6903 changes the default value of hive.metastore.execute.setugi to true in trunk (for 0.14.0).;;;","23/Apr/14 07:49;leftyl;* HIVE-6743 added hive.tez.log.level (for 0.13.0).

* HIVE-6447 added hive.convert.join.bucket.mapjoin.tez (for 0.13.0).;;;","01/May/14 23:49;leftyl;HIVE-6741 added more information to the description of hive.server2.thrift.sasl.qop (for 0.14.0).;;;","18/May/14 23:12;leftyl;The old hive-default.xml.template file includes two entries for *hive.metastore.integral.jdo.pushdown* (from HIVE-6070 and HIVE-6188).

That will be fixed by HIVE-6037, but we should add the name of another parameter to the description as done in the HIVE-6188 patch and the wiki:

* [HIVE-6188 patch | https://issues.apache.org/jira/secure/attachment/12637478/HIVE-6188.patch]
* [wikidoc for hive.metastore.integral.jdo.pushdown | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.metastore.integral.jdo.pushdown]

For details see:

* [HIVE-6188 comment about duplicate parameter | https://issues.apache.org/jira/browse/HIVE-6188?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14001236#comment-14001236];;;","19/May/14 00:09;leftyl;According to a comment in HiveConf.java (from HIVE-4440), *hive.mapjoin.bucket.cache.size* should be removed: 

{quote}
+ // hive.mapjoin.bucket.cache.size has been replaced by hive.smbjoin.cache.row,
+ // need to remove by hive .13. Also, do not change default (see SMB operator)
{quote}

Also, that comment has a typo in the name of the parameter replacing *hive.mapjoin.bucket.cache.size* and the typo is replicated in the HIVE-6037 patch -- the new parameter is *hive.smbjoin.cache.rows*, not hive.smbjoin.cache.row.;;;","20/May/14 08:36;leftyl;The fix for hive.user.install.directory in HIVE-6582 has been included in release 0.13.0, so it isn't needed here.;;;","20/May/14 09:00;leftyl;bq. The fix for hive.user.install.directory in HIVE-6582 has been included in release 0.13.0, so it isn't needed here.

Actually, the patch for HIVE-6037 should be checked for four semicolons ( &lt ; and &gt ; ) in the description of hive.user.install.directory -- they are missing from HiveConf.java in HIVE-6037.17.patch although HIVE-6037-0.13.0 has ""<"" and "">"".;;;","20/May/14 09:26;leftyl;Descriptions are available for hive.metastore.try.direct.sql & hive.metastore.try.direct.sql.ddl (see patch for HIVE-6188 or wiki).  They are included in the 0.13.0 hive-default.xml.template file.;;;","08/Jun/14 09:09;leftyl;Descriptions for hive.mapjoin.optimized.keys & hive.mapjoin.lazy.hashtable were added to hive-default.xml.template in Hive 0.13.0 by HIVE-6188.  They are also in the wiki:  

* [hive.mapjoin.optimized.keys | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.mapjoin.optimized.keys]
* [hive.mapjoin.lazy.hashtable | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.mapjoin.lazy.hashtable]
;;;","08/Jun/14 23:05;leftyl;HIVE-7140 changes the default value for hive.metastore.client.socket.timeout from 20 seconds to 600 seconds in Hive 0.14.0.  The change was made in HiveConf.java but not in hive-default.xml.template.;;;","13/Jun/14 10:24;leftyl;See HIVE-7227 for a list of parameters that don't have descriptions yet.;;;","18/Jun/14 07:59;leftyl;HIVE-7219 adds hive.exec.orc.encoding.strategy with a description in hive-default.xml.template for release 0.14.0.;;;","30/Jun/14 00:28;leftyl;Transfers from HIVE-6037 comments (through Feb. 23, 2014):

HIVE-860 might add hive.cache.runtime.jars in release 0.14.0 with a description in hive-default.xml.template.

HIVE-6429 added hive.mapjoin.optimized.keys in 0.13.0.  HIVE-6188 added a description to hive-default.xml.template so it's in the 0.13.0 release but isn't in patch HIVE-6037-0.13.0 (link below).

HIVE-6455 added hive.optimize.sort.dynamic.partition in 0.13.0 with a description, which is in the release but isn't in patch HIVE-6037-0.13.0.

HIVE-3635 added hive.lazysimple.extended_boolean_literal in 0.14.0 with a description in hive-default.xml.template.

* [patch HIVE-6037-0.13.0 | https://issues.apache.org/jira/secure/attachment/12634329/HIVE-6037-0.13.0];;;","30/Jun/14 04:23;leftyl;That's it for transfers of parameter comments from HIVE-6037 (through June 30, 2014).  Other parameters mentioned in HIVE-6037's comments are already included in release 0.13.0 and their descriptions are in patch HIVE-6037-0.13.0 (https://issues.apache.org/jira/secure/attachment/12634329/HIVE-6037-0.13.0).

Just for completeness:  HIVE-7227 has a list of parameters without descriptions.  They don't have to be fixed with this jira ticket.;;;","30/Jun/14 05:56;leftyl;HIVE-6689 added hive.display.partition.cols.separately in 0.13.0.  It isn't in patch HIVE-6037-0.13.0 (https://issues.apache.org/jira/secure/attachment/12634329/HIVE-6037-0.13.0).;;;","01/Jul/14 05:02;leftyl;HIVE-6492 added hive.limit.query.max.table.partition in 0.13.0. It isn't in patch HIVE-6037-0.13.0 (https://issues.apache.org/jira/secure/attachment/12634329/HIVE-6037-0.13.0).;;;","01/Jul/14 08:40;leftyl;HIVE-6782 added hive.localize.resource.wait.interval & hive.localize.resource.num.wait.attempts in 0.13.0. They aren't in patch HIVE-6037-0.13.0.;;;","02/Jul/14 08:06;leftyl;HIVE-6749 changed the default value of hive.auto.convert.join.use.nonstaged to false in 0.13.0.  The change isn't in patch HIVE-6037-0.13.0.;;;","03/Jul/14 06:45;leftyl;HIVE-6697 added hive.server2.authentication.spnego.keytab and hive.server2.authentication.spnego.principal in 0.13.0. They aren't in patch HIVE-6037-0.13.0.;;;","03/Jul/14 08:42;leftyl;HIVE-5351 added three HiveServer2 configuration parameters in 0.13.0.  Patch HIVE-6037-0.13.0 includes them without their descriptions, which are:

* hive.server2.use.SSL:  Set this to true for using SSL encryption in HiveServer2.
* hive.server2.keystore.path:  SSL certificate keystore location.
* hive.server2.keystore.password:  SSL certificate keystore password.;;;","03/Jul/14 08:53;leftyl;HIVE-6643 added hive.exec.check.crossproducts in 0.13.0.  It isn't in patch HIVE-6037-0.13.0.;;;","03/Jul/14 23:40;leftyl;HIVE-7209 changes the description of hive.security.metastore.authorization.manager in 0.14.0.;;;","04/Jul/14 06:29;leftyl;HIVE-4209 added hive.cache.expr.evaluation in 0.12.0, but patch HIVE-6037-0.13.0 truncates its description to just the first sentence.  The full description should be:

{quote}
If true, the evaluation result of a deterministic expression referenced twice or more will be cached. For example, in a filter condition like "".. where key + 10 > 10 or key + 10 = 0"" the expression ""key + 10"" will be evaluated/cached once and reused for the following expression (""key + 10 = 0""). Currently, this is applied only to expressions in select or filter operators.
{quote};;;","07/Jul/14 07:30;leftyl;HIVE-7231 adds hive.exec.orc.default.block.size & hive.exec.orc.block.padding.tolerance in 0.14.0 with descriptions in hive-default.xml.template.  It also changes the default for hive.exec.orc.default.stripe.size to ""64L * 1024 * 1024"" (HiveConf.java) or ""67108864"" (template, same value).

Note:  The description of hive.exec.orc.block.padding.tolerance is slightly inaccurate -- instead of saying ""as a percentage of stripe size"" it should say ""as a decimal fraction of stripe size.""

Update 28/Jul/14:  HIVE-7490 changed the default of hive.exec.orc.default.stripe.size, so that doesn't have to be done here.;;;","09/Jul/14 06:59;leftyl;HIVE-6846 added hive.security.authorization.sqlstd.confwhitelist in 0.13.0, with a description in the HiveConf.java comment which isn't in hive-default.xml.template (nor in patch HIVE-6037-0.13.0).

The wiki has a revised description -- I recommend using it without the full parameter list, just refer to HIVE-6846 for the list:

* [Configuration Properties -- hive.security.authorization.sqlstd.confwhitelist | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.security.authorization.sqlstd.confwhitelist];;;","14/Jul/14 04:22;leftyl;/**
 *===============   Patch 20 for HIVE-6037 has been committed.   ===============
 */

* [Patch 20 | https://issues.apache.org/jira/secure/attachment/12655439/HIVE-6037.20.patch.txt];;;","24/Jul/14 18:14;leftyl;HIVE-6503 updated the description of hive.server2.authentication to include PAM (for HIVE-6466).  That update needs to be added to the new, improved HiveConf.java.;;;","24/Jul/14 18:16;leftyl;Also, a new value for hive.server2.authentication needs a description:  NOSASL.  It was added in 0.13.0 by a JIRA that I can't find, although there's helpful information in HIVE-4232 including this from [~cdrome]:

* [comment on HIVE-4232 from Chris Drome, 04/Apr/13 | https://issues.apache.org/jira/browse/HIVE-4232?focusedCommentId=13622931&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13622931]

{quote}
...to summarize the current state:

hive-site.xml -> transport -> JDBC connection string

1. hive.server2.authentication=NOSASL -> raw transport -> jdbc:hive2://host:port/dbname;auth=noSasl
2. hive.server2.authentication=NONE -> plain SASL transport -> jdbc:hive2://host:port/dbname
  (*DEFAULT*)
3. hive.server2.authentication=KERBEROS -> Kerberos SASL transport -> jdbc:hive2://host:port/dbname;principal=<principal>
{quote}

Googling ""Hive NOSASL"" also turned up some information in Cloudera & Hortonworks documentation.  Cloudera says Beeline doesn't work with NOSASL, and Hortonworks says hive.server2.enable.doAs can't be true when hive.server2.authentication is NOSASL.

* [Cloudera's Hue issues -- Beeline conflict | http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.3.0/CDH4-Release-Notes/cdh4ki_topic_1_14.html]
* [Hortonworks' ODBC Driver doc -- hive.server2.enable.doAs | http://hortonworks.com/wp-content/uploads/2013/04/Hortonworks-Hive-ODBC-Driver-User-Guide.pdf]

What JIRA ticket added NOSASL to hive.server2.authentication in 0.13.0?  (It was committed before 26 Sept. 2013 -- see NOSASL in HIVE-4763 patch.);;;","09/Aug/14 21:00;leftyl;HIVE-4123 added hive.exec.orc.write.format (default changed by HIVE-5091﻿) with a skimpy description.  An improved description is available in the comments to HIVE-4123 or the wiki.;;;","10/Aug/14 04:23;leftyl;Descriptions of some of the parameters added by HIVE-5728 in 0.13.0 have been fleshed out in the wiki:

* [Configuration Properties -- hive.exec.orc.default.row.index.stride | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.orc.default.row.index.stride] 
* [Configuration Properties -- hive.exec.orc.default.buffer.size | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.orc.default.buffer.size] 
* [Configuration Properties -- hive.exec.orc.default.block.padding | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.orc.default.block.padding];;;","10/Aug/14 04:28;leftyl;The description of hive.exec.orc.default.stripe.size (added by HIVE-5425 in 0.13.0) should specify units:

bq.  Define the default ORC stripe size, in bytes.;;;","12/Aug/14 02:20;leftyl;Some parameter descriptions from HIVE-6360 could be improved, although two of these are trivial:

* hive.exec.orc.zerocopy should be augmented with Hadoop version information:  ""Use zerocopy reads with ORC. (This requires Hadoop 2.3 or later.)""
* hive.tez.input.format should capitalize Tez and explain what AM means:  ""The default input format for Tez. Tez groups splits in the AM (ApplicationMaster).""
* hive.tez.container.size should add ""... the default"":  ""By default Tez will spawn containers of the size of a mapper. This can be used to overwrite the default.""
* hive.tez.java.opts, also add ""... the default"":  ""By default Tez will use the Java options from map tasks. This can be used to overwrite the default."";;;","15/Sep/14 07:08;leftyl;HIVE-6586.patch addresses all the fixes listed in the comments (except ones that had already been fixed), tidies up some line breaks, and makes minor edits to parameter descriptions.

The template file generated from the revised HiveConf.java is also attached.;;;","15/Sep/14 08:21;leftyl;I'm going to need some help posting the patch on RB.  Its UI kept cycling back to ""Select a repository"" after I gave it the base directory.  And apparently RBTools for Mac OS X needs Debian or Ubuntu, which I don't have and don't want to install just for one tool.  Should I try a browser other than Safari?;;;","16/Sep/14 05:18;leftyl;No help needed -- Firefox solved my UI problems on the Review Board.;;;","16/Sep/14 05:24;leftyl;Added review https://reviews.apache.org/r/25682/.;;;","01/Oct/14 06:52;leftyl;Rebased and fixed a few issues, including inability of template file to display angle brackets properly.  Generated and verified a new template file (also attached).  I'll update the review board.  Another comment will list the parameters that have significant changes to their descriptions -- no new parameters are added by this patch.;;;","01/Oct/14 07:08;leftyl;Patch 2 adds descriptions for 7 parameters, removes angle brackets from descriptions of 2 parameters, improves descriptions of 24 parameters, and makes trivial edits and format changes to many more.

If that's too much to review, I can strip out the trivial changes and create a new patch.  (But I'd only try to get the changes in at a later time, so why not deal with them now?)

If you don't have time to review the whole patch, please review the parameters you created.

7 descriptions added:

* hive.metastore.try.direct.sql -- HIVE-4051, [~sershe]
* hive.metastore.try.direct.sql.ddl -- HIVE-5626, [~sershe]
* hive.txn.manager -- HIVE-5843, [~gates]
* hive.server2.authentication -- HIVE-2935, [~cwsteinbach]
* hive.server2.use.SSL -- HIVE-5351, [~prasadm]
* hive.server2.keystore.path -- HIVE-5351, [~prasadm]
* hive.server2.keystore.password -- HIVE-5351, [~prasadm]

2 sets of angle brackets removed:

* hive.metastore.disallow.incompatible.col.type.changes -- HIVE-4409, [~dilipjoseph]
* hive.cache.expr.evaluation -- HIVE-4209, [~navis]

24 improvements: 

* hive.exec.orc.write.format
* hive.exec.orc.default.stripe.size
* hive.exec.orc.default.row.index.stride
* hive.exec.orc.default.buffer.size
* hive.exec.orc.default.block.padding
* hive.exec.orc.block.padding.tolerance
* hive.exec.orc.default.buffer.size
* hive.exec.orc.default.block.padding
* hive.exec.orc.block.padding.tolerance
* hive.exec.orc.zerocopy
* hive.txn.timeout 
* hive.txn.max.open.batch
* hive.compactor.initiator.on
* hive.compactor.worker.threads
* hive.compactor.worker.timeout
* hive.compactor.check.interval
* hive.compactor.delta.num.threshold
* hive.compactor.delta.pct.threshold
* hive.compactor.abortedtxn.threshold
* hive.security.metastore.authorization.manager
* hive.security.authorization.createtable.owner.grants
* hive.security.authorization.sqlstd.confwhitelist 
* hive.server2.thrift.sasl.qop
* hive.server2.async.exec.shutdown.timeout



;;;","01/Oct/14 07:30;leftyl;Patch 2 has been uploaded to the review board:  https://reviews.apache.org/r/25682/.;;;","02/Oct/14 22:08;gates;A few small comments left on review board.;;;","03/Oct/14 02:00;leftyl;Marking as blocker to get included in 0.14.0.

Error in previous change list:  hive.server2.authentication doesn't have a new description, it just adds PAM and NOSASL to the existing description.  PAM was added by HIVE-6466 ([~vgumashta]) but I don't know which jira added NOSASL.  Sorry about the false alarm, [~cwsteinbach].;;;","27/Oct/14 00:30;leftyl;Giving this jira ticket a new title, since no parameters are being added.

New patch coming soon....;;;","27/Oct/14 07:40;leftyl;Cancel patch2 in favor of patch3.;;;","27/Oct/14 07:43;leftyl;Patch 3 is for branch-0.14, not for trunk.;;;","27/Oct/14 07:46;leftyl;Patch 3 incorporates Alan Gates' review suggestions and omits inessential changes (minor edits and line breaks).;;;","27/Oct/14 08:35;leftyl;My attempts to upload patch 3 into the RB failed.  I'll try again tomorrow.

I suppose the problem was that I said trunk was the base directory, which it was for the first two patches but for patch 3 the base directory was branch-0.14.  Entering ""branch-0.14"" in the RB's ""Update Diff"" window gave an error, though.  I think I need remedial RB training.  Or do I just need to open a new review request based on branch-0.14?;;;","27/Oct/14 08:45;leftyl;Here's the list of changes to parameter descriptions in patch 3 (in order of appearance in HiveConf.java and the template file):

hive.exec.dynamic.partition.mode        -- add nonstrict value to description
hive.metastore.try.direct.sql                -- add description
hive.metastore.try.direct.sql.ddl           -- add description
hive.metastore.disallow.incompatible.col.type.changes    -- change MAP<..> to MAP to avoid & lt; and & gt; in template 
hive.jar.path                                       -- add description
hive.aux.jars.path                               -- add description
hive.added.files.path                           -- add ""This an internal parameter.""
hive.added.jars.path                            -- add ""This an internal parameter.""
hive.added.archives.path                     -- add ""This an internal parameter.""
hive.merge.orcfile.stripe.level               -- add ""or hive.merge.tezfiles"" to description 
hive.exec.orc.write.format                    -- explain possible values 
hive.exec.orc.default.stripe.size           -- add ""in bytes"" to description
hive.exec.orc.default.row.index.stride   -- add ""in number of rows"" and explain stride
hive.exec.orc.default.buffer.size           -- add ""in bytes"" to description
hive.exec.orc.default.block.padding      -- explain block padding 
hive.exec.orc.block.padding.tolerance   -- improve description, fix line breaks 
hive.exec.orc.zerocopy                        -- add ""(This requires Hadoop 2.3 or later.)""
hive.tez.cpu.vcores                             -- add line break
hive.stats.dbclass                               -- add ""fs"" to description and explain values 
hive.zookeeper.client.port                    -- add line breaks and space between words 
hive.txn.manager                                -- revise description per Alan Gates' review 
hive.txn.max.open.batch                     -- improve description (already reviewed by Alan Gates)
hive.compactor.initiator.on                   -- improve description, revise per Alan Gates' review 
hive.compactor.worker.threads             -- improve description, revise per Alan Gates' review 
hive.compactor.worker.timeout             -- improve description (already reviewed by Alan Gates)
hive.compactor.check.interval              -- improve description (already reviewed by Alan Gates)
hive.compactor.delta.num.threshold      -- improve description (already reviewed by Alan Gates)
hive.compactor.delta.pct.threshold        -- improve description (already reviewed by Alan Gates)
hive.compactor.abortedtxn.threshold     -- improve description (already reviewed by Alan Gates)
hive.cache.expr.evaluation                   -- improve description, avoiding ""<"" and "">"" due to & lt; and & gt; in template 
hive.security.metastore.authorization.manager           -- improve description
hive.security.authorization.createtable.owner.grants   -- improve description
hive.server2.thrift.sasl.qop                                       -- move comment into the description, fix leading spaces
hive.server2.async.exec.shutdown.timeout               -- improve description
hive.server2.authentication                                      -- add PAM and NOSASL to description
hive.server2.use.SSL                           -- add description
hive.server2.keystore.path                    -- add description
hive.server2.keystore.password             -- add description
hive.tez.dynamic.partition.pruning          -- add line breaks (needed \n), capitalize Tez;;;","28/Oct/14 04:04;hagleitn;Patch 3 doesn't apply cleanly on trunk.;;;","28/Oct/14 04:05;hagleitn;[~leftylev] - just make your patch work against trunk. I'll take care of committing it to .14 when it's ready to go.;;;","28/Oct/14 05:59;leftyl;Patch 4 has the same changes as patch 3 but it's based on trunk instead of branch-0.14.

It revises some transaction parameter descriptions per [~gates]' review, and omits most of patch 2's minor edits and line-break changes.;;;","28/Oct/14 06:15;leftyl;Patch 4 uploaded successfully to the review board:  https://reviews.apache.org/r/25682/.

(They're calling it #5.)

Could someone please review it?;;;","28/Oct/14 20:22;hagleitn;+1 LGTM;;;","28/Oct/14 20:30;hagleitn;.5 is the same as .4 - just reuploading to trigger a build. ([~leftylev] please do not upload another hive-default.xml.template, I think that's confusing the system).;;;","29/Oct/14 03:26;leftyl;bq.  ... do not upload another hive-default.xml.template ...

Okay, sorry about that.  Thanks [~hagleitn].;;;","29/Oct/14 03:33;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12677680/HIVE-6586.5.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 6580 tests executed
*Failed tests:*
{noformat}
org.apache.hive.minikdc.TestJdbcWithMiniKdc.testNegativeTokenAuth
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1513/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1513/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1513/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12677680 - PreCommit-HIVE-TRUNK-Build;;;","29/Oct/14 04:35;hagleitn;Committed to branch and trunk. Thanks [~leftylev]!;;;","29/Oct/14 04:49;leftyl;What is MiniKdc?  (Does Kdc mean key distribution center?);;;","29/Oct/14 04:55;leftyl;Woohoo!  That's a big load off my mind, thanks for the review & commit, [~hagleitn].;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;"
bucket map join fails in presence of _SUCCESS file,HIVE-6585,12699483,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,07/Mar/14 19:11,11/Mar/14 01:23,14/Jul/23 06:14,10/Mar/14 22:41,0.12.0,0.13.0,,,,,,,,0.13.0,,File Formats,,,,0,,,Reason is missing path filters.,,rhbutani,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/14 19:14;ashutoshc;HIVE-6585.patch;https://issues.apache.org/jira/secure/attachment/12633425/HIVE-6585.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377830,,,,Tue Mar 11 01:23:54 UTC 2014,,,,,,,,,,"0|i1t3s7:",378122,,,,,,,,,,,,,,,,,,,,,"07/Mar/14 19:14;ashutoshc;Simple fix.;;;","07/Mar/14 23:55;ashutoshc;https://reviews.apache.org/r/18916/;;;","09/Mar/14 15:02;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633425/HIVE-6585.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5375 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1675/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1675/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633425;;;","10/Mar/14 18:46;vikram.dixit;LGTM +1;;;","10/Mar/14 22:41;ashutoshc;Committed to trunk. 
[~rhbutani] Its a long standing bug, good to have it fixed in 0.13 as well.;;;","11/Mar/14 00:04;rhbutani;+1 for 0.13;;;","11/Mar/14 01:23;ashutoshc;Committed to 0.13 as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wrong sql comments : ----... instead of -- ---...,HIVE-6583,12699431,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,pNerzic,pNerzic,pNerzic,07/Mar/14 15:00,13/Nov/14 19:43,14/Jul/23 06:14,09/Mar/14 02:37,0.14.0,,,,,,,,,0.14.0,,Database/Schema,,,,0,,,"In file metastore/scripts/upgrade/mysql/hive-schema-0.13.0.mysql.sql, lines 799 and 801, a comment is written as ---------- (uninterrupted line of dashes) and should be -- ----------  (a space after 2 dashes)

(source https://dev.mysql.com/doc/refman/5.7/en/comments.html)",,pNerzic,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/14 15:02;pNerzic;HIVE-6583.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12633382/HIVE-6583.1.patch.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377778,,,,Thu Nov 13 19:43:18 UTC 2014,,,,,,,,,,"0|i1t3gn:",378070,,,,,,,,,,,,,,,,,,,,,"09/Mar/14 01:29;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633382/HIVE-6583.1.patch.txt

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5374 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_dyn_part
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1667/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1667/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633382;;;","09/Mar/14 02:12;ashutoshc;+1;;;","09/Mar/14 02:37;ashutoshc;Committed to trunk. Thanks, Pierre!;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"missing ; in HTML entities like &lt; in conf file",HIVE-6582,12699408,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,Fixed,,pNerzic,pNerzic,07/Mar/14 13:10,21/May/14 04:21,14/Jul/23 06:14,20/May/14 08:43,0.14.0,,,,,,,,,0.13.0,,Configuration,,,,0,,,"In conf/hive-default.xml.template, line 2392, the description of the property is malformed : &lthive... instead of &lt ;hive... and 3 other like this : &gt on the same line.
(I have problems with wikification to display &lt ; and not <)

This causes an error when launching hive : org.xml.sax.SAXParseException (translated from french) reference to entity ""lthive.user.install.directory"" must end with ';'.
",,leftyl,pNerzic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6586,,,,,,,,,,,,,,,,,,,,,"07/Mar/14 13:11;pNerzic;HIVE-6582.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12633369/HIVE-6582.1.patch.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377755,,,,Wed May 21 04:21:09 UTC 2014,,,,,,,,,,"0|i1t3bj:",378047,,,,,,,,,,,,,,,,,,,,,"07/Mar/14 20:53;leftyl;These four errors are in the description of *hive.user.install.directory* and the same errors exist in the current patch to HiveConf.java for HIVE-6037 (https://issues.apache.org/jira/secure/attachment/12631336/HIVE-6037.17.patch).

After HIVE-6037 commits, hive-default.xml.template will be generated from HiveConf.java so that's where these changes need to be made for Hive 0.13.0 and future releases.  But this patch could be applied to hive-default.xml.template to fix the problem in release 0.12.0 and previous releases.

I'll add a comment to HIVE-6037 so these corrections can be included with other fixes that HiveConf.java will need after HIVE-6037 gets committed.;;;","07/Mar/14 21:11;leftyl;Opened HIVE-6586 to deal with this in HiveConf.java.;;;","08/Mar/14 23:50;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633369/HIVE-6582.1.patch.txt

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5373 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1666/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1666/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633369;;;","20/May/14 08:38;leftyl;Release 0.13.0 includes these fixes in the description of hive.user.install.directory.

This jira can be closed without committing the patch.;;;","21/May/14 04:21;leftyl;Fixed in Hive 0.13.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor ThriftBinaryCLIService and ThriftHttpCLIService tests.,HIVE-6580,12699350,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vgumashta,vgumashta,vgumashta,07/Mar/14 06:16,21/Mar/14 07:24,14/Jul/23 06:14,21/Mar/14 07:24,0.13.0,,,,,,,,,0.13.0,,HiveServer2,,,,0,,,Refactor ThriftBinaryCLIService and ThriftHttpCLIService tests.,,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/14 23:51;vgumashta;HIVE-6580.1.patch;https://issues.apache.org/jira/secure/attachment/12635201/HIVE-6580.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377697,,,,Fri Mar 21 07:24:31 UTC 2014,,,,,,,,,,"0|i1t2yv:",377989,,,,,,,,,,,,,,,,,,,,,"17/Mar/14 23:52;vgumashta;[~thejas] This is up for review whenever you have time. Thanks!;;;","20/Mar/14 23:07;ashutoshc;+1;;;","21/Mar/14 03:29;vgumashta;Reattaching for pre-commit run;;;","21/Mar/14 05:56;thejas;cc [~rhbutani] Without this change, ThriftHttpCLIService tests hang. I think we should include this in 0.13.
;;;","21/Mar/14 07:24;ashutoshc;Committed to trunk & 0.13. Thanks, Vaibhav!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
change WebHCat reference examples to show user.name as a query paramter,HIVE-6577,12699322,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,07/Mar/14 02:04,13/Jun/15 17:50,14/Jul/23 06:14,12/Jun/15 23:48,,,,,,,,,,,,Documentation,WebHCat,,,0,,,"as of HIVE-6576, submitting POST requests to webhcat with user.name as a form parameter is deprecated.  user.name should instead be in the query string 'http://.../templeton/v1/ddl?use.name=foo'

Most WebHCat reference doc examples show 'curl -d user.name=foo', so they need to be updated.",,ekoifman,leftyl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6576,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377669,,,,Sat Jun 13 17:50:22 UTC 2015,,,,,,,,,,"0|i1t2sn:",377961,,,,,,,,,,,,,,,,,,,,,"12/Jun/15 23:48;ekoifman;updated examples remaining https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference;;;","13/Jun/15 06:38;leftyl;[~ekoifman]  I added a version note to all six POST docs -- please review.;;;","13/Jun/15 17:50;ekoifman;[~leftylev], LGTM.  Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sending user.name as a form parameter in POST doesn't work post HADOOP-10193,HIVE-6576,12699319,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,ekoifman,ekoifman,ekoifman,07/Mar/14 01:49,01/Feb/16 17:15,14/Jul/23 06:14,14/Mar/14 09:44,0.13.0,,,,,,,,,0.13.0,,WebHCat,,,,0,TODOC13,,"WebHCat uses AuthFilter to handle authentication.  In simple mode that means using PseudoAuthenticationHandler.  Prior to HADOOP-10193, the latter handled user.name as form parameter in a POST request.  Now it only handles it as a query parameter.  

to maintain webhcat backwards compat, we need to make WebHCat still extract it from form param.  This will be deprecated immediately and removed in 0.15

Also, all examples in WebHCat reference manual should be updated to use user.name in query string from current form param (curl -d user.name=foo)",,ekoifman,leftyl,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6577,,,HADOOP-10193,HIVE-12420,,,,,,,,,,,,,,,,,,,,"07/Mar/14 02:21;ekoifman;HIVE-6576.patch;https://issues.apache.org/jira/secure/attachment/12633293/HIVE-6576.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377666,,,,Sat Jun 13 06:53:29 UTC 2015,,,,,,,,,,"0|i1t2rz:",377958,,,,,,,,,,,,,,,,,,,,,"08/Mar/14 12:07;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633293/HIVE-6576.patch

{color:green}SUCCESS:{color} +1 5373 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1657/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1657/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633293;;;","13/Mar/14 15:46;thejas;+1;;;","14/Mar/14 09:44;thejas;Patch committed to trunk and 0.13 branch (included in 0.13 wiki ).
Thanks for the contribution Eugene!
;;;","13/Jun/15 06:53;leftyl;Doc note:  [~ekoifman] documented this for all of the POST request docs, so I'm removing the TODOC13 label.  (See doc jira HIVE-6577.)

* [WebHCat Reference -- POST ddl -- Curl Command | https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference+DDL#WebHCatReferenceDDL-CurlCommand]
* [WebHCat Reference -- POST table -- Curl Command | https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference+PostTable#WebHCatReferencePostTable-CurlCommand]
* [WebHCat Reference -- POST mapreduce/streaming -- Curl Command | https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference+MapReduceStream#WebHCatReferenceMapReduceStream-CurlCommand]
* [WebHCat Reference -- POST mapreduce/jar -- Curl Command | https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference+MapReduceJar#WebHCatReferenceMapReduceJar-CurlCommand]
* [WebHCat Reference -- POST pig -- Curl Command | https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference+Pig#WebHCatReferencePig-CurlCommand]
* [WebHCat Reference -- POST hive -- Curl Command | https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference+Hive#WebHCatReferenceHive-CurlCommand];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
select * fails on parquet table with map datatype,HIVE-6575,12699315,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,szehon,szehon,szehon,07/Mar/14 01:23,31/Mar/14 21:29,14/Jul/23 06:14,12/Mar/14 17:48,0.13.0,,,,,,,,,0.13.0,,Serializers/Deserializers,,,,0,parquet,,"Create parquet table with map and run select * from parquet_table, returns following exception:

{noformat}
 FAILED: RuntimeException java.lang.ClassCastException: org.apache.hadoop.hive.ql.io.parquet.serde.DeepParquetHiveMapInspector cannot be cast to org.apache.hadoop.hive.ql.io.parquet.serde.StandardParquetHiveMapInspector
{noformat}

However select <mapcol> from parquet_table seems to work, and thus joins will work.",,mdominguez@cloudera.com,rhbutani,szehon,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6491,HIVE-6794,,,,,HIVE-6634,,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/14 22:20;szehon;HIVE-6575.2.patch;https://issues.apache.org/jira/secure/attachment/12633475/HIVE-6575.2.patch","08/Mar/14 23:43;szehon;HIVE-6575.3.patch;https://issues.apache.org/jira/secure/attachment/12633569/HIVE-6575.3.patch","07/Mar/14 22:12;szehon;HIVE-6575.patch;https://issues.apache.org/jira/secure/attachment/12633473/HIVE-6575.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377662,,,,Thu Mar 13 00:52:23 UTC 2014,,,,,,,,,,"0|i1t2r3:",377954,,,,,,,,,,,,,,,,,,,,,"07/Mar/14 22:14;szehon;Review board is here: [https://reviews.apache.org/r/18925/|https://reviews.apache.org/r/18925/];;;","07/Mar/14 22:20;szehon;Trimmed whitespace.;;;","07/Mar/14 23:08;xuefuz;Thanks for working on this, [~szehon]. I have left some minor comments on RB for your consideration.;;;","08/Mar/14 23:43;szehon;Thanks Xuefu for review, I responded to latest comment + updated the latest patch, not sure if you had a chance to take a look.;;;","09/Mar/14 01:13;xuefuz;+1 pending test result.;;;","09/Mar/14 20:05;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633569/HIVE-6575.3.patch

{color:green}SUCCESS:{color} +1 5374 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1680/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1680/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633569;;;","12/Mar/14 17:07;szehon;Hi [~xuefuz] can we commit this when you get a chance?  If possible it would be good to get it in for 0.13.  Thanks.;;;","12/Mar/14 17:40;xuefuz;[~szehon] I just notice that there is no test case for this. A test case would be good to prevent future breakage. I can commit this, but could you please create and work on a followup JIRA adding the missing test case?;;;","12/Mar/14 17:44;szehon;OK I created it, will work on that one next.;;;","12/Mar/14 17:48;xuefuz;Patch committed to trunk. Thanks goes to Szehon for the contribution.;;;","13/Mar/14 00:52;rhbutani;committed to 0.13.0 branch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Type in ql/pom.xml prevents jobs from parquet queries from running on a cluster,HIVE-6574,12699290,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,xuefuz,xuefuz,xuefuz,06/Mar/14 23:23,10/Mar/14 18:04,14/Jul/23 06:14,08/Mar/14 00:44,0.13.0,,,,,,,,,0.13.0,,Serializers/Deserializers,,,,0,,,"MR job complains about not found class:
{code}
Caused by: java.lang.ClassNotFoundException: parquet.hadoop.ParquetInputFormat
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:423)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
	... 64 more
{code}",,brocknoland,rhbutani,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 23:30;xuefuz;HIVE-6574.patch;https://issues.apache.org/jira/secure/attachment/12633254/HIVE-6574.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377637,,,,Mon Mar 10 18:04:49 UTC 2014,,,,,,,,,,"0|i1t2lj:",377929,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 23:31;brocknoland;My mistake. +1;;;","07/Mar/14 22:40;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633254/HIVE-6574.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5360 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1651/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1651/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633254;;;","08/Mar/14 00:44;xuefuz;The above test failure is unrelated. Patched committed to trunk. Thanks Brock for the review.;;;","08/Mar/14 00:52;brocknoland;Thanks! I think we should also commit this to 0.13.;;;","08/Mar/14 01:00;xuefuz;Yes. I communicated this in the release email thread. Do I need to actually commit it or the release ""manager"" will?;;;","10/Mar/14 18:02;rhbutani;ported to 0.13;;;","10/Mar/14 18:04;xuefuz;Thanks, Harish!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Oracle metastore doesnt come up when hive.cluster.delegation.token.store.class is set to DBTokenStore,HIVE-6573,12699289,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,ashutoshc,ashutoshc,ashutoshc,06/Mar/14 23:21,09/Mar/14 01:40,14/Jul/23 06:14,08/Mar/14 04:50,0.12.0,,,,,,,,,0.13.0,,Metastore,Security,,,0,,,This config {{hive.cluster.delegation.token.store.class}} was introduced in HIVE-3255 and is useful only if oracle metastore is used in secure setup with HA config.,,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 23:27;ashutoshc;HIVE-6573.patch;https://issues.apache.org/jira/secure/attachment/12633252/HIVE-6573.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377636,,,,Sun Mar 09 01:40:59 UTC 2014,,,,,,,,,,"0|i1t2lb:",377928,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 23:27;ashutoshc;Simple fix. Once we use native strategy as value generator for primary key, sequence strategy will be used for Oracle. For others it will continue to be identity as previously. This got introduced in DN 3.x so we can use it now that we have switched to DN 3.x More info: http://www.datanucleus.org/products/datanucleus/jdo/value_generation.html;;;","07/Mar/14 18:45;thejas;+1;;;","08/Mar/14 00:15;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633252/HIVE-6573.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5360 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1652/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1652/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633252;;;","08/Mar/14 04:50;ashutoshc;Committed to trunk. 
[~rhbutani] This is an existing bug on 0.12. It will be good to have it fixed in 0.13. I can commit to branch if you are on board.;;;","08/Mar/14 15:41;rhbutani;+1 for 0.13;;;","09/Mar/14 01:40;ashutoshc;Committed to 0.13 branch as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Use shimmed version of hadoop conf names for mapred.{min,max}.split.size{.*}",HIVE-6572,12699287,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sushanth,sushanth,sushanth,06/Mar/14 23:09,20/Mar/14 22:01,14/Jul/23 06:14,12/Mar/14 22:54,0.13.0,0.14.0,,,,,,,,0.13.0,,,,,,0,,,"HadoopShims has a method to fetch config parameters by name so that they return the appropriate config param name for the appropriate hadoop version. We need to be consistent about using these versions.

For eg:. mapred.min.split.size is deprecated with hadoop 2.x, and is instead called mapreduce.input.fileinputformat.split.minsize .

Also, there is a bug in Hadoop23Shims, Hadoop20SShims and Hadoop20Shims that defines MAPREDMINSPLITSIZEPERNODE as mapred.min.split.size.per.rack and MAPREDMINSPLITSIZEPERRACK as mapred.min.split.size.per.node. This is wrong and confusing.",,ashutoshc,leftyl,omalley,rhbutani,sushanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-5910,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 23:16;sushanth;HIVE-6572.patch;https://issues.apache.org/jira/secure/attachment/12633249/HIVE-6572.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377634,,,,Wed Mar 12 22:54:57 UTC 2014,,,,,,,,,,"0|i1t2kv:",377926,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 23:16;sushanth;Patch attached.

[~owen.omalley], could you please check this patch real quick?

[~rhbutani], I would like this patch to also be included in the hive-0.13 release, since it fixes some erroneous config usage introduced in the 0.13 timeframe, and we don't want to have a release with them, which might make this behaviour something we have to support in the longer run.;;;","07/Mar/14 00:04;sushanth;Review board link : https://reviews.apache.org/r/18878/;;;","07/Mar/14 00:05;sushanth;[~ashutoshc], I'd appreciate your eyes on this for a review as well. :);;;","08/Mar/14 05:20;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633249/HIVE-6572.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5373 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_merge
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1654/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1654/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633249;;;","12/Mar/14 20:31;ashutoshc;+1;;;","12/Mar/14 22:54;ashutoshc;Committed to trunk & 0.13 Thanks, Sushanth!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
query id should be available for logging during query compilation,HIVE-6571,12699275,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,hagleitn,hagleitn,hagleitn,06/Mar/14 22:07,13/Nov/14 19:42,14/Jul/23 06:14,07/Mar/14 20:55,,,,,,,,,,0.14.0,,,,,,0,,,Would be nice to have the query id set during compilation to tie logs together etc.,,hagleitn,leftyl,sershe,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 22:08;hagleitn;HIVE-6571.1.patch;https://issues.apache.org/jira/secure/attachment/12633236/HIVE-6571.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377622,,,,Thu Nov 13 19:42:30 UTC 2014,,,,,,,,,,"0|i1t2if:",377915,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 22:09;sershe;+1;;;","06/Mar/14 22:12;sershe;this queryId really ties the logs together...;;;","07/Mar/14 17:33;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633236/HIVE-6571.1.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5358 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hive.beeline.TestSchemaTool.testSchemaInit
org.apache.hive.beeline.TestSchemaTool.testSchemaUpgrade
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1647/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1647/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633236;;;","07/Mar/14 20:50;hagleitn;Test failures are unrelated.;;;","07/Mar/14 20:55;hagleitn;Committed to trunk;;;","12/Mar/14 07:39;leftyl;Q1:  Why doesn't this jira have a fix version number?

Q2:  Should this change be mentioned in the wiki?  For example, here:

* [Getting Started:  Error Logs |https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-ErrorLogs];;;","25/Apr/14 08:20;leftyl;I'll assume this doesn't need to be documented in the wiki.;;;","30/Jun/14 22:20;sushanth;Changing fix version to 0.14.0, since this is not committed on branch-0.13, and was only committed on 0.14 trunk.;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Hive variable substitution does not work with the ""source"" command",HIVE-6570,12699267,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,erwaman,erwaman,erwaman,06/Mar/14 21:18,13/Nov/14 19:43,14/Jul/23 06:14,30/Mar/14 15:00,,,,,,,,,,0.14.0,,,,,,0,TODOC14,,"The following does not work:
{code}
source ${hivevar:test-dir}/test.q;
{code}",,appodictic,erwaman,leftyl,qwertymaniac,szehon,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-1014,HIVE-6791,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/14 20:40;erwaman;HIVE-6570.1.patch;https://issues.apache.org/jira/secure/attachment/12633453/HIVE-6570.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377614,,,,Thu Nov 13 19:43:36 UTC 2014,,,,,,,,,,"0|i1t2gn:",377907,"This patch adds Hive variable substitution support to the ""source"" command.  For example, you will now be able to use a statement such as:
source ${hivevar:test-dir}/test.q; ",,,,,,,,,,,,,,,,,,,,"06/Mar/14 21:21;erwaman;I have a fix for this issue and will upload a patch shortly.;;;","06/Mar/14 23:14;erwaman;Added support for Hive variable substitution with the ""source"" command, and added a test for this in source.q.;;;","06/Mar/14 23:41;erwaman;Review Board code review: https://reviews.apache.org/r/18874/;;;","07/Mar/14 00:26;szehon;Looks like this will work, +1 (non-binding);;;","07/Mar/14 20:40;erwaman;Reuploading identical patch without the "".txt"" suffix so that the precommit tests will run.;;;","07/Mar/14 20:56;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633248/HIVE-6570.1.patch.txt

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5360 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hive.beeline.TestSchemaTool.testSchemaInit
org.apache.hive.beeline.TestSchemaTool.testSchemaUpgrade
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1650/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1650/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633248;;;","09/Mar/14 16:44;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633453/HIVE-6570.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5374 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1678/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1678/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633453;;;","12/Mar/14 20:44;ashutoshc;+1;;;","12/Mar/14 22:10;erwaman;The single test failure seems unrelated to my change.

Ashutosh, can you commit my change for me?;;;","12/Mar/14 22:12;ashutoshc;yeah, failure is unrelated. I will commit this shortly.;;;","12/Mar/14 22:13;appodictic;WE should make a release note if someone has  $ in there file hive might now try to interpret it.;;;","12/Mar/14 23:26;ashutoshc;If I am reading the patch correctly, it does that interpretation only for {{source}} statement, not for anything else.;;;","17/Mar/14 21:22;erwaman;Added a Release Note explaining the changes in this patch.;;;","17/Mar/14 21:30;ashutoshc;[~appodictic] Let us know if you have any more feedback.;;;","21/Mar/14 23:52;erwaman;Ping;;;","22/Mar/14 00:01;ashutoshc;I am waiting to hear from [~appodictic] seems like he had some concerns.;;;","28/Mar/14 20:39;erwaman;What concerns does [~appodictic] have?;;;","29/Mar/14 01:22;appodictic;No major concern the release note is enough information. Sorry I was not paying attention to this thread. Please proceed.;;;","29/Mar/14 16:26;erwaman;Thanks.  Could one of you guys commit the patch for me please?;;;","29/Mar/14 20:56;leftyl;Do the bug & fix need to be mentioned in the wiki?  A version note could be added in the CLI doc and/or Variable Substitution:

* [CLI:  Hive Interactive Shell Commands |https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli#LanguageManualCli-HiveInteractiveShellCommands]
* [Variable Substitution |https://cwiki.apache.org/confluence/display/Hive/LanguageManual+VariableSubstitution];;;","30/Mar/14 08:18;erwaman;It would probably be nice to add an example to the Variable Substitution page that uses variable substitution with the source command.

On a side note, how does one get edit privileges for the wiki?;;;","30/Mar/14 08:49;leftyl;""AboutThisWiki"" gives these instructions:

{quote}
*How to get permission to edit*

* Create a Confluence account
* Sign up for the user mailing list by sending a message to user-subscribe@hive.apache.org
* Send a message to user@hive.apache.org requesting write access
{quote}

[AboutThisWiki |https://cwiki.apache.org/confluence/display/Hive/AboutThisWiki];;;","30/Mar/14 15:00;xuefuz;Patch committed to trunk. Thanks to Anthony for the contribution.;;;","31/Mar/14 16:49;erwaman;[~leftylev] - Thanks for the instructions!
[~xuefuz] - Thanks for committing this!;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HCatalog still has references to deprecated property hive.metastore.local,HIVE-6569,12699265,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,sushanth,sushanth,sushanth,06/Mar/14 21:07,13/Nov/14 19:41,14/Jul/23 06:14,12/Mar/14 23:03,,,,,,,,,,0.14.0,,,,,,0,cleanup,hcatalog,"HIVE-2585 removed the conf parameter hive.metastore.local, but HCatalog still has references to it. Most of it is in tests, but one is in PigHCatUtil, which leads to HCatLoader/HCatStorer jobs giving warnings. We need to remove them.",,ekoifman,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 21:47;sushanth;HIVE-6569.2.patch;https://issues.apache.org/jira/secure/attachment/12633230/HIVE-6569.2.patch","06/Mar/14 21:09;sushanth;HIVE-6569.patch;https://issues.apache.org/jira/secure/attachment/12633224/HIVE-6569.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377612,,,,Thu Nov 13 19:41:15 UTC 2014,,,,,,,,,,"0|i1t2g7:",377905,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 21:09;sushanth;Patch attached.;;;","06/Mar/14 21:17;ekoifman;webhcat-default.xml has a ref to it as well.  Should probably be removed as well;;;","06/Mar/14 21:46;sushanth;Good catch - updating patch with a couple more instances I found in .xml files.;;;","07/Mar/14 12:23;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633230/HIVE-6569.2.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5357 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
org.apache.hive.beeline.TestSchemaTool.testSchemaInit
org.apache.hive.beeline.TestSchemaTool.testSchemaUpgrade
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1644/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1644/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633230;;;","07/Mar/14 23:40;sushanth;Note : None of the test failures reported are to do with this patch.;;;","12/Mar/14 22:13;ashutoshc;+1;;;","12/Mar/14 23:03;ashutoshc;Committed to trunk. Thanks, Sushanth!;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vectorized cast of decimal to string and timestamp produces incorrect result.,HIVE-6568,12699203,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jnp,jnp,jnp,06/Mar/14 17:36,12/Mar/14 20:04,14/Jul/23 06:14,12/Mar/14 20:04,0.13.0,,,,,,,,,0.13.0,,Vectorization,,,,0,,,"A decimal value 1.23 with scale 5 is represented in string as 1.23000. This behavior is different from HiveDecimal behavior.
The difference in cast to timestamp is due to more aggressive rounding in vectorized expression.",,ehans,jnp,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6511,HIVE-6531,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/14 19:36;jnp;HIVE-6568.1.patch;https://issues.apache.org/jira/secure/attachment/12633436/HIVE-6568.1.patch","09/Mar/14 17:50;jnp;HIVE-6568.2.patch;https://issues.apache.org/jira/secure/attachment/12633603/HIVE-6568.2.patch","10/Mar/14 16:58;jnp;HIVE-6568.3.patch;https://issues.apache.org/jira/secure/attachment/12633731/HIVE-6568.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377550,,,,Wed Mar 12 19:39:09 UTC 2014,,,,,,,,,,"0|i1t22n:",377844,,,,,,,,,,,,,,,,,,,,,"09/Mar/14 15:03;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633436/HIVE-6568.1.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1677/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1677/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-1677/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'common/src/java/org/apache/hadoop/hive/common/FileUtils.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/io/BucketizedHiveInputFormat.java'
++ egrep -v '^X|^Performing status on external'
++ awk '{print $2}'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/storage-handlers/hbase/target hcatalog/server-extensions/target hcatalog/core/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target hwi/target common/target common/src/gen service/target contrib/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target ql/src/test/results/clientpositive/bucket_if_with_path_filter.q.out ql/src/test/queries/clientpositive/bucket_if_with_path_filter.q
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1575712.

At revision 1575712.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633436;;;","09/Mar/14 17:50;jnp;Updated patch re-based against latest trunk.;;;","10/Mar/14 12:47;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633603/HIVE-6568.2.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5375 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastDecimalToString
org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1692/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1692/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633603;;;","10/Mar/14 16:59;jnp;Updated patch fixes the test TestVectorTypeCasts.testCastDecimalToString.
testExecuteStatementAsync is not related to the patch.;;;","10/Mar/14 17:02;jnp;Review board: https://reviews.apache.org/r/18972/;;;","11/Mar/14 16:55;ehans;+1;;;","11/Mar/14 20:56;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633731/HIVE-6568.3.patch

{color:green}SUCCESS:{color} +1 5378 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1703/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1703/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633731;;;","12/Mar/14 18:51;jnp;[~rhbutani] This is a serious bug in hive-0.13 and should be ported to branch-0.13 as well. I will commit it to both trunk and branch-0.13.;;;","12/Mar/14 19:39;rhbutani;+1 for 0.13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect union-all plan with map-joins on Tez,HIVE-6566,12699093,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,06/Mar/14 08:29,22/Mar/14 06:19,14/Jul/23 06:14,07/Mar/14 20:36,0.13.0,,,,,,,,,0.13.0,,,,,,0,,,The tez dag is hooked up incorrectly for some union all queries involving map joins. That's quite common and results in either NPE or invalid results.,,hagleitn,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 08:30;hagleitn;HIVE-6566.1.patch;https://issues.apache.org/jira/secure/attachment/12633075/HIVE-6566.1.patch","06/Mar/14 19:50;hagleitn;HIVE-6566.2.patch;https://issues.apache.org/jira/secure/attachment/12633213/HIVE-6566.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377440,,,,Fri Mar 07 20:36:42 UTC 2014,,,,,,,,,,"0|i1t1ef:",377734,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 19:50;hagleitn;.2 adds comments (per review request).;;;","06/Mar/14 20:46;sershe;+1;;;","07/Mar/14 06:36;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633213/HIVE-6566.2.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5358 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_groupby2
org.apache.hive.beeline.TestSchemaTool.testSchemaInit
org.apache.hive.beeline.TestSchemaTool.testSchemaUpgrade
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1643/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1643/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633213;;;","07/Mar/14 20:02;hagleitn;Failures are unrelated.;;;","07/Mar/14 20:36;hagleitn;Committed to trunk and branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WebHCat E2E tests that launch MR jobs fail on check job completion timeout,HIVE-6564,12699086,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,deepesh,deepesh,deepesh,06/Mar/14 07:56,13/Nov/14 19:41,14/Jul/23 06:14,24/Jun/14 05:40,0.13.0,,,,,,,,,0.14.0,,Tests,WebHCat,,,0,,,"WebHCat E2E tests that fire off an MR job are not correctly being detected as complete so those tests are timing out.
The problem is happening because of JSON module available through cpan which returns 1 or 0 instead of true or false.
NO PRECOMMIT TESTS
",,deepesh,ekoifman,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/14 03:53;deepesh;HIVE-6564.2.patch;https://issues.apache.org/jira/secure/attachment/12633306/HIVE-6564.2.patch","06/Mar/14 07:58;deepesh;HIVE-6564.patch;https://issues.apache.org/jira/secure/attachment/12633069/HIVE-6564.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377433,,,,Thu Nov 13 19:41:08 UTC 2014,,,,,,,,,,"0|i1t1cv:",377727,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 07:58;deepesh;Attaching a patch that deals with JSON module incompatibility.;;;","07/Mar/14 03:53;deepesh;Attaching a new patch as the previous one had a small syntax issue.;;;","10/Jun/14 14:35;ashutoshc;cc: [~ekoifman] Can you take a quick look?;;;","10/Jun/14 18:44;ekoifman;+1;;;","24/Jun/14 04:45;ashutoshc;+1;;;","24/Jun/14 05:40;ashutoshc;Committed to trunk. Thanks, Deepesh!;;;","24/Jun/14 18:25;deepesh;Thanks Eugene and Ashutosh!;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hdfs jar being pulled in when creating a hadoop-2 based hive tar ball,HIVE-6563,12699059,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,vikram.dixit,vikram.dixit,vikram.dixit,06/Mar/14 02:53,13/Nov/14 19:41,14/Jul/23 06:14,11/Mar/14 20:12,0.13.0,0.14.0,,,,,,,,0.13.0,0.14.0,Build Infrastructure,,,,0,,,Looks like some dependency issue is causing hadoop-hdfs jar to be packaged in the hive tar ball.,,rhbutani,thejas,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 02:54;vikram.dixit;HIVE-6563.1.patch;https://issues.apache.org/jira/secure/attachment/12633020/HIVE-6563.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377406,,,,Thu Nov 13 19:41:52 UTC 2014,,,,,,,,,,"0|i1t16v:",377700,,,,,,,,,,,,,,,,,,,,,"11/Mar/14 00:59;rhbutani;+1;;;","11/Mar/14 20:12;vikram.dixit;Committed to trunk and branch-0.13.;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Protection from exceptions in ORC predicate evaluation,HIVE-6562,12699056,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,prasanth_j,prasanth_j,prasanth_j,06/Mar/14 02:40,18/Mar/14 01:19,14/Jul/23 06:14,18/Mar/14 01:19,0.13.0,,,,,,,,,0.13.0,,,,,,0,orcfile,,"ORC evaluates predicate expressions to select row groups that satisfy predicate condition. There can be exceptions (mostly ClassCastException) when data types of predicate constant and min/max values are different. 
To avoid this patch catches any such exception and provides a default behaviour i.e; selecting the row group.",,hagleitn,prasanth_j,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 02:42;prasanth_j;HIVE-6562.1.patch;https://issues.apache.org/jira/secure/attachment/12633017/HIVE-6562.1.patch","12/Mar/14 01:18;prasanth_j;HIVE-6562.2.patch;https://issues.apache.org/jira/secure/attachment/12634071/HIVE-6562.2.patch","13/Mar/14 18:33;prasanth_j;HIVE-6562.3.patch;https://issues.apache.org/jira/secure/attachment/12634506/HIVE-6562.3.patch","12/Mar/14 01:19;prasanth_j;HIVE-6562.3.patch;https://issues.apache.org/jira/secure/attachment/12634072/HIVE-6562.3.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377403,,,,Tue Mar 18 01:19:14 UTC 2014,,,,,,,,,,"0|i1t167:",377697,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 02:42;prasanth_j;Initial patch. [~owen.omalley]/[~hagleitn] can you please take a look at it?;;;","07/Mar/14 03:25;prasanth_j;This is related to HIVE-4177. But this patch uses the same code for partialscan and noscan.;;;","07/Mar/14 22:20;hagleitn;[~prasanth_j] this sounds like a bad idea, because it has the potential of masking random errors. Can't you do a check for types (or avoid having types that don't match altogether?);;;","07/Mar/14 22:34;prasanth_j;[~hagleitn] i agree that it will mask other errors too.. adding checks for valid types is also error prone..
instead of checking for different valid types for comparison.. how about just looking for ClassCastException and just ignore it i.e, select that particular row group..?;;;","07/Mar/14 22:36;prasanth_j;[~hagleitn] selection row group is our best effort to choose row groups based on predicates.. so it shouldn't throw in any case.. ;;;","08/Mar/14 13:46;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633017/HIVE-6562.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5374 tests executed
*Failed tests:*
{noformat}
org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1658/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1658/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633017;;;","10/Mar/14 19:03;prasanth_j;Addressed [~hagleitn] initial review comments.;;;","10/Mar/14 19:04;prasanth_j;attaching RB link;;;","11/Mar/14 21:54;sershe;minor comment on RB. Does any test (I just skimmed them) actually test the new path w/exception?;;;","11/Mar/14 21:57;prasanth_j;Yes. testPredEvalWithDateStats() has some invalid cases that should throw.;;;","12/Mar/14 01:18;prasanth_j;Earlier patch had issues with Date conversions. Fixed them in this patch.;;;","12/Mar/14 01:19;prasanth_j;Patch version was wrong. Reuploading again.;;;","12/Mar/14 01:31;sershe;lgtm;;;","12/Mar/14 01:32;sershe;(as in, +1);;;","13/Mar/14 18:33;prasanth_j;Reuploading patch for precommit tests.;;;","15/Mar/14 04:50;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634506/HIVE-6562.3.patch

{color:green}SUCCESS:{color} +1 5412 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1786/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1786/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634506;;;","18/Mar/14 01:19;sershe;in trunk and 13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
varchar and char types cannot be cast to binary,HIVE-6560,12699037,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,csun,xuefuz,xuefuz,06/Mar/14 01:25,13/Nov/14 19:42,14/Jul/23 06:14,16/Jul/14 22:44,0.12.0,0.13.0,0.13.1,,,,,,,0.14.0,,Types,UDF,,,0,,,"GenericUDFToBinary can convert string to binary. VARCHAR and CHAR are substitutable with string. Thus, GenericUDFToBinary should also be able to convert VARCHAR and CHAR to binary. However,

{code}
hive> select binary(cast('abc' as varchar(5)) from decimal_udf limit 1;
FAILED: ParseException line 1:40 missing ) at 'from' near '<EOF>'
hive> select binary(cast('abc' as varchar(5))) from decimal_udf limit 1;
FAILED: SemanticException Line 0:-1 Wrong arguments ''abc'': Only string or binary data can be cast into binary data types.
{code}",,csun,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/14 21:58;csun;HIVE-6560.1.patch;https://issues.apache.org/jira/secure/attachment/12655887/HIVE-6560.1.patch","16/Jul/14 15:32;csun;HIVE-6560.2.patch;https://issues.apache.org/jira/secure/attachment/12656059/HIVE-6560.2.patch","16/Jul/14 17:54;csun;HIVE-6560.3.patch;https://issues.apache.org/jira/secure/attachment/12656086/HIVE-6560.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377384,,,,Thu Nov 13 19:42:49 UTC 2014,,,,,,,,,,"0|i1t11z:",377678,,,,,,,,,,,,TODOC14,,,,,,,,,"15/Jul/14 21:58;csun;I've proposed a patch for this issue,. Can someone please give a review for this? Thanks.;;;","15/Jul/14 22:16;xuefuz;Patch looks good to me. +1 pending on tests.;;;","16/Jul/14 01:09;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12655887/HIVE-6560.1.patch

{color:red}ERROR:{color} -1 due to 11 failed/errored test(s), 5735 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_binary
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_temp_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_to_binary_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_to_binary_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_to_binary_3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_to_binary_4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_to_binary_5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_to_binary_6
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/801/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/801/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-801/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 11 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12655887;;;","16/Jul/14 01:13;xuefuz;[~csun] Looks like some failures are related to your patch. Could you do an analysis and address them accordingly?;;;","16/Jul/14 15:32;csun;Attempting to address test failures due to a unsynced output file.;;;","16/Jul/14 17:10;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12656059/HIVE-6560.2.patch

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 5735 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_temp_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_to_binary_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_to_binary_2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_to_binary_3
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_to_binary_4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_to_binary_5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_invalid_cast_to_binary_6
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataPrimitiveTypes
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/811/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/811/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-811/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12656059;;;","16/Jul/14 17:54;csun;Forgot to address invalid_cast_to_binary_N.q. Now fixed.;;;","16/Jul/14 21:19;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12656086/HIVE-6560.3.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5720 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_temp_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.testRevokeTimedOutWorkers
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/812/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/812/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-812/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12656086;;;","16/Jul/14 21:55;csun;The rest of test failures do not appear to be related to this patch.;;;","16/Jul/14 22:28;xuefuz;Patch committed to trunk. Thanks Chao for the contribution.;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sourcing txn-script from schema script results in failure for mysql & oracle,HIVE-6559,12699016,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,gates,ashutoshc,ashutoshc,05/Mar/14 23:48,01/Oct/19 22:07,14/Jul/23 06:14,12/Mar/14 05:05,0.13.0,,,,,,,,,0.13.0,,Metastore,,,,0,,,"On mysql, I got:
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '----------------------------

----------------------------
SOURCE hive-txn-schem' at line 1

On Oracle, I got:
SP2-0310: unable to open file ""hive-txn-schema-0.13.0.oracle.sql"" 
",,gates,prasadm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6669,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/14 17:59;gates;HIVE-6559.patch;https://issues.apache.org/jira/secure/attachment/12633741/HIVE-6559.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377363,,,,Wed Mar 12 05:05:29 UTC 2014,,,,,,,,,,"0|i1t0xb:",377657,,,,,,,,,,,,,,,,,,,,,"05/Mar/14 23:56;ashutoshc;Also, Prasad [brought up|https://issues.apache.org/jira/browse/HIVE-6555?focusedCommentId=13921608&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13921608] on HIVE-6555 that schema version update should happen at last.
[~gates] Can you take a look?;;;","06/Mar/14 00:01;prasadm;Thanks [~ashutoshc] !
It's a pretty simple change. I can take care of it.;;;","06/Mar/14 00:05;ashutoshc;Sorry, [~prasadm] I mixed up two issues here. One which you brought up. Another different issue is calling out txn-script from schema script is not working as intended.  See description of this issue. Feel free to pick either one or both of them.;;;","06/Mar/14 00:09;prasadm;The changes will be done on top of the 0.14 scripts that are being added by HIVE-6555 patch.;;;","06/Mar/14 00:12;prasadm;ah ok. sorry, didn't notice that. I was talking about the script ordering issue. will create a separate ticket for that. Thanks
;;;","09/Mar/14 02:19;ashutoshc;Although, HIVE-6583 will help one get past problem I pasted above, that doesn't help completely. New requirement because of sourcing another script leads one into a situation that you need to be in same dir to perform this sourcing correctly. Otherwise, you will get 
{code}
ERROR: 
Failed to open file 'hive-txn-schema-0.13.0.mysql.sql', error: 2
{code}
This is inconvenience for user, but real problems is for tools because they now need to cd into correct dir to invoke this script.
I believe we should not source another script, but inline all sql statements in existing script.;;;","10/Mar/14 17:59;gates;I fixed the issue by removing the invocations from the hive-schema-0.13 scripts and inlining the table creation.;;;","11/Mar/14 22:45;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633741/HIVE-6559.patch

{color:green}SUCCESS:{color} +1 5377 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1704/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1704/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633741;;;","11/Mar/14 23:02;prasadm;Looks fine to me. 
The same changes will be needed in hive-schema-0.14.0.*.sql files in trunk. I guess this patch can be applied to 0.13 as is.;;;","11/Mar/14 23:18;ashutoshc;+1. yeah.. we should get this in 0.13 as well.;;;","11/Mar/14 23:20;ashutoshc;For 0.14.* sql files we can track in separate jira because I guess Alan is thinking on how to do it better in trunk.;;;","11/Mar/14 23:22;prasadm;Sounds good.

+1
;;;","12/Mar/14 05:05;ashutoshc;Committed to 0.13 & trunk. Thanks, Alan!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2 Plain SASL authentication broken after hadoop 2.3 upgrade,HIVE-6558,12699001,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,prasadm,prasadm,prasadm,05/Mar/14 22:37,12/Mar/14 05:14,14/Jul/23 06:14,12/Mar/14 05:14,0.13.0,,,,,,,,,0.13.0,,Authentication,HiveServer2,,,0,,,"Java only includes Plain SASL client and not server. Hence HiveServer2 includes a Plain SASL server implementation. Now Hadoop has its own Plain SASL server [HADOOP-9020|https://issues.apache.org/jira/browse/HADOOP-9020] which is part of Hadoop 2.3 [release|http://hadoop.apache.org/docs/r2.3.0/hadoop-project-dist/hadoop-common/releasenotes.html].
The two servers use different Sasl callbacks and the servers are registered in java.security.Provider via static code. As a result the HiveServer2 instance could be using Hadoop's Plain SASL server which breaks the authentication.",,prasadm,rhbutani,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/14 16:44;prasadm;HIVE-6558.2.patch;https://issues.apache.org/jira/secure/attachment/12633729/HIVE-6558.2.patch","05/Mar/14 22:56;prasadm;HIVE-6558.2.patch;https://issues.apache.org/jira/secure/attachment/12632937/HIVE-6558.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377348,,,,Wed Mar 12 05:14:57 UTC 2014,,,,,,,,,,"0|i1t0uf:",377643,,,,,,,,,,,,,,,,,,,,,"05/Mar/14 22:59;prasadm;[~rhbutani] This is a regression in 0.13. Requesting backport once the patch is committed to trunk.;;;","05/Mar/14 23:06;rhbutani;+1 for porting to 0.13;;;","06/Mar/14 18:04;prasadm;[~thejas],  [~ashutoshc] would you mind taking a look. This should be a blocker for 0.13 release. Thanks!;;;","09/Mar/14 19:45;ashutoshc;Looks good to me. +1 cc: [~thejas];;;","10/Mar/14 10:37;thejas;+1;;;","10/Mar/14 15:28;ashutoshc;[~prasadm] I guess you need to reupload the patch for Hive QA to pick it up.;;;","10/Mar/14 16:44;prasadm;Re-attaching the patch for pre-commit test run;;;","11/Mar/14 19:09;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633729/HIVE-6558.2.patch

{color:green}SUCCESS:{color} +1 5380 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1702/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1702/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633729;;;","12/Mar/14 05:14;ashutoshc;Committed to trunk & 0.13. Thanks, Prasad!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestSchemaTool is failing on trunk after branching,HIVE-6555,12698911,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,05/Mar/14 17:22,13/Nov/14 19:41,14/Jul/23 06:14,07/Mar/14 19:32,,,,,,,,,,0.13.0,0.14.0,,,,,0,,,This is because version was bumped to 0.14 in pom file and there are no metastore scripts for 0.14 yet.,,prasadm,rhbutani,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6557,,,,,,,,,,HIVE-6666,,,,,,,,,,,,,,,,,,,,,"05/Mar/14 23:42;ashutoshc;HIVE-6555-branch13.patch;https://issues.apache.org/jira/secure/attachment/12632962/HIVE-6555-branch13.patch","06/Mar/14 22:17;ashutoshc;HIVE-6555.1.patch;https://issues.apache.org/jira/secure/attachment/12633238/HIVE-6555.1.patch","05/Mar/14 23:07;ashutoshc;HIVE-6555.patch;https://issues.apache.org/jira/secure/attachment/12632940/HIVE-6555.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377258,,,,Thu Nov 13 19:41:05 UTC 2014,,,,,,,,,,"0|i1t0an:",377553,,,,,,,,,,,,,,,,,,,,,"05/Mar/14 21:13;ashutoshc;Patch forthcoming.;;;","05/Mar/14 23:07;ashutoshc;[~prasadm] Can you take a look?
Also, it seems version # on 0.13 branch is wrong for mysql script, so I guess we need to patch 0.13 for mysql as well. See, 1-line diff in  metastore/scripts/upgrade/mysql/hive-schema-0.13.0.mysql.sql
Once you confirm, I can generate patch for 0.13 branch for that.;;;","05/Mar/14 23:08;ashutoshc;https://reviews.apache.org/r/18818/;;;","05/Mar/14 23:21;prasadm;+1

[~ashutoshc] added the same comment when looking at the review. Didn't realize you had already called it. 
yes, the mysql script change should be backported to 0.13 release. Are you planning to create a new jira or use the same one ?
[~rhbutani] requesting approval for backporting the 0.13 specific change.

Also I see the schema script for transaction support feature is executed after the version is inserted into version table. Ideally marking the schema version should be the last step in the script. Otherwise we might have an inconsistent metastore with valid version information. That could be a problem in troubleshooting.

I will create a new ticket to track that.;;;","05/Mar/14 23:30;rhbutani;+1 for 0.13
nice catch, will add this to Release management steps.;;;","05/Mar/14 23:42;ashutoshc;[~prasadm] including branch 13 fix in this jira only. Will commit both patches once tests comes back. 
Thanks, Harish for taking a look.;;;","05/Mar/14 23:58;ashutoshc;[~prasadm] I filed HIVE-6559 for the issue you brought up.;;;","06/Mar/14 22:17;ashutoshc;Same patch. Reupload for Hive QA to pick up.;;;","07/Mar/14 19:15;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633238/HIVE-6555.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5360 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1648/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1648/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633238;;;","07/Mar/14 19:32;ashutoshc;Committed to trunk.;;;","07/Mar/14 19:35;ashutoshc;Committed 0.13 version on 0.13 branch as well.;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to generate new mapJoin operator by exception : Big Table Alias is null,HIVE-6552,12698828,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,,mkudlej,mkudlej,05/Mar/14 09:01,16/Mar/14 12:52,14/Jul/23 06:14,16/Mar/14 12:52,0.12.0,,,,,,,,,0.13.0,,HiveServer2,,,,0,,,"I've tried BigTop test for UNIQUEJOIN:
CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;                                               CREATE TABLE T3(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH 'seed_data_files/T1.txt' INTO TABLE T1;                                            LOAD DATA LOCAL INPATH 'seed_data_files/T2.txt' INTO TABLE T2;
LOAD DATA LOCAL INPATH 'seed_data_files/T3.txt' INTO TABLE T3;                                             
FROM UNIQUEJOIN PRESERVE T1 a (a.key), PRESERVE T2 b (b.key), PRESERVE T3 c (c.key)
SELECT a.key, b.key, c.key;

where T1.txt is:
111
212
313
717
818
828
and T2.txt is:
222
313
414
515
818
818
and T3.txt is:
212
414
616
717

if hive.auto.convert.join=false it works and result is:
1	NULL	NULL
2	2	2
3	3	NULL
NULL	4	4
NULL	5	NULL
NULL	NULL	6
7	NULL	7
8	8	NULL
8	8	NULL
8	8	NULL
8	8	NULL

but hive.auto.convert.join=true it failed:
> FROM UNIQUEJOIN PRESERVE T1 a (a.key), PRESERVE T2 b (b.key), PRESERVE T3 c (c.key) SELECT a.key, b.key, c.key
org.apache.hadoop.hive.ql.parse.SemanticException: Big Table Alias is null                                  at org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.genMapJoinLocalWork(MapJoinProcessor.java:225)
  at org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.genLocalWorkForMapJoin(MapJoinProcessor.java:256)
  at org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.genMapJoinOpAndLocalWork(MapJoinProcessor.java:248)
  at org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.convertTaskToMapJoinTask(CommonJoinTaskDispatcher.java:191)
  at org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.processCurrentTask(CommonJoinTaskDispatcher.java:480)
  at org.apache.hadoop.hive.ql.optimizer.physical.AbstractJoinTaskDispatcher.dispatch(AbstractJoinTaskDispatcher.java:182)
  at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111)                       at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:194)
  at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:139)                   at org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver.resolve(CommonJoinResolver.java:79)
  at org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.optimize(PhysicalOptimizer.java:90)
  at org.apache.hadoop.hive.ql.parse.MapReduceCompiler.compile(MapReduceCompiler.java:300)
  at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8410)
  at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:284)
  at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:441)
  at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:342)
  at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1000)
  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
  at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)
  at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)
  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:348)
  at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:446)
  at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:456)
  at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:737)
  at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
  at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)                                          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  at java.lang.reflect.Method.invoke(Method.java:597)
  at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
org.apache.hadoop.hive.ql.parse.SemanticException: Failed to generate new mapJoin operator by exception : Big Table Alias is null
","Hive version:
getBranch()      : bigwheel-m16-0.12.0
getBuildVersion(): 0.12.0.2.0.6.1-101 from 8b1b43ece7c96d3cf38fda84414b23e3b707026e by jenkins source checksum 1c1e5eb051cefce14af4d621654dc423
getDate()        : Wed Jan 8 22:20:16 PST 2014
getRevision()    : 8b1b43ece7c96d3cf38fda84414b23e3b707026e
getSrcChecksum() : 1c1e5eb051cefce14af4d621654dc423
getUrl()         : git://c64-s17/grid/0/workspace/BIGTOP-HDP_RPM_REPO-bigwheel-M16/label/centos6-builds/bigtop-0.5/build/hive/rpm/BUILD/hive-0.12.0.2.0.6.1
getUser()        : jenkins
getVersion()     : 0.12.0.2.0.6.1-101

OS:  Red Hat Enterprise Linux Server release 6.4 x86_64

JVM: java version ""1.6.0_31""
Java(TM) SE Runtime Environment (build 1.6.0_31-b04)
Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode)

Hadoop:
Hadoop 2.2.0.2.0.6.0-101
Subversion git@github.com:hortonworks/hadoop.git -r b07b2906c36defd389c8b5bd22bebc1bead8115b
Compiled by jenkins on 2014-01-09T05:18Z
Compiled with protoc 2.5.0
From source with checksum 704f1e463ebc4fb89353011407e965",mkudlej,navis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377176,,,,Sun Mar 16 12:52:07 UTC 2014,,,,,,,,,,"0|i1szsf:",377471,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 05:41;navis;Seemed fixed by HIVE-5945. ;;;","16/Mar/14 12:52;navis;Fixed by HIVE-6403;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
group by after join with skew join optimization references invalid task sometimes,HIVE-6551,12698812,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,Fixed,navis,navis,navis,05/Mar/14 05:08,10/Mar/14 17:57,14/Jul/23 06:14,07/Mar/14 02:54,,,,,,,,,,0.13.0,,,,,,0,,,"For example,
{noformat}
hive> set hive.auto.convert.join = true;
hive> set hive.optimize.skewjoin = true;
hive> set hive.skewjoin.key = 3;
hive> 
    > EXPLAIN FROM 
    > (SELECT src.* FROM src) x
    > JOIN 
    > (SELECT src.* FROM src) Y
    > ON (x.key = Y.key)
    > SELECT sum(hash(Y.key)), sum(hash(Y.value));
OK
STAGE DEPENDENCIES:
  Stage-8 is a root stage
  Stage-6 depends on stages: Stage-8
  Stage-5 depends on stages: Stage-6 , consists of Stage-4, Stage-2
  Stage-4
  Stage-2 depends on stages: Stage-4, Stage-1
  Stage-0 is a root stage
...
{noformat}

Stage-2 references not-existing Stage-1",,navis,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/14 07:11;navis;HIVE-6551.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12632789/HIVE-6551.1.patch.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377160,,,,Mon Mar 10 17:57:11 UTC 2014,,,,,,,,,,"0|i1szov:",377455,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 10:48;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12632789/HIVE-6551.1.patch.txt

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5358 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket5
org.apache.hive.beeline.TestSchemaTool.testSchemaInit
org.apache.hive.beeline.TestSchemaTool.testSchemaUpgrade
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1637/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1637/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12632789;;;","06/Mar/14 17:52;ashutoshc;+1;;;","07/Mar/14 02:54;ashutoshc;Committed to trunk. Thanks, Navis!;;;","10/Mar/14 17:57;rhbutani;ported to 0.13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SemanticAnalyzer.reset() doesn't clear all the state,HIVE-6550,12698797,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sershe,jpullokkaran,jpullokkaran,05/Mar/14 03:16,13/Nov/14 19:42,14/Jul/23 06:14,10/Sep/14 07:45,0.12.0,0.13.0,0.13.1,,,,,,,0.14.0,,Query Processor,,,,0,,,,,jpullokkaran,sershe,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/14 23:08;sershe;HIVE-6550.01.patch;https://issues.apache.org/jira/secure/attachment/12666610/HIVE-6550.01.patch","05/Sep/14 19:01;sershe;HIVE-6550.02.patch;https://issues.apache.org/jira/secure/attachment/12666866/HIVE-6550.02.patch","09/Sep/14 02:18;sershe;HIVE-6550.03.patch;https://issues.apache.org/jira/secure/attachment/12667326/HIVE-6550.03.patch","11/Mar/14 17:45;jpullokkaran;HIVE-6550.patch;https://issues.apache.org/jira/secure/attachment/12633954/HIVE-6550.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377145,,,,Thu Nov 13 19:42:58 UTC 2014,,,,,,,,,,"0|i1szlj:",377440,,,,,,,,,,,,,,,,,,,,,"11/Mar/14 01:38;ashutoshc;+1 
[~jpullokkaran] You need to reupload the patch according to naming convention : https://cwiki.apache.org/confluence/display/Hive/Hive+PreCommit+Patch+Testing for Hive QA to pick it up.;;;","11/Mar/14 17:40;jpullokkaran;Renamed patch to follow naming convention;;;","10/Jun/14 16:19;ashutoshc;Patch needs to be rebased.;;;","04/Sep/14 23:08;sershe;auto-rebased patch with some cleanup added;;;","05/Sep/14 02:09;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12666610/HIVE-6550.01.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/646/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/646/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-646/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Tests exited with: AWSResponseException: request POST https://ec2.sa-east-1.amazonaws.com/ HTTP/1.1 failed with code 500, error: AWSError{requestId='9cb79712-847c-40f2-9857-a783de37c55b', requestToken='null', code='InternalError', message='An internal error has occurred', context='{Response=, Errors=}'}
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12666610;;;","05/Sep/14 19:01;sershe;trying again with almost the same patch, looks like HiveQA was broken;;;","06/Sep/14 06:39;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12666866/HIVE-6550.02.patch

{color:red}ERROR:{color} -1 due to 12 failed/errored test(s), 6171 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_merge_stats_orc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_analyze
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge_incompat1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_merge_incompat2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_noscan_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_noscan_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_alter_merge_stats_orc
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_orc_analyze
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_stats_noscan_1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partscan_norcfile
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataPrimitiveTypes
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/663/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/663/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-663/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 12 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12666866;;;","06/Sep/14 19:46;ashutoshc;Test failures needs to be looked at.;;;","09/Sep/14 01:38;sershe;It seems we clear too much state, in cases where several analyzers are getting used (e.g. Explain+ColumnStats, noscan so far seems the one to blame). I'll investigate the code further;;;","09/Sep/14 02:18;sershe;the tests failed because ColumnStatsSA initializes stuff in ctor and then we reset it. Trying to change it to init in initialize; let's see if it breaks anything (it fixes some of the above tests);;;","09/Sep/14 19:31;sershe;forgot to submit patch;;;","09/Sep/14 22:28;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12667326/HIVE-6550.03.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 6192 tests executed
*Failed tests:*
{noformat}
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/713/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/713/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-713/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12667326;;;","09/Sep/14 23:18;sershe;I don;t think this failure is related. [~ashutoshc] [~jpullokkaran] can you please review?;;;","10/Sep/14 01:38;ashutoshc;+1;;;","10/Sep/14 07:45;ashutoshc;Committed to trunk. Thanks, Sergey!;;;","10/Sep/14 20:15;sershe;Thanks;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"remove templeton.jar from webhcat-default.xml, remove hcatalog/bin/hive-config.sh",HIVE-6549,12698776,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,ekoifman,ekoifman,ekoifman,05/Mar/14 00:42,13/Nov/14 19:41,14/Jul/23 06:14,12/May/14 18:56,0.12.0,,,,,,,,,0.14.0,,WebHCat,,,,0,,,"this property is no longer used
also removed corresponding AppConfig.TEMPLETON_JAR_NAME

hcatalog/bin/hive-config.sh is not used

NO PRECOMMIT TESTS
",,ekoifman,leftyl,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-8807,,,,,,,,,,,,,,,,,,,,,"09/May/14 20:19;ekoifman;HIVE-6549.2.patch;https://issues.apache.org/jira/secure/attachment/12644180/HIVE-6549.2.patch","15/Mar/14 01:24;ekoifman;HIVE-6549.patch;https://issues.apache.org/jira/secure/attachment/12634881/HIVE-6549.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377124,,,,Thu Nov 13 19:41:22 UTC 2014,,,,,,,,,,"0|i1szgv:",377419,,,,,,,,,,,,,,,,,,,,,"05/Mar/14 03:08;leftyl;When this gets committed, the wiki needs to be edited (with version information):

* [WebHCat Configuration:  Configuration Variables |https://cwiki.apache.org/confluence/display/Hive/WebHCat+Configure#WebHCatConfigure-ConfigurationVariables]

The existing table shows configuration defaults for Hive 0.11.0, so they ought to be updated too.  But if the only changes are ""11"" or ""12"" or ""13"" in file names and paths, then a note could explain that in the intro to the table.;;;","05/Mar/14 04:07;ekoifman;I'm not sure it's useful to maintain ""Configuration Variables"" section.  Each variable is/should be documented in webhcat-default.xml (there is a special 'description' xml element there for it).  Copying it to wiki only adds maintenance effort.  The rest of the page is useful.;;;","05/Mar/14 04:34;leftyl;Readability is the main advantage of putting config variables in the wiki.  Some readers might also like seeing all the variables along with general configuration information, without having to hunt for webhcat-default.xml.  But you're right about the maintenance problem.

I'd say go ahead and remove the table from the wiki, but perhaps we need a few more opinions.;;;","06/Mar/14 19:14;leftyl;More advantages of wikidocs:

* Access -- you can look up the information even when the code isn't available.
* Elaboration -- additional notes and guidance.
* Search -- well, I'd like to say you can always find a config variable by googling it, but a random check of Hive config properties had more misses than hits.  And one search found an svn copy of hive-default.xml.
* Review -- after initial release, descriptions are more likely to get reviewed in the wiki and corrections are easier.  Of course, that leads to a major disadvantage:  divergence of the wikidoc from the source file.;;;","15/Mar/14 01:24;ekoifman;no pre commit tests;;;","15/Mar/14 10:43;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634881/HIVE-6549.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1826/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1826/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [8.712s]
[INFO] Hive Ant Utilities ................................ SUCCESS [5.674s]
[INFO] Hive Shims Common ................................. SUCCESS [3.770s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.611s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.226s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.588s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [8.111s]
[INFO] Hive Shims ........................................ SUCCESS [1.252s]
[INFO] Hive Common ....................................... SUCCESS [6.806s]
[INFO] Hive Serde ........................................ SUCCESS [12.715s]
[INFO] Hive Metastore .................................... SUCCESS [32.348s]
[INFO] Hive Query Language ............................... SUCCESS [1:19.497s]
[INFO] Hive Service ...................................... SUCCESS [8.036s]
[INFO] Hive JDBC ......................................... SUCCESS [2.906s]
[INFO] Hive Beeline ...................................... SUCCESS [2.686s]
[INFO] Hive CLI .......................................... SUCCESS [2.635s]
[INFO] Hive Contrib ...................................... SUCCESS [1.761s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.652s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.626s]
[INFO] Hive HCatalog Core ................................ SUCCESS [1.961s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.465s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.370s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [2.063s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [9.759s]
[INFO] Hive HWI .......................................... SUCCESS [1.329s]
[INFO] Hive ODBC ......................................... SUCCESS [0.692s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.286s]
[INFO] Hive TestUtils .................................... SUCCESS [0.668s]
[INFO] Hive Packaging .................................... FAILURE [1.713s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:36.455s
[INFO] Finished at: Sat Mar 15 06:43:49 EDT 2014
[INFO] Final Memory: 74M/426M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634881;;;","14/Apr/14 19:01;ekoifman;[~leftylev] If [WebHCat Configuration: Configuration Variables|https://cwiki.apache.org/confluence/display/Hive/WebHCat+Configure#WebHCatConfigure-ConfigurationVariables] is maintained then, ""hive.metastore.local=false"" in ""templeton.hive.properties"" should be removed.  It's be deprecated/removed since Hive 0.10;;;","16/Apr/14 19:17;leftyl;Thanks [~eugene.koifman].  I've removed it.

We never decided whether to maintain the doc or just refer readers to webhcat-default.xml.  Now that the HCatalog 0.5.0 docs have gone missing, I'm inclined to remove the default values from the table but keep the variable names and descriptions, which seem to be fairly stable.  Then instead of the introduction discussing default values, it could give the path to webhcat-default.xml.

What do you think?

By the way, I notice SNAPSHOT in the templeton.jar default in 0.13.0 RC2 (escape chars added):

{quote}
  <property>
    <name>templeton.jar</name>
    <value>$\{env.TEMPLETON_HOME\}/share/webhcat/svr/webhcat-0.6.0-SNAPSHOT.jar</value>
    <description>The path to the Templeton jar file.</description>
  </property>
{quote}

It's also in branch-0.11 and branch-0.12 even though the doc for branch-0.11 gives ""webhcat-0.11.0.jar"" instead of ""webhcat-0.6.0-SNAPSHOT.jar"".;;;","16/Apr/14 20:04;ekoifman;the value for templeton.jar is not relevant - it's not used (HIVE-6549 covers removing it)

I agree that that listing defaults is not useful.  I think including a link on github/svn to webhcat-default.xml would useful.;;;","16/Apr/14 23:24;leftyl;My github knowledge is rusty and never was much to begin with, so what would the link be?

For svn, I've found this:  http://svn.apache.org/repos/asf/hive/trunk/hcatalog/webhcat/svr/src/main/config/webhcat-default.xml (or equivalent files in branches, such as http://svn.apache.org/repos/asf/hive/branches/branch-0.12/hcatalog/webhcat/svr/src/main/config/webhcat-default.xml).  Is that what you mean?;;;","16/Apr/14 23:31;ekoifman;I think http://svn.apache.org/repos/asf/hive/trunk/hcatalog/webhcat/svr/src/main/config/webhcat-default.xml is perfect;;;","16/Apr/14 23:41;leftyl;Good, will do.

In the meantime I found this on github:  https://github.com/apache/hive/blob/trunk/hcatalog/webhcat/svr/src/main/config/webhcat-default.xml (or for branch-0.12 it's https://github.com/apache/hive/blob/branch-0.12/hcatalog/webhcat/svr/src/main/config/webhcat-default.xml).

Should we give both github and svn, in trunk?  Or mention how to find the branches too?;;;","16/Apr/14 23:51;ekoifman;github repo is a mirror of the svn repo so one should be sufficient.  

Now that you mention it, I think http://svn.apache.org/repos/asf/hive/branches/branch-0.12/hcatalog/webhcat/svr/src/main/config/webhcat-default.xml would be better (or perhaps 0.13) since 'trunk' is the version currently in develpment.  I think users will be able to look at the URL and figure out how to change it to get to a different branch.;;;","17/Apr/14 05:07;leftyl;Here's what I put in the wiki after the table:

[Default Values |https://cwiki.apache.org/confluence/display/Hive/WebHCat+Configure#WebHCatConfigure-DefaultValues]

{quote}
Some of the default values for WebHCat configuration variables depend on the release number. For the default values in the Hive release you are using, see the webhcat-default.xml file. It can be found in the SVN repository at:
http://svn.apache.org/repos/asf/hive/branches/branch-<release_number>/hcatalog/webhcat/svr/src/main/config/webhcat-default.xml
where <release_number> is 0.11, 0.12, and so on. (Prior to Hive 0.11, WebHCat was in the Apache incubator.) For example, the file for Hive 0.12 is at http://svn.apache.org/repos/asf/hive/branches/branch-0.12/hcatalog/webhcat/svr/src/main/config/webhcat-default.xml.
{quote}

I'd like to include a link to the file in HCatalog 0.5.0 too, but don't know where to find it.  I sent email to dev@hive yesterday and might open a JIRA, because lots of wiki links to the HCat 0.5.0 docs are broken.;;;","08/May/14 20:08;thejas;I think we should remove the related code from AppConfig.java as well.
;;;","09/May/14 20:19;ekoifman;adressed [~thejas]'s comments;;;","09/May/14 21:40;thejas;+1;;;","12/May/14 18:56;thejas;Patch committed to trunk. Thanks for the contribution Eugene!
;;;","15/May/14 08:44;leftyl;I've added links to the HCat 0.5.0 doc and Hive 0.13.0 webhcat-default.xml file:

* [WebHCat Configuration -- Default Values |https://cwiki.apache.org/confluence/display/Hive/WebHCat+Configure#WebHCatConfigure-DefaultValues]

So that takes care of the documentation for this jira.

But the webhcat-default.xml files for Hive 0.12 and 0.13 both show a Hive 0.11 default value for templeton.hive.path:

{code}
    <name>templeton.hive.path</name>
    <value>hive-0.11.0.tar.gz/hive-0.11.0/bin/hive</value>
{code}

Is that another unused parameter?;;;","16/May/14 22:55;ekoifman;no, this is definitely used.  I guess the proper version number is being set in the release brunch but not trunk;;;","17/May/14 05:18;leftyl;I checked webhcat-default.xml here:

* branch 11:  [http://svn.apache.org/repos/asf/hive/branches/branch-0.11/hcatalog/webhcat/svr/src/main/config/webhcat-default.xml]
{code}
  <property>
    <name>templeton.hive.path</name>
    <value>hive-0.10.0.tar.gz/hive-0.10.0/bin/hive</value>
    <description>The path to the Hive executable.</description>
  </property>
{code}
* branch 12:  [http://svn.apache.org/repos/asf/hive/branches/branch-0.12/hcatalog/webhcat/svr/src/main/config/webhcat-default.xml]
{code}
  <property>
    <name>templeton.hive.path</name>
    <value>hive-0.11.0.tar.gz/hive-0.11.0/bin/hive</value>
    <description>The path to the Hive executable.</description>
  </property>
{code}
* branch 13:  [http://svn.apache.org/repos/asf/hive/branches/branch-0.13/hcatalog/webhcat/svr/src/main/config/webhcat-default.xml]
{code}
  <property>
    <name>templeton.hive.path</name>
    <value>hive-0.11.0.tar.gz/hive-0.11.0/bin/hive</value>
    <description>The path to the Hive executable.</description>
  </property>
{code}
* trunk:  [http://svn.apache.org/repos/asf/hive/trunk/hcatalog/webhcat/svr/src/main/config/webhcat-default.xml]
{code}
  <property>
    <name>templeton.hive.path</name>
    <value>hive-0.11.0.tar.gz/hive-0.11.0/bin/hive</value>
    <description>The path to the Hive executable.</description>
  </property>
{code};;;","06/Aug/14 05:58;leftyl;The default for templeton.hive.path is still set to hive-0.11.0-* in branch-0.13.1 as well as trunk.  Do we need a new JIRA ticket?;;;","06/Aug/14 06:13;leftyl;Somehow I forgot to remove templeton.jar from the wiki.  Now instead of removing it I've marked it as obsolete, with a link back to this issue.  We can weed it out later.

* [WebHCat Configuration -- Configuration Variables | https://cwiki.apache.org/confluence/display/Hive/WebHCat+Configure#WebHCatConfigure-ConfigurationVariables];;;","10/Nov/14 21:44;leftyl;HIVE-8807 will fix the obsolete default value for templeton.hive.path.;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing owner name and type fields in schema script for DBS table ,HIVE-6548,12698771,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,04/Mar/14 23:58,05/Mar/14 20:03,14/Jul/23 06:14,05/Mar/14 17:27,0.13.0,,,,,,,,,0.13.0,,Metastore,,,,0,,,"HIVE-6386 introduced new columns in DBS table, but those are missing from schema scripts.",,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/14 00:01;ashutoshc;HIVE-6548.patch;https://issues.apache.org/jira/secure/attachment/12632711/HIVE-6548.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377119,,,,Wed Mar 05 20:03:28 UTC 2014,,,,,,,,,,"0|i1szfr:",377414,,,,,,,,,,,,,,,,,,,,,"05/Mar/14 00:01;ashutoshc;Patch to add missing columns in schema scripts.;;;","05/Mar/14 03:02;thejas;+1;;;","05/Mar/14 15:49;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12632711/HIVE-6548.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5355 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket5
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hive.beeline.TestSchemaTool.testSchemaInit
org.apache.hive.beeline.TestSchemaTool.testSchemaUpgrade
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1630/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1630/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12632711;;;","05/Mar/14 17:23;ashutoshc;Verified that TestSchemaTool is failing on trunk as well. Created, HIVE-6555 to track that.;;;","05/Mar/14 17:27;ashutoshc;Committed to trunk. 
[~rhbutani] Please consider this for inclusion in 0.13 since its a bug which results in metastore failure to come up.;;;","05/Mar/14 19:57;rhbutani;+1;;;","05/Mar/14 20:03;ashutoshc;Thanks, Harish! I committed to 0.13 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
normalize struct Role in metastore thrift interface,HIVE-6547,12698716,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,thejas,thejas,thejas,04/Mar/14 20:09,28/Mar/14 23:35,14/Jul/23 06:14,28/Mar/14 23:35,0.13.0,,,,,,,,,0.13.0,,Metastore,Thrift API,,,0,,,"As discussed in HIVE-5931, it will be cleaner to have the information about Role to role member mapping removed from the Role object, as it is not part of a logical Role. This information not relevant for actions such as creating a Role.
As part of this change  get_role_grants_for_principal api will be added, so that it can be used in place of  list_roles, when role mapping information is desired.
",,ashutoshc,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-5931,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/14 19:43;thejas;HIVE-6547.1.patch;https://issues.apache.org/jira/secure/attachment/12637223/HIVE-6547.1.patch","27/Mar/14 19:42;thejas;HIVE-6547.nothriftgen.1.patch;https://issues.apache.org/jira/secure/attachment/12637222/HIVE-6547.nothriftgen.1.patch","14/Mar/14 20:02;thejas;HIVE-6547.thriftapi.2.patch;https://issues.apache.org/jira/secure/attachment/12634816/HIVE-6547.thriftapi.2.patch","13/Mar/14 19:28;thejas;HIVE-6547.thriftapi.patch;https://issues.apache.org/jira/secure/attachment/12634522/HIVE-6547.thriftapi.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377074,,,,Fri Mar 28 23:35:07 UTC 2014,,,,,,,,,,"0|i1sz5r:",377369,,,,,,,,,,,,,,,,,,,,,"13/Mar/14 19:28;thejas;HIVE-6547.thriftapi.patch - thrift api change for review
;;;","14/Mar/14 15:05;ashutoshc;Looks good. Do you want to qualify fields in new structs with required ?;;;","14/Mar/14 15:22;thejas;Thanks for the feedback. Will make them required.
While I am at it , i will also make the fields in GetPrincipalsInRoleResponse,GetPrincipalsInRoleRequest required. With the semantics of the functions, it does not make any sense for them to be non-required.
Those are similar functions that were added recently;;;","14/Mar/14 20:02;thejas;HIVE-6547.thriftapi.2.patch - addressed thrift api feedback.
;;;","27/Mar/14 18:49;thejas;HIVE-6547.1.patch - includes thrift gen files
;;;","27/Mar/14 19:44;thejas;The patch also remioves additional fields  - principalname and principaltype in 'show role grant user user2"" output, as that is redundant information. Also removes role createtime from this command output as that is not relevant to role grant information.;;;","28/Mar/14 22:52;thejas;Ran tests locally and they passed.
;;;","28/Mar/14 23:35;thejas;Patch committed to 0.13 branch and trunk.
Thanks for the review Ashutosh!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WebHCat job submission for pig with -useHCatalog argument fails on Windows,HIVE-6546,12698715,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ehans,ehans,ehans,04/Mar/14 20:05,27/Mar/14 20:49,14/Jul/23 06:14,27/Mar/14 19:13,0.11.0,0.12.0,0.13.0,0.14.0,,,,,,0.13.0,,WebHCat,,,,0,,,"On a one-box windows setup, do the following from a powershell prompt:

cmd /c curl.exe -s `
  -d user.name=hadoop `
  -d arg=-useHCatalog `
  -d execute=""emp = load '/data/emp/emp_0.dat'; dump emp;"" `
  -d statusdir=""/tmp/webhcat.output01"" `
  'http://localhost:50111/templeton/v1/pig' -v

The job fails with error code 7, but it should run. 

I traced this down to the following. In the job configuration for the TempletonJobController, we have templeton.args set to

cmd,/c,call,C:\\hadoop\\\\pig-0.11.0.1.3.0.0-0846/bin/pig.cmd,-D__WEBHCAT_TOKEN_FILE_LOCATION__=""-useHCatalog"",-execute,""emp = load '/data/emp/emp_0.dat'; dump emp;""

Notice the = sign before ""-useHCatalog"". I think this should be a comma.

The bad string D__WEBHCAT_TOKEN_FILE_LOCATION__=""-useHCatalog"" gets created in  org.apache.hadoop.util.GenericOptionsParser.preProcessForWindows().

It happens at line 434:
{code}
      } else {
          if (i < args.length - 1) {
            prop += ""="" + args[++i];   // RIGHT HERE! at iterations i = 37, 38
          }
        }
{code}

Bug is here:
{code}
      if (prop != null) {
        if (prop.contains(""="")) {  // -D__WEBHCAT_TOKEN_FILE_LOCATION__ does not contain equal, so else branch is run and appends =""-useHCatalog"",
          // everything good
        } else {
          if (i < args.length - 1) {
            prop += ""="" + args[++i];
          }
        }
        newArgs.add(prop);
      }
{code}
One possible fix is to change the string constant org.apache.hcatalog.templeton.tool.TempletonControllerJob.TOKEN_FILE_ARG_PLACEHOLDER to have an ""="" sign in it. Or, preProcessForWindows() itself could be changed.
","HDInsight deploying HDP 1.3:  c:\apps\dist\pig-0.11.0.1.3.2.0-05
Also on Windows HDP 1.3 one-box configuration.",ehans,ekoifman,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/14 22:25;ehans;HIVE-6546.01.patch;https://issues.apache.org/jira/secure/attachment/12632932/HIVE-6546.01.patch","05/Mar/14 22:42;ehans;HIVE-6546.02.patch;https://issues.apache.org/jira/secure/attachment/12632934/HIVE-6546.02.patch","26/Mar/14 22:10;ehans;HIVE-6546.03.patch;https://issues.apache.org/jira/secure/attachment/12637024/HIVE-6546.03.patch","11/Mar/14 23:12;ehans;HIVE-6546.03.patch;https://issues.apache.org/jira/secure/attachment/12634041/HIVE-6546.03.patch","05/Mar/14 22:55;ehans;HIVE-6546.03.patch;https://issues.apache.org/jira/secure/attachment/12632935/HIVE-6546.03.patch",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377073,,,,Thu Mar 27 20:49:54 UTC 2014,,,,,,,,,,"0|i1sz5j:",377368,,,,,,,,,,,,,,,,,,,,,"05/Mar/14 22:25;ehans;Changed constant placeholder to include = sign;;;","05/Mar/14 22:42;ehans;removed trailing white space;;;","05/Mar/14 22:46;ehans;Code review at https://reviews.apache.org/r/18816/;;;","05/Mar/14 22:55;ehans;fix typo;;;","11/Mar/14 23:12;ehans;Upload again to try to kick off pre-commit tests;;;","18/Mar/14 00:04;ekoifman;+1;;;","24/Mar/14 22:19;ehans;[~thejas] Can you take a look?;;;","25/Mar/14 18:29;thejas;The code here is supposed to remove this argument if token file argument is not set, do you know why it is not kicking in this case ? https://github.com/apache/hive/blob/trunk/hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/LaunchMapper.java#L127
;;;","25/Mar/14 18:48;thejas;I think the right fix is to not add this param to add this placeholder to command line arguments, it has not been replaced with a real value.
;;;","26/Mar/14 21:44;ehans;I'm not sure I understand what you mean. Can you elaborate? The placeholder is getting substituted or eliminated by the templeton controller job. 

If I run this simple Pig script from WebHCat:

emp = load 'wasbs://ehans1@ehans7.blob.core.windows.net/data/emp_0.dat'; dump emp;

Then I see this in the templeton controller job configuration:

templeton.args   cmd,/c,call,C:\\apps\\dist\\pig-0.12.0.2.0.7.0-1551/bin/pig.cmd,-D__WEBHCAT_TOKEN_FILE_LOCATION__=-execute,""emp = load 'wasbs://ehans1@ehans7.blob.core.windows.net/data/emp_0.dat'; dump emp;"" 

And I see this in the Pig job configuration for the job spawned by the templeton controller job:

pig.cmd.args    -Dmapreduce.job.credentials.binary=/c:/hdfs/nm-local-dir/usercache/ehans/appcache/application_1395867453549_0007/container_1395867453549_0007_01_000002/container_tokens -execute emp = load 'wasbs://ehans1@ehans7.blob.core.windows.net/data/emp_0.dat'; dump emp; 


;;;","26/Mar/14 22:07;thejas;Thanks for the clarification Eric. I thought that the problem was with parsing the command launched from the map task, but looks like this is happening before that. I was worried that having ""="" sign could cause problems in other systems, but since the shell never sees this arguement, it is fine.

+1 .
;;;","26/Mar/14 22:10;ehans;Uploading patch yet again to try to kick off pre-commit tests.;;;","27/Mar/14 17:36;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637024/HIVE-6546.03.patch

{color:green}SUCCESS:{color} +1 5491 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1985/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1985/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637024;;;","27/Mar/14 17:50;thejas;[~rhbutani] This is a small change, but it will be very useful to have it in 0.13, as it affects the working on windows.
;;;","27/Mar/14 18:53;rhbutani;+1 for 0.13;;;","27/Mar/14 19:13;ehans;Committed to trunk;;;","27/Mar/14 20:49;thejas;Patch committed to 0.13 branch .
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
analyze table throws NPE for non-existent tables.,HIVE-6545,12698706,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,04/Mar/14 19:17,04/Mar/14 23:52,14/Jul/23 06:14,04/Mar/14 23:52,,,,,,,,,,0.13.0,,Statistics,,,,0,,,"Instead of NPE, we should give error message to user.",,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/14 19:21;ashutoshc;HIVE-6545.patch;https://issues.apache.org/jira/secure/attachment/12632573/HIVE-6545.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,377064,,,,Tue Mar 04 23:52:28 UTC 2014,,,,,,,,,,"0|i1sz3j:",377359,,,,,,,,,,,,,,,,,,,,,"04/Mar/14 19:21;ashutoshc;Simple fix.;;;","04/Mar/14 19:25;ashutoshc;https://reviews.apache.org/r/18745/;;;","04/Mar/14 19:40;rhbutani;+1;;;","04/Mar/14 23:42;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12632573/HIVE-6545.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5240 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_parallel_orderby
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1621/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1621/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12632573;;;","04/Mar/14 23:52;ashutoshc;Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Couple of issues in fs based stats collection,HIVE-6539,12698501,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,03/Mar/14 19:52,04/Mar/14 23:41,14/Jul/23 06:14,04/Mar/14 23:41,0.13.0,,,,,,,,,0.13.0,,,,,,0,,,"While testing on cluster found couple of bugs:
* NPE in certain case.
* map object reuse causing problem",,hagleitn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/14 19:54;ashutoshc;HIVE-6539.patch;https://issues.apache.org/jira/secure/attachment/12632345/HIVE-6539.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,376859,,,,Tue Mar 04 23:41:22 UTC 2014,,,,,,,,,,"0|i1sxtz:",377154,,,,,,,,,,,,,,,,,,,,,"03/Mar/14 19:54;ashutoshc;* Fixes NPE
* Do new Hashmap to avoid object reuse issue.
* Increased test coverage by putting fs based stats collection in tests.;;;","03/Mar/14 21:47;ashutoshc;https://reviews.apache.org/r/18709/;;;","03/Mar/14 23:38;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12632345/HIVE-6539.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5236 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hive.hcatalog.mapreduce.TestHCatMutablePartitioned.testHCatPartitionedTable
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1607/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1607/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12632345;;;","04/Mar/14 19:02;hagleitn;+1 LGTM;;;","04/Mar/14 23:41;ashutoshc;Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yet another annoying exception in test logs,HIVE-6538,12698499,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,Fixed,szehon,sershe,sershe,03/Mar/14 19:38,13/Nov/14 19:42,14/Jul/23 06:14,12/Mar/14 20:38,,,,,,,,,,0.14.0,,,,,,0,,,"Whenever you look at failed q tests you have to go thru this useless exception.

{noformat}
2014-03-03 11:22:54,872 ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(143)) - MetaException(message:NoSuchObjectException(message:Function default.qtest_get_java_boolean does not exist))
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:4575)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_function(HiveMetaStore.java:4702)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
	at $Proxy8.get_function(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getFunction(HiveMetaStoreClient.java:1526)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at $Proxy9.getFunction(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getFunction(Hive.java:2603)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getFunctionInfoFromMetastore(FunctionRegistry.java:546)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getQualifiedFunctionInfo(FunctionRegistry.java:578)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getFunctionInfo(FunctionRegistry.java:599)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getFunctionInfo(FunctionRegistry.java:606)
	at org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.analyzeDropFunction(FunctionSemanticAnalyzer.java:94)
	at org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.analyzeInternal(FunctionSemanticAnalyzer.java:60)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:445)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:345)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1078)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1121)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1014)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1004)
	at org.apache.hadoop.hive.ql.QTestUtil.runCmd(QTestUtil.java:655)
	at org.apache.hadoop.hive.ql.QTestUtil.createSources(QTestUtil.java:772)
	at org.apache.hadoop.hive.cli.TestCliDriver.<clinit>(TestCliDriver.java:46)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.internal.runners.SuiteMethod.testFromSuiteMethod(SuiteMethod.java:34)
	at org.junit.internal.runners.SuiteMethod.<init>(SuiteMethod.java:23)
	at org.junit.internal.builders.SuiteMethodBuilder.runnerForClass(SuiteMethodBuilder.java:14)
	at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:57)
	at org.junit.internal.builders.AllDefaultPossibilitiesBuilder.runnerForClass(AllDefaultPossibilitiesBuilder.java:29)
	at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:57)
	at org.junit.internal.requests.ClassRequest.getRunner(ClassRequest.java:24)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:262)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
Caused by: NoSuchObjectException(message:Function default.qtest_get_java_boolean does not exist)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_function(HiveMetaStore.java:4697)
	... 47 more
{noformat}

I'll take a look when I have time, or someone else can take a look.",,jdere,qwertymaniac,sershe,szehon,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6330,,,,,,,,,"06/Mar/14 18:55;szehon;HIVE-6538.2.patch;https://issues.apache.org/jira/secure/attachment/12633205/HIVE-6538.2.patch","05/Mar/14 21:49;szehon;HIVE-6538.2.patch;https://issues.apache.org/jira/secure/attachment/12632921/HIVE-6538.2.patch","03/Mar/14 22:38;szehon;HIVE-6538.patch;https://issues.apache.org/jira/secure/attachment/12632374/HIVE-6538.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,376857,,,,Thu Nov 13 19:42:30 UTC 2014,,,,,,,,,,"0|i1sxtj:",377152,,,,,,,,,,,,,,,,,,,,,"03/Mar/14 21:30;szehon;In RetryingHMSHandler, there are checks for get_table and get_partitions not to log to error, I guess similar thing needs to apply for new method ""get_function"".;;;","03/Mar/14 22:38;szehon;This seems to work and get rid of the stack trace in the log.  

However, FunctionRegistry.getFunctionInfoFromMetastore() still seems to print the top-level error, not sure if its by design. + [~jdere];;;","03/Mar/14 22:45;sershe;+1 assuming tests pass, esp. negative cli ;;;","04/Mar/14 06:34;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12632374/HIVE-6538.patch

{color:green}SUCCESS:{color} +1 5238 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1609/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1609/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12632374;;;","05/Mar/14 19:34;jdere;Hi [~szehon], given that logic in FunctionRegistry.getFunctionInfoFromMetastore() does not propagate the exception I did feel like in the general case we should log exceptions, though I could see us not doing that for that particular error.  I suppose there could be a couple of ways to change getFunctionInfoFromMetastore():
1. Set the logging level to DEBUG (not sure if the message will still show up in the test logs).
2. Add a special catch for for NoSuchObjectException, so that particular error is not logged.;;;","05/Mar/14 19:51;sershe;1 would still appear in logs;;;","05/Mar/14 19:53;jdere;yeah, just do (2);;;","05/Mar/14 20:00;szehon;Yea makes sense, let's do (2) to keep it consistent with getting non-existent table/partition and keep the logs clean.;;;","05/Mar/14 21:50;szehon;Added a check to not log for NoSuchObjectException in FunctionRegistry.;;;","06/Mar/14 18:27;sershe;Looks like this was done as part of 60ff41c Tue Feb 25 07:58:52 2014 +0000 Merge latest trunk into branch. (Gunther Hagleitner);;;","06/Mar/14 18:27;sershe;sorry wrong jira;;;","06/Mar/14 18:55;szehon;Looks like pre-commit queue died last night, I'm resubmitting;;;","06/Mar/14 21:27;sershe;has long line, +1 otherwise;;;","11/Mar/14 17:33;sershe;Will commit tomorrow and fix long line on commit if there are no objections;;;","11/Mar/14 18:37;szehon;Yea, Im not sure where this is in queue, or even if it got lost in queue.;;;","11/Mar/14 18:47;sershe;well, patch was not submitted :P;;;","11/Mar/14 18:48;sershe;I'll wait for HiveQA good point... just submitted the patch;;;","11/Mar/14 18:50;szehon;Oh I missed that, thanks.;;;","12/Mar/14 19:29;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633205/HIVE-6538.2.patch

{color:green}SUCCESS:{color} +1 5381 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1712/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1712/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633205;;;","12/Mar/14 20:38;sershe;committed to trunk;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException when loading hashtable for MapJoin directly,HIVE-6537,12698486,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sershe,sershe,sershe,03/Mar/14 18:57,07/Mar/14 18:29,14/Jul/23 06:14,07/Mar/14 18:29,,,,,,,,,,0.13.0,,,,,,0,,,"We see the following error:
{noformat}
2014-02-20 23:33:15,743 FATAL [main] org.apache.hadoop.hive.ql.exec.mr.ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.load(HashTableLoader.java:103)
        at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:149)
        at org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp(MapJoinOperator.java:164)
        at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1026)
        at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1030)
        at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1030)
        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:489)
        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)
Caused by: java.lang.NullPointerException
        at java.util.Arrays.fill(Arrays.java:2685)
        at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.loadDirectly(HashTableLoader.java:155)
        at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.load(HashTableLoader.java:81)
        ... 15 more
{noformat}

It appears that the tables in Arrays.fill call is nulls. I don't really have full understanding of this path, but what I gleaned so far is this...
From what I see, tables would be set unconditionally in initializeOp of the sink, and in no other place, so I assume for this code to ever  work that startForward calls it at least some time.
Here, it doesn't call it, so it's null. 
Previous loop also uses tables, and should have NPE-d before fill was ever called; it didn't, so I'd assume it never executed. 
There's a little bit of inconsistency in the above code where directWorks are added to parents unconditionally but sink is only added as child conditionally. I think it may be that some of the direct works are not table scans; in fact given that loop never executes they may be null (which is rather strange). 
Regardless, it seems that the logic should be fixed, it may be the root cause",,hagleitn,navis,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6144,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/14 01:47;sershe;HIVE-6537.01.patch;https://issues.apache.org/jira/secure/attachment/12632421/HIVE-6537.01.patch","04/Mar/14 05:25;navis;HIVE-6537.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12632443/HIVE-6537.2.patch.txt","03/Mar/14 19:04;sershe;HIVE-6537.patch;https://issues.apache.org/jira/secure/attachment/12632331/HIVE-6537.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,376844,,,,Fri Mar 07 18:29:46 UTC 2014,,,,,,,,,,"0|i1sxqn:",377139,,,,,,,,,,,,,,,,,,,,,"03/Mar/14 19:26;sershe;[~navis] [~vikram.dixit] fyi;;;","03/Mar/14 21:55;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12632331/HIVE-6537.patch

{color:red}ERROR:{color} -1 due to 61 failed/errored test(s), 5236 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join29
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_nulls
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_without_localtask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cp_mj_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32_lessSize
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join35
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_nulls
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_nullsafe
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_reorder4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_star
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_hook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_subquery
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_subquery2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats11
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin7
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_reduce_deduplicate
org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1606/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1606/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 61 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12632331;;;","04/Mar/14 00:05;sershe;Tests break because initializeOp is not being called with this change... [~navis] can you comment about the type check when adding children? Why is it there (asymmetric with adding parents).;;;","04/Mar/14 01:47;sershe;attempt #2... presumably not only TableScans can be valid parents, because if I remove all other operators (as in the initial version) the tests fail. The input from someone with better knowledge of the original path would be helpful;;;","04/Mar/14 02:14;navis;[~sershe] Hard to recall context of HIVE-6144. Could you show me the query which made the exception? Or could you try change the code in HashtableLoader#loadDirectly(),
{code}
  for (Operator<?> operator : directWorks) {
    if (operator instanceof TableScanOperator) {
      operator.setChildOperators(Arrays.<Operator<? extends OperatorDesc>>asList(sink));
    }
  }
{code}

like below? (looks like apparent mistake)
{code}
  for (Operator<?> operator : directWorks) {
    if (operator instanceof TableScanOperator || operator instanceof MapJoinOperator) {
      operator.setChildOperators(Arrays.<Operator<? extends OperatorDesc>>asList(sink));
    }
  }
{code};;;","04/Mar/14 02:22;sershe;Example query is hard to set up... let me try tomorrow if necessary 
Why should not other operator types be valid here? If I add this condition parent-child set calls are still not symmetric (even disregarding null parents) -  should they also not be added to parents then?;;;","04/Mar/14 02:29;navis;It might good just removing those if conditions (except null check). 
In LocalMapjoinProcFactory$LocalMapJoinProcessor#process
{code}
boolean directFetchable = useNontaged &&
            (parent instanceof TableScanOperator || parent instanceof MapJoinOperator);
{code};;;","04/Mar/14 02:44;sershe;That's what I did. The new check is for nulls only, and that has to be here as we do apparently have null parents in the tests. I am assuming in our original query all parents are null, because there's no other way to get to the stack in description (if there were non-null parents the NPE would have happened in the loop right before fill call, where we assign from tables; and if there were no parents directWorks empty check would have caused it to exit);;;","04/Mar/14 02:45;sershe;See 01 patch :);;;","04/Mar/14 04:51;navis;Ah, you are right. Direct works for some join operator can be nulls only in mapjoin-mapjoin case. I might think of test case as an addendum of you patch.;;;","04/Mar/14 18:25;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12632443/HIVE-6537.2.patch.txt

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5238 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_groupby2
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1615/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1615/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12632443;;;","04/Mar/14 20:51;sershe;I don't think failure is related (passed for me, too). New test looks good for me. Do I have +1 for the patch itself? I can commit (after 24 hours :));;;","06/Mar/14 19:27;hagleitn;+1;;;","07/Mar/14 18:29;sershe;committed to trunk and 13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Keep guava on v11 in tez branch,HIVE-6534,12698153,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,02/Mar/14 01:49,02/Mar/14 04:59,14/Jul/23 06:14,02/Mar/14 01:59,,,,,,,,,,tez-branch,,,,,,0,,,Needed to upgrade guava for tez - but the 0.3 release rolled that back.,,hagleitn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6360,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,376622,,,,Sun Mar 02 04:59:32 UTC 2014,,,,,,,,,,"0|i1swdb:",376917,,,,,,,,,,,,,,,,,,,,,"02/Mar/14 01:59;hagleitn;Committed to branch.;;;","02/Mar/14 04:59;hagleitn;[~jenjen01] - any particular reason for the changes to this jira?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Switch to released tez 0.3,HIVE-6533,12698151,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,02/Mar/14 01:36,02/Mar/14 01:39,14/Jul/23 06:14,02/Mar/14 01:39,,,,,,,,,,tez-branch,,,,,,0,,,,,hagleitn,jenjen01,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6360,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/14 01:38;hagleitn;HIVE-6533.1.patch;https://issues.apache.org/jira/secure/attachment/12631986/HIVE-6533.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,376620,,,,Sun Mar 02 01:39:07 UTC 2014,,,,,,,,,,"0|i1swcv:",376915,,,,,,,,,,,,,,,,,,,,,"02/Mar/14 01:39;hagleitn;Committed to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Runtime errors in vectorized execution.,HIVE-6531,12698104,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jnp,jnp,jnp,01/Mar/14 09:45,13/Nov/14 19:42,14/Jul/23 06:14,10/Mar/14 01:08,,,,,,,,,,0.13.0,0.14.0,,,,,0,,,"There are a few runtime errors observed in some of the tpcds queries for following reasons:
1) VectorFileSinkOperator fails with LazyBinarySerde.
2) Decimal128 and Unsigned128 don't serialize correctly.",,jnp,rhbutani,rusanu,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6511,,,HIVE-6568,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/14 09:54;jnp;HIVE-6531.1.patch;https://issues.apache.org/jira/secure/attachment/12631936/HIVE-6531.1.patch","06/Mar/14 07:29;jnp;HIVE-6531.2.patch;https://issues.apache.org/jira/secure/attachment/12633066/HIVE-6531.2.patch","07/Mar/14 04:50;jnp;HIVE-6531.3.patch;https://issues.apache.org/jira/secure/attachment/12633314/HIVE-6531.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,376573,,,,Thu Nov 13 19:42:08 UTC 2014,,,,,,,,,,"0|i1sw2f:",376868,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 07:29;jnp;Updated patch with tests.;;;","07/Mar/14 04:50;jnp;Updated patch.;;;","07/Mar/14 08:59;rusanu;+1;;;","08/Mar/14 20:29;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633314/HIVE-6531.3.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5373 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_expressions
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1664/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1664/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633314;;;","08/Mar/14 23:01;jnp;The failed test is addressed by HIVE-6511. I have tested it after applying HIVE-6511 patch.;;;","09/Mar/14 17:25;jnp;I have committed this to trunk.

[~rhbutani] This is a serious bug failing many queries in vectorized mode. It will be good to have it fixed in 0.13. I can commit to branch if you agree. The same patch applies to 0.13.
;;;","09/Mar/14 19:48;rhbutani;+1 for 0.13;;;","10/Mar/14 01:08;jnp;Committed to branch-0.13 as well.;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDK 7 trunk build fails after HIVE-6418 patch,HIVE-6530,12698095,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,navis,prasadm,prasadm,01/Mar/14 05:22,22/Mar/14 06:25,14/Jul/23 06:14,04/Mar/14 17:56,0.13.0,,,,,,,,,0.13.0,,Query Processor,,,,0,,,"JDK7 build fails with following error 
{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-exec: Compilation failure
[ERROR] /home/prasadm/repos/apache/hive-trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/LazyFlatRowContainer.java:[118,15] name clash: add(java.util.List<java.lang.Object>) in org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer overrides a method whose erasure is the same as another method, yet neither overrides the other
[ERROR] first method:  add(E) in java.util.AbstractCollection
[ERROR] second method: add(ROW) in org.apache.hadoop.hive.ql.exec.persistence.AbstractRowContainer
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-exec
{noformat}

This LazyFlatRowContainer.java is  a new file added as part of  HIVE-6418 patch. It's extending AbstractCollection and implements AbstractRowContainer. Looks like the both these have a add() method that's conflicting.",,gates,navis,prasadm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6542,,,,,,,HIVE-6418,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/14 04:24;navis;HIVE-6530.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12631989/HIVE-6530.1.patch.txt","03/Mar/14 02:08;navis;HIVE-6530.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12632172/HIVE-6530.2.patch.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,376564,,,,Tue Mar 04 17:56:33 UTC 2014,,,,,,,,,,"0|i1sw0f:",376859,,,,,,,,,,,,,,,,,,,,,"02/Mar/14 14:22;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12631989/HIVE-6530.1.patch.txt

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1589/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1589/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
Decision can match input such as ""LPAREN KW_NOT KW_CASE"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_CASE KW_CASE"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN LPAREN KW_CASE"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:108:5: 
Decision can match input such as ""KW_ORDER KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:121:5: 
Decision can match input such as ""KW_CLUSTER KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:133:5: 
Decision can match input such as ""KW_PARTITION KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:144:5: 
Decision can match input such as ""KW_DISTRIBUTE KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:155:5: 
Decision can match input such as ""KW_SORT KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:172:7: 
Decision can match input such as ""STAR"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:185:5: 
Decision can match input such as ""KW_ARRAY"" using multiple alternatives: 2, 6

As a result, alternative(s) 6 were disabled for that input
warning(200): IdentifiersParser.g:185:5: 
Decision can match input such as ""KW_UNIONTYPE"" using multiple alternatives: 5, 6

As a result, alternative(s) 6 were disabled for that input
warning(200): IdentifiersParser.g:185:5: 
Decision can match input such as ""KW_STRUCT"" using multiple alternatives: 4, 6

As a result, alternative(s) 6 were disabled for that input
warning(200): IdentifiersParser.g:267:5: 
Decision can match input such as ""KW_NULL"" using multiple alternatives: 1, 8

As a result, alternative(s) 8 were disabled for that input
warning(200): IdentifiersParser.g:267:5: 
Decision can match input such as ""KW_FALSE"" using multiple alternatives: 3, 8

As a result, alternative(s) 8 were disabled for that input
warning(200): IdentifiersParser.g:267:5: 
Decision can match input such as ""KW_TRUE"" using multiple alternatives: 3, 8

As a result, alternative(s) 8 were disabled for that input
warning(200): IdentifiersParser.g:267:5: 
Decision can match input such as ""KW_DATE StringLiteral"" using multiple alternatives: 2, 3

As a result, alternative(s) 3 were disabled for that input
warning(200): IdentifiersParser.g:399:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_MAP LPAREN"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:399:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_LATERAL KW_VIEW"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:399:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_CLUSTER KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:399:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_DISTRIBUTE KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:399:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_ORDER KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:399:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_GROUP KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:399:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_INSERT KW_INTO"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:399:5: 
Decision can match input such as ""KW_BETWEEN KW_MAP LPAREN"" using multiple alternatives: 8, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:399:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_INSERT KW_OVERWRITE"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:399:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_SORT KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:524:5: 
Decision can match input such as ""{AMPERSAND..BITWISEXOR, DIV..DIVIDE, EQUAL..EQUAL_NS, GREATERTHAN..GREATERTHANOREQUALTO, KW_AND, KW_ARRAY, KW_BETWEEN..KW_BOOLEAN, KW_CASE, KW_DOUBLE, KW_FLOAT, KW_IF, KW_IN, KW_INT, KW_LIKE, KW_MAP, KW_NOT, KW_OR, KW_REGEXP, KW_RLIKE, KW_SMALLINT, KW_STRING..KW_STRUCT, KW_TINYINT, KW_UNIONTYPE, KW_WHEN, LESSTHAN..LESSTHANOREQUALTO, MINUS..NOTEQUAL, PLUS, STAR, TILDE}"" using multiple alternatives: 1, 3

As a result, alternative(s) 3 were disabled for that input
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-exec ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-exec ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-exec ---
[INFO] Compiling 1620 source files to /data/hive-ptest/working/apache-svn-trunk-source/ql/target/classes
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] Note: Some input files use or override a deprecated API.
[WARNING] Note: Recompile with -Xlint:deprecation for details.
[WARNING] Note: Some input files use unchecked or unsafe operations.
[WARNING] Note: Recompile with -Xlint:unchecked for details.
[INFO] 4 warnings 
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java:[275,13] cannot find symbol
symbol  : method add(java.util.ArrayList<java.lang.Object>)
location: class org.apache.hadoop.hive.ql.exec.persistence.RowContainer<java.util.List<java.lang.Object>>
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFPartition.java:[93,10] cannot find symbol
symbol  : method add(java.util.List<java.lang.Object>)
location: class org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer<java.util.List<java.lang.Object>>
[INFO] 2 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [3.964s]
[INFO] Hive Ant Utilities ................................ SUCCESS [7.029s]
[INFO] Hive Shims Common ................................. SUCCESS [3.344s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.031s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [2.779s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [1.457s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [4.142s]
[INFO] Hive Shims ........................................ SUCCESS [0.613s]
[INFO] Hive Common ....................................... SUCCESS [10.711s]
[INFO] Hive Serde ........................................ SUCCESS [8.855s]
[INFO] Hive Metastore .................................... SUCCESS [29.410s]
[INFO] Hive Query Language ............................... FAILURE [38.886s]
[INFO] Hive Service ...................................... SKIPPED
[INFO] Hive JDBC ......................................... SKIPPED
[INFO] Hive Beeline ...................................... SKIPPED
[INFO] Hive CLI .......................................... SKIPPED
[INFO] Hive Contrib ...................................... SKIPPED
[INFO] Hive HBase Handler ................................ SKIPPED
[INFO] Hive HCatalog ..................................... SKIPPED
[INFO] Hive HCatalog Core ................................ SKIPPED
[INFO] Hive HCatalog Pig Adapter ......................... SKIPPED
[INFO] Hive HCatalog Server Extensions ................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client ................. SKIPPED
[INFO] Hive HCatalog Webhcat ............................. SKIPPED
[INFO] Hive HCatalog HBase Storage Handler ............... SKIPPED
[INFO] Hive HWI .......................................... SKIPPED
[INFO] Hive ODBC ......................................... SKIPPED
[INFO] Hive Shims Aggregator ............................. SKIPPED
[INFO] Hive TestUtils .................................... SKIPPED
[INFO] Hive Packaging .................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1:57.968s
[INFO] Finished at: Sun Mar 02 09:22:01 EST 2014
[INFO] Final Memory: 65M/569M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-exec: Compilation failure: Compilation failure:
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java:[275,13] cannot find symbol
[ERROR] symbol  : method add(java.util.ArrayList<java.lang.Object>)
[ERROR] location: class org.apache.hadoop.hive.ql.exec.persistence.RowContainer<java.util.List<java.lang.Object>>
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFPartition.java:[93,10] cannot find symbol
[ERROR] symbol  : method add(java.util.List<java.lang.Object>)
[ERROR] location: class org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer<java.util.List<java.lang.Object>>
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-exec
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12631989;;;","03/Mar/14 02:08;navis;Rebased to trunk;;;","03/Mar/14 10:20;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12632172/HIVE-6530.2.patch.txt

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5208 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1599/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1599/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12632172;;;","03/Mar/14 18:37;prasadm;+1

[~navis] Thanks for taking care of it! I guess test failures are unrelated ..;;;","04/Mar/14 06:06;navis;Confirmed TestEmbeddedThriftBinaryCLIService.testExecuteStatementAsync is not related. Thanks for a review.;;;","04/Mar/14 17:56;prasadm;Pach committed to trunk.
Thanks Navis!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add maven compiler plugin to ptest2 pom,HIVE-6528,12698056,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,szehon,szehon,szehon,28/Feb/14 23:59,01/Mar/14 17:14,14/Jul/23 06:14,01/Mar/14 17:14,0.13.0,,,,,,,,,0.13.0,,Testing Infrastructure,,,,0,,,"NO PRECOMMIT TESTS

Maven-compiler-plugin and java versions needs to be added to ptest2 pom.

Without this, will pick up random version of javac when trying to build this project.",,brocknoland,szehon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/14 00:04;szehon;HIVE-6528.patch;https://issues.apache.org/jira/secure/attachment/12631875/HIVE-6528.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,376530,,,,Sat Mar 01 17:14:11 UTC 2014,,,,,,,,,,"0|i1svsv:",376825,,,,,,,,,,,,,,,,,,,,,"01/Mar/14 00:35;brocknoland;+1;;;","01/Mar/14 17:14;brocknoland;Thank you Szehon! I have committed this to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix some whitespace issues in GenTezUtils,HIVE-6525,12698029,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,hagleitn,hagleitn,hagleitn,28/Feb/14 21:29,02/Mar/14 01:48,14/Jul/23 06:14,02/Mar/14 01:48,,,,,,,,,,tez-branch,,,,,,0,,,,,hagleitn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6360,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/14 21:31;hagleitn;HIVE-6525.1.patch;https://issues.apache.org/jira/secure/attachment/12631845/HIVE-6525.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,376503,,,,Sun Mar 02 01:48:20 UTC 2014,,,,,,,,,,"0|i1svn3:",376799,,,,,,,,,,,,,,,,,,,,,"02/Mar/14 01:48;hagleitn;Committed to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update ORC Filedump stripe sizes to match the memory manager changes,HIVE-6524,12698007,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,gopalv,gopalv,gopalv,28/Feb/14 20:14,28/Feb/14 21:18,14/Jul/23 06:14,28/Feb/14 21:18,tez-branch,,,,,,,,,tez-branch,,Tests,,,,0,,,"The MemoryManager in ORC now resets to default whenever we close all open writers. 

This results in consistent (but different from test golden) stripe sizes for all files being written.

Fix the goldens.",,gopalv,hagleitn,prasanth_j,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6360,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/14 20:44;gopalv;HIVE-6524.1-tez.patch;https://issues.apache.org/jira/secure/attachment/12631832/HIVE-6524.1-tez.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,376481,,,,Fri Feb 28 21:18:53 UTC 2014,,,,,,,,,,"0|i1svi7:",376777,Fix orc filedump tests to check for 5k row stripes for all but the final stripe,,,,,,,,,,,,,,,,,,,,"28/Feb/14 20:44;gopalv;{code}
 Stripes:
-  Stripe: offset: 3 data: 102311 rows: 4000 tail: 68 index: 224
+  Stripe: offset: 3 data: 144733 rows: 5000 tail: 68 index: 235
{code}

And everything else changes because of the first stripe being 5k rows.

A previous 21k orc writer was causing a leak into the next file, which ended up with 4k rows for 1st stream instead of the full 5k.;;;","28/Feb/14 20:50;prasanth_j;LGTM. +1 (non-binding);;;","28/Feb/14 21:18;hagleitn;Committed to branch. Thanks Gopal!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AVG() failure with decimal type,HIVE-6522,12697894,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,,jdere,jdere,28/Feb/14 09:47,05/Mar/14 18:49,14/Jul/23 06:14,05/Mar/14 18:48,0.13.0,,,,,,,,,0.13.0,,UDF,,,,0,,,"The following test fails:

{code}
hive> describe dec4;
OK
key                 	string              	from deserializer   
c1                  	string              	from deserializer   
c2                  	decimal(10,2)       	from deserializer   
Time taken: 0.716 seconds, Fetched: 3 row(s)
hive> select * from dec4;
OK
484	484	484
98	NULL	NULL
278	NULL	NULL
255	255	255
409	NULL	NULL
165	165	165
27	27	27
311	NULL	NULL
86	NULL	NULL
238	NULL	NULL
Time taken: 0.262 seconds, Fetched: 10 row(s)
hive> select avg(cast(key as decimal(3,0))) from dec4;
...

Task failed!
Task ID:
  Stage-1

Logs:

/tmp/jdere/hive.log
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
{code}

The logs show the following stack trace. 

{noformat}
java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) [Error getting row data with exception java.lang.NumberFormatException: Zero length BigInteger
	at java.math.BigInteger.<init>(BigInteger.java:171)
	at org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.getHiveDecimal(HiveDecimalWritable.java:85)
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector.getPrimitiveJavaObject(WritableHiveDecimalObjectInspector.java:43)
	at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:322)
	at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:392)
	at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:392)
	at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:392)
	at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:236)
	at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:222)
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:265)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:462)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:443)
 ]
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:282)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:462)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:443)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) [Error getting row data with exception java.lang.NumberFormatException: Zero length BigInteger
	at java.math.BigInteger.<init>(BigInteger.java:171)
	at org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.getHiveDecimal(HiveDecimalWritable.java:85)
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector.getPrimitiveJavaObject(WritableHiveDecimalObjectInspector.java:43)
	at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:322)
	at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:392)
	at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:392)
	at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:392)
	at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:236)
	at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:222)
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:265)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:462)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:443)
 ]
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:270)
	... 3 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NumberFormatException: Zero length BigInteger
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:808)
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:261)
	... 3 more
Caused by: java.lang.NumberFormatException: Zero length BigInteger
	at java.math.BigInteger.<init>(BigInteger.java:171)
	at org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.getHiveDecimal(HiveDecimalWritable.java:96)
	at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryHiveDecimal.init(LazyBinaryHiveDecimal.java:48)
	at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.uncheckedGetField(LazyBinaryStruct.java:216)
	at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.getField(LazyBinaryStruct.java:197)
	at org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector.getStructFieldData(LazyBinaryStructObjectInspector.java:64)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage$AbstractGenericUDAFAverageEvaluator.merge(GenericUDAFAverage.java:353)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:186)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.updateAggregations(GroupByOperator.java:641)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processAggr(GroupByOperator.java:905)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(GroupByOperator.java:737)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:803)
	... 4 more
{noformat}",,jdere,richardatcloudera,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6459,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,376368,,,,Wed Mar 05 18:47:54 UTC 2014,,,,,,,,,,"0|i1sut3:",376664,,,,,,,,,,,,,,,,,,,,,"28/Feb/14 09:51;jdere;I suspect this has something to do with the sum of the column values exceeding the precision/scale of the value type (decimal(3,0 in this case), though I haven't really looked into this so far.  [~xuefuz], does HIVE-6459 solve this problem?;;;","28/Feb/14 15:06;xuefuz;Yes. I saw this error message when working on HIVE-6459. It should be addressed with the patch there.;;;","02/Mar/14 16:55;xuefuz;[~jdere] HIVE-6459 is now closed. Could you please verify if the problem here goes away?;;;","05/Mar/14 18:47;jdere;Yeah, this looks like it's fixed, thanks Xuefu.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WebHCat cannot fetch correct percentComplete for Hive jobs,HIVE-6521,12697880,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,deepesh,deepesh,deepesh,28/Feb/14 07:56,13/Nov/14 19:40,14/Jul/23 06:14,26/Mar/14 02:19,0.13.0,,,,,,,,,0.14.0,,WebHCat,,,,0,,,"WebHCat E2E test TestHive_7 failed because percentComplete wasn't returned as expected.
{noformat}
check_job_percent_complete failed. got percentComplete ""map 0% reduce 0%"",  expected  ""map 100% reduce 100%""
{noformat}
So, there are two problems here.
# The log parsing is broken for status of percentComplete. In the stderr of the job we see:
{noformat}
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1393486488858_0691, Tracking URL = http://ambari-sec-1393480847-others-2-4.cs1cloud.internal:8088/proxy/application_1393486488858_0691/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1393486488858_0691
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2014-02-27 18:40:50,166 Stage-1 map = 0%,  reduce = 0%
2014-02-27 18:40:56,599 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.87 sec
2014-02-27 18:40:57,656 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.87 sec
2014-02-27 18:40:58,706 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.87 sec
MapReduce Total cumulative CPU time: 870 msec
Ended Job = job_1393486488858_0691
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.87 sec   HDFS Read: 305 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 870 msec
{noformat}
The assumption in the code is that the line containing the percent status will end after ""reduce = \d+%"" but that fails with the above.
# The last status from Hive job is ""map = 100%,  reduce = 0%"" instead of expected ""map = 100%,  reduce = 100%"".",,deepesh,ekoifman,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/14 18:02;deepesh;HIVE-6521.2.patch;https://issues.apache.org/jira/secure/attachment/12631797/HIVE-6521.2.patch","28/Feb/14 07:59;deepesh;HIVE-6521.patch;https://issues.apache.org/jira/secure/attachment/12631690/HIVE-6521.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,376354,,,,Thu Nov 13 19:40:41 UTC 2014,,,,,,,,,,"0|i1supz:",376650,,,,,,,,,,,,,,,,,,,,,"28/Feb/14 07:59;deepesh;Attached patch fixes two things:
# The regular expression to comply with the Hive percent status logging.
# Modified the test so that we also exercise the reduce step.
Please review.;;;","28/Feb/14 18:02;deepesh;The previous patch was getting the ""Cumulative CPU ..."" in the percentComplete string. New patch does better job in parsing.;;;","01/Mar/14 04:57;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12631797/HIVE-6521.2.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5185 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1560/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1560/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12631797;;;","10/Mar/14 18:37;ekoifman;+1;;;","25/Mar/14 22:54;sushanth;I think there's an issue here with a unit test failure with -Phadoop-2, looking into this.;;;","25/Mar/14 23:51;sushanth;Okay, nvm, my apologies, this patch is okay - my issue was from another source. +1 to this patch.;;;","26/Mar/14 02:19;sushanth;Committed to trunk. Thanks, Deepesh for the patch, and Thanks Eugene, for the review.;;;","26/Mar/14 02:27;deepesh;Thanks Eugene for the review and Sushanth for the committ.;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Allow optional ""as"" in subquery definition",HIVE-6519,12697828,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,hagleitn,hagleitn,hagleitn,27/Feb/14 23:47,05/Mar/14 05:06,14/Jul/23 06:14,01/Mar/14 01:00,,,,,,,,,,0.13.0,,,,,,0,,,"Allow both:
select * from (select * from foo) bar 
select * from (select * from foo) as bar 
",,hagleitn,leftyl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/14 23:48;hagleitn;HIVE-6519.1.patch;https://issues.apache.org/jira/secure/attachment/12631625/HIVE-6519.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,376302,,,,Wed Mar 05 05:06:38 UTC 2014,,,,,,,,,,"0|i1suef:",376598,,,,,,,,,,,,,,,,,,,,,"27/Feb/14 23:58;ashutoshc;+1;;;","28/Feb/14 16:14;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12631625/HIVE-6519.1.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5181 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1553/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1553/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12631625;;;","28/Feb/14 20:08;hagleitn;Test failures are unrelated...;;;","01/Mar/14 01:00;ashutoshc;Committed to trunk.;;;","05/Mar/14 05:06;leftyl;Documented in the wiki here:

* [SubQueries:  Subqueries in the FROM Clause |https://cwiki.apache.org/confluence/display/Hive/LanguageManual+SubQueries#LanguageManualSubQueries-SubqueriesintheFROMClause]

by adding a second line of syntax:

{code}
SELECT ... FROM (subquery) name ...
SELECT ... FROM (subquery) AS name ...   (Note: Only valid starting with Hive 0.13.0)
{code}

and this text:

bq. The optional keyword ""AS"" can be included before the subquery name in Hive 0.13.0 and later versions (HIVE-6519).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a GC canary to the VectorGroupByOperator to flush whenever a GC is triggered,HIVE-6518,12697823,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,gopalv,gopalv,gopalv,27/Feb/14 23:16,17/Mar/14 18:41,14/Jul/23 06:14,17/Mar/14 16:42,0.13.0,,,,,,,,,0.13.0,,Query Processor,,,,0,,,"The current VectorGroupByOperator implementation flushes the in-memory hashes when the maximum entries or fraction of memory is hit.

This works for most cases, but there are some corner cases where we hit GC ovehead limits or heap size limits before either of those conditions are reached due to the rest of the pipeline.

This patch adds a SoftReference as a GC canary. If the soft reference is dead, then a full GC pass happened sometime in the near past & the aggregation hashtables should be flushed immediately before another full GC is triggered.",,gopalv,hagleitn,jnp,rhbutani,rusanu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/14 23:23;gopalv;HIVE-6518.1-tez.patch;https://issues.apache.org/jira/secure/attachment/12631621/HIVE-6518.1-tez.patch","28/Feb/14 23:42;gopalv;HIVE-6518.2-tez.patch;https://issues.apache.org/jira/secure/attachment/12631872/HIVE-6518.2-tez.patch","02/Mar/14 05:02;hagleitn;HIVE-6518.2.patch;https://issues.apache.org/jira/secure/attachment/12631991/HIVE-6518.2.patch","13/Mar/14 08:53;hagleitn;HIVE-6518.3.patch;https://issues.apache.org/jira/secure/attachment/12634401/HIVE-6518.3.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,376297,,,,Mon Mar 17 18:41:09 UTC 2014,,,,,,,,,,"0|i1sudb:",376593,Flush VectorGroupBy aggregation hashes in case of a full GC,,,,,,,,,,,,,,,,,,,,"27/Feb/14 23:36;hagleitn;I like it. Sounds like this will allow you to be more aggressive with caching/flushing params, while having a trigger that will flush out stuff when necessary.

+1 (assuming tests pass);;;","28/Feb/14 00:00;gopalv;Yes, also the ORC scenario is more complex for strings in dictionaries. 

A substring does not drop the rest of the data off the memory overhead because in vectorized mode, only the start:len get modified, no new allocations are made.

So a group by SUBSTR() will keep the entire string in  memory, except the VGBY does not know that it does.;;;","28/Feb/14 19:57;rusanu;Can you somehow modify the LOG.debug at top of flush() to call out that the flush was triggered by the gcCanary.get() == null? I was thinking: keep a count of gcCanary allocations and print it in the LOG.debug message, this will tell us if the GC is the trigger and also will tell how often has occured in the operator lifetime, when debugging etc.
+1;;;","28/Feb/14 23:42;gopalv;Add DEBUG lines;;;","02/Mar/14 05:02;hagleitn;Reuploading .2 for precommit.;;;","10/Mar/14 21:53;gopalv;Resubmit for pre-commit tests;;;","13/Mar/14 08:53;hagleitn;pre-commit is back. let's try again.;;;","14/Mar/14 06:09;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634401/HIVE-6518.3.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5389 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1769/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1769/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634401;;;","14/Mar/14 18:52;gopalv;The test failures don't seem to be related to this fix - they aren't vectorized.;;;","17/Mar/14 16:42;jnp;I have committed this trunk. Thanks to Gopal!

[~rhbutani] This is an important fix to vector group by because the aggregates must flush more aggressively in case of GC. Therefore, I intend to commit it to branch-0.13. as well.;;;","17/Mar/14 17:13;rhbutani;+1 for port to 0.13;;;","17/Mar/14 18:41;jnp;Committed to branch-0.13 as well. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom vertex in hive-tez should be able to accept multiple MR-inputs,HIVE-6515,12697791,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vikram.dixit,vikram.dixit,vikram.dixit,27/Feb/14 20:59,13/Nov/14 19:40,14/Jul/23 06:14,24/Sep/14 07:06,tez-branch,,,,,,,,,0.14.0,,Tez,,,,0,,,Custom vertex in hive-tez should be able to accept multiple MR-inputs. Currently this is restricted to only 1 input.,,thejas,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7430,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,376265,,,,Thu Nov 13 19:40:43 UTC 2014,,,,,,,,,,"0|i1su5z:",376560,,,,,,,,,,,,,,,,,,,,,"13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2 ThriftCLIServiceTest#testDoAs is an invalid test,HIVE-6512,12697637,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vgumashta,vgumashta,vgumashta,27/Feb/14 09:09,12/Mar/14 11:35,14/Jul/23 06:14,12/Mar/14 11:35,0.13.0,,,,,,,,,0.13.0,,HiveServer2,,,,0,,,Basically the test tries to test a kerberos doAs which is invalid since it doesn't do a kerberos login and it's not possible to unit test a kerberos setup. Surprisingly it has been hanging around for a while. Needs to be removed from the test suite.,,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-4764,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/14 09:15;vgumashta;HIVE-6512.1.patch;https://issues.apache.org/jira/secure/attachment/12631484/HIVE-6512.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,376111,,,,Wed Mar 12 11:35:04 UTC 2014,,,,,,,,,,"0|i1st87:",376407,,,,,,,,,,,,,,,,,,,,,"11/Mar/14 12:42;thejas;+1;;;","12/Mar/14 11:35;thejas;Patch committed to trunk and 0.13 branch. This is a blocker for HIVE-4764 which is tracked in the 0.13 cwiki, which needs this test case fix. cc [~rhbutani].

Thanks for the contribution Vaibhav !
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"casting from decimal to tinyint,smallint, int and bigint generates different result when vectorization is on",HIVE-6511,12697459,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jnp,jnp,jnp,26/Feb/14 21:33,13/Nov/14 19:40,14/Jul/23 06:14,09/Mar/14 17:44,0.13.0,,,,,,,,,0.13.0,0.14.0,,,,,0,,,"select dc,cast(dc as int), cast(dc as smallint),cast(dc as tinyint) from vectortab10korc limit 20 generates following result when vectorization is enabled:
{code}
4619756289662.078125	-1628520834	-16770	126
1553532646710.316406	-1245514442	-2762	54
3367942487288.360352	688127224	-776	-8
4386447830839.337891	1286221623	12087	55
-3234165331139.458008	-54957251	27453	61
-488378613475.326172	1247658269	-16099	29
-493942492598.691406	-21253559	-19895	73
3101852523586.039062	886135874	23618	66
2544105595941.381836	1484956709	-23515	37
-3997512403067.0625	1102149509	30597	-123
-1183754978977.589355	1655994718	31070	94
1408783849655.676758	34576568	-26440	-72
-2993175106993.426758	417098319	27215	79
3004723551798.100586	-1753555402	-8650	54
1103792083527.786133	-14511544	-28088	72
469767055288.485352	1615620024	26552	-72
-1263700791098.294434	-980406074	12486	-58
-4244889766496.484375	-1462078048	30112	-96
-3962729491139.782715	1525323068	-27332	60
NULL	NULL	NULL	NULL
{code}

When vectorization is disabled, result looks like this:
{code}
4619756289662.078125	-1628520834	-16770	126
1553532646710.316406	-1245514442	-2762	54
3367942487288.360352	688127224	-776	-8
4386447830839.337891	1286221623	12087	55
-3234165331139.458008	-54957251	27453	61
-488378613475.326172	1247658269	-16099	29
-493942492598.691406	-21253558	-19894	74
3101852523586.039062	886135874	23618	66
2544105595941.381836	1484956709	-23515	37
-3997512403067.0625	1102149509	30597	-123
-1183754978977.589355	1655994719	31071	95
1408783849655.676758	34576567	-26441	-73
-2993175106993.426758	417098319	27215	79
3004723551798.100586	-1753555402	-8650	54
1103792083527.786133	-14511545	-28089	71
469767055288.485352	1615620024	26552	-72
-1263700791098.294434	-980406074	12486	-58
-4244889766496.484375	-1462078048	30112	-96
-3962729491139.782715	1525323069	-27331	61
NULL	NULL	NULL	NULL
{code}

This issue is visible only for certain decimal values. In above example, row 7,11,12, and 15 generates different results.

vectortab10korc table schema:
{code}
t                   	tinyint             	from deserializer   
si                  	smallint            	from deserializer   
i                   	int                 	from deserializer   
b                   	bigint              	from deserializer   
f                   	float               	from deserializer   
d                   	double              	from deserializer   
dc                  	decimal(38,18)      	from deserializer   
bo                  	boolean             	from deserializer   
s                   	string              	from deserializer   
s2                  	string              	from deserializer   
ts                  	timestamp           	from deserializer   
	 	 
# Detailed Table Information	 	 
Database:           	default             	 
Owner:              	xyz              	 
CreateTime:         	Tue Feb 25 21:54:28 UTC 2014	 
LastAccessTime:     	UNKNOWN             	 
Protect Mode:       	None                	 
Retention:          	0                   	 
Location:           	hdfs://host1.domain.com:8020/apps/hive/warehouse/vectortab10korc	 
Table Type:         	MANAGED_TABLE       	 
Table Parameters:	 	 
	COLUMN_STATS_ACCURATE	true                
	numFiles            	1                   
	numRows             	10000               
	rawDataSize         	0                   
	totalSize           	344748              
	transient_lastDdlTime	1393365281          
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.ql.io.orc.OrcSerde	 
InputFormat:        	org.apache.hadoop.hive.ql.io.orc.OrcInputFormat	 
OutputFormat:       	org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	serialization.format	1                   
Time taken: 0.196 seconds, Fetched: 41 row(s
{code}



",,ehans,jnp,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6568,HIVE-6531,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/14 05:36;jnp;HIVE-6511.1.patch;https://issues.apache.org/jira/secure/attachment/12632184/HIVE-6511.1.patch","05/Mar/14 21:25;jnp;HIVE-6511.2.patch;https://issues.apache.org/jira/secure/attachment/12632916/HIVE-6511.2.patch","05/Mar/14 21:40;jnp;HIVE-6511.3.patch;https://issues.apache.org/jira/secure/attachment/12632919/HIVE-6511.3.patch","07/Mar/14 04:38;jnp;HIVE-6511.4.patch;https://issues.apache.org/jira/secure/attachment/12633312/HIVE-6511.4.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,375933,,,,Thu Nov 13 19:40:49 UTC 2014,,,,,,,,,,"0|i1ss4v:",376229,,,,,,,,,,,,,,,,,,,,,"03/Mar/14 05:36;jnp;The longValue function in Decimal128 rounds the value. HiveDecimal just discards the fractional part. This patch adds another method to Decimal128, that discards the fractional part, and is used in the CastDecimalToLong expression.;;;","03/Mar/14 17:24;ehans;Can you put this up on ReviewBoard?;;;","05/Mar/14 21:40;jnp;Review board: https://reviews.apache.org/r/18808/;;;","07/Mar/14 04:38;jnp;Updated patch addresses review comments.;;;","07/Mar/14 19:17;ehans;+1;;;","09/Mar/14 13:21;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633312/HIVE-6511.4.patch

{color:green}SUCCESS:{color} +1 5374 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1674/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1674/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633312;;;","09/Mar/14 17:44;jnp;I have committed this.;;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mismatched results between vector and non-vector mode with decimal field,HIVE-6508,12697316,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,rusanu,rusanu,rusanu,26/Feb/14 07:30,13/Nov/14 19:42,14/Jul/23 06:14,08/Mar/14 23:05,0.13.0,,,,,,,,,0.13.0,0.14.0,Query Processor,,,,0,,,"Following query has a little mismatch in result as compared to the non-vector mode.
{code}
select d_year, i_brand_id, i_brand,
       sum(ss_ext_sales_price) as sum_agg
from date_dim
join store_sales on date_dim.d_date_sk = store_sales.ss_sold_date_sk
join item on store_sales.ss_item_sk = item.i_item_sk
where i_manufact_id = 128
  and d_moy = 11
group by d_year, i_brand, i_brand_id
order by d_year, sum_agg desc, i_brand_id
limit 100;
{code}
This query is on tpcds data.
The field ss_ext_sales_price is of type decimal(7,2) and everything else is an integer.
",,jnp,rusanu,sershe,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/14 02:33;jnp;HIVE-6508.1.patch;https://issues.apache.org/jira/secure/attachment/12633294/HIVE-6508.1.patch","06/Mar/14 12:30;rusanu;HIVE-6508.1.patch;https://issues.apache.org/jira/secure/attachment/12633112/HIVE-6508.1.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,375790,,,,Thu Nov 13 19:42:04 UTC 2014,,,,,,,,,,"0|i1sr93:",376086,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 10:59;rusanu;The value 0 comes in the input vector unscaled (scale 0). As aggregates (SUM, STDxx) are being updated, they us the scale of the input value, not the scale of the input column. So any 0 in the input will round the intermediate fractional part of the intermediate. Final result is off. AVG uses a special scale so is not affected. MIN/MAX use the input value scale, but has no side effects. Fix is to pass in the column scale explictly, rather than assume the input value scale has the column scale. Ultimately the behavior of passing in unscaled 0s is wrong, but this comes from the row-mode join modus-operandi and I don't want to change that. Hardening the aggregates against this case is more robust.;;;","06/Mar/14 17:23;jnp;+1. The patch looks good to me.;;;","07/Mar/14 02:33;jnp;Same patch re-uploaded to trigger pre-commit build.;;;","08/Mar/14 01:35;sershe;It might make sense to add a test to ./ql/src/test/queries/clientcompare :);;;","08/Mar/14 08:56;rusanu;[~sershe] There is a new test case testSumDecimalHive6508 the covers possible regression;;;","08/Mar/14 17:08;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633294/HIVE-6508.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5374 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_parallel_orderby
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1660/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1660/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633294;;;","08/Mar/14 18:35;rusanu;The failure is unrelated to the patch;;;","08/Mar/14 23:05;jnp;Committed to trunk. It is a correctness bug, therefore I will port it to hive-13 branch as well.;;;","09/Mar/14 00:05;jnp;Committed to branch-0.13;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OrcFile table property names are specified as strings,HIVE-6507,12697273,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sushanth,sushanth,sushanth,26/Feb/14 01:00,12/Mar/14 12:50,14/Jul/23 06:14,12/Mar/14 12:50,0.13.0,,,,,,,,,0.13.0,,HCatalog,Serializers/Deserializers,,,0,,,"In HIVE-5504, we had to do some special casing in HCatalog to add a particular set of orc table properties from table properties to job properties.

In doing so, it's obvious that that is a bit cumbersome, and ideally, the list of all orc file table properties should really be an enum, rather than individual loosely tied constant strings. If we were to clean this up, we can clean up other code that references this to reference the entire enum, and avoid future errors when new table properties are introduced, but other referencing code is not updated.",,omalley,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6506,,,,,,,HIVE-5504,,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/14 19:02;sushanth;HIVE-6507.2.patch;https://issues.apache.org/jira/secure/attachment/12632882/HIVE-6507.2.patch","26/Feb/14 01:02;sushanth;HIVE-6507.patch;https://issues.apache.org/jira/secure/attachment/12631113/HIVE-6507.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,375747,,,,Wed Mar 12 12:50:43 UTC 2014,,,,,,,,,,"0|i1sqzj:",376043,"(none, this is minor refactoring so as to avoid future issues where new orc properties are added and not accounted for in HCatalog/etc)",,,,,,,,,,,,,,,,,,,,"26/Feb/14 01:02;sushanth;Attaching patch. This applies on top of HIVE-5504, and depends on that being committed first.;;;","26/Feb/14 02:34;thejas;+1 ;;;","26/Feb/14 23:00;sushanth;Marking as patch-available so the precommit test queue picks it up when it's time - I figure 5504 will have been committed by the time this comes up.;;;","05/Mar/14 17:52;omalley;This breaks the API compatibility. You need to leave the strings.;;;","05/Mar/14 19:01;sushanth;Ah, ok, how about if we reintroduce the strings, but mark them deprecated so that new parameters are henceforth introduced in the enum instead, as in the .2.patch?;;;","05/Mar/14 19:04;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12631113/HIVE-6507.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5354 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hive.beeline.TestSchemaTool.testSchemaInit
org.apache.hive.beeline.TestSchemaTool.testSchemaUpgrade
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1631/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1631/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12631113;;;","05/Mar/14 20:52;sushanth;Noting that 2 of the test failures reported are due to the upgrade to 0.14, and one is disconnected from this issue. Also, the new patch does not change behaviour from the previous patch except for adding back in the string constants with deprecation notices, and thus, should not change test behaviour.;;;","12/Mar/14 12:50;thejas;Patch committed to trunk and 0.13 branch (in list of 0.13 patches maintained by Harish).
Thanks for the contribution Sushanth!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make stats optimizer more robust in presence of distinct clause,HIVE-6505,12697259,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,26/Feb/14 00:16,03/Mar/14 16:02,14/Jul/23 06:14,03/Mar/14 16:02,,,,,,,,,,0.13.0,,Statistics,,,,0,,,Currently it throws exceptions in few cases.,,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/14 15:54;ashutoshc;HIVE-6505.1.patch;https://issues.apache.org/jira/secure/attachment/12631763/HIVE-6505.1.patch","26/Feb/14 00:20;ashutoshc;HIVE-6505.patch;https://issues.apache.org/jira/secure/attachment/12631104/HIVE-6505.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,375733,,,,Mon Mar 03 16:02:44 UTC 2014,,,,,,,,,,"0|i1sqwf:",376029,,,,,,,,,,,,,,,,,,,,,"26/Feb/14 00:20;ashutoshc;More checks to make sure stats optimizer fires correctly.;;;","26/Feb/14 00:28;ashutoshc;https://reviews.apache.org/r/18494/;;;","28/Feb/14 15:54;ashutoshc;Reattaching same patch for Hive QA to pick up.;;;","01/Mar/14 12:45;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12631763/HIVE-6505.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5186 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_quotedid_smb
org.apache.hcatalog.hbase.snapshot.lock.TestWriteLock.testRun
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1562/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1562/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12631763;;;","03/Mar/14 06:27;thejas;+1
Ashutosh, Can you verify that the test failures are not related ?
;;;","03/Mar/14 16:02;ashutoshc;Reran the tests. They passed locally. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change hadoop dependency on tez branch,HIVE-6501,12697098,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,25/Feb/14 10:34,27/Feb/14 19:23,14/Jul/23 06:14,27/Feb/14 19:23,,,,,,,,,,tez-branch,,,,,,0,,,"Now that 2.3.0 is out, we no longer need to pull the snapshot.",,hagleitn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6360,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/14 10:35;hagleitn;HIVE-6501.1.patch;https://issues.apache.org/jira/secure/attachment/12630924/HIVE-6501.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,375572,,,,2014-02-25 10:34:32.0,,,,,,,,,,"0|i1spwv:",375868,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using Metastore-side Auth errors on non-resolvable IF/OF/SerDe,HIVE-6499,12697082,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sushanth,sushanth,sushanth,25/Feb/14 06:43,01/Oct/19 22:07,14/Jul/23 06:14,26/Mar/14 16:56,,,,,,,,,,0.13.0,,Metastore,Security,,,0,,,"In cases where a user needs to use a custom IF/OF/SerDe that is not accessible from the metastore, calls like msc.createTable and msc.dropTable should still work without being able to load the class. This is possible as long as one does not enable MetaStore-side authorization, at which point this becomes impossible, erroring out with a ClassNotFoundException.

The reason this happens is that since the AuthorizationProvider interface is defined against a ql.metadata.Table, we wind up needing to instantiate a ql.metadata.Table object, which, in its constructor tries to instantiate IF/OF/SerDe elements in an attempt to pre-load those fields. And if we do not have access to those classes in the metastore, this is when that fails. The constructor/initialize methods of Table and Partition do not really need to pre-initialize these fields, since the fields are accessed only through the accessor, and will be instantiated on first-use.
",,gates,rhbutani,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 00:44;sushanth;HIVE-6499.2.patch;https://issues.apache.org/jira/secure/attachment/12634599/HIVE-6499.2.patch","12/Mar/14 12:44;thejas;HIVE-6499.patch;https://issues.apache.org/jira/secure/attachment/12634158/HIVE-6499.patch","25/Feb/14 06:46;sushanth;HIVE-6499.patch;https://issues.apache.org/jira/secure/attachment/12630899/HIVE-6499.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,375556,,,,Wed Mar 26 16:56:05 UTC 2014,,,,,,,,,,"0|i1sptb:",375852,"Fixes bug where if Metastore-side authorization was used, tables that used a custom IF/OF/SerDe that was not accessible from the metastore-side could not be created or dropped.",,,,,,,,,,,,,,,,,,,,"25/Feb/14 06:46;sushanth;Attaching patch.;;;","25/Feb/14 06:57;sushanth;Created review board link : https://reviews.apache.org/r/18456/

[~thejas], could you please review this?

[~owen.omalley], this is a bug that you pinged me about, if you could test that it works for your usecase as well, that'd be great.;;;","25/Feb/14 22:35;thejas;+1;;;","01/Mar/14 00:11;sushanth;canceling patch and re-marking as available to make the precommit tests pick it up, now that it's running again.;;;","12/Mar/14 12:44;thejas;HIVE-6499.patch - uploading a copy of the patch, in case the pre-commit tests have marked earlier file as processed.;;;","13/Mar/14 22:19;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634158/HIVE-6499.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1767/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1767/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-1767/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'contrib/src/test/results/clientnegative/udtf_explode2.q.out'
Reverted 'contrib/src/test/results/clientnegative/case_with_row_sequence.q.out'
Reverted 'contrib/src/test/results/clientnegative/invalid_row_sequence.q.out'
Reverted 'contrib/src/test/results/clientpositive/udaf_example_min_n.q.out'
Reverted 'contrib/src/test/results/clientpositive/udaf_example_max_n.q.out'
Reverted 'contrib/src/test/results/clientpositive/lateral_view_explode2.q.out'
Reverted 'contrib/src/test/results/clientpositive/udaf_example_avg.q.out'
Reverted 'contrib/src/test/results/clientpositive/udf_example_arraymapstruct.q.out'
Reverted 'contrib/src/test/results/clientpositive/udaf_example_group_concat.q.out'
Reverted 'contrib/src/test/results/clientpositive/udaf_example_min.q.out'
Reverted 'contrib/src/test/results/clientpositive/udf_row_sequence.q.out'
Reverted 'contrib/src/test/results/clientpositive/udaf_example_max.q.out'
Reverted 'contrib/src/test/results/clientpositive/udf_example_add.q.out'
Reverted 'contrib/src/test/results/clientpositive/udf_example_format.q.out'
Reverted 'contrib/src/test/results/clientpositive/udtf_output_on_close.q.out'
Reverted 'contrib/src/test/results/clientpositive/udtf_explode2.q.out'
Reverted 'contrib/src/test/results/clientpositive/dboutput.q.out'
Reverted 'ql/src/test/results/clientnegative/udf_local_resource.q.out'
Reverted 'ql/src/test/results/clientnegative/udf_function_does_not_implement_udf.q.out'
Reverted 'ql/src/test/results/clientnegative/create_unknown_udf_udaf.q.out'
Reverted 'ql/src/test/results/clientnegative/create_function_nonudf_class.q.out'
Reverted 'ql/src/test/results/clientnegative/create_function_nonexistent_db.q.out'
Reverted 'ql/src/test/results/clientnegative/udf_test_error_reduce.q.out'
Reverted 'ql/src/test/results/clientnegative/drop_native_udf.q.out'
Reverted 'ql/src/test/results/clientnegative/create_udaf_failure.q.out'
Reverted 'ql/src/test/results/clientnegative/udf_nonexistent_resource.q.out'
Reverted 'ql/src/test/results/clientnegative/create_unknown_genericudf.q.out'
Reverted 'ql/src/test/results/clientnegative/cluster_tasklog_retrieval.q.out'
Reverted 'ql/src/test/results/clientnegative/udf_test_error.q.out'
Reverted 'ql/src/test/results/clientnegative/create_function_nonexistent_class.q.out'
Reverted 'ql/src/test/results/clientpositive/create_func1.q.out'
Reverted 'ql/src/test/results/clientpositive/macro.q.out'
Reverted 'ql/src/test/results/clientpositive/create_view.q.out'
Reverted 'ql/src/test/results/clientpositive/udaf_sum_list.q.out'
Reverted 'ql/src/test/results/clientpositive/create_genericudaf.q.out'
Reverted 'ql/src/test/results/clientpositive/udf_logic_java_boolean.q.out'
Reverted 'ql/src/test/results/clientpositive/udf_compare_java_string.q.out'
Reverted 'ql/src/test/results/clientpositive/udf_testlength.q.out'
Reverted 'ql/src/test/results/clientpositive/udf_testlength2.q.out'
Reverted 'ql/src/test/results/clientpositive/drop_udf.q.out'
Reverted 'ql/src/test/results/clientpositive/autogen_colalias.q.out'
Reverted 'ql/src/test/results/clientpositive/create_udaf.q.out'
Reverted 'ql/src/test/results/clientpositive/create_genericudf.q.out'
Reverted 'ql/src/test/results/clientpositive/ptf_register_tblfn.q.out'
Reverted 'ql/src/test/results/clientpositive/udf_context_aware.q.out'
Reverted 'ql/src/test/results/clientpositive/windowing_udaf2.q.out'
Reverted 'ql/src/test/results/clientpositive/compile_processor.q.out'
Reverted 'ql/src/test/results/clientpositive/udf_using.q.out'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/FunctionSemanticAnalyzer.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/MacroSemanticAnalyzer.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/Operation2Privilege.java'
++ awk '{print $2}'
++ egrep -v '^X|^Performing status on external'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/storage-handlers/hbase/target hcatalog/server-extensions/target hcatalog/core/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target hwi/target common/target common/src/gen service/target contrib/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target ql/src/test/results/clientnegative/authorization_create_macro1.q.out ql/src/test/results/clientnegative/authorization_create_func1.q.out ql/src/test/results/clientnegative/authorization_create_func2.q.out ql/src/test/results/clientpositive/authorization_create_func1.q.out ql/src/test/results/clientpositive/authorization_create_macro1.q.out ql/src/test/queries/clientnegative/authorization_create_macro1.q ql/src/test/queries/clientnegative/authorization_create_func1.q ql/src/test/queries/clientnegative/authorization_create_func2.q ql/src/test/queries/clientpositive/authorization_create_macro1.q ql/src/test/queries/clientpositive/authorization_create_func1.q
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1577348.

At revision 1577348.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634158;;;","14/Mar/14 00:09;sushanth;Looks like the patch needed to be updated after the changes in HIVE-3938. Regenerating.;;;","14/Mar/14 00:44;sushanth;Updated patch to latest trunk.;;;","15/Mar/14 07:25;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634599/HIVE-6499.2.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1795/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1795/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hwi ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hwi ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hwi ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hwi ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hwi ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hwi ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/target/hive-hwi-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hwi/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hwi/0.14.0-SNAPSHOT/hive-hwi-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive ODBC 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-odbc ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/odbc (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-odbc ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-odbc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-odbc ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/odbc/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-odbc ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-odbc ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/odbc/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-odbc/0.14.0-SNAPSHOT/hive-odbc-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Aggregator 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-aggregator ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-aggregator ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims-aggregator ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-aggregator ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims-aggregator/0.14.0-SNAPSHOT/hive-shims-aggregator-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive TestUtils 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-testutils ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/testutils (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-testutils ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-testutils ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-testutils ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-testutils ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-testutils ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-testutils ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-testutils ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-testutils ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-testutils ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-testutils ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/target/hive-testutils-0.14.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/testutils/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-testutils/0.14.0-SNAPSHOT/hive-testutils-0.14.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Packaging 0.14.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.pom
[WARNING] The POM for org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/0.14.0-SNAPSHOT/hive-hcatalog-hbase-storage-handler-0.14.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [9.127s]
[INFO] Hive Ant Utilities ................................ SUCCESS [5.951s]
[INFO] Hive Shims Common ................................. SUCCESS [3.471s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.533s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.117s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.592s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [7.958s]
[INFO] Hive Shims ........................................ SUCCESS [1.290s]
[INFO] Hive Common ....................................... SUCCESS [7.409s]
[INFO] Hive Serde ........................................ SUCCESS [10.438s]
[INFO] Hive Metastore .................................... SUCCESS [32.268s]
[INFO] Hive Query Language ............................... SUCCESS [1:21.803s]
[INFO] Hive Service ...................................... SUCCESS [7.569s]
[INFO] Hive JDBC ......................................... SUCCESS [2.978s]
[INFO] Hive Beeline ...................................... SUCCESS [2.881s]
[INFO] Hive CLI .......................................... SUCCESS [1.891s]
[INFO] Hive Contrib ...................................... SUCCESS [2.513s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.736s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.586s]
[INFO] Hive HCatalog Core ................................ SUCCESS [1.839s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.468s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.244s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [2.251s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [10.352s]
[INFO] Hive HWI .......................................... SUCCESS [1.245s]
[INFO] Hive ODBC ......................................... SUCCESS [0.883s]
[INFO] Hive Shims Aggregator ............................. SUCCESS [0.166s]
[INFO] Hive TestUtils .................................... SUCCESS [0.625s]
[INFO] Hive Packaging .................................... FAILURE [1.676s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:37.399s
[INFO] Finished at: Sat Mar 15 03:25:32 EDT 2014
[INFO] Final Memory: 74M/440M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-packaging: Could not resolve dependencies for project org.apache.hive:hive-packaging:pom:0.14.0-SNAPSHOT: Could not find artifact org.apache.hive.hcatalog:hive-hcatalog-hbase-storage-handler:jar:0.14.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634599;;;","26/Mar/14 04:53;gates;Ran the tests locally, all tests pass.;;;","26/Mar/14 16:56;rhbutani;Committed to trunk and 0.13 branches.
[~gates] thanks for running the tests;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add config vars to be able to set different defaults for tez and mapred,HIVE-6498,12697076,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,25/Feb/14 06:16,12/Aug/14 02:39,14/Jul/23 06:14,27/Feb/14 19:22,,,,,,,,,,tez-branch,,,,,,0,,,"should be able to set container size, whether to merge, input format differently for mr and tez (at the same time).",,apivovarov,hagleitn,leftyl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6360,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/14 06:18;hagleitn;HIVE-6498.1.patch;https://issues.apache.org/jira/secure/attachment/12630894/HIVE-6498.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,375550,,,,Tue Aug 12 02:39:39 UTC 2014,,,,,,,,,,"0|i1sprz:",375846,,,,,,,,,,,,,,,,,,,,,"27/Feb/14 19:22;hagleitn;Committed to branch.;;;","23/Apr/14 07:35;leftyl;This adds configuration parameters *hive.merge.tezfiles*, *hive.tez.input.format*, *hive.tez.container.size*, and *hive.tez.java.opts* to HiveConf.java and hive-default.xml.template.  They are documented in the wiki here:  https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-Tez.;;;","12/Aug/14 01:13;apivovarov;Lefy, can you fix hive.hive.merge.tezfiles on wiki?;;;","12/Aug/14 02:39;leftyl;I'm seeing double ... but it's fixed now, thanks [~apivovarov].

* [Configuration Properties -- hive.merge.tezfiles | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.merge.tezfiles];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableDesc.getDeserializer() should use correct classloader when calling Class.forName(),HIVE-6495,12697020,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,24/Feb/14 22:37,12/Mar/14 17:54,14/Jul/23 06:14,12/Mar/14 17:54,,,,,,,,,,0.13.0,,Serializers/Deserializers,,,,0,,,"User is getting an error with the following stack trace below.  It looks like when Class.forName() is called, it may not be using the correct class loader (JavaUtils.getClassLoader() is used in other contexts when the loaded jar may be required).

{noformat}
FAILED: RuntimeException org.apache.hadoop.hive.ql.metadata.HiveException: Failed with exception java.lang.ClassNotFoundException: my.serde.ColonSerdejava.lang.RuntimeException: java.lang.ClassNotFoundException: my.serde.ColonSerde
    at org.apache.hadoop.hive.ql.plan.TableDesc.getDeserializerClass(TableDesc.java:68)
    at org.apache.hadoop.hive.ql.exec.FetchOperator.getRowInspectorFromTable(FetchOperator.java:231)
    at org.apache.hadoop.hive.ql.exec.FetchOperator.getOutputObjectInspector(FetchOperator.java:608)
    at org.apache.hadoop.hive.ql.exec.FetchTask.initialize(FetchTask.java:80)
    at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:497)
    at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:352)
    at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:995)
    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1038)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:931)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:921)
    at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)
    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)
    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)
    at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:790)
    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)
    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:623)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
Caused by: java.lang.ClassNotFoundException: my.serde.ColonSerde
    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:190)
    at org.apache.hadoop.hive.ql.plan.TableDesc.getDeserializerClass(TableDesc.java:66)
    ... 20 more
{noformat}",,jdere,qwertymaniac,wzheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/14 22:52;jdere;HIVE-6495.1.patch;https://issues.apache.org/jira/secure/attachment/12630826/HIVE-6495.1.patch","10/Mar/14 19:56;jdere;HIVE-6495.2.patch;https://issues.apache.org/jira/secure/attachment/12633762/HIVE-6495.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,375495,,,,Wed Mar 12 17:54:31 UTC 2014,,,,,,,,,,"0|i1spfr:",375791,,,,,,,,,,,,,,,,,,,,,"25/Feb/14 18:13;ashutoshc;+1;;;","06/Mar/14 16:09;ashutoshc;[~jdere] Looks like this need to be reuploaded for jenkins to pick it up.;;;","10/Mar/14 19:56;jdere;re-uploading patch to kick off precommit tests;;;","12/Mar/14 17:43;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633762/HIVE-6495.2.patch

{color:green}SUCCESS:{color} +1 5381 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1710/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1710/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633762;;;","12/Mar/14 17:54;ashutoshc;Committed to 0.13 & trunk. Thanks, Jason!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tez 0.3.0 API change,HIVE-6493,12696992,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,24/Feb/14 19:57,24/Feb/14 20:17,14/Jul/23 06:14,24/Feb/14 20:17,tez-branch,,,,,,,,,tez-branch,,,,,,0,,,Minor api change. Affects subclasses of logical input.,,hagleitn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6360,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/14 20:00;hagleitn;HIVE-6493.1.patch;https://issues.apache.org/jira/secure/attachment/12630787/HIVE-6493.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,375467,,,,Mon Feb 24 20:17:27 UTC 2014,,,,,,,,,,"0|i1sp9j:",375763,,,,,,,,,,,,,,,,,,,,,"24/Feb/14 20:17;hagleitn;Committed to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Investigate TestBeeLineWithArgs,HIVE-6488,12696735,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Fixed,jdere,brocknoland,brocknoland,22/Feb/14 16:33,20/Mar/14 17:12,14/Jul/23 06:14,20/Mar/14 16:08,,,,,,,,,,0.13.0,,Tests,,,,0,,,"TestBeeLineWithArgs started taking many, many hours and eventually timing out which is one cause of precommit runs taking a long time. For now I have skipped it in for precommit tests so we should figure out what is going on so we can re-enable the test.",,brocknoland,jdere,szehon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/14 17:53;jdere;HIVE-6488.1.patch;https://issues.apache.org/jira/secure/attachment/12635127/HIVE-6488.1.patch","12/Mar/14 18:15;jdere;HIVE-6488.1.patch;https://issues.apache.org/jira/secure/attachment/12634216/HIVE-6488.1.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,375211,,,,Thu Mar 20 17:12:38 UTC 2014,,,,,,,,,,"0|i1snov:",375508,,,,,,,,,,,,,,,,,,,,,"12/Mar/14 09:50;jdere;Looks like HS2 is continually trying/failing to connect to a zookeeper instance at a default port 2181.  Not sure when this started occurring or whether the root cause of this needs to be fixed.  Creating a zookeeper instance and configuring the tests to use it allows this test to pass in under a minute.

{noformat}
2014-03-12 02:34:25,167 INFO  zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(966)) - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (Unable to locate a login configuration)
2014-03-12 02:34:25,177 WARN  zookeeper.ClientCnxn (ClientCnxn.java:run(1089)) - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:599)
        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068)
2014-03-12 02:34:25,285 WARN  ZooKeeperHiveLockManager (ZooKeeperHiveLockManager.java:setContext(121)) - Unexpected ZK exception when creating parent node /hive_zookeeper_namespace
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hive_zookeeper_namespace
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)
        at org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.setContext(ZooKeeperHiveLockManager.java:117)
        at org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.getLockManager(DummyTxnManager.java:74)
        at org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.acquireLocks(DummyTxnManager.java:103)
        at org.apache.hadoop.hive.ql.Driver.acquireReadWriteLocks(Driver.java:840)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1046)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:884)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:879)
        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:139)
        at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:62)
        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:168)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        at java.lang.Thread.run(Thread.java:680)
{noformat};;;","12/Mar/14 18:15;jdere;Patch v1 fixes the test by using non-zookeeper lock manager, avoiding the need to connect to zookeeper when creating table.;;;","12/Mar/14 18:23;ashutoshc;+1 test is about Beeline, not about ZK or lock management, so ok to test with Embedded LM.;;;","17/Mar/14 17:53;jdere;re-upload patch for precommit tests;;;","20/Mar/14 04:29;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635127/HIVE-6488.1.patch

{color:green}SUCCESS:{color} +1 5413 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1873/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1873/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635127;;;","20/Mar/14 16:08;ashutoshc;Committed to 0.13 & trunk!;;;","20/Mar/14 17:12;szehon;Thanks a lot for the fix, I re-enabled this test in pre-commit and trunk build, hope it works.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PTest2 do not copy failed source directories,HIVE-6487,12696734,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,szehon,brocknoland,brocknoland,22/Feb/14 16:19,13/Nov/14 19:42,14/Jul/23 06:14,06/Mar/14 16:01,,,,,,,,,,0.14.0,,,,,,0,,,"Right now we copy the entire source directory for failed tests back to the master (up to 5). They are 10GB per so it takes a very long time. We should remove this feature.

Remove the cp command from batch-exec.vm:
https://github.com/apache/hive/blob/trunk/testutils/ptest2/src/main/resources/batch-exec.vm#L91
also don't publish the number of failed tests as a template variable:


NO_PRECOMMIT_TESTS",,brocknoland,szehon,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 02:50;szehon;HIVE-6487.2.patch;https://issues.apache.org/jira/secure/attachment/12633019/HIVE-6487.2.patch","06/Mar/14 00:37;szehon;HIVE-6487.patch;https://issues.apache.org/jira/secure/attachment/12632983/HIVE-6487.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,375210,,,,Thu Nov 13 19:42:29 UTC 2014,,,,,,,,,,"0|i1snon:",375507,,,,,,,,,,,,,,,,,,,,,"22/Feb/14 16:20;brocknoland;I put a break fix into place for this but we should make the change and commit this so we don't regress.

[~szehon] mentioned he was potentially interested in this.;;;","06/Mar/14 00:39;szehon;Removed cp task from the batch-exec.vm template, also fixed the unit tests.

Tested manually on my own cluster, and ran existing unit tests.  Let me know if more needs to be done.;;;","06/Mar/14 01:12;brocknoland;Looks great! 

Can we also stop publishing the variables numOfFailedTests and maxSourceDirs to the template?;;;","06/Mar/14 02:50;szehon;Sure, no longer publishing those variables.;;;","06/Mar/14 02:51;szehon;Adding link to RB;;;","06/Mar/14 16:01;brocknoland;Thank you Szehon! I have committed this to trunk!!;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Downgrade to httpclient-4.2.5 in JDBC from httpclient-4.3.2,HIVE-6485,12696696,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vgumashta,vgumashta,vgumashta,22/Feb/14 01:26,17/Mar/14 17:47,14/Jul/23 06:14,17/Mar/14 17:47,0.13.0,,,,,,,,,0.13.0,,JDBC,,,,0,,,"Had upgraded to the new version while adding SSL over Http mode support for HiveServer2. But that conflicts with httpclient-4.2.5 which is in hadoop classpath. I don't have a good reason to use httpclient-4.3.2, so it's better to match hadoop.",,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/14 11:18;vgumashta;HIVE-6485.1.patch;https://issues.apache.org/jira/secure/attachment/12631505/HIVE-6485.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,375172,,,,Mon Mar 17 17:47:56 UTC 2014,,,,,,,,,,"0|i1sng7:",375469,,,,,,,,,,,,,,,,,,,,,"27/Feb/14 11:12;vgumashta;Patch on top of HIVE-6350. ;;;","09/Mar/14 11:06;vgumashta;Changes will get in as part of HIVE-4764.;;;","10/Mar/14 06:41;thejas;Lets mark it as fixed only after HIVE-4764 goes in. (alternatively, maybe mark it as duplicate ).
;;;","17/Mar/14 17:47;vgumashta;Fixed in HIVE-4764.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix NOTICE file: pre release task,HIVE-6482,12696642,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,Fixed,rhbutani,rhbutani,rhbutani,21/Feb/14 21:31,04/Mar/14 18:05,14/Jul/23 06:14,04/Mar/14 18:05,,,,,,,,,,0.13.0,,,,,,0,,,"As per steps in Release doc: https://cwiki.apache.org/confluence/display/Hive/HowToRelease

Removed projects with Apache license as per [~thejas] suggestion.",,cwsteinbach,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/14 21:38;rhbutani;HIVE-6482.1.patch;https://issues.apache.org/jira/secure/attachment/12630387/HIVE-6482.1.patch","27/Feb/14 18:07;rhbutani;HIVE-6482.2.patch;https://issues.apache.org/jira/secure/attachment/12631568/HIVE-6482.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,375118,,,,Mon Mar 03 06:37:52 UTC 2014,,,,,,,,,,"0|i1sn47:",375415,,,,,,,,,,,,,,,,,,,,,"21/Feb/14 22:17;thejas;All these libraries except for jersey are under MIT/BSD license or derivatives. 
Only one that I am not sure of is jsersey library license (CDDL) its too long to read. Might as well put it in the NOTICE section to be safe.

An interesting note : JSON license also adds that ""The Software shall be used for Good, not Evil"" !! :)
;;;","21/Feb/14 22:18;thejas;I mean to say that we can remove all the 3rd party library notices from the NOTICE file except for maybe jersey. If someone can verify that it also does not have such an attribution required, we can remove that from NOTICE file as well.

See http://apache.org/legal/resolved.html#required-third-party-notices
cc [~cwsteinbach]
;;;","27/Feb/14 18:09;rhbutani;made the change based on your feedback.
[~cwsteinbach] do you agree? ;;;","03/Mar/14 02:28;cwsteinbach;[~rhbutani] Looks good to me.;;;","03/Mar/14 06:37;thejas;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metastore server startup script ignores ENV settings,HIVE-6480,12696620,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,farisa,farisa,farisa,21/Feb/14 19:54,13/Nov/14 19:41,14/Jul/23 06:14,12/Apr/14 01:39,0.12.0,,,,,,,,,0.14.0,,HCatalog,,,,0,,,"This is a minor issue with hcat_server.sh.  Currently the startup script has HADOOP_HEAPSIZE hardcoded to 2048, causing administrators to hand edit the script.  As hcat_server.sh reads hcat-env.sh, it makes sense to allow an administrator to define HADOOP_HEAPSIZE in hcat-env.sh (or other location like /etc/profile).

Secondly, there is no defined default for METASTORE_PORT in hcat_server.sh.  If METASTORE_PORT is missing, the metastore server fails to start.

I will attach a patch in my next update, once this jira is opened.

",,erwaman,farisa,leftyl,sushanth,thejas,tthompso,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/14 19:58;farisa;HIVE-6480.01.patch;https://issues.apache.org/jira/secure/attachment/12630362/HIVE-6480.01.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,375096,,,,Thu Nov 13 19:41:26 UTC 2014,,,,,,,,,,"0|i1smzj:",375394,,,,,,,,,,,,,,,,,,,,,"21/Feb/14 19:58;farisa;Attaching 'git diff' output against trunk.;;;","21/Feb/14 20:59;farisa;Reviewboard link https://reviews.apache.org/r/18373/;;;","24/Feb/14 00:36;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12630362/HIVE-6480.01.patch

{color:green}SUCCESS:{color} +1 5176 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1463/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1463/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12630362;;;","12/Apr/14 01:23;sushanth;+1, Looks good to me, will commit.;;;","12/Apr/14 01:29;sushanth;Committed. Thanks for the contribution, Adam!;;;","12/Apr/14 03:09;leftyl;Does this need any user doc?;;;","12/Apr/14 09:17;sushanth;Shouldn't need any userdoc, this simply changes the semantic to a ""set if not already set"" mode rather than a blind set. Most people will not even run this script, since the bigtop installations of hive won't use this script to start the metastore - this is a leftover script from the days hcat was a separate project - it's use now is primarily so devs can use for testing the metastore without too much setup.;;;","12/Apr/14 21:41;leftyl;Good to know.  Thanks, [~sushanth].;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Few .q.out files need to be updated post HIVE-5958,HIVE-6479,12696432,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,21/Feb/14 00:38,21/Feb/14 21:47,14/Jul/23 06:14,21/Feb/14 21:47,,,,,,,,,,0.13.0,,,,,,0,,,See my comment https://issues.apache.org/jira/browse/HIVE-6433?focusedCommentId=13907782&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13907782,,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/14 00:59;ashutoshc;HIVE-6479.patch;https://issues.apache.org/jira/secure/attachment/12630211/HIVE-6479.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,374908,,,,Fri Feb 21 21:47:58 UTC 2014,,,,,,,,,,"0|i1sltz:",375207,,,,,,,,,,,,,,,,,,,,,"21/Feb/14 00:40;ashutoshc;Another failed build https://issues.apache.org/jira/browse/HIVE-6375?focusedCommentId=13907770&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13907770;;;","21/Feb/14 00:59;ashutoshc;Straightfwd patch. Better to commit asap, so not to generate false alarm for future builds.;;;","21/Feb/14 20:55;thejas;+1 . I don't think we need to wait for the full unit test suite to kick in or for 24hours for this one, as it just updates 2 .q files. I will commit it after verifying that these two tests pass with this change.
;;;","21/Feb/14 20:58;thejas;Verified that the 2 tests pass with this .q.out file update . I am planning to commit this in another 1/2 hour. It will get rid of false alarms for the pending precommit tests.
;;;","21/Feb/14 21:47;thejas;Patch committed to trunk.
Thanks for the patch Ashutosh!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC cancel will not work with current HiveServer2,HIVE-6472,12696155,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vgumashta,vgumashta,vgumashta,20/Feb/14 03:01,22/Mar/14 17:29,14/Jul/23 06:14,22/Mar/14 17:29,0.13.0,,,,,,,,,0.13.0,,HiveServer2,JDBC,,,0,,,"Creating this JIRA to add missing pieces for query cancel capability to JDBC. HIVE-5232 should however fix the core issue. Typical use case is when the client calls Statement#execute in one thread to execute a long running query, and Statement#cancel in another thread to cancel the execution before it is complete.",,h_o,jdbcworries,prasadm,rhbutani,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-5232,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/14 11:02;vgumashta;HIVE-6472.1.patch;https://issues.apache.org/jira/secure/attachment/12635527/HIVE-6472.1.patch","20/Mar/14 07:45;vgumashta;HIVE-6472.2.patch;https://issues.apache.org/jira/secure/attachment/12635738/HIVE-6472.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,374632,,,,Sat Mar 22 17:29:52 UTC 2014,,,,,,,,,,"0|i1sk4v:",374932,,,,,,,,,,,,,,,,,,,,,"19/Mar/14 15:01;prasadm;[~vaibhavgumashta] I left some minor comments on the RB;;;","19/Mar/14 18:06;vgumashta;Thanks for the feedback [~prasadm]. I'll upload a new patch with the test changes.;;;","20/Mar/14 07:43;vgumashta;[~prasadm] Addressed the review comments. Added a fair locking for client transport which otherwise might have caused starvation. Thanks!;;;","20/Mar/14 17:45;prasadm;+1 pending test run.;;;","20/Mar/14 20:03;vgumashta;Thanks for the feedback [~prasadm]. Do you think this can be part of 13? This is more of a bug fix since the cancel jdbc api already exists, but will not work if used in current state.

cc [~rhbutani];;;","20/Mar/14 20:33;rhbutani;+1 for 0.13;;;","22/Mar/14 12:24;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635738/HIVE-6472.2.patch

{color:green}SUCCESS:{color} +1 5440 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1902/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1902/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635738;;;","22/Mar/14 17:29;prasadm;Patch committed to trunk and 0.13
Thanks [~vaibhavgumashta]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HS2 & Metastore using SASL out of memory error when curl sends a get request,HIVE-6468,12696116,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,navis,ashahab,ashahab,20/Feb/14 00:06,19/Feb/15 18:21,14/Jul/23 06:14,15/Dec/14 19:41,0.12.0,0.13.0,0.13.1,0.14.0,,,,,,1.0.0,,HiveServer2,Metastore,,,1,TODOC14,,"We see an out of memory error when we run simple beeline calls.
(The hive.server2.transport.mode is binary)

curl localhost:10000

Exception in thread ""pool-2-thread-8"" java.lang.OutOfMemoryError: Java heap space
	at org.apache.thrift.transport.TSaslTransport.receiveSaslMessage(TSaslTransport.java:181)
	at org.apache.thrift.transport.TSaslServerTransport.handleSaslStartMessage(TSaslServerTransport.java:125)
	at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:253)
	at org.apache.thrift.transport.TSaslServerTransport.open(TSaslServerTransport.java:41)
	at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:216)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:189)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)","Centos 6.3, hive 12, hadoop-2.2",ashahab,brocknoland,glenn.strycker,krisden,leftyl,mdominguez@cloudera.com,navis,qwertymaniac,raviprak,thejas,vgumashta,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-8829,,,THRIFT-2660,THRIFT-2678,,,,,,,,,,,,,,,,,,,,"08/Dec/14 08:03;vgumashta;HIVE-6468.0.patch;https://issues.apache.org/jira/secure/attachment/12685699/HIVE-6468.0.patch","06/Mar/14 07:19;navis;HIVE-6468.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12633065/HIVE-6468.1.patch.txt","26/Jun/14 06:03;navis;HIVE-6468.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12652560/HIVE-6468.2.patch.txt","13/Nov/14 22:42;vgumashta;HIVE-6468.3.patch;https://issues.apache.org/jira/secure/attachment/12681419/HIVE-6468.3.patch","14/Nov/14 00:32;vgumashta;HIVE-6468.4.patch;https://issues.apache.org/jira/secure/attachment/12681451/HIVE-6468.4.patch","18/Nov/14 21:52;vgumashta;HIVE-6468.5.patch;https://issues.apache.org/jira/secure/attachment/12682251/HIVE-6468.5.patch","09/Dec/14 13:44;vgumashta;HIVE-6468.branch-0.14.patch;https://issues.apache.org/jira/secure/attachment/12686008/HIVE-6468.branch-0.14.patch","05/Dec/14 19:02;vgumashta;HIVE-6468.branch-0.14.patch;https://issues.apache.org/jira/secure/attachment/12685378/HIVE-6468.branch-0.14.patch",,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,374593,,,,Thu Feb 19 18:21:49 UTC 2015,,,,,,,,,,"0|i1sjw7:",374893,,,,,,,,,,,,,,,,,,,,,"20/Feb/14 01:03;navis;Possibly version mismatch including thrift. Could you check that first?;;;","20/Feb/14 01:09;ashahab;I am suspecting that too. But which component versions must match?

;;;","20/Feb/14 01:16;navis;Thrift related ones. libfb303-*.jar, libthrift-*.jar and hive-service-*.jar. ;;;","20/Feb/14 02:33;ashahab;Hmm, I'm seeing: libfb303-0.9.0.jar,  libthrift-0.9.0.jar,
and hive-service-0.12.0.jar
Are these not correct?



;;;","20/Feb/14 05:50;ashahab;This is how we build hive:
export HADOOP_VERSION=2.2.0
ant clean package tar -Dhadoop.version=${HADOOP_VERSION} -Dhadoop-0.23.version=${HADOOP_VERSION} -Dhadoop.mr.rev=23 -Dmvn.hadoop.profile=hadoop23 -Dhadoop23.version=${HADOOP_VERSION} ;;;","21/Feb/14 22:23;ashahab;We realized the issue was with a curl we were doing to ensure that hiveserver2 is up.
/usr/bin/curl --silent localhost:10000  causes this.;;;","24/Feb/14 02:17;navis;Ah, yes. You should never call  via http or telnet to hiveserver2, which only accepts valid thrift call. It would be good to there is some guard code for that case in thrift server but there is no such thing in current implementation (in thrift) as I know.;;;","24/Feb/14 02:57;leftyl;bq.  You should never call via http or telnet to hiveserver2, which only accepts valid thrift call.

That could be noted in the user docs:

*  [HiveServer2 Clients |https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients]

Perhaps it belongs after the first paragraph of the Beeline section, before the example.
;;;","06/Mar/14 07:15;navis;Made some guard code in hacky way. If you set hive.server2.sasl.message.limit with positive value, any SASL message bigger than than would not be parsed (and closes underlying transport).

{noformat}
navis@navis-book:~/apache/oss-hive(HIVE-6468)$ curl localhost:10000
curl: (52) Empty reply from server
navis@navis-book:~/apache/oss-hive(HIVE-6468)$ telnet localhost 10000
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
asd
Connection closed by foreign host.
{noformat};;;","06/Mar/14 18:44;xuefuz;I agree that guard against this is good. Just curious,  however, why a http get request would put HS2 in OOM? It's understandable that HS2 doesn't understand the request, but how it runs out of memory seems interesting.;;;","07/Mar/14 01:00;navis;The format of sasl message in SaslTransport is something like this : status(1)+length(4)+payload(n). There is no magic header, no CRC, no validity check in it. It just creates byte[] after reading int value from offset 1, resulting OOM. Even when I've coded first client/server program 20 years ago, I didn't make things like this.;;;","17/Mar/14 05:31;leftyl;I added this sentence to the second paragraph of the wiki's Beeline section:

{quote}
In remote mode HiveServer2 only accepts valid Thrift calls; you cannot call it via http or telnet (HIVE-6468).
{quote}

Please review and correct if necessary.

* [Beeline -- New Command Line Shell |https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-Beeline–NewCommandLineShell];;;","18/Mar/14 11:32;navis;[~leftylev] ""cannot"" -> ""must not"" would be better. You can call it but that will make hiveserver die.;;;","18/Mar/14 22:53;leftyl;Thanks, I put it in a warning box with this wording:  ""In remote mode HiveServer2 only accepts valid Thrift calls – do not attempt to call it via http or telnet (HIVE-6468).""

Should it also explain that HS2 will die, or is that just until this jira's patch gets added?  Readers can click the link to this jira if they want to know the reason for the warning, but we could make it explicit if you think that's better.

By the way, *hive.server2.sasl.message.limit* needs some user doc.  It can go in a HiveConf.java comment for now, or in a release note, until we know when HIVE-6037 will get committed.

Quick ref:

* [new warning in Beeline – New Command Line Shell |https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-Beeline–NewCommandLineShell]
* [page history:  new changes |https://cwiki.apache.org/confluence/pages/diffpages.action?pageId=30758725&originalId=40505296];;;","03/Apr/14 01:34;vgumashta;[~navis] HiveServer2 can run in http mode (I just updated the wiki [Setting+up+HiveServer2|https://cwiki.apache.org/confluence/display/Hive/Setting+up+HiveServer2]), where basically the thrift RPC calls happen over http. In the http mode, there is an embedded jetty instance listening on the port which HiveServer2 was started on. 

[~leftylev] I will modify the warning message to indicate that even in http mode, the message body is thrift payloads. 
Thanks!;;;","06/Apr/14 21:20;leftyl;Good doc, [~vaibhavgumashta].  I did some light editing (mostly capitalization) and fixed a typo.  Were the empty links on ""jdbc:hive2://"" intentional?  I removed them, but I'll put them back if you wanted them there for emphasis.  Your use of color on a default value is nice -- we might want to use that in other wikidocs.;;;","17/Apr/14 07:07;vgumashta;Thanks a lot for the edits & corrections [~leftylev]! The doc looks good.;;;","19/Apr/14 21:46;leftyl;Just a reminder:

bq.  By the way, *hive.server2.sasl.message.limit* needs some user doc.  It can go in a HiveConf.java comment for now, or in a release note, until we know when HIVE-6037 will get committed.;;;","25/Jun/14 15:24;raviprak;This is a pretty egregious bug! Any reviews for the patch Navis posted? Can we please check in some guard? ;;;","25/Jun/14 15:47;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633065/HIVE-6468.1.patch.txt

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/590/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/590/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-590/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]
+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-590/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
++ egrep -v '^X|^Performing status on external'
++ awk '{print $2}'
++ svn status --no-ignore
+ rm -rf
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1605453.

At revision 1605453.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633065;;;","26/Jun/14 06:03;navis;Good to know that someone is interested in this. Rebased to trunk.;;;","26/Jun/14 15:01;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12652560/HIVE-6468.2.patch.txt

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5654 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats_empty_partition
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/600/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/600/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-600/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12652560;;;","30/Jun/14 17:38;raviprak;Hi Navis! Thanks a lot for your patch! I'm glad at least someone is cognizant of the severity of this problem. Do you think we should increase the priority for this JIRA?

The patch looks good to me overall except for a few nits. FWIW, here they are:
HiveAuthFactory:
1. Could we just rewrite authTypeStr.equalsIgnoreCase(AuthTypes.KERBEROS.name()) -> authType == AuthTypes.KERBEROS
2. You're catching Exception. Any reason you changed this from IOException? IMHO IOException would have been preferable
3. This block is unnecessary {code}     } else {
      saslServer = null;
    } {code}
4. getAuthTransFactory() is throwing Exception now. Why did you have to change that from LoginException?

PlainSaslHelper:
1. public String mechanism; is never used
2. I'm not sure what the consequences of using WeakReferences and WeakHashMap will be. Could you please comment on that?
3. Can we also put a timeout on {code} underlyingTransport.readAll(messageHeader, 0, messageHeader.length); {code}  if there isn't one already?
;;;","01/Jul/14 04:31;navis;1. ok, sure.
2. just wanted not to miss any runtime exceptions. Not much differences in action.
3. saslServer is final field. So must be assigned to null.
4. Thought wrapping AuthenticationException into LoginException is not that useful.

For PlainSaslHelper, most of codes are copied as-is from TSaslServerTransport.Factory. So, I believe there's a reason for weak references. 
For socket timeout, there are other issues for it (HIVE-6679, etc.). ;;;","02/Jul/14 21:07;raviprak;Thanks Navis!
2. IMHO We should not catch RuntimeExceptions. There's a danger that we might end up covering the real exception (the OOM). Otherwise what's the point of having an exception hierarchy?
3. Good point. Thanks
4. Same as 2. getAuthTransFactory() could throw all the necessary exceptions
{code}public TTransportFactory getAuthTransFactory() throws TTransportException, AuthenticationException, LoginException {code}

I'm afraid I don't know enough about writing Thrift servers to review that code. 
Thanks for the pointer. I'm happy to add the timeout there.;;;","03/Jul/14 08:16;leftyl;Thanks for the description of *hive.server2.sasl.message.limit* in hive-default.xml.template.  (If you're doing another patch, it would be good to capitalize SASL in the description.);;;","13/Aug/14 03:02;qwertymaniac;It would also help add a check for the payload length detected as < 0 (i.e. negative number) aside of an upper message cap.

It also seems Thrift is doing this incorrectly so we should rather fix it there and consume it in, than duplicate its code at our end. I found this JIRA after I'd filed THRIFT-2660.;;;","26/Aug/14 16:11;brocknoland;Since THRIFT-2660 has been committed, it appears we can resolve this issue by upgrading thrift when released?;;;","11/Nov/14 20:38;leftyl;bq.  Since THRIFT-2660 has been committed, it appears we can resolve this issue by upgrading thrift when released?

Is this issue resolved?  (So *hive.server2.sasl.message.limit* won't ever be added to HiveConf.java?);;;","11/Nov/14 23:16;qwertymaniac;The issue is not resolved. For the issue to be resolved, either Hive should update its thrift dependency to 0.9.2 (or higher), to pull in the THFIT-2660 fix, or Hive should RTC the alternative patch created here.

If the former is done, the config will not be introduced, nor needed.;;;","12/Nov/14 09:26;leftyl;Okay, thanks.  I'll keep it on my ""pending"" list for docs.;;;","13/Nov/14 19:58;vgumashta;If there is a 0.14.1, I think we should get this in. For 0.15.0, bumping up the thrift version would work. However, this patch doesn't take care of having a max payload length for Kerberos auth mode. In addition, even Metastore running SASL (does that when it uses Kerberos) will go OOM if it gets non-thrift packets. 

[~navis] [~thejas] [~qwertymaniac] What do you guys think? I can put up a patch on top of #2 patch uploaded here which takes care of the remaining cases as well.;;;","13/Nov/14 22:44;vgumashta;Patch attached is based on 0.14.0 geared towards 0.14.1. Added fix for Metastore SASL and HiveServer2 Kerberos SASL in addition to the Plain SASL fix that the previous patch had.

cc [~thejas] [~navis];;;","14/Nov/14 05:44;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12681451/HIVE-6468.4.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1780/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1780/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1780/

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
Starting Generation of: TestCompareCliDriver
Include Files: null
Excluded Files: null
Query Files: 
Query Files Regex: 
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestCompareCliDriver.java from template TestCompareCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/ql/src/test/templates
Starting Generation of: TestMinimrCliDriver
Include Files: auto_sortmerge_join_16.q,bucket4.q,bucket5.q,bucket6.q,bucket_num_reducers.q,bucket_num_reducers2.q,bucketizedhiveinputformat.q,bucketmapjoin6.q,bucketmapjoin7.q,disable_merge_for_bucketing.q,empty_dir_in_table.q,external_table_with_space_in_location_path.q,file_with_header_footer.q,groupby2.q,import_exported_table.q,index_bitmap3.q,index_bitmap_auto.q,infer_bucket_sort_bucketed_table.q,infer_bucket_sort_dyn_part.q,infer_bucket_sort_map_operators.q,infer_bucket_sort_merge.q,infer_bucket_sort_num_buckets.q,infer_bucket_sort_reducers_power_two.q,input16_cc.q,join1.q,leftsemijoin_mr.q,list_bucket_dml_10.q,load_fs2.q,load_hdfs_file_with_space_in_the_name.q,optrstat_groupby.q,parallel_orderby.q,ql_rewrite_gbtoidx.q,quotedid_smb.q,reduce_deduplicate.q,remote_script.q,root_dir_external_table.q,schemeAuthority.q,schemeAuthority2.q,scriptfile1.q,scriptfile1_win.q,smb_mapjoin_8.q,stats_counter.q,stats_counter_partitioned.q,temp_table_external.q,truncate_column_buckets.q,uber_reduce.q,udf_using.q
Excluded Files: null
Query Files: 
Query Files Regex: 
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestMinimrCliDriver.java from template TestCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/ql/src/test/templates
Starting Generation of: TestMiniTezCliDriver
Include Files: bucket_map_join_tez1.q,bucket_map_join_tez2.q,dynamic_partition_pruning.q,dynamic_partition_pruning_2.q,mapjoin_decimal.q,mrr.q,tez_bmj_schema_evolution.q,tez_dml.q,tez_fsstat.q,tez_insert_overwrite_local_directory_1.q,tez_join_hash.q,tez_join_tests.q,tez_joins_explain.q,tez_schema_evolution.q,tez_union.q,tez_union_decimal.q,tez_union_group_by.q,tez_smb_main.q,tez_smb_1.q,vectorized_dynamic_partition_pruning.q,alter_merge_2_orc.q,alter_merge_orc.q,alter_merge_stats_orc.q,auto_join0.q,auto_join1.q,bucket2.q,bucket3.q,bucket4.q,cbo_gby.q,cbo_gby_empty.q,cbo_join.q,cbo_limit.q,cbo_semijoin.q,cbo_simple_select.q,cbo_stats.q,cbo_subq_exists.q,cbo_subq_in.q,cbo_subq_not_in.q,cbo_udf_udaf.q,cbo_union.q,cbo_views.q,cbo_windowing.q,correlationoptimizer1.q,count.q,create_merge_compressed.q,cross_join.q,cross_product_check_1.q,cross_product_check_2.q,ctas.q,custom_input_output_format.q,delete_all_non_partitioned.q,delete_all_partitioned.q,delete_orig_table.q,delete_tmp_table.q,delete_where_no_match.q,delete_where_non_partitioned.q,delete_where_partitioned.q,delete_whole_partition.q,disable_merge_for_bucketing.q,dynpart_sort_opt_vectorization.q,dynpart_sort_optimization.q,dynpart_sort_optimization2.q,enforce_order.q,filter_join_breaktask.q,filter_join_breaktask2.q,groupby1.q,groupby2.q,groupby3.q,having.q,insert1.q,insert_into1.q,insert_into2.q,insert_orig_table.q,insert_values_dynamic_partitioned.q,insert_values_non_partitioned.q,insert_values_orig_table.qinsert_values_partitioned.q,insert_values_tmp_table.q,insert_update_delete.q,join0.q,join1.q,join_nullsafe.q,leftsemijoin.q,limit_pushdown.q,load_dyn_part1.q,load_dyn_part2.q,load_dyn_part3.q,mapjoin_mapjoin.q,mapreduce1.q,mapreduce2.q,merge1.q,merge2.q,metadataonly1.q,metadata_only_queries.q,optimize_nullscan.q,orc_analyze.q,orc_merge1.q,orc_merge2.q,orc_merge3.q,orc_merge4.q,orc_merge5.q,orc_merge6.q,orc_merge7.q,orc_merge_incompat1.q,orc_merge_incompat2.q,orc_vectorization_ppd.q,parallel.q,ptf.q,sample1.q,script_env_var1.q,script_env_var2.q,script_pipe.q,scriptfile1.q,select_dummy_source.q,skewjoin.q,stats_counter.q,stats_counter_partitioned.q,stats_noscan_1.q,subquery_exists.q,subquery_in.q,temp_table.q,transform1.q,transform2.q,transform_ppr1.q,transform_ppr2.q,union2.q,union3.q,union4.q,union5.q,union6.q,union7.q,union8.q,union9.q,update_after_multiple_inserts.q,update_all_non_partitioned.q,update_all_partitioned.q,update_all_types.q,update_orig_table.q,update_tmp_table.q,update_where_no_match.q,update_where_non_partitioned.q,update_where_partitioned.q,update_two_cols.q,vector_between_in.q,vector_bucket.q,vector_cast_constant.q,vector_char_4.q,vector_char_simple.q,vector_coalesce.q,vector_count_distinct.q,vector_data_types.q,vector_decimal_1.q,vector_decimal_10_0.q,vector_decimal_2.q,vector_decimal_3.q,vector_decimal_4.q,vector_decimal_5.q,vector_decimal_6.q,vector_decimal_aggregate.q,vector_decimal_cast.q,vector_decimal_expressions.q,vector_decimal_mapjoin.q,vector_decimal_math_funcs.q,vector_decimal_precision.q,vector_decimal_trailing.q,vector_decimal_udf.q,vector_decimal_udf2.q,vector_distinct_2.q,vector_elt.q,vector_groupby_3.q,vector_groupby_reduce.q,vector_left_outer_join.q,vector_mapjoin_reduce.q,vector_non_string_partition.q,vector_orderby_5.q,vector_partitioned_date_time.q,vector_reduce_groupby_decimal.q,vector_string_concat.q,vector_varchar_4.q,vector_varchar_simple.q,vectorization_0.q,vectorization_1.q,vectorization_10.q,vectorization_11.q,vectorization_12.q,vectorization_13.q,vectorization_14.q,vectorization_15.q,vectorization_16.q,vectorization_2.q,vectorization_3.q,vectorization_4.q,vectorization_5.q,vectorization_6.q,vectorization_7.q,vectorization_8.q,vectorization_9.q,vectorization_decimal_date.q,vectorization_div0.q,vectorization_limit.q,vectorization_nested_udf.q,vectorization_not.q,vectorization_part.q,vectorization_part_project.q,vectorization_pushdown.q,vectorization_short_regress.q,vectorized_bucketmapjoin1.q,vectorized_case.q,vectorized_casts.q,vectorized_context.q,vectorized_date_funcs.q,vectorized_distinct_gby.q,vectorized_mapjoin.q,vectorized_math_funcs.q,vectorized_nested_mapjoin.q,vectorized_parquet.q,vectorized_ptf.q,vectorized_rcfile_columnar.q,vectorized_shufflejoin.q,vectorized_string_funcs.q,vectorized_timestamp_funcs.q,auto_sortmerge_join_1.q,auto_sortmerge_join_10.q,auto_sortmerge_join_11.q,auto_sortmerge_join_12.q,auto_sortmerge_join_13.q,auto_sortmerge_join_14.q,auto_sortmerge_join_15.q,auto_sortmerge_join_16.q,auto_sortmerge_join_2.q,auto_sortmerge_join_3.q,auto_sortmerge_join_4.q,auto_sortmerge_join_5.q,auto_sortmerge_join_7.q,auto_sortmerge_join_8.q,auto_sortmerge_join_9.q
Excluded Files: null
Query Files: 
Query Files Regex: 
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestMiniTezCliDriver.java from template TestCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/ql/src/test/templates
Starting Generation of: TestNegativeMinimrCliDriver
Include Files: cluster_tasklog_retrieval.q,file_with_header_footer_negative.q,local_mapred_error_cache.q,mapreduce_stack_trace.q,mapreduce_stack_trace_hadoop20.q,mapreduce_stack_trace_turnoff.q,mapreduce_stack_trace_turnoff_hadoop20.q,minimr_broken_pipe.q,udf_local_resource.q
Excluded Files: null
Query Files: 
Query Files Regex: 
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestNegativeMinimrCliDriver.java from template TestNegativeCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/test/templates
Starting Generation of: TestHBaseCliDriver
Include Files: null
Excluded Files: null
Query Files: 
Query Files Regex: null
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestHBaseCliDriver.java from template TestHBaseCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/test/templates
Starting Generation of: TestHBaseMinimrCliDriver
Include Files: null
Excluded Files: null
Query Files: hbase_bulk.m
Query Files Regex: null
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestHBaseMinimrCliDriver.java from template TestHBaseCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/test/templates
Starting Generation of: TestHBaseNegativeCliDriver
Include Files: null
Excluded Files: null
Query Files: 
Query Files Regex: null
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestHBaseNegativeCliDriver.java from template TestHBaseNegativeCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/accumulo-handler/src/test/templates
Starting Generation of: TestAccumuloCliDriver
Include Files: null
Excluded Files: null
Query Files: 
Query Files Regex: null
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestAccumuloCliDriver.java from template TestAccumuloCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/ql/src/test/templates
Starting Generation of: TestContribCliDriver
Include Files: null
Excluded Files: null
Query Files: 
Query Files Regex: 
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestContribCliDriver.java from template TestCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/ql/src/test/templates
Starting Generation of: TestContribNegativeCliDriver
Include Files: null
Excluded Files: null
Query Files: 
Query Files Regex: 
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestContribNegativeCliDriver.java from template TestNegativeCliDriver.vm
[INFO] Executed tasks
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-test-source (add-test-sources) @ hive-it-qfile ---
[INFO] Test Source directory: /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java added.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-it-qfile ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-it-qfile ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/tmp/conf
     [copy] Copying 8 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-it-qfile ---
[INFO] Compiling 15 source files to /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-it-qfile ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-it-qfile ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/hive-it-qfile-0.15.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-it-qfile ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-it-qfile ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/hive-it-qfile-0.15.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-it-qfile/0.15.0-SNAPSHOT/hive-it-qfile-0.15.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-it-qfile/0.15.0-SNAPSHOT/hive-it-qfile-0.15.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - Unit Tests - Hadoop 2 0.15.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-unit-hadoop2 ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2 (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-it-unit-hadoop2 ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-it-unit-hadoop2 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-it-unit-hadoop2 ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-it-unit-hadoop2 ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-it-unit-hadoop2 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-it-unit-hadoop2 ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/tmp/conf
     [copy] Copying 8 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-metastore-scripts) @ hive-it-unit-hadoop2 ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/tmp/scripts/metastore
     [copy] Copying 190 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/tmp/scripts/metastore
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-it-unit-hadoop2 ---
[INFO] Compiling 5 source files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/test-classes
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java uses or overrides a deprecated API.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java: Recompile with -Xlint:deprecation for details.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/ql/security/TestPasswordWithCredentialProvider.java: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/ql/security/TestPasswordWithCredentialProvider.java uses unchecked or unsafe operations.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/ql/security/TestPasswordWithCredentialProvider.java: Recompile with -Xlint:unchecked for details.
[INFO] 4 warnings 
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java:[82,7] method does not override or implement a method from a supertype
[INFO] 1 error
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive Integration - Parent ......................... SUCCESS [7.095s]
[INFO] Hive Integration - Custom Serde ................... SUCCESS [14.309s]
[INFO] Hive Integration - HCatalog Unit Tests ............ SUCCESS [17.487s]
[INFO] Hive Integration - Testing Utilities .............. SUCCESS [15.790s]
[INFO] Hive Integration - Unit Tests ..................... SUCCESS [12.896s]
[INFO] Hive Integration - Test Serde ..................... SUCCESS [1.267s]
[INFO] Hive Integration - QFile Tests .................... SUCCESS [9.204s]
[INFO] Hive Integration - Unit Tests - Hadoop 2 .......... FAILURE [4.064s]
[INFO] Hive Integration - Unit Tests with miniKdc ........ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1:25.967s
[INFO] Finished at: Fri Nov 14 00:43:23 EST 2014
[INFO] Final Memory: 67M/214M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hive-it-unit-hadoop2: Compilation failure
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java:[82,7] method does not override or implement a method from a supertype
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-it-unit-hadoop2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12681451 - PreCommit-HIVE-TRUNK-Build;;;","18/Nov/14 21:52;vgumashta;Revised patch for 14.1. I'll upload one based on trunk just for precommit run (we're upgrading thrift version for trunk - not planning to use this patch).;;;","05/Dec/14 18:09;vgumashta;Patch .0 based on trunk just for getting a precommit run. On trunk this issue has been resolved with a thrify version upgrade.

[~thejas] Does the latest patch (.5) look good for 14.1?;;;","05/Dec/14 18:51;thejas;+1  pending tests.
Can you upload the patch file with the branch name to kick off tests against 0.14 branch ? As per - 
https://cwiki.apache.org/confluence/display/Hive/Hive+PreCommit+Patch+Testing
;;;","05/Dec/14 19:03;vgumashta;[~thejas] Done.;;;","05/Dec/14 19:05;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12685369/HIVE-6468.0.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1970/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1970/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1970/

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
Excluded Files: null
Query Files: 
Query Files Regex: 
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestCompareCliDriver.java from template TestCompareCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/ql/src/test/templates
Starting Generation of: TestMinimrCliDriver
Include Files: auto_sortmerge_join_16.q,bucket4.q,bucket5.q,bucket6.q,bucket_num_reducers.q,bucket_num_reducers2.q,bucketizedhiveinputformat.q,bucketmapjoin6.q,bucketmapjoin7.q,disable_merge_for_bucketing.q,empty_dir_in_table.q,external_table_with_space_in_location_path.q,file_with_header_footer.q,groupby2.q,import_exported_table.q,index_bitmap3.q,index_bitmap_auto.q,infer_bucket_sort_bucketed_table.q,infer_bucket_sort_dyn_part.q,infer_bucket_sort_map_operators.q,infer_bucket_sort_merge.q,infer_bucket_sort_num_buckets.q,infer_bucket_sort_reducers_power_two.q,input16_cc.q,join1.q,leftsemijoin_mr.q,list_bucket_dml_10.q,load_fs2.q,load_hdfs_file_with_space_in_the_name.q,optrstat_groupby.q,parallel_orderby.q,ql_rewrite_gbtoidx.q,quotedid_smb.q,reduce_deduplicate.q,remote_script.q,root_dir_external_table.q,schemeAuthority.q,schemeAuthority2.q,scriptfile1.q,scriptfile1_win.q,smb_mapjoin_8.q,stats_counter.q,stats_counter_partitioned.q,temp_table_external.q,truncate_column_buckets.q,uber_reduce.q,udf_using.q
Excluded Files: null
Query Files: 
Query Files Regex: 
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestMinimrCliDriver.java from template TestCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/ql/src/test/templates
Starting Generation of: TestMiniTezCliDriver
Include Files: bucket_map_join_tez1.q,bucket_map_join_tez2.q,dynamic_partition_pruning.q,dynamic_partition_pruning_2.q,mapjoin_decimal.q,lvj_mapjoin.q, mrr.q,tez_bmj_schema_evolution.q,tez_dml.q,tez_fsstat.q,tez_insert_overwrite_local_directory_1.q,tez_join_hash.q,tez_join_tests.q,tez_joins_explain.q,tez_schema_evolution.q,tez_union.q,tez_union_decimal.q,tez_union_group_by.q,tez_smb_main.q,tez_smb_1.q,vectorized_dynamic_partition_pruning.q,alter_merge_2_orc.q,alter_merge_orc.q,alter_merge_stats_orc.q,auto_join0.q,auto_join1.q,bucket2.q,bucket3.q,bucket4.q,cbo_gby.q,cbo_gby_empty.q,cbo_join.q,cbo_limit.q,cbo_semijoin.q,cbo_simple_select.q,cbo_stats.q,cbo_subq_exists.q,cbo_subq_in.q,cbo_subq_not_in.q,cbo_udf_udaf.q,cbo_union.q,cbo_views.q,cbo_windowing.q,correlationoptimizer1.q,count.q,create_merge_compressed.q,cross_join.q,cross_product_check_1.q,cross_product_check_2.q,ctas.q,custom_input_output_format.q,delete_all_non_partitioned.q,delete_all_partitioned.q,delete_orig_table.q,delete_tmp_table.q,delete_where_no_match.q,delete_where_non_partitioned.q,delete_where_partitioned.q,delete_whole_partition.q,disable_merge_for_bucketing.q,dynpart_sort_opt_vectorization.q,dynpart_sort_optimization.q,dynpart_sort_optimization2.q,enforce_order.q,filter_join_breaktask.q,filter_join_breaktask2.q,groupby1.q,groupby2.q,groupby3.q,having.q,insert1.q,insert_into1.q,insert_into2.q,insert_orig_table.q,insert_values_dynamic_partitioned.q,insert_values_non_partitioned.q,insert_values_orig_table.qinsert_values_partitioned.q,insert_values_tmp_table.q,insert_update_delete.q,join0.q,join1.q,join_nullsafe.q,leftsemijoin.q,limit_pushdown.q,load_dyn_part1.q,load_dyn_part2.q,load_dyn_part3.q,mapjoin_mapjoin.q,mapreduce1.q,mapreduce2.q,merge1.q,merge2.q,metadataonly1.q,metadata_only_queries.q,optimize_nullscan.q,orc_analyze.q,orc_merge1.q,orc_merge2.q,orc_merge3.q,orc_merge4.q,orc_merge5.q,orc_merge6.q,orc_merge7.q,orc_merge_incompat1.q,orc_merge_incompat2.q,orc_vectorization_ppd.q,parallel.q,ptf.q,sample1.q,script_env_var1.q,script_env_var2.q,script_pipe.q,scriptfile1.q,select_dummy_source.q,skewjoin.q,stats_counter.q,stats_counter_partitioned.q,stats_noscan_1.q,subquery_exists.q,subquery_in.q,temp_table.q,transform1.q,transform2.q,transform_ppr1.q,transform_ppr2.q,union2.q,union3.q,union4.q,union5.q,union6.q,union7.q,union8.q,union9.q,update_after_multiple_inserts.q,update_all_non_partitioned.q,update_all_partitioned.q,update_all_types.q,update_orig_table.q,update_tmp_table.q,update_where_no_match.q,update_where_non_partitioned.q,update_where_partitioned.q,update_two_cols.q,vector_between_in.q,vector_bucket.q,vector_cast_constant.q,vector_char_4.q,vector_char_simple.q,vector_coalesce.q,vector_count_distinct.q,vector_data_types.q,vector_decimal_1.q,vector_decimal_10_0.q,vector_decimal_2.q,vector_decimal_3.q,vector_decimal_4.q,vector_decimal_5.q,vector_decimal_6.q,vector_decimal_aggregate.q,vector_decimal_cast.q,vector_decimal_expressions.q,vector_decimal_mapjoin.q,vector_decimal_math_funcs.q,vector_decimal_precision.q,vector_decimal_trailing.q,vector_decimal_udf.q,vector_decimal_udf2.q,vector_distinct_2.q,vector_elt.q,vector_groupby_3.q,vector_groupby_reduce.q,vector_left_outer_join.q,vector_mapjoin_reduce.q,vector_non_string_partition.q,vector_orderby_5.q,vector_partitioned_date_time.q,vector_reduce_groupby_decimal.q,vector_string_concat.q,vector_varchar_4.q,vector_varchar_simple.q,vectorization_0.q,vectorization_1.q,vectorization_10.q,vectorization_11.q,vectorization_12.q,vectorization_13.q,vectorization_14.q,vectorization_15.q,vectorization_16.q,vectorization_2.q,vectorization_3.q,vectorization_4.q,vectorization_5.q,vectorization_6.q,vectorization_7.q,vectorization_8.q,vectorization_9.q,vectorization_decimal_date.q,vectorization_div0.q,vectorization_limit.q,vectorization_nested_udf.q,vectorization_not.q,vectorization_part.q,vectorization_part_project.q,vectorization_pushdown.q,vectorization_short_regress.q,vectorized_bucketmapjoin1.q,vectorized_case.q,vectorized_casts.q,vectorized_context.q,vectorized_date_funcs.q,vectorized_distinct_gby.q,vectorized_mapjoin.q,vectorized_math_funcs.q,vectorized_nested_mapjoin.q,vectorized_parquet.q,vectorized_ptf.q,vectorized_rcfile_columnar.q,vectorized_shufflejoin.q,vectorized_string_funcs.q,vectorized_timestamp_funcs.q,auto_sortmerge_join_1.q,auto_sortmerge_join_10.q,auto_sortmerge_join_11.q,auto_sortmerge_join_12.q,auto_sortmerge_join_13.q,auto_sortmerge_join_14.q,auto_sortmerge_join_15.q,auto_sortmerge_join_16.q,auto_sortmerge_join_2.q,auto_sortmerge_join_3.q,auto_sortmerge_join_4.q,auto_sortmerge_join_5.q,auto_sortmerge_join_7.q,auto_sortmerge_join_8.q,auto_sortmerge_join_9.q
Excluded Files: null
Query Files: 
Query Files Regex: 
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestMiniTezCliDriver.java from template TestCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/ql/src/test/templates
Starting Generation of: TestNegativeMinimrCliDriver
Include Files: cluster_tasklog_retrieval.q,file_with_header_footer_negative.q,local_mapred_error_cache.q,mapreduce_stack_trace.q,mapreduce_stack_trace_hadoop20.q,mapreduce_stack_trace_turnoff.q,mapreduce_stack_trace_turnoff_hadoop20.q,minimr_broken_pipe.q,udf_local_resource.q
Excluded Files: null
Query Files: 
Query Files Regex: 
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestNegativeMinimrCliDriver.java from template TestNegativeCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/test/templates
Starting Generation of: TestHBaseCliDriver
Include Files: null
Excluded Files: null
Query Files: 
Query Files Regex: null
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestHBaseCliDriver.java from template TestHBaseCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/test/templates
Starting Generation of: TestHBaseMinimrCliDriver
Include Files: null
Excluded Files: null
Query Files: hbase_bulk.m
Query Files Regex: null
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestHBaseMinimrCliDriver.java from template TestHBaseCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/test/templates
Starting Generation of: TestHBaseNegativeCliDriver
Include Files: null
Excluded Files: null
Query Files: 
Query Files Regex: null
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestHBaseNegativeCliDriver.java from template TestHBaseNegativeCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/accumulo-handler/src/test/templates
Starting Generation of: TestAccumuloCliDriver
Include Files: null
Excluded Files: null
Query Files: 
Query Files Regex: null
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestAccumuloCliDriver.java from template TestAccumuloCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/ql/src/test/templates
Starting Generation of: TestContribCliDriver
Include Files: null
Excluded Files: null
Query Files: 
Query Files Regex: 
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestContribCliDriver.java from template TestCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/ql/src/test/templates
Starting Generation of: TestContribNegativeCliDriver
Include Files: null
Excluded Files: null
Query Files: 
Query Files Regex: 
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestContribNegativeCliDriver.java from template TestNegativeCliDriver.vm
[INFO] Executed tasks
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-test-source (add-test-sources) @ hive-it-qfile ---
[INFO] Test Source directory: /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java added.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-it-qfile ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-it-qfile ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/tmp/conf
     [copy] Copying 8 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-it-qfile ---
[INFO] Compiling 14 source files to /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-it-qfile ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-it-qfile ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/hive-it-qfile-0.15.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-it-qfile ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-it-qfile ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/hive-it-qfile-0.15.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-it-qfile/0.15.0-SNAPSHOT/hive-it-qfile-0.15.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-it-qfile/0.15.0-SNAPSHOT/hive-it-qfile-0.15.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - Unit Tests - Hadoop 2 0.15.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-unit-hadoop2 ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2 (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-it-unit-hadoop2 ---
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-it-unit-hadoop2 ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-it-unit-hadoop2 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-it-unit-hadoop2 ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-it-unit-hadoop2 ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-it-unit-hadoop2 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-it-unit-hadoop2 ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/tmp/conf
     [copy] Copying 8 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-metastore-scripts) @ hive-it-unit-hadoop2 ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/tmp/scripts/metastore
     [copy] Copying 192 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/tmp/scripts/metastore
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-it-unit-hadoop2 ---
[INFO] Compiling 5 source files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/test-classes
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java uses or overrides a deprecated API.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java: Recompile with -Xlint:deprecation for details.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/ql/security/TestPasswordWithCredentialProvider.java: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/ql/security/TestPasswordWithCredentialProvider.java uses unchecked or unsafe operations.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/ql/security/TestPasswordWithCredentialProvider.java: Recompile with -Xlint:unchecked for details.
[INFO] 4 warnings 
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java:[82,7] method does not override or implement a method from a supertype
[INFO] 1 error
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive Integration - Parent ......................... SUCCESS [7.867s]
[INFO] Hive Integration - Custom Serde ................... SUCCESS [12.792s]
[INFO] Hive Integration - HCatalog Unit Tests ............ SUCCESS [15.058s]
[INFO] Hive Integration - Testing Utilities .............. SUCCESS [14.114s]
[INFO] Hive Integration - Unit Tests ..................... SUCCESS [12.109s]
[INFO] Hive Integration - Test Serde ..................... SUCCESS [1.505s]
[INFO] Hive Integration - QFile Tests .................... SUCCESS [8.816s]
[INFO] Hive Integration - Unit Tests - Hadoop 2 .......... FAILURE [2.729s]
[INFO] Hive Integration - Unit Tests with miniKdc ........ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1:19.016s
[INFO] Finished at: Fri Dec 05 14:04:09 EST 2014
[INFO] Final Memory: 70M/184M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hive-it-unit-hadoop2: Compilation failure
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java:[82,7] method does not override or implement a method from a supertype
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-it-unit-hadoop2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12685369 - PreCommit-HIVE-TRUNK-Build;;;","05/Dec/14 19:10;vgumashta;Removing .0 since this is only for 14.1. Thanks for the link Thejas.;;;","05/Dec/14 19:15;brocknoland;FYI - I don't think precommit is setup for the 0.14 branch.;;;","06/Dec/14 12:22;vgumashta;Thanks [~brocknoland]. I'll run it on the trunk then.  ;;;","06/Dec/14 14:27;vgumashta;Patch based on trunk for precommit run.;;;","06/Dec/14 15:09;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12685543/HIVE-6468.0.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1981/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1981/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1981/

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
Excluded Files: null
Query Files: 
Query Files Regex: 
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestCompareCliDriver.java from template TestCompareCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/ql/src/test/templates
Starting Generation of: TestMinimrCliDriver
Include Files: auto_sortmerge_join_16.q,bucket4.q,bucket5.q,bucket6.q,bucket_num_reducers.q,bucket_num_reducers2.q,bucketizedhiveinputformat.q,bucketmapjoin6.q,bucketmapjoin7.q,disable_merge_for_bucketing.q,empty_dir_in_table.q,external_table_with_space_in_location_path.q,file_with_header_footer.q,groupby2.q,import_exported_table.q,index_bitmap3.q,index_bitmap_auto.q,infer_bucket_sort_bucketed_table.q,infer_bucket_sort_dyn_part.q,infer_bucket_sort_map_operators.q,infer_bucket_sort_merge.q,infer_bucket_sort_num_buckets.q,infer_bucket_sort_reducers_power_two.q,input16_cc.q,join1.q,leftsemijoin_mr.q,list_bucket_dml_10.q,load_fs2.q,load_hdfs_file_with_space_in_the_name.q,optrstat_groupby.q,parallel_orderby.q,ql_rewrite_gbtoidx.q,quotedid_smb.q,reduce_deduplicate.q,remote_script.q,root_dir_external_table.q,schemeAuthority.q,schemeAuthority2.q,scriptfile1.q,scriptfile1_win.q,smb_mapjoin_8.q,stats_counter.q,stats_counter_partitioned.q,temp_table_external.q,truncate_column_buckets.q,uber_reduce.q,udf_using.q
Excluded Files: null
Query Files: 
Query Files Regex: 
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestMinimrCliDriver.java from template TestCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/ql/src/test/templates
Starting Generation of: TestMiniTezCliDriver
Include Files: bucket_map_join_tez1.q,bucket_map_join_tez2.q,dynamic_partition_pruning.q,dynamic_partition_pruning_2.q,mapjoin_decimal.q,lvj_mapjoin.q, mrr.q,tez_bmj_schema_evolution.q,tez_dml.q,tez_fsstat.q,tez_insert_overwrite_local_directory_1.q,tez_join_hash.q,tez_join_tests.q,tez_joins_explain.q,tez_schema_evolution.q,tez_union.q,tez_union_decimal.q,tez_union_group_by.q,tez_smb_main.q,tez_smb_1.q,vectorized_dynamic_partition_pruning.q,alter_merge_2_orc.q,alter_merge_orc.q,alter_merge_stats_orc.q,auto_join0.q,auto_join1.q,bucket2.q,bucket3.q,bucket4.q,cbo_gby.q,cbo_gby_empty.q,cbo_join.q,cbo_limit.q,cbo_semijoin.q,cbo_simple_select.q,cbo_stats.q,cbo_subq_exists.q,cbo_subq_in.q,cbo_subq_not_in.q,cbo_udf_udaf.q,cbo_union.q,cbo_views.q,cbo_windowing.q,correlationoptimizer1.q,count.q,create_merge_compressed.q,cross_join.q,cross_product_check_1.q,cross_product_check_2.q,ctas.q,custom_input_output_format.q,delete_all_non_partitioned.q,delete_all_partitioned.q,delete_orig_table.q,delete_tmp_table.q,delete_where_no_match.q,delete_where_non_partitioned.q,delete_where_partitioned.q,delete_whole_partition.q,disable_merge_for_bucketing.q,dynpart_sort_opt_vectorization.q,dynpart_sort_optimization.q,dynpart_sort_optimization2.q,enforce_order.q,filter_join_breaktask.q,filter_join_breaktask2.q,groupby1.q,groupby2.q,groupby3.q,having.q,insert1.q,insert_into1.q,insert_into2.q,insert_orig_table.q,insert_values_dynamic_partitioned.q,insert_values_non_partitioned.q,insert_values_orig_table.qinsert_values_partitioned.q,insert_values_tmp_table.q,insert_update_delete.q,join0.q,join1.q,join_nullsafe.q,leftsemijoin.q,limit_pushdown.q,load_dyn_part1.q,load_dyn_part2.q,load_dyn_part3.q,mapjoin_mapjoin.q,mapreduce1.q,mapreduce2.q,merge1.q,merge2.q,metadataonly1.q,metadata_only_queries.q,optimize_nullscan.q,orc_analyze.q,orc_merge1.q,orc_merge2.q,orc_merge3.q,orc_merge4.q,orc_merge5.q,orc_merge6.q,orc_merge7.q,orc_merge_incompat1.q,orc_merge_incompat2.q,orc_vectorization_ppd.q,parallel.q,ptf.q,sample1.q,script_env_var1.q,script_env_var2.q,script_pipe.q,scriptfile1.q,select_dummy_source.q,skewjoin.q,stats_counter.q,stats_counter_partitioned.q,stats_noscan_1.q,subquery_exists.q,subquery_in.q,temp_table.q,transform1.q,transform2.q,transform_ppr1.q,transform_ppr2.q,union2.q,union3.q,union4.q,union5.q,union6.q,union7.q,union8.q,union9.q,update_after_multiple_inserts.q,update_all_non_partitioned.q,update_all_partitioned.q,update_all_types.q,update_orig_table.q,update_tmp_table.q,update_where_no_match.q,update_where_non_partitioned.q,update_where_partitioned.q,update_two_cols.q,vector_between_in.q,vector_bucket.q,vector_cast_constant.q,vector_char_4.q,vector_char_simple.q,vector_coalesce.q,vector_count_distinct.q,vector_data_types.q,vector_decimal_1.q,vector_decimal_10_0.q,vector_decimal_2.q,vector_decimal_3.q,vector_decimal_4.q,vector_decimal_5.q,vector_decimal_6.q,vector_decimal_aggregate.q,vector_decimal_cast.q,vector_decimal_expressions.q,vector_decimal_mapjoin.q,vector_decimal_math_funcs.q,vector_decimal_precision.q,vector_decimal_trailing.q,vector_decimal_udf.q,vector_decimal_udf2.q,vector_distinct_2.q,vector_elt.q,vector_groupby_3.q,vector_groupby_reduce.q,vector_left_outer_join.q,vector_mapjoin_reduce.q,vector_non_string_partition.q,vector_orderby_5.q,vector_partitioned_date_time.q,vector_reduce_groupby_decimal.q,vector_string_concat.q,vector_varchar_4.q,vector_varchar_simple.q,vectorization_0.q,vectorization_1.q,vectorization_10.q,vectorization_11.q,vectorization_12.q,vectorization_13.q,vectorization_14.q,vectorization_15.q,vectorization_16.q,vectorization_2.q,vectorization_3.q,vectorization_4.q,vectorization_5.q,vectorization_6.q,vectorization_7.q,vectorization_8.q,vectorization_9.q,vectorization_decimal_date.q,vectorization_div0.q,vectorization_limit.q,vectorization_nested_udf.q,vectorization_not.q,vectorization_part.q,vectorization_part_project.q,vectorization_pushdown.q,vectorization_short_regress.q,vectorized_bucketmapjoin1.q,vectorized_case.q,vectorized_casts.q,vectorized_context.q,vectorized_date_funcs.q,vectorized_distinct_gby.q,vectorized_mapjoin.q,vectorized_math_funcs.q,vectorized_nested_mapjoin.q,vectorized_parquet.q,vectorized_ptf.q,vectorized_rcfile_columnar.q,vectorized_shufflejoin.q,vectorized_string_funcs.q,vectorized_timestamp_funcs.q,auto_sortmerge_join_1.q,auto_sortmerge_join_10.q,auto_sortmerge_join_11.q,auto_sortmerge_join_12.q,auto_sortmerge_join_13.q,auto_sortmerge_join_14.q,auto_sortmerge_join_15.q,auto_sortmerge_join_16.q,auto_sortmerge_join_2.q,auto_sortmerge_join_3.q,auto_sortmerge_join_4.q,auto_sortmerge_join_5.q,auto_sortmerge_join_7.q,auto_sortmerge_join_8.q,auto_sortmerge_join_9.q
Excluded Files: null
Query Files: 
Query Files Regex: 
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestMiniTezCliDriver.java from template TestCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/ql/src/test/templates
Starting Generation of: TestNegativeMinimrCliDriver
Include Files: cluster_tasklog_retrieval.q,file_with_header_footer_negative.q,local_mapred_error_cache.q,mapreduce_stack_trace.q,mapreduce_stack_trace_hadoop20.q,mapreduce_stack_trace_turnoff.q,mapreduce_stack_trace_turnoff_hadoop20.q,minimr_broken_pipe.q,udf_local_resource.q
Excluded Files: null
Query Files: 
Query Files Regex: 
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestNegativeMinimrCliDriver.java from template TestNegativeCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/test/templates
Starting Generation of: TestHBaseCliDriver
Include Files: null
Excluded Files: null
Query Files: 
Query Files Regex: null
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestHBaseCliDriver.java from template TestHBaseCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/test/templates
Starting Generation of: TestHBaseMinimrCliDriver
Include Files: null
Excluded Files: null
Query Files: hbase_bulk.m
Query Files Regex: null
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestHBaseMinimrCliDriver.java from template TestHBaseCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/test/templates
Starting Generation of: TestHBaseNegativeCliDriver
Include Files: null
Excluded Files: null
Query Files: 
Query Files Regex: null
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestHBaseNegativeCliDriver.java from template TestHBaseNegativeCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/accumulo-handler/src/test/templates
Starting Generation of: TestAccumuloCliDriver
Include Files: null
Excluded Files: null
Query Files: 
Query Files Regex: null
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestAccumuloCliDriver.java from template TestAccumuloCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/ql/src/test/templates
Starting Generation of: TestContribCliDriver
Include Files: null
Excluded Files: null
Query Files: 
Query Files Regex: 
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestContribCliDriver.java from template TestCliDriver.vm
Template Path:/data/hive-ptest/working/apache-svn-trunk-source/ql/src/test/templates
Starting Generation of: TestContribNegativeCliDriver
Include Files: null
Excluded Files: null
Query Files: 
Query Files Regex: 
Generated /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestContribNegativeCliDriver.java from template TestNegativeCliDriver.vm
[INFO] Executed tasks
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-test-source (add-test-sources) @ hive-it-qfile ---
[INFO] Test Source directory: /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/generated-test-sources/java added.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-it-qfile ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-it-qfile ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/tmp/conf
     [copy] Copying 8 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-it-qfile ---
[INFO] Compiling 14 source files to /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-it-qfile ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-it-qfile ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/hive-it-qfile-0.15.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-it-qfile ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-it-qfile ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/target/hive-it-qfile-0.15.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-it-qfile/0.15.0-SNAPSHOT/hive-it-qfile-0.15.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/qtest/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-it-qfile/0.15.0-SNAPSHOT/hive-it-qfile-0.15.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - Unit Tests - Hadoop 2 0.15.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-unit-hadoop2 ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2 (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-it-unit-hadoop2 ---
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-it-unit-hadoop2 ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-it-unit-hadoop2 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-it-unit-hadoop2 ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-it-unit-hadoop2 ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-it-unit-hadoop2 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-it-unit-hadoop2 ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/tmp/conf
     [copy] Copying 8 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-metastore-scripts) @ hive-it-unit-hadoop2 ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/tmp/scripts/metastore
     [copy] Copying 192 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/tmp/scripts/metastore
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-it-unit-hadoop2 ---
[INFO] Compiling 5 source files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/target/test-classes
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java uses or overrides a deprecated API.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java: Recompile with -Xlint:deprecation for details.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/ql/security/TestPasswordWithCredentialProvider.java: /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/ql/security/TestPasswordWithCredentialProvider.java uses unchecked or unsafe operations.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/ql/security/TestPasswordWithCredentialProvider.java: Recompile with -Xlint:unchecked for details.
[INFO] 4 warnings 
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java:[82,7] method does not override or implement a method from a supertype
[INFO] 1 error
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive Integration - Parent ......................... SUCCESS [7.558s]
[INFO] Hive Integration - Custom Serde ................... SUCCESS [12.972s]
[INFO] Hive Integration - HCatalog Unit Tests ............ SUCCESS [17.416s]
[INFO] Hive Integration - Testing Utilities .............. SUCCESS [15.094s]
[INFO] Hive Integration - Unit Tests ..................... SUCCESS [13.740s]
[INFO] Hive Integration - Test Serde ..................... SUCCESS [1.446s]
[INFO] Hive Integration - QFile Tests .................... SUCCESS [10.098s]
[INFO] Hive Integration - Unit Tests - Hadoop 2 .......... FAILURE [3.625s]
[INFO] Hive Integration - Unit Tests with miniKdc ........ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1:25.846s
[INFO] Finished at: Sat Dec 06 10:08:11 EST 2014
[INFO] Final Memory: 69M/183M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hive-it-unit-hadoop2: Compilation failure
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java:[82,7] method does not override or implement a method from a supertype
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-it-unit-hadoop2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12685543 - PreCommit-HIVE-TRUNK-Build;;;","08/Dec/14 17:10;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12685699/HIVE-6468.0.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6698 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_aggregate
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_optimize_nullscan
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1998/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1998/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1998/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12685699 - PreCommit-HIVE-TRUNK-Build;;;","09/Dec/14 10:50;vgumashta;Test failures are unrelated.;;;","09/Dec/14 13:44;vgumashta;Updated patch for 14. Will commit this shortly to 14.;;;","15/Dec/14 19:41;vgumashta;Committed to 14.1. Thanks for the patch [~navis] and thanks for reviewing [~thejas], [~raviprak].;;;","16/Dec/14 09:29;leftyl;Doc note:  Add *hive.thrift.sasl.message.limit* to the wiki in Configuration Properties.  But which section?  Perhaps it belongs in the Metastore section after *hive.metastore.sasl.enabled* -- most other mentions of SASL occur in the HiveServer2 section.

* [Configuration Properties -- Metastore | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-MetaStore]
** [hive.metastore.sasl.enabled | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.metastore.sasl.enabled]
* [Configuration Properties -- HiveServer2 | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-HiveServer2]
** [hive.server2.authentication | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.server2.authentication]
** [hive.server2.thrift.sasl.qop | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.server2.thrift.sasl.qop]

bq.  ""On trunk this issue has been resolved with a thrift version upgrade.""

Does that mean *hive.thrift.sasl.message.limit* will only exist in the 0.14.1 release?;;;","19/Feb/15 14:44;thejas;Updating release version for jiras resolved in 1.0.0 .
;;;","19/Feb/15 18:21;thejas;This issue has been fixed in Apache Hive 1.0.0. If there is any issue with the fix, please open a new jira to address it.
;;;",,,,,,,,,,
metastore upgrade script 016-HIVE-6386.derby.sql uses char rather than varchar,HIVE-6467,12696104,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,19/Feb/14 23:04,21/Feb/14 15:28,14/Jul/23 06:14,21/Feb/14 15:28,,,,,,,,,,0.13.0,,Metastore,,,,0,,,"Trying to tinker with the metastore upgrade scripts and did the following steps on a brand new Derby DB:

From derby:
{noformat}
run 'hive-schema-0.12.0.derby.sql';
run 'upgrade-0.12.0-to-0.13.0.derby.sql';
{noformat}

From Hive:
{noformat}
show tables;
{noformat}

I then hit the following error below.  It appears that in the metastore DBS table, the row with defaultdb was created with the value ""ROLE      "", with spaces at the end, where it was expecting ""ROLE"".

{noformat}
2014-02-19 14:49:19,824 ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(143)) - java.lang.IllegalArgumentException: No enum const class org.apache.hadoop.hive.metastore.api.PrincipalType.ROLE      
	at java.lang.Enum.valueOf(Enum.java:196)
	at org.apache.hadoop.hive.metastore.api.PrincipalType.valueOf(PrincipalType.java:14)
	at org.apache.hadoop.hive.metastore.ObjectStore.getDatabase(ObjectStore.java:521)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)
	at com.sun.proxy.$Proxy7.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:753)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
	at com.sun.proxy.$Proxy8.get_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:895)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at com.sun.proxy.$Proxy9.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1150)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1139)
	at org.apache.hadoop.hive.ql.exec.DDLTask.showTables(DDLTask.java:2372)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:354)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1566)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1339)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1010)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1000)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:424)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:793)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:687)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:626)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
{noformat}",,jdere,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/14 23:45;jdere;HIVE-6467.1.patch;https://issues.apache.org/jira/secure/attachment/12629932/HIVE-6467.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,374581,,,,Fri Feb 21 15:28:44 UTC 2014,,,,,,,,,,"0|i1sjtj:",374881,,,,,,,,,,,,,,,,,,,,,"19/Feb/14 23:38;jdere;I think this has to do with the Derby metastore upgrade script 016-HIVE-6386.derby.sql - it specifies CHAR type whereas package.jdo specifies that these columns are supposed to be varchar:

{noformat}
ALTER TABLE ""DBS"" ADD ""OWNER_NAME"" CHAR(128);
ALTER TABLE ""DBS"" ADD ""OWNER_TYPE"" CHAR(10);
{noformat}
;;;","20/Feb/14 00:41;ashutoshc;+1 thanks Jason for fixing this one.;;;","21/Feb/14 14:36;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12629932/HIVE-6467.1.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5141 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_parallel_orderby
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into6
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1433/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1433/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12629932;;;","21/Feb/14 15:28;ashutoshc;Committed to trunk. Thanks, Jason!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test configuration: reduce the duration for which lock attempts are retried,HIVE-6464,12695998,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,thejas,thejas,thejas,19/Feb/14 16:07,21/Feb/14 21:02,14/Jul/23 06:14,21/Feb/14 21:01,,,,,,,,,,0.13.0,,,,,,0,,,"Lock attempts are being done for 60 seconds * 100 before it gives up. Most tests attempt to disable locking but sometimes don't do it correctly and changes can cause the locking to kick in. Locking fails, (at least in the HS2 related tests) because of problems in creating the zookeeper entries in test mode. When locking attempt kicks in and that fails, it can end up waiting for 6000 seconds before failing.

As the tests are not trying to test parallel locking, there is no reason to wait this long in the tests. 
We should update hive-site.xml used by tests for smaller duration.",,navis,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/14 16:07;thejas;HIVE-6464.1.patch;https://issues.apache.org/jira/secure/attachment/12629799/HIVE-6464.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,374476,,,,Fri Feb 21 21:01:57 UTC 2014,,,,,,,,,,"0|i1sj67:",374776,,,,,,,,,,,,,,,,,,,,,"20/Feb/14 01:08;navis;+1;;;","21/Feb/14 17:47;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12629799/HIVE-6464.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5169 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into6
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1435/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1435/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12629799;;;","21/Feb/14 21:01;thejas;The two test failures are unrelated. See HIVE-6479 .

Patch committed to trunk. Thanks for the review [~navis]!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Run Release Audit tool, fix missing license issues",HIVE-6461,12695841,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,Fixed,rhbutani,rhbutani,rhbutani,19/Feb/14 00:17,21/Feb/14 22:31,14/Jul/23 06:14,21/Feb/14 22:31,,,,,,,,,,0.13.0,,,,,,0,,,run mvn apache-rat:check and add apache license in flagged files.,,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/14 01:08;rhbutani;HIVE-6461.1.patch;https://issues.apache.org/jira/secure/attachment/12629703/HIVE-6461.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,374345,,,,Fri Feb 21 22:31:55 UTC 2014,,,,,,,,,,"0|i1sid3:",374645,,,,,,,,,,,,,,,,,,,,,"19/Feb/14 01:26;thejas;+1;;;","19/Feb/14 14:17;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12629703/HIVE-6461.1.patch

{color:green}SUCCESS:{color} +1 5133 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1409/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1409/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12629703;;;","21/Feb/14 22:31;thejas;Patch committed to trunk.
Thanks Harish!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bucket map joins in hive-tez,HIVE-6447,12695480,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vikram.dixit,vikram.dixit,vikram.dixit,17/Feb/14 10:54,01/Oct/19 22:07,14/Jul/23 06:14,28/Mar/14 06:13,tez-branch,,,,,,,,,0.13.0,0.14.0,Tez,,,,0,,,Support bucket map joins in tez.,,cdrome,gates,hagleitn,leftyl,rhbutani,sseth,thejas,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/14 21:58;vikram.dixit;HIVE-6447.1.patch;https://issues.apache.org/jira/secure/attachment/12633788/HIVE-6447.1.patch","26/Mar/14 02:02;vikram.dixit;HIVE-6447.10.patch;https://issues.apache.org/jira/secure/attachment/12636844/HIVE-6447.10.patch","26/Mar/14 20:52;vikram.dixit;HIVE-6447.11.patch;https://issues.apache.org/jira/secure/attachment/12637003/HIVE-6447.11.patch","26/Mar/14 22:06;vikram.dixit;HIVE-6447.12.patch;https://issues.apache.org/jira/secure/attachment/12637022/HIVE-6447.12.patch","27/Mar/14 23:35;vikram.dixit;HIVE-6447.13.patch;https://issues.apache.org/jira/secure/attachment/12637287/HIVE-6447.13.patch","11/Mar/14 05:08;vikram.dixit;HIVE-6447.2.patch;https://issues.apache.org/jira/secure/attachment/12633855/HIVE-6447.2.patch","13/Mar/14 18:14;vikram.dixit;HIVE-6447.3.patch;https://issues.apache.org/jira/secure/attachment/12634499/HIVE-6447.3.patch","20/Mar/14 07:11;vikram.dixit;HIVE-6447.4.patch;https://issues.apache.org/jira/secure/attachment/12635732/HIVE-6447.4.patch","24/Mar/14 09:49;vikram.dixit;HIVE-6447.5.patch;https://issues.apache.org/jira/secure/attachment/12636317/HIVE-6447.5.patch","25/Mar/14 04:26;vikram.dixit;HIVE-6447.6.patch;https://issues.apache.org/jira/secure/attachment/12636519/HIVE-6447.6.patch","25/Mar/14 11:13;vikram.dixit;HIVE-6447.7.patch;https://issues.apache.org/jira/secure/attachment/12636662/HIVE-6447.7.patch","25/Mar/14 16:56;vikram.dixit;HIVE-6447.8.patch;https://issues.apache.org/jira/secure/attachment/12636729/HIVE-6447.8.patch","25/Mar/14 22:33;vikram.dixit;HIVE-6447.9.patch;https://issues.apache.org/jira/secure/attachment/12636799/HIVE-6447.9.patch","18/Feb/14 08:36;vikram.dixit;HIVE-6447.WIP.patch;https://issues.apache.org/jira/secure/attachment/12629495/HIVE-6447.WIP.patch",,14.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,373988,,,,Thu Nov 13 19:40:21 UTC 2014,,,,,,,,,,"0|i1sg5z:",374288,,,,,,,,,,,,,,,,,,,,,"17/Feb/14 11:02;vikram.dixit;Cannot create a review request at this time for some reason. Will try again and update the jira.;;;","18/Feb/14 08:34;vikram.dixit;Wrong file.;;;","11/Mar/14 05:08;vikram.dixit;Updated golden test file results and minor changes.;;;","12/Mar/14 21:15;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633855/HIVE-6447.2.patch

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 5387 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testBuildDag
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testClose
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testEmptyWork
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hadoop.hive.ql.plan.TestTezWork.testBroadcastConnect
org.apache.hadoop.hive.ql.plan.TestTezWork.testConnect
org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1714/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1714/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633855;;;","13/Mar/14 18:13;vikram.dixit;Address failing tests.;;;","13/Mar/14 18:14;vikram.dixit;Fix failing tests.;;;","15/Mar/14 10:30;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634499/HIVE-6447.3.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1823/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1823/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-1823/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
++ egrep -v '^X|^Performing status on external'
++ awk '{print $2}'
++ svn status --no-ignore
+ rm -rf
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1577820.

At revision 1577820.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634499;;;","20/Mar/14 07:11;vikram.dixit;Address review comments.;;;","23/Mar/14 16:52;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635732/HIVE-6447.4.patch

{color:green}SUCCESS:{color} +1 5440 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1921/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1921/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635732;;;","24/Mar/14 08:09;vikram.dixit;Addressed review comments.;;;","25/Mar/14 04:26;vikram.dixit;Address review comments.;;;","25/Mar/14 07:03;sseth;The tez side of things in the CustomEdge and CustomVertex look good. +1. Left some minor comments.;;;","25/Mar/14 11:13;vikram.dixit;Address Harish and Sid's comments.;;;","25/Mar/14 22:33;vikram.dixit;More tests added and some minor fixes.;;;","25/Mar/14 23:26;rhbutani;lgtm, +1 for the traits propagation and Join conversion pieces.;;;","26/Mar/14 02:02;vikram.dixit;During run-time, there is an error without setting up the edge property correctly in the ""sub-query as big table"" case.;;;","26/Mar/14 06:20;gates;Saw following failures when running tests:
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer1
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer10
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer11
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer14
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer15
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer2
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer3
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer4
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer5
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer6
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer7
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer8
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer9
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_innerjoin
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input20
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input33
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into3
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_limit_pushdown_negative
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce1
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce3
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce4
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce5
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce6
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce7
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce8
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin2
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_gby
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_skew
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt1
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt10
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt11
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt12
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt14
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt15
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt16
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt17
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt19
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt2
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt20
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt3
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt4
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt5
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt6
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt7
2014-03-26 02:40:03 WARN  PTest:205 - org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt8
;;;","26/Mar/14 16:38;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12636844/HIVE-6447.10.patch

{color:red}ERROR:{color} -1 due to 49 failed/errored test(s), 5457 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_innerjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_limit_pushdown_negative
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_gby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt8
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket5
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1965/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1965/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 49 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12636844;;;","26/Mar/14 20:52;vikram.dixit;Fix failing tests.;;;","27/Mar/14 04:09;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637022/HIVE-6447.12.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5491 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_parallel_orderby
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1974/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1974/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637022;;;","27/Mar/14 05:31;vikram.dixit;Failures unrelated. They pass locally.;;;","27/Mar/14 23:35;vikram.dixit;Fix for final stage reducer having no #reducer settings.;;;","28/Mar/14 01:20;rhbutani;+1 for 0.13;;;","28/Mar/14 01:55;thejas;Ran tests in my setup and the tests passed.
;;;","28/Mar/14 06:13;vikram.dixit;Committed to trunk and branch-0.13. Thanks for the reviews [~sseth], [~hagleitn], [~rhbutani]. Thanks [~gates] [~thejas] for the test runs.;;;","29/Mar/14 02:05;leftyl;This adds *hive.convert.join.bucket.mapjoin.tez* to HiveConf.java without a description.  If you write a release note and include the description, I'll put it in the wiki and make sure it's added to the new HiveConf.java & hive-default.xml.template via HIVE-6586 after HIVE-6037 gets committed.;;;","04/Apr/14 08:35;vikram.dixit;Hi Lefty,

This would be the change to the template file:
{code}
<property>
  <name>hive.convert.join.bucket.mapjoin.tez</name>
  <value>false</value>
  <description>Whether joins can be automatically converted to bucket map joins in hive when tez is used as the execution engine.</description>
</property>
{code}

Thanks
Vikram.;;;","11/Apr/14 23:53;cdrome;A method call in ql/src/java/org/apache/hadoop/hive/ql/exec/tez/CustomPartitionVertex.java is mispelled causing a compilation failure when building against Hadoop-2.x.

Line 681 of the patch is:

+    int totalResource = context.getTotalAVailableResource().getMemory();

but should be:

+    int totalResource = context.getTotalAvailableResource().getMemory();;;;","12/Apr/14 00:02;vikram.dixit;Hi Chris,

I completely cleaned my .m2 and built this and it passes. I think the tez folks fixed this spelling issue in 0.4.0-incubating-SNAPSHOT and beyond. Hive is currently dependent on 0.4.0-incubating only. This works with that version of tez.

Thanks
Vikram.;;;","12/Apr/14 00:04;vikram.dixit;Hi Chris,

I completely cleaned my .m2 and built this and it passes. I think the tez
folks fixed this spelling issue in 0.4.0-incubating-SNAPSHOT and beyond.
Hive is currently dependent on 0.4.0-incubating only. This works with that
version of tez.

Thanks
Vikram.






-- 
Nothing better than when appreciated for hard work.
-Mark
;;;","12/Apr/14 00:24;cdrome;Thanks for the clarification. We are using a newer version of Tez, which caused the problem.;;;","23/Apr/14 07:39;leftyl;*hive.convert.join.bucket.mapjoin.tez* is documented in the wiki here:

* [Configuration Properties -- Tez -- hive.convert.join.bucket.mapjoin.tez |https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.convert.join.bucket.mapjoin.tez];;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ability to specify hadoop.bin.path from command line -D,HIVE-6446,12695477,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,rusanu,rusanu,rusanu,17/Feb/14 10:16,13/Nov/14 19:42,14/Jul/23 06:14,06/Mar/14 16:28,,,,,,,,,,0.14.0,,Build Infrastructure,,,,0,,,"the surefire plugin configures hadoop.bin.path as a system property:
{code}
<hadoop.bin.path>${basedir}/${hive.path.to.root}/testutils/hadoop</hadoop.bin.path>
{code}
On Windows testing, this should be: 
{code}
<hadoop.bin.path>${basedir}/${hive.path.to.root}/testutils/hadoop.cmd</hadoop.bin.path>
{code}
Additionally, it would be useful to be able to  specify the Hadoop CLI location from -D mvn command line.",,brocknoland,leftyl,rusanu,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/14 10:20;rusanu;HIVE-6446.1.patch;https://issues.apache.org/jira/secure/attachment/12629358/HIVE-6446.1.patch","17/Feb/14 10:22;rusanu;HIVE-6446.2.patch;https://issues.apache.org/jira/secure/attachment/12629359/HIVE-6446.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,373985,,,,Thu Nov 13 19:42:14 UTC 2014,,,,,,,,,,"0|i1sg5b:",374285,,,,,,,,,,,,,,,,,,,,,"17/Feb/14 10:20;rusanu;Made hadoop.bin.path a POM property, used this in the surefire plugin systemPropertyVariables. This way it has the same default as before, but it can be explicitly overwritten with -D;;;","17/Feb/14 10:22;rusanu;moved the property in the right place in alphabetic order;;;","17/Feb/14 16:01;brocknoland;+1 pending tests;;;","17/Feb/14 23:07;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12629359/HIVE-6446.2.patch

{color:green}SUCCESS:{color} +1 5127 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1366/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1366/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12629359;;;","06/Mar/14 16:28;ashutoshc;Committed to trunk. Thanks, Remus!;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create unique dag id for tez 0.3,HIVE-6444,12695458,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,17/Feb/14 08:03,24/Feb/14 21:52,14/Jul/23 06:14,24/Feb/14 21:52,,,,,,,,,,tez-branch,,,,,,0,,,DagID is abbreviated query right now. Need to switch to real id.,,hagleitn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6360,,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/14 10:06;hagleitn;HIVE-6444.1.patch;https://issues.apache.org/jira/secure/attachment/12629356/HIVE-6444.1.patch","24/Feb/14 21:51;hagleitn;HIVE-6444.2.patch;https://issues.apache.org/jira/secure/attachment/12630808/HIVE-6444.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,373966,,,,Mon Feb 24 21:52:33 UTC 2014,,,,,,,,,,"0|i1sg13:",374266,,,,,,,,,,,,,,,,,,,,,"17/Feb/14 10:06;hagleitn;.1 adds dag id related to query id.;;;","24/Feb/14 21:52;hagleitn;Committed to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
load_dyn_part1 is flaky on Tez because it doesn't have the stage re-arranger,HIVE-6442,12695456,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,17/Feb/14 08:00,27/Feb/14 19:20,14/Jul/23 06:14,27/Feb/14 19:20,,,,,,,,,,tez-branch,,,,,,0,,,Need to use the stage re-arranger on tez as well. That will give predictable order of the stages.,,hagleitn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6360,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/14 23:00;hagleitn;HIVE-6442.1.patch;https://issues.apache.org/jira/secure/attachment/12630829/HIVE-6442.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,373964,,,,Thu Feb 27 19:20:47 UTC 2014,,,,,,,,,,"0|i1sg0n:",374264,,,,,,,,,,,,,,,,,,,,,"27/Feb/14 19:20;hagleitn;Committed to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultHiveAuthorizationProvider should not initialize a new HiveConf,HIVE-6437,12695347,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,Fixed,navis,qwertymaniac,qwertymaniac,15/Feb/14 06:59,22/Oct/16 00:35,14/Jul/23 06:14,31/Jul/14 16:11,0.13.0,,,,,,,,,0.14.0,,Configuration,,,,0,,,"During a HS2 connection, every SessionState got initializes a new DefaultHiveAuthorizationProvider object (on stock configs).

In turn, DefaultHiveAuthorizationProvider carries a {{new HiveConf(…)}} that may prove too expensive, and unnecessary to do, since SessionState itself sends in a fully applied HiveConf to it in the first place.",,ctang,leftyl,navis,qwertymaniac,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-14099,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/14 06:51;navis;HIVE-6437.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12657557/HIVE-6437.1.patch.txt","25/Jul/14 05:31;navis;HIVE-6437.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12657776/HIVE-6437.2.patch.txt","28/Jul/14 05:03;navis;HIVE-6437.3.patch.txt;https://issues.apache.org/jira/secure/attachment/12658073/HIVE-6437.3.patch.txt","28/Jul/14 07:46;navis;HIVE-6437.4.patch.txt;https://issues.apache.org/jira/secure/attachment/12658101/HIVE-6437.4.patch.txt","29/Jul/14 01:12;navis;HIVE-6437.5.patch.txt;https://issues.apache.org/jira/secure/attachment/12658319/HIVE-6437.5.patch.txt","30/Jul/14 02:17;navis;HIVE-6437.6.patch.txt;https://issues.apache.org/jira/secure/attachment/12658572/HIVE-6437.6.patch.txt","31/Jul/14 02:05;navis;HIVE-6437.7.patch.txt;https://issues.apache.org/jira/secure/attachment/12658819/HIVE-6437.7.patch.txt",,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,373855,,,,Sat Oct 22 00:35:48 UTC 2016,,,,,,,,,,"0|i1sfcf:",374155,,,,,,,,,,,,,,,,,,,,,"15/Feb/14 07:08;qwertymaniac;This appears to be done cause {{Hive#get(…)}} expects a HiveConf parameter and not a Configuration one.

I don't see {{Hive#get(…)}} particularly relying on a HiveConf specific method, but since the change is deeper than what I envisioned earlier, I'll leave it to the more knowledgeable folks to decide here.;;;","24/Jul/14 19:31;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12657557/HIVE-6437.1.patch.txt

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5756 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestContribNegativeCliDriver.testNegativeCliDriver_url_hook
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/43/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/43/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-43/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12657557;;;","25/Jul/14 05:31;navis;Changed to reconnect automatically on any meta conf changes, removing test only configuration ""hive.metastore.force.reload.conf"";;;","25/Jul/14 11:14;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12657776/HIVE-6437.2.patch.txt

{color:red}ERROR:{color} -1 due to 103 failed/errored test(s), 5756 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_1_sql_std
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_admin_almighty1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_admin_almighty2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_create_func1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_create_macro1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_insert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_owner_actions_db
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_role_grant1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_role_grant2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_set_show_current_role
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_show_grant
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_view_sqlstd
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lb_fs_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_addjar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_tblproperty
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_custom_key
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_custom_key2
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_handler_bulk
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_pushdown
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_scan_params
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_single_sourced_multi_insert
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats3
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_ppd_key_ranges
org.apache.hadoop.hive.cli.TestHBaseNegativeCliDriver.testCliDriver_generatehfiles_require_family_path
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_join_hash
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union4
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_input16_cc
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_list_bucket_dml_10
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_cannot_create_all_role
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_cannot_create_default_role
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_cannot_create_none_role
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_caseinsensitivity
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_drop_db_cascade
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_drop_db_empty
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_drop_role_no_admin
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_priv_current_role_neg
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_role_cycles1
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_role_cycles2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_role_grant
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_role_grant2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_role_grant_nosuchrole
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_role_grant_otherrole
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_role_grant_otheruser
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_rolehierarchy_privs
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_set_role_neg2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_show_grant_otherrole
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dyn_part_max
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_truncate_column_list_bucketing
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_cluster_tasklog_retrieval
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testMetastoreVersion
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testVersionMatching
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testVersionMisMatch
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testVersionRestriction
org.apache.hadoop.hive.ql.metadata.TestHive.testHiveRefreshOnConfChange
org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testHiveRefreshOnConfChange
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/55/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/55/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-55/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 103 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12657776;;;","28/Jul/14 05:58;leftyl;*hive.metastore.force.reload.conf* was introduced in Hive 0.6.0 (default false) but isn't documented in the wiki.  Even though it's going to be removed, it should probably be documented with version information -- I suggest the Test Properties section, not the Metastore section even though it's a hive.metastore.* property.

* [Configuration Properties -- Test Properties | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-TestProperties]
* [Configuration Properties -- Metastore | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-MetaStore];;;","28/Jul/14 06:09;navis;It was needed only for a test case (url_hook.q), which seemed not useful and should not be used in any other cases. I think it should be removed from wiki rather than make new description.;;;","28/Jul/14 06:33;leftyl;Okay, it never got documented and it will stay undocumented.  Thanks, [~navis].;;;","28/Jul/14 07:41;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12658073/HIVE-6437.3.patch.txt

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/74/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/74/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-74/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]
+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-maven-3.0.5/bin:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-maven-3.0.5/bin:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-TRUNK-Build-74/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'conf/hive-default.xml.template'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeObject.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/Driver.java'
++ awk '{print $2}'
++ egrep -v '^X|^Performing status on external'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit-hadoop2/target itests/hive-minikdc/target itests/hive-unit/target itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/authorization itests/custom-serde/target itests/util/target hcatalog/target hcatalog/core/target hcatalog/streaming/target hcatalog/server-extensions/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target hwi/target common/target common/src/gen contrib/target service/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target
+ svn update
U    .gitignore
D    conf/hive-default.xml.template
U    common/pom.xml

Fetching external item into 'hcatalog/src/test/e2e/harness'
Updated external to revision 1613903.

Updated to revision 1613903.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12658073;;;","28/Jul/14 07:46;navis;fixed conflict on HIVE-7496;;;","28/Jul/14 14:27;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12658101/HIVE-6437.4.patch.txt

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 5770 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample_islocalmode_hook
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testMetastoreVersion
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testVersionMatching
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testVersionMisMatch
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testVersionRestriction
org.apache.hadoop.hive.ql.metadata.TestHive.testHiveRefreshOnConfChange
org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testHiveRefreshOnConfChange
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/77/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/77/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-77/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12658101;;;","29/Jul/14 05:42;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12658319/HIVE-6437.5.patch.txt

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5787 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/87/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/87/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-87/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12658319;;;","29/Jul/14 15:47;ashutoshc;This seems like a useful change. [~thejas] would you like to review this one?;;;","29/Jul/14 18:54;thejas;Added comments to review board.
;;;","30/Jul/14 14:55;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12658572/HIVE-6437.6.patch.txt

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5838 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataPrimitiveTypes
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/103/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/103/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-103/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12658572;;;","30/Jul/14 17:12;thejas;[~navis] The latest patch also has this change in SQLStdHiveAccessController.java to make admin role comparison case sensitive. But role names are not case sensitive in sql std auth mode (also documented in the wiki).
{code}
-        if (!HiveMetaStore.ADMIN.equalsIgnoreCase(role.getRoleName())) {
+        if (!HiveMetaStore.ADMIN.equals(role.getRoleName())) {
{code};;;","30/Jul/14 17:29;thejas;Can you also please update the reviewboard with new patch ?
;;;","31/Jul/14 02:06;navis;[~thejas] Updated the patch, thanks.;;;","31/Jul/14 15:24;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12658819/HIVE-6437.7.patch.txt

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5842 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/120/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/120/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-120/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12658819;;;","31/Jul/14 16:11;thejas;Patch committed to trunk.
Thanks for the contribution Navis!
;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;","22/Oct/16 00:35;leftyl;Doc note:  This adds ""hive.users.in.admin.role"" to the default value of *hive.conf.restricted.list* which was created by HIVE-2935 in release 0.11.0 and changed by HIVE-5953 in 0.13.0.

* [Configuration Properties -- hive.conf.restricted.list | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.conf.restricted.list]

The default value is changed again by HIVE-13853 (2.1.0) and HIVE-14099 (2.2.0).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
abs() should preserve precision/scale of decimal input,HIVE-6421,12694876,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,12/Feb/14 23:24,12/Feb/15 23:40,14/Jul/23 06:14,01/Dec/14 22:55,,,,,,,,,,1.1.0,,UDF,,,,0,,,"{noformat}
hive> describe dec1;
OK
c1                  	decimal(10,2)       	None 

hive> explain select c1, abs(c1) from dec1;
 ...
            Select Operator
              expressions: c1 (type: decimal(10,2)), abs(c1) (type: decimal(38,18))

{noformat}

Given that abs() is a GenericUDF it should be possible for the return type precision/scale to match the input precision/scale.",,jdere,leftyl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/14 01:44;jdere;HIVE-6421.1.txt;https://issues.apache.org/jira/secure/attachment/12628644/HIVE-6421.1.txt","19/Nov/14 23:02;jdere;HIVE-6421.2.patch;https://issues.apache.org/jira/secure/attachment/12682515/HIVE-6421.2.patch","25/Nov/14 00:17;jdere;HIVE-6421.3.patch;https://issues.apache.org/jira/secure/attachment/12683449/HIVE-6421.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,373384,,,,Tue Dec 02 07:25:48 UTC 2014,,,,,,,,,,"0|i1scgv:",373685,,,,,,,,,,,,,,,,,,,,,"19/Nov/14 23:02;jdere;re-upload to run precommit tests;;;","20/Nov/14 09:49;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12682515/HIVE-6421.2.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 6655 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_math_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_udf
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_decimal_math_funcs
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_decimal_udf
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1846/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1846/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1846/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12682515 - PreCommit-HIVE-TRUNK-Build;;;","25/Nov/14 00:17;jdere;Fix test failures;;;","26/Nov/14 06:10;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12683449/HIVE-6421.3.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 6683 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_optimize_nullscan
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1903/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1903/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1903/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12683449 - PreCommit-HIVE-TRUNK-Build;;;","26/Nov/14 18:08;jdere;Failure has been occurring in other precommit runs and does not appear to be related. [~ashutoshc], does this one look ok?;;;","26/Nov/14 19:04;ashutoshc;yup. +1;;;","01/Dec/14 22:55;jdere;Committed to trunk. Thanks for review Ashutosh;;;","02/Dec/14 06:27;leftyl;No doc needed?;;;","02/Dec/14 07:25;jdere;Not really, more of a bug fix;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ParquetInputFormat provides data values that do not match the object inspectors,HIVE-6414,12694778,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jcoffey,rusanu,rusanu,12/Feb/14 16:50,10/Mar/14 18:22,14/Jul/23 06:14,08/Mar/14 00:57,0.13.0,,,,,,,,,0.13.0,,Serializers/Deserializers,,,,0,Parquet,,"While working on HIVE-5998 I noticed that the ParquetRecordReader returns IntWritable for all 'int like' types, in disaccord with the row object inspectors. I though fine, and I worked my way around it. But I see now that the issue trigger failuers in other places, eg. in aggregates:

{noformat}
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""cint"":528534767,""ctinyint"":31,""csmallint"":4963,""cfloat"":31.0,""cdouble"":4963.0,""cstring1"":""cvLH6Eat2yFsyy7p""}
        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:534)
        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)
        ... 8 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to java.lang.Short
        at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:808)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)
        at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)
        at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)
        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:524)
        ... 9 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to java.lang.Short
        at org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaShortObjectInspector.get(JavaShortObjectInspector.java:41)
        at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.compare(ObjectInspectorUtils.java:671)
        at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.compare(ObjectInspectorUtils.java:631)
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin$GenericUDAFMinEvaluator.merge(GenericUDAFMin.java:109)
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin$GenericUDAFMinEvaluator.iterate(GenericUDAFMin.java:96)
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:183)
        at org.apache.hadoop.hive.ql.exec.GroupByOperator.updateAggregations(GroupByOperator.java:641)
        at org.apache.hadoop.hive.ql.exec.GroupByOperator.processHashAggr(GroupByOperator.java:838)
        at org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(GroupByOperator.java:735)
        at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:803)
        ... 15 more
{noformat}

My test is (I'm writing a test .q from HIVE-5998, but the repro does not involve vectorization):

{noformat}
create table if not exists alltypes_parquet (
  cint int,
  ctinyint tinyint,
  csmallint smallint,
  cfloat float,
  cdouble double,
  cstring1 string) stored as parquet;

insert overwrite table alltypes_parquet
  select cint,
    ctinyint,
    csmallint,
    cfloat,
    cdouble,
    cstring1
  from alltypesorc;

explain select * from alltypes_parquet limit 10; select * from alltypes_parquet limit 10;

explain select ctinyint,
  max(cint),
  min(csmallint),
  count(cstring1),
  avg(cfloat),
  stddev_pop(cdouble)
  from alltypes_parquet
  group by ctinyint;
select ctinyint,
  max(cint),
  min(csmallint),
  count(cstring1),
  avg(cfloat),
  stddev_pop(cdouble)
  from alltypes_parquet
  group by ctinyint;
{noformat}",,brocknoland,jcoffey,prasadm,rhbutani,rusanu,szehon,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6477,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/14 14:12;jcoffey;HIVE-6414.2.patch;https://issues.apache.org/jira/secure/attachment/12630955/HIVE-6414.2.patch","07/Mar/14 17:42;jcoffey;HIVE-6414.3.patch;https://issues.apache.org/jira/secure/attachment/12633406/HIVE-6414.3.patch","06/Mar/14 19:09;szehon;HIVE-6414.3.patch;https://issues.apache.org/jira/secure/attachment/12633208/HIVE-6414.3.patch","27/Feb/14 17:15;jcoffey;HIVE-6414.3.patch;https://issues.apache.org/jira/secure/attachment/12631559/HIVE-6414.3.patch","24/Feb/14 13:35;jcoffey;HIVE-6414.patch;https://issues.apache.org/jira/secure/attachment/12630660/HIVE-6414.patch",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,373286,,,,Mon Mar 10 18:22:41 UTC 2014,,,,,,,,,,"0|i1sbv3:",373587,,,,,,,,,,,,,,,,,,,,,"12/Feb/14 16:56;brocknoland;FYI [~jcoffey] [~xuefuz];;;","12/Feb/14 17:04;jcoffey;I'll investigate.;;;","24/Feb/14 13:34;jcoffey;the patch was developed against this commit: b05004a863b09cbe5f4b734c5474092f328f0c41

unit tests and qtests run fine against this commit.

the latest commit (as of today): 1a3608d8b1f8cf41e9ba2fc7e9bacdecf271bb92

Appears to have broken qtests (none will run) and so I can't verify the patch specific qtest.  Unit tests, however, execute without error.;;;","24/Feb/14 13:35;jcoffey;Credit should be given to Remy Pecqueur <r.pecqueur@criteo.com>;;;","24/Feb/14 19:16;szehon;Hi, for aggregator functions we saw the issue as well and I made a fix in HIVE-6477, I guess it is the same fix.

;;;","24/Feb/14 21:07;szehon;Hi Justin, I think this version of the fix looks more complete than mine, let's go with this one if it works.  

But just some comments.  Which version of the branch did you create the branch from?  There are some late changes that added new output to all q.out file in HIVE-5958, and this might need to regenerate based on that.  

Also does the query need a ""sort by"" after ""group by"" to guarantee deterministic result of the q.out file?  Thanks.;;;","24/Feb/14 21:13;szehon;Sorry, I meant ""order by"".;;;","25/Feb/14 13:50;jcoffey;Hi Szehon, I worked off of the trunk on this.  We are applying cleanly to the latest commit and unit tests pass, but our qtest fails after the commit for #HIVE-5958.  qtests for parquet_create.q work just fine though.

We're digging into it.;;;","25/Feb/14 13:51;jcoffey;Oh, and we don't appear to need the order by for deterministic tests, but I have added it and will submit an updated patch with it (once we have gotten to the bottom of these failures).

btw are your qtests passing in #HIVE-6477?;;;","25/Feb/14 14:12;jcoffey;Updated patch with working unit and qtests applicable to trunk commit: 6010e22bd24d5004990c63f0aeb232d75693dd94 (#HIVE-5954);;;","25/Feb/14 19:53;szehon;Hmm, I applied your patch on trunk, and new test (parquet_types) still fails for me with missing output due to HIVE-5958.  Let's see how pre-commit tests go.  Yea my tests pass pre-commit test in HIVE-6477, I had added regeneration of output.

Other than that, +1 (non-binding).  Thanks for doing order-by, from my experience its useful for group by, as each group goes to one reducer, and no guarantee from MR framework that they wont run in parallel.

;;;","26/Feb/14 00:30;xuefuz;Quick comment on the code change:

Hive doesn't throw runtime exception when the data isn't right. In terms of error handling, Hive returns null for data errors, including data-out-of-bound as in this case.;;;","26/Feb/14 00:32;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12630955/HIVE-6414.2.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5196 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_types
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1491/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1491/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12630955;;;","26/Feb/14 08:36;jcoffey;[~xuefuz] ok will recheck qtest and resubmit with nulls and not exceptions.  I wasn't sure what the behavior should be in the case of an overflow.;;;","27/Feb/14 17:15;jcoffey;Update patch based on comments from Xuefu.;;;","04/Mar/14 21:29;szehon;Hi Justin, thanks for taking care of it.  Do you want resubmit the patch for testing for this issue?  There had been an issue where the pre-commit test queue got lost.;;;","04/Mar/14 21:46;xuefuz;+1 to the patch #3.;;;","06/Mar/14 19:09;szehon;Re-submitting the patch, on behalf of Justin, to retrigger the pre-commit test.;;;","06/Mar/14 23:50;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633208/HIVE-6414.3.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5371 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
org.apache.hive.beeline.TestSchemaTool.testSchemaInit
org.apache.hive.beeline.TestSchemaTool.testSchemaUpgrade
org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1641/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1641/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633208;;;","07/Mar/14 17:40;jcoffey;hello, I don't think these are related to the patch, so resubmitting for retesting.;;;","08/Mar/14 00:46;xuefuz;TestMinimrCliDriver.testCliDriver_bucket_num_reducers is flaky. Other failures are appearing in other test runs as well.;;;","08/Mar/14 00:57;xuefuz;Patch committed to trunk. Thanks to Justin for the contribution.;;;","10/Mar/14 18:22;rhbutani;ported to 0.13 branch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileOutputCommitterContainer::commitJob() cancels delegation tokens too early.,HIVE-6409,12694623,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,mithun,mithun,mithun,11/Feb/14 23:52,03/Mar/14 21:53,14/Jul/23 06:14,03/Mar/14 21:53,0.12.0,0.13.0,,,,,,,,0.13.0,,HCatalog,,,,0,,,"When HCatalog's FileOutputCommitterContainer::commitJob() is run, it calls the underlying OutputCommitter and then attempts to register partitions in HCatalog.

If the commit fails (for example, because of HIVE-4996), commitJob() cancels delegation tokens retrieved from HCatalog before the exception is rethrown.

{code}
java.io.IOException: java.lang.reflect.InvocationTargetException
    at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter.commitJob(PigOutputCommitter.java:185)
    at
org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.handleJobCommit(CommitterEventHandler.java:249)
    at
org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.run(CommitterEventHandler.java:212)
    at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:601)
    at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter.commitJob(PigOutputCommitter.java:183)
    ... 5 more
Caused by: org.apache.hcatalog.common.HCatException : 2006 : Error adding
partition to metastore. Cause :
MetaException(message:java.lang.RuntimeException: commitTransaction was called
but openTransactionCalls = 0. This probably indicates that there are unbalanced
calls to openTransaction/commitTransaction)
    at
org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.registerPartitions(FileOutputCommitterContainer.java:712)
{code}

The problem is that this happens before abortJob() has had a chance to run, thus yielding the following error:

{code}
MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: Peer indicated failure: DIGEST-MD5: IO error acquiring password
	at org.apache.thrift.transport.TSaslTransport.receiveSaslMessage(TSaslTransport.java:190)
	at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:266)
	at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)
	at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52)
	at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49)
	at java.security.AccessController.doPrivileged(Native Method)
...
{code}

I'll have a patch out that only cancels delegation tokens if the commitJob() has succeeded.",,mithun,sushanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/14 18:29;mithun;HIVE-6409.patch;https://issues.apache.org/jira/secure/attachment/12632065/HIVE-6409.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,373131,,,,Mon Mar 03 19:00:07 UTC 2014,,,,,,,,,,"0|i1sawn:",373432,,,,,,,,,,,,FileOutputCommitterContainer delegation tokens,,,,,,,,,"12/Feb/14 00:02;mithun;Removed the finally{} block from commitJob(). If commit succeeds, there are no exceptions, and the tokens are cancelled. If commit fails, the cancel is avoided, and abortJob() takes care of cancelling the token.

I've also removed an annoying typo. I would also like to remove the following  catch blocks:
{code}
catch(TException e){} 
{code}

TException isA MetaException, so this is redundant. The error handling is identical in both cases. I'd like to remove this cruft, if it's alright.;;;","28/Feb/14 23:16;sushanth;Hi Mithun, could you please cancel your patch, rename your patch to HIVE-6409.patch, and mark it as patch available again? That helps the pre-commit tests pick up the patch and run it with it.;;;","28/Feb/14 23:18;sushanth;Also, actually, you may have to regenerate this patch real quick off trunk now, since FileOutputCommitterContainer has changed a bit.;;;","28/Feb/14 23:20;sushanth;Also, please do not patch any of the org.apache.hcatalog.* classes - they are deprecated and are being maintained in parity with the state they were in as of hive-0.11. The only reason we'd change them is if they do not build any more due to issues with hadoop or hive/etc. Bugfixes are to go on to org.apache.hive.hcatalog.* only for now.;;;","02/Mar/14 18:29;mithun;I've renamed the patch, removed the changes to the deprecated code and resubmitted.

About not making changes to org.apache.hcatalog: It is really quite dangerous to patch fixes only into org.apache.hive.hcatalog. :/ We'll have to make sure that projects like Pig are using the right version of the packages, for fixes.

Wouldn't it be preferable to remove the package entirely (and visibly break code) than to silently compile and produce something unusable? (Has this been discussed already?);;;","02/Mar/14 20:39;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12632065/HIVE-6409.patch

{color:green}SUCCESS:{color} +1 5201 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1594/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1594/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12632065;;;","03/Mar/14 18:51;sushanth;I'm +1 on this new patch, I'll go ahead and commit it.

As to your other concerns, this is per HIVE-5274, and per discussions that I brought up in the next hive-dev meetup(after that patch) as well.

As to visibly breaking code, we will do that, as of 0.14, which will be trunk in a couple more days as soon as we branch out for 0.13 - we have HIVE-6432 open for just that. The idea is that org.apache.hcatalog.* works predictably in the way it has for all of those that have already been using it and find it usable in the state it is in, whereas to all new users who may be evaluating, they must be moving to org.apache.hive.hcatalog.
;;;","03/Mar/14 19:00;sushanth;Committed (with some minor indenting fixes).

Thanks, Mithun!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
uncorrelated subquery is failing with auto.convert.join=true,HIVE-6403,12694413,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,navis,navis,navis,11/Feb/14 02:33,08/Mar/14 17:24,14/Jul/23 06:14,08/Mar/14 17:24,,,,,,,,,,0.13.0,,Query Processor,,,,0,,,"Fixing HIVE-5690, I've found query in subquery_multiinsert.q is not working with hive.auto.convert.join=true 
{noformat}
set hive.auto.convert.join=true;
hive> explain
    > from src b 
    > INSERT OVERWRITE TABLE src_4 
    >   select * 
    >   where b.key in 
    >    (select a.key 
    >     from src a 
    >     where b.value = a.value and a.key > '9'
    >    ) 
    > INSERT OVERWRITE TABLE src_5 
    >   select *  
    >   where b.key not in  ( select key from src s1 where s1.key > '2') 
    >   order by key 
    > ;
java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at java.util.ArrayList.rangeCheck(ArrayList.java:635)
	at java.util.ArrayList.get(ArrayList.java:411)
	at org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.genMapJoinLocalWork(MapJoinProcessor.java:149)
	at org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.genLocalWorkForMapJoin(MapJoinProcessor.java:256)
	at org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.genMapJoinOpAndLocalWork(MapJoinProcessor.java:248)
	at org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.convertTaskToMapJoinTask(CommonJoinTaskDispatcher.java:191)
	at org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.processCurrentTask(CommonJoinTaskDispatcher.java:481)
	at org.apache.hadoop.hive.ql.optimizer.physical.AbstractJoinTaskDispatcher.dispatch(AbstractJoinTaskDispatcher.java:182)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:194)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:139)
	at org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver.resolve(CommonJoinResolver.java:79)
	at org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.optimize(PhysicalOptimizer.java:100)
	at org.apache.hadoop.hive.ql.parse.MapReduceCompiler.optimizeTaskPlan(MapReduceCompiler.java:290)
	at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:216)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9167)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
	at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:64)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:446)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:346)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1056)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1099)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:992)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:982)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:424)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:793)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:687)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:626)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:160)
org.apache.hadoop.hive.ql.parse.SemanticException: Failed to generate new mapJoin operator by exception : Index: 0, Size: 0
	at org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.genLocalWorkForMapJoin(MapJoinProcessor.java:266)
	at org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.genMapJoinOpAndLocalWork(MapJoinProcessor.java:248)
	at org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.convertTaskToMapJoinTask(CommonJoinTaskDispatcher.java:191)
	at org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.processCurrentTask(CommonJoinTaskDispatcher.java:481)
	at org.apache.hadoop.hive.ql.optimizer.physical.AbstractJoinTaskDispatcher.dispatch(AbstractJoinTaskDispatcher.java:182)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:194)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:139)
	at org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver.resolve(CommonJoinResolver.java:79)
	at org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.optimize(PhysicalOptimizer.java:100)
	at org.apache.hadoop.hive.ql.parse.MapReduceCompiler.optimizeTaskPlan(MapReduceCompiler.java:290)
	at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:216)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9167)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
	at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:64)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:446)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:346)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1056)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1099)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:992)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:982)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:424)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:793)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:687)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:626)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:160)
FAILED: SemanticException Generate Map Join Task Error: Failed to generate new mapJoin operator by exception : Index: 0, Size: 0
{noformat}

",,navis,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/14 06:28;rhbutani;HIVE-6403.1.patch;https://issues.apache.org/jira/secure/attachment/12628444/HIVE-6403.1.patch","14/Feb/14 04:12;rhbutani;HIVE-6403.2.patch;https://issues.apache.org/jira/secure/attachment/12628947/HIVE-6403.2.patch","17/Feb/14 02:25;navis;HIVE-6403.3.patch.txt;https://issues.apache.org/jira/secure/attachment/12629298/HIVE-6403.3.patch.txt","17/Feb/14 04:14;navis;HIVE-6403.4.patch.txt;https://issues.apache.org/jira/secure/attachment/12629301/HIVE-6403.4.patch.txt","18/Feb/14 02:12;navis;HIVE-6403.5.patch.txt;https://issues.apache.org/jira/secure/attachment/12629459/HIVE-6403.5.patch.txt","06/Mar/14 01:47;navis;HIVE-6403.6.patch.txt;https://issues.apache.org/jira/secure/attachment/12633001/HIVE-6403.6.patch.txt","13/Feb/14 04:39;navis;navis.patch;https://issues.apache.org/jira/secure/attachment/12628677/navis.patch","14/Feb/14 05:46;navis;navis2.patch;https://issues.apache.org/jira/secure/attachment/12628955/navis2.patch",,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,372921,,,,Sat Mar 08 08:46:56 UTC 2014,,,,,,,,,,"0|i1s9m7:",373223,"also added to 0.13
thanks Navis",,,,,,,,,,,,,,,,,,,,"11/Feb/14 02:33;navis;I wish I could fix this but it's way out of my ability. Could you check this [~rhbutani] ?;;;","11/Feb/14 07:38;rhbutani;[~navis] I am no expert on the MapJoinProcessor. Following is what I see; I will need to spend more time on this.
Maybe from my comments you can see the issue.

1. The Plan generated at genPlan is:
TS[0] (the scan for b) has 2 child operators: [RS[4], RS[25]] 
These are for the joins for each of the SubQuery expressions:
b.key in 
   (select a.key 
    from src a 
    where b.value = a.value and a.key > '9'
   ) 
and 
b.key not in  ( select key from src s1 where s1.key > '2') 

The plan looks complex because the handling of not in requires the null check. This issue will occur even if the second insert is a 'in' subquery predicate. It will be easier to follow for such an e.g.

2. With set hive.auto.convert.join=false
The second RS gets converted to a FileSink. You can observe this from the explain output. A subsequent Stage reads this intermediate output to perform the processing for the 2nd SubQuery.

3. With set hive.auto.convert.join=true;
When it comes to CommonJoinResolver the TS[0] has children [RS[4], FS[44]] ie the 2nd ReduceSink is converted to a FileSink
The MapJoinProcessor:genMapJoinLocalWork line 145 it is assuming that a TableScanOp can only have 1 child. 
The fix maybe to ignore any FileSink operators that are children of TableScan. Another test to add is a multi insert on 3 tables.

;;;","12/Feb/14 06:29;rhbutani;[~navis] could you please review.;;;","12/Feb/14 07:14;navis;I think CommonJoinTaskDispatcher#getPosition() depends on wrong assumption that top operator has only a child (or resultingly a child like LV cases). We should track up (child to parent) big aliases from bigTableCandidates, opposed to current way.;;;","12/Feb/14 17:22;rhbutani;What I see is that for the Multi Insert with SubQuery case:
- the first child will be a ReduceSink; other child is converted to FileSink by the time it gets to CommonJoinTaskDispatcher.
- so for the Multi Insert SubQuery case checking the first child is still ok.

But yes this check that other children are FileSink, should be done in CommonJoinTaskDispatcher#getPosition

Beyond the above point; I am sorry, I don't follow what else you are proposing. Can you please elaborate.;;;","13/Feb/14 04:39;navis;This is the change what I've meant. Could you check this?;;;","13/Feb/14 20:02;rhbutani;Ok, yes I see what you mean. I incorporated your findAlias changes. and created a new patch.
I think the 'multiInsertBigTableCheck' check needs to be still done, because in case of multi insert we want the 'source' table to be the only big table choice. Do you agree?;;;","13/Feb/14 22:27;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12628832/HIVE-6403.2.patch

{color:red}ERROR:{color} -1 due to 22 failed/errored test(s), 5095 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join27
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join29
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_reorder4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_join_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_mapjoin
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1315/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1315/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 22 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12628832;;;","14/Feb/14 04:12;rhbutani;Resubmitting patch. Ran several of the tests locally, they succeed.
The previous run was aborted.;;;","14/Feb/14 05:50;navis;[~rhbutani] Took some time to understand multiInsertBigTableCheck() and now got it. But seeing auto_join17.q, I've realized that we missed handling map-side union cases. Updated in navis2.patch. Could you check that too?;;;","14/Feb/14 21:18;rhbutani;[~navis] i went through the patch; I think I understand the reason for the change. Couple of questions:
- i don't see a union e.g. Can an e.g. be added that exercises the map-side union case.
- i see several plans changed in .out files to scan src1 in place of src2.  But the check at CommonJoinTaskDispatcher::cannotConvert line 447, should favor the right alias as the big table. So I don't understand why these plans are changed with your patch. For my understanding, can you please explain this change.

Assigning the jira to you; you have made most of the changes, my contribution is very tiny; and I am just becoming familiar with the pieces you have touched in this patch.;;;","17/Feb/14 02:06;navis;[~rhbutani] 
bq. i don't see a union
Ah, it's ""auto_join27.q"", ""not auto_join17.q"". Sorry for that.
bq. should favor the right alias as the big table
Seemed a bug which was not fixed properly in HIVE-5945. I'll check that, too.
bq. my contribution is very tiny
I would have never thought of multiInsertBigTableCheck() will be needed. Most of codes I've suggested are just easier part. I'll make this issue done. Thanks.;;;","17/Feb/14 04:14;navis;Add/fix comments and minor refactorings;;;","17/Feb/14 10:21;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12629301/HIVE-6403.4.patch.txt

{color:red}ERROR:{color} -1 due to 21 failed/errored test(s), 5127 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_without_localtask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables_compact
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_update
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_convert_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join35
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_hook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_parallel_orderby
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1356/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1356/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 21 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12629301;;;","17/Feb/14 16:55;rhbutani;The patch looks good.
I looked at the failures, most of them are assertion failures because of the join reorder fix you put in.
There are a few failures to do with Alter Index Rebuild, like index_bitmap_rc.q.;;;","18/Feb/14 01:25;navis;Hive index system seemed broken badly in somewhere. 
Even in trunk,
{noformat}
hive> explain extended ALTER INDEX srcpart_index ON srcpart REBUILD;

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1
  Stage-2 depends on stages: Stage-0
  null depends on stages: Stage-1
  Stage-3 depends on stages: Stage-1, Stage-1, Stage-1, Stage-1
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1
  Stage-2 depends on stages: Stage-0
  null depends on stages: Stage-1
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1
  Stage-2 depends on stages: Stage-0
  null depends on stages: Stage-1
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1
  Stage-2 depends on stages: Stage-0
  null depends on stages: Stage-1
{noformat}
I've never used index before but it seemed not a valid plan. I'll dig into this for a while.;;;","18/Feb/14 02:01;navis;It was not that bad as it first looked. I've used task ID for equality of task but index builder resets ID (even not assigns ID for index metadata updator). Changed not to reset ID and seemed working good. I'll update the patch shortly after confirming that.
{noformat}
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1
  Stage-2 depends on stages: Stage-0
  Stage-3 depends on stages: Stage-1
  Stage-16 depends on stages: Stage-1, Stage-5, Stage-9, Stage-13
  Stage-5 is a root stage
  Stage-4 depends on stages: Stage-5
  Stage-6 depends on stages: Stage-4
  Stage-7 depends on stages: Stage-5
  Stage-9 is a root stage
  Stage-8 depends on stages: Stage-9
  Stage-10 depends on stages: Stage-8
  Stage-11 depends on stages: Stage-9
  Stage-13 is a root stage
  Stage-12 depends on stages: Stage-13
  Stage-14 depends on stages: Stage-12
  Stage-15 depends on stages: Stage-13
{noformat};;;","18/Feb/14 14:10;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12629459/HIVE-6403.5.patch.txt

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1378/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1378/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
Reverted 'ql/src/test/results/clientpositive/decimal_5.q.out'
Reverted 'ql/src/test/results/clientpositive/bucketmapjoin8.q.out'
Reverted 'ql/src/test/results/clientpositive/progress_1.q.out'
Reverted 'ql/src/test/results/clientpositive/nestedvirtual.q.out'
Reverted 'ql/src/test/results/clientpositive/parallel.q.out'
Reverted 'ql/src/test/results/clientpositive/convert_enum_to_string.q.out'
Reverted 'ql/src/test/results/clientpositive/annotate_stats_filter.q.out'
Reverted 'ql/src/test/results/clientpositive/bucketmapjoin12.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby8_map_skew.q.out'
Reverted 'ql/src/test/results/clientpositive/varchar_1.q.out'
Reverted 'ql/src/test/results/clientpositive/insert1_overwrite_partitions.q.out'
Reverted 'ql/src/test/results/clientpositive/join27.q.out'
Reverted 'ql/src/test/results/clientpositive/load_file_with_space_in_the_name.q.out'
Reverted 'ql/src/test/results/clientpositive/load_dyn_part11.q.out'
Reverted 'ql/src/test/results/clientpositive/input4.q.out'
Reverted 'ql/src/test/results/clientpositive/stats6.q.out'
Reverted 'ql/src/test/results/clientpositive/leadlag_queries.q.out'
Reverted 'ql/src/test/results/clientpositive/rename_partition_location.q.out'
Reverted 'ql/src/test/results/clientpositive/udf_translate.q.out'
Reverted 'ql/src/test/results/clientpositive/union12.q.out'
Reverted 'ql/src/test/results/clientpositive/index_compact_2.q.out'
Reverted 'ql/src/test/results/clientpositive/udf2.q.out'
Reverted 'ql/src/test/results/clientpositive/sort_merge_join_desc_1.q.out'
Reverted 'ql/src/test/results/clientpositive/windowing_udaf.q.out'
Reverted 'ql/src/test/results/clientpositive/smb_mapjoin_17.q.out'
Reverted 'ql/src/test/results/clientpositive/ppd_repeated_alias.q.out'
Reverted 'ql/src/test/results/clientpositive/join36.q.out'
Reverted 'ql/src/test/results/clientpositive/avro_joins.q.out'
Reverted 'ql/src/test/results/clientpositive/drop_with_concurrency.q.out'
Reverted 'ql/src/test/results/clientpositive/lock2.q.out'
Reverted 'ql/src/test/results/clientpositive/input_part5.q.out'
Reverted 'ql/src/test/results/clientpositive/bucketmapjoin3.q.out'
Reverted 'ql/src/test/results/clientpositive/insert_into2.q.out'
Reverted 'ql/src/test/results/clientpositive/alter_skewed_table.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby_grouping_sets2.q.out'
Reverted 'ql/src/test/results/clientpositive/timestamp_1.q.out'
Reverted 'ql/src/test/results/clientpositive/partition_vs_table_metadata.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby12.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby1_limit.q.out'
Reverted 'ql/src/test/results/clientpositive/cast1.q.out'
Reverted 'ql/src/test/results/clientpositive/input16_cc.q.out'
Reverted 'ql/src/test/results/clientpositive/annotate_stats_part.q.out'
Reverted 'ql/src/test/results/clientpositive/partition_wise_fileformat15.q.out'
Reverted 'ql/src/test/results/clientpositive/stats1.q.out'
Reverted 'ql/src/test/results/clientpositive/symlink_text_input_format.q.out'
Reverted 'ql/src/test/results/clientpositive/ptf_register_tblfn.q.out'
Reverted 'ql/src/test/results/clientpositive/ddltime.q.out'
Reverted 'ql/src/test/results/clientpositive/uniquejoin.q.out'
Reverted 'ql/src/test/results/clientpositive/ppd_constant_expr.q.out'
Reverted 'ql/src/test/results/clientpositive/union30.q.out'
Reverted 'ql/src/test/results/clientpositive/timestamp_null.q.out'
Reverted 'ql/src/test/results/clientpositive/combine1.q.out'
Reverted 'ql/src/test/results/clientpositive/avro_sanity_test.q.out'
Reverted 'ql/src/test/results/clientpositive/exim_06_one_part.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby_multi_insert_common_distinct.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby2_noskew.q.out'
Reverted 'ql/src/test/results/clientpositive/index_stale.q.out'
Reverted 'ql/src/test/results/clientpositive/vectorized_timestamp_funcs.q.out'
Reverted 'ql/src/test/results/clientpositive/smb_mapjoin_12.q.out'
Reverted 'ql/src/test/results/clientpositive/metadataonly1.q.out'
Reverted 'ql/src/test/results/clientpositive/join31.q.out'
Reverted 'ql/src/test/results/clientpositive/concatenate_inherit_table_location.q.out'
Reverted 'ql/src/test/results/clientpositive/alter_rename_partition_authorization.q.out'
Reverted 'ql/src/test/results/clientpositive/rcfile_toleratecorruptions.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_join7.q.out'
Reverted 'ql/src/test/results/clientpositive/nonreserved_keywords_insert_into1.q.out'
Reverted 'ql/src/test/results/clientpositive/bucketsortoptimize_insert_7.q.out'
Reverted 'ql/src/test/results/clientpositive/tablename_with_select.q.out'
Reverted 'ql/src/test/results/clientpositive/exim_04_all_part.q.out'
Reverted 'ql/src/test/results/clientpositive/join32_lessSize.q.out'
Reverted 'ql/src/test/results/clientpositive/correlationoptimizer11.q.out'
Reverted 'ql/src/test/results/clientpositive/multiMapJoin1.q.out'
Reverted 'ql/src/test/results/clientpositive/alter3.q.out'
Reverted 'ql/src/test/results/clientpositive/smb_mapjoin_21.q.out'
Reverted 'ql/src/test/results/clientpositive/exim_23_import_part_authsuccess.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby4_map_skew.q.out'
Reverted 'ql/src/test/results/clientpositive/exim_01_nonpart.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_sortmerge_join_7.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby7_noskew.q.out'
Reverted 'ql/src/test/results/clientpositive/test_boolean_whereclause.q.out'
Reverted 'ql/src/test/results/clientpositive/drop_index_removes_partition_dirs.q.out'
Reverted 'ql/src/test/results/clientpositive/partition_wise_fileformat10.q.out'
Reverted 'ql/src/test/results/clientpositive/compute_stats_long.q.out'
Reverted 'ql/src/test/results/clientpositive/inputddl7.q.out'
Reverted 'ql/src/test/results/clientpositive/join5.q.out'
Reverted 'ql/src/test/results/clientpositive/describe_syntax.q.out'
Reverted 'ql/src/test/results/clientpositive/add_part_multiple.q.out'
Reverted 'ql/src/test/results/clientpositive/external_table_with_space_in_location_path.q.out'
Reverted 'ql/src/test/results/clientpositive/fileformat_text.q.out'
Reverted 'ql/src/test/results/clientpositive/char_udf1.q.out'
Reverted 'ql/src/test/results/clientpositive/date_1.q.out'
Reverted 'ql/src/test/results/clientpositive/exim_08_nonpart_rename.q.out'
Reverted 'ql/src/test/results/clientpositive/list_bucket_dml_10.q.out'
Reverted 'ql/src/test/results/clientpositive/windowing.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_join2.q.out'
Reverted 'ql/src/test/results/clientpositive/udf_reverse.q.out'
Reverted 'ql/src/test/results/clientpositive/bucketsortoptimize_insert_2.q.out'
Reverted 'ql/src/test/results/clientpositive/union3.q.out'
Reverted 'ql/src/test/results/clientpositive/show_create_table_partitioned.q.out'
Reverted 'ql/src/test/results/clientpositive/input1_limit.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby8.q.out'
Reverted 'ql/src/test/results/clientpositive/scriptfile1.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby_sort_9.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby_grouping_id1.q.out'
Reverted 'ql/src/test/results/clientpositive/bucketmapjoin_negative2.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_sortmerge_join_2.q.out'
Reverted 'ql/src/test/results/clientpositive/alter_varchar2.q.out'
Reverted 'ql/src/test/results/clientpositive/skewjoinopt6.q.out'
Reverted 'ql/src/test/results/clientpositive/load_dyn_part6.q.out'
Reverted 'ql/src/test/results/clientpositive/inputddl2.q.out'
Reverted 'ql/src/test/results/clientpositive/drop_partitions_filter2.q.out'
Reverted 'ql/src/test/results/clientpositive/sample7.q.out'
Reverted 'ql/src/test/results/clientpositive/rcfile_union.q.out'
Reverted 'ql/src/test/results/clientpositive/nullinput2.q.out'
Reverted 'ql/src/test/results/clientpositive/exim_10_external_managed.q.out'
Reverted 'ql/src/test/results/clientpositive/ptf_general_queries.q.out'
Reverted 'ql/src/test/results/clientpositive/create_escape.q.out'
Reverted 'ql/src/test/results/clientpositive/exim_07_all_part_over_nonoverlap.q.out'
Reverted 'ql/src/test/results/clientpositive/annotate_stats_limit.q.out'
Reverted 'ql/src/test/results/clientpositive/alter_char2.q.out'
Reverted 'ql/src/test/results/clientpositive/input_testsequencefile.q.out'
Reverted 'ql/src/test/results/clientpositive/join_view.q.out'
Reverted 'ql/src/test/results/clientpositive/udf_get_json_object.q.out'
Reverted 'ql/src/test/results/clientpositive/input_testxpath.q.out'
Reverted 'ql/src/test/results/clientpositive/input19.q.out'
Reverted 'ql/src/test/results/clientpositive/bucket_map_join_2.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby3.q.out'
Reverted 'ql/src/test/results/clientpositive/ba_table3.q.out'
Reverted 'ql/src/test/results/clientpositive/join_empty.q.out'
Reverted 'ql/src/test/results/clientpositive/windowing_rank.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby_sort_4.q.out'
Reverted 'ql/src/test/results/clientpositive/parquet_partitioned.q.out'
Reverted 'ql/src/test/results/clientpositive/reduce_deduplicate_exclude_gby.q.out'
Reverted 'ql/src/test/results/clientpositive/skewjoinopt18.q.out'
Reverted 'ql/src/test/results/clientpositive/archive_excludeHadoop20.q.out'
Reverted 'ql/src/test/results/clientpositive/skewjoinopt1.q.out'
Reverted 'ql/src/test/results/clientpositive/load_dyn_part1.q.out'
Reverted 'ql/src/test/results/clientpositive/sample2.q.out'
Reverted 'ql/src/test/results/clientpositive/exim_03_nonpart_over_compat.q.out'
Reverted 'ql/src/test/results/clientpositive/stats16.q.out'
Reverted 'ql/src/test/results/clientpositive/input28.q.out'
Reverted 'ql/src/test/results/clientpositive/smb_mapjoin_4.q.out'
Reverted 'ql/src/test/results/clientpositive/auto_sortmerge_join_14.q.out'
Reverted 'ql/src/test/results/clientpositive/disable_merge_for_bucketing.q.out'
Reverted 'ql/src/test/results/clientpositive/ppd_union_view.q.out'
Reverted 'ql/src/test/results/clientpositive/input_testxpath2.q.out'
Reverted 'ql/src/test/results/clientpositive/create_like_tbl_props.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby7_map_multi_single_reducer.q.out'
Reverted 'ql/src/test/results/clientpositive/partition_wise_fileformat9.q.out'
Reverted 'ql/src/test/results/clientpositive/truncate_column.q.out'
Reverted 'ql/src/test/results/clientpositive/mergejoins.q.out'
Reverted 'ql/src/test/results/clientpositive/groupby_complex_types.q.out'
Reverted 'ql/src/test/results/clientpositive/input14.q.out'
Reverted 'ql/src/test/results/clientpositive/bucketcontext_5.q.out'
Reverted 'ql/src/test/results/clientpositive/input37.q.out'
Reverted 'ql/src/test/results/clientpositive/binary_table_colserde.q.out'
Reverted 'ql/src/test/results/clientpositive/skewjoinopt13.q.out'
Reverted 'ql/src/test/results/clientpositive/describe_comment_nonascii.q.out'
Reverted 'ql/src/test/results/clientpositive/mapreduce4.q.out'
Reverted 'ql/src/test/results/clientpositive/orc_predicate_pushdown.q.out'
Reverted 'ql/src/test/results/clientpositive/nullscript.q.out'
Reverted 'ql/src/test/results/clientpositive/stats11.q.out'
Reverted 'ql/src/test/results/clientpositive/bucket5.q.out'
Reverted 'ql/src/test/results/clientpositive/alter_merge_stats.q.out'
Reverted 'ql/src/test/results/clientpositive/udf_round_2.q.out'
Reverted 'ql/src/test/results/clientpositive/filter_numeric.q.out'
Reverted 'ql/src/test/results/clientpositive/input46.q.out'
Reverted 'ql/src/test/queries/clientnegative/authorization_addpartition.q'
Reverted 'ql/src/test/queries/clientnegative/authorization_droppartition.q'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/hooks/ReadEntity.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/hooks/WriteEntity.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/Operation2Privilege.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/RequiredPrivileges.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidator.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/security/SessionStateConfigUserAuthenticator.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/Driver.java'
++ awk '{print $2}'
++ egrep -v '^X|^Performing status on external'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/storage-handlers/hbase/target hcatalog/server-extensions/target hcatalog/core/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target hwi/target common/target common/src/gen service/target contrib/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target ql/src/test/results/clientnegative/authorization_uri_create_table_ext.q.out ql/src/test/results/clientnegative/authorization_uri_insert.q.out ql/src/test/results/clientnegative/authorization_uri_load_data.q.out ql/src/test/results/clientnegative/authorization_uri_insert_local.q.out ql/src/test/results/clientnegative/authorization_uri_create_table1.q.out ql/src/test/results/clientnegative/authorization_uri_altertab_setloc.q.out ql/src/test/results/clientnegative/authorization_uri_createdb.q.out ql/src/test/results/clientnegative/authorization_uri_index.q.out ql/src/test/results/clientnegative/authorization_uri_add_partition.q.out ql/src/test/results/clientnegative/authorization_uri_alterpart_loc.q.out ql/src/test/queries/clientnegative/authorization_uri_load_data.q ql/src/test/queries/clientnegative/authorization_uri_createdb.q ql/src/test/queries/clientnegative/authorization_uri_alterpart_loc.q ql/src/test/queries/clientnegative/authorization_uri_insert_local.q ql/src/test/queries/clientnegative/authorization_uri_add_partition.q ql/src/test/queries/clientnegative/authorization_uri_altertab_setloc.q ql/src/test/queries/clientnegative/authorization_uri_insert.q ql/src/test/queries/clientnegative/authorization_uri_index.q ql/src/test/queries/clientnegative/authorization_uri_create_table_ext.q ql/src/test/queries/clientnegative/authorization_uri_create_table1.q ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java.orig ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/Operation2Privilege.java.orig
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1569331.

At revision 1569331.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12629459;;;","18/Feb/14 18:59;rhbutani;+1
will checkin once tests pass.;;;","19/Feb/14 01:36;navis;HIVE-6037 is reverted. I've kicked test. ;;;","06/Mar/14 01:47;navis;Kick test;;;","08/Mar/14 08:46;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12633001/HIVE-6403.6.patch.txt

{color:green}SUCCESS:{color} +1 5373 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1655/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1655/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12633001;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Apache Rat plugin to pom.xml,HIVE-6400,12694356,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,rhbutani,rhbutani,rhbutani,10/Feb/14 21:32,14/Feb/14 22:04,14/Jul/23 06:14,14/Feb/14 22:04,,,,,,,,,,0.13.0,,,,,,0,,,So we can generate the Release Audit report. ,,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/14 04:15;rhbutani;HIVE-6400.1.patch;https://issues.apache.org/jira/secure/attachment/12628949/HIVE-6400.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,372865,,,,Fri Feb 14 22:04:21 UTC 2014,,,,,,,,,,"0|i1s9a7:",373169,,,,,,,,,,,,,,,,,,,,,"11/Feb/14 23:22;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12628061/HIVE-6400.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5083 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_revoke_table_priv
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1274/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1274/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12628061;;;","14/Feb/14 16:53;ashutoshc;+1;;;","14/Feb/14 21:03;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12628949/HIVE-6400.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5119 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_quotedid_smb
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1325/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1325/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12628949;;;","14/Feb/14 22:04;ashutoshc;Committed to trunk. Thanks, Harish!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MapRedTask.configureDebugVariablesForChildJVM mixes HIVE_CHILD_CLIENT_DEBUG_OPTS and HIVE_MAIN_CLIENT_DEBUG_OPTS in env check,HIVE-6398,12694217,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,Fixed,rusanu,rusanu,rusanu,10/Feb/14 08:11,12/Feb/14 23:39,14/Jul/23 06:14,12/Feb/14 23:39,,,,,,,,,,0.13.0,,,,,,0,,,"@328:

     assert environmentVariables.containsKey(HIVE_CHILD_CLIENT_DEBUG_OPTS)
          && environmentVariables.get(HIVE_MAIN_CLIENT_DEBUG_OPTS) != null : HIVE_CHILD_CLIENT_DEBUG_OPTS
",,rusanu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/14 08:16;rusanu;HIVE-6398.1.patch;https://issues.apache.org/jira/secure/attachment/12627941/HIVE-6398.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,372726,,,,Wed Feb 12 23:39:36 UTC 2014,,,,,,,,,,"0|i1s8fb:",373030,,,,,,,,,,,,,,,,,,,,,"10/Feb/14 15:37;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12627941/HIVE-6398.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5082 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_revoke_table_priv
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1270/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1270/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12627941;;;","12/Feb/14 18:32;ashutoshc;+1;;;","12/Feb/14 23:39;ashutoshc;Committed to trunk. Thanks, Remus!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
multi-table insert from select transform fails if optimize.ppd enabled,HIVE-6395,12694074,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,szehon,szehon,szehon,08/Feb/14 05:34,22/Mar/14 05:31,14/Jul/23 06:14,22/Mar/14 05:31,0.13.0,,,,,,,,,0.13.0,,Query Processor,,,,0,,,"{noformat}
set hive.optimize.ppd=true;
add file ./test.py;

from (select transform(test.*) using 'python ./test.py'
as id,name,state from test) t0
insert overwrite table test2 select * where state=1
insert overwrite table test3 select * where state=2;
{noformat}

In the above example, the select transform returns an extra column, and that column is used in where clause of the multi-insert selects.  However, if optimize is on, the query plan is wrong:

filter (state=1 and state=2) //impossible
--> select, insert into test1
--> select, insert into test2

The correct query plan for hive.optimize.ppd=false is:
filter (state=1)
--> select, insert into test1
filter (state=2)
--> select, insert into test2

For reference
{noformat}
create table test (id int, name string)
create table test2(id int, name string, state int)
create table test3(id int, name string, state int)
{noformat}",,ctang,prasadm,qwertymaniac,richardatcloudera,szehon,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-4293,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/14 20:49;szehon;HIVE-6395.patch;https://issues.apache.org/jira/secure/attachment/12636105/HIVE-6395.patch","08/Feb/14 05:35;szehon;test.py;https://issues.apache.org/jira/secure/attachment/12627781/test.py",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,372583,,,,Sat Mar 22 05:31:17 UTC 2014,,,,,,,,,,"0|i1s7k7:",372887,,,,,,,,,,,,,,,,,,,,,"21/Mar/14 21:03;szehon;Attaching review board for review.

[~navis] can you also review too , as you had done some fixes for ppd optimizer.;;;","21/Mar/14 21:50;szehon;Actually, I just saw your fix for HIVE-4293, is it a more complete fix for the same situation ?;;;","21/Mar/14 22:08;xuefuz;[~szehon] The patch looks good to me. However, just one thing to consider: your patch would make PPD off in the given case, which has performance implications. However, if we push down ( state == 1 || state == 2 ) in the test case while keeping the filter at the filter, wouldn't that work? However, I'm not sure if this is possible or have covets. What you think?;;;","21/Mar/14 22:14;ashutoshc;I think HIVE-4293 indeed fixes this and is more complete. [~rhbutani] / [~navis] Can you verify ?;;;","22/Mar/14 01:48;szehon;Thanks Xuefu, its not possible unless I'm misunderstanding the question, the entire FilterOp is pushed down, the predicate is just a part of it.  Navis's patch also seems to remove the candidate if its multi-insert case, as its not implemented yet in ppd.  (see OpProcFactory.getChildWalkerInfo).

Although I am curious why this is duplicated in HIVE-4293 which is about subquery + udtf, I wonder if its a part of the main fix, or just an additional fix that got added?;;;","22/Mar/14 02:34;xuefuz;{quote}
Although I am curious why this is duplicated in HIVE-4293 which is about subquery + udtf, I wonder if its a part of the main fix, or just an additional fix that got added?
{quote}

I didn't read the patch in HIVE-4293, but from Hive's perspective, UDTF is very similar to TRANSFORM() except that the former is is done via UDTF's java code, and the later in external script via streaming. For this reason, the problem here might be a sub-problem of HIVE-4293. This is just my guess.;;;","22/Mar/14 05:31;ashutoshc;Tested the test case in patch on trunk which now has HIVE-4293. It now passes. Feel free to reopen if you can still repro in some other form.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive (and HCatalog) don't allow super-users to add partitions to tables.,HIVE-6392,12693855,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,mithun,mithun,mithun,07/Feb/14 10:28,14/Mar/14 21:34,14/Jul/23 06:14,05/Mar/14 03:08,0.12.0,0.13.0,,,,,,,,0.13.0,,Authorization,,,,0,,,"HDFS allows for users to be added to a ""supergroup"" (identified by the ""dfs.permissions.superusergroup"" key in hdfs-site.xml). Users in this group are allowed to modify HDFS contents regardless of the path's ogw permissions.

However, Hive's StorageBasedAuthProvider disallows such a superuser from adding partitions to any table that doesn't explicitly grant write permissions to said superuser. This causes the odd scenario where the superuser writes data to a partition-directory (under the table's path), but can't register the appropriate partition.

I have a patch that brings the Metastore's behaviour in line with what the HDFS allows.",,mithun,rhbutani,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/14 10:39;mithun;HIVE-6392.branch-0.12.patch;https://issues.apache.org/jira/secure/attachment/12627586/HIVE-6392.branch-0.12.patch","04/Mar/14 19:26;mithun;HIVE-6392.patch;https://issues.apache.org/jira/secure/attachment/12632575/HIVE-6392.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,372364,,,,Fri Mar 14 21:34:16 UTC 2014,,,,,,,,,,"0|i1s67r:",372668,,,,,,,,,,,,,,,,,,,,,"07/Feb/14 10:39;mithun;Fixes for trunk and branch-0.12/.;;;","28/Feb/14 11:35;thejas;+1;;;","02/Mar/14 18:32;mithun;Attaching a renamed patch, to put through regressions.;;;","02/Mar/14 22:21;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12632066/HIVE-6392.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5202 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1595/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1595/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12632066;;;","03/Mar/14 23:11;mithun;Hey, Thejas. This failure doesn't look related to the code change. I've resubmitted the patch. I'm hoping for a cleaner run.;;;","04/Mar/14 19:26;mithun;Resubmitting, to avoid transient test failures.;;;","05/Mar/14 01:25;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12632575/HIVE-6392.patch

{color:green}SUCCESS:{color} +1 5244 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1622/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1622/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12632575;;;","05/Mar/14 03:08;thejas;Patch committed to trunk. Thanks for the contribution Mithun!
;;;","14/Mar/14 21:34;rhbutani;patch applied  to 0.13 branch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LazyBinaryColumnarSerDe-based RCFile tables break when looking up elements in null-maps.,HIVE-6389,12693800,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,mithun,mithun,mithun,07/Feb/14 01:33,03/Mar/14 18:06,14/Jul/23 06:14,03/Mar/14 18:06,0.10.0,0.11.0,0.12.0,0.13.0,,,,,,0.13.0,,Serializers/Deserializers,,,,0,,,"RCFile tables that use the LazyBinaryColumnarSerDe don't seem to handle look-ups into map-columns when the value of the column is null.

When an RCFile table is created with LazyBinaryColumnarSerDe (as is default in 0.12), and queried as follows:

{code}
select mymap['1024'] from mytable;
{code}

and if the mymap column has nulls, then one is treated to the following guttural utterance:

{code}
2014-02-05 21:50:25,050 FATAL mr.ExecMapper (ExecMapper.java:map(194)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""id"":null,""mymap"":null,""isnull"":null}
  at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:534)
  at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)
  at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
  at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429)
  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
  at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:235)
  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
  at java.util.concurrent.FutureTask.run(FutureTask.java:262)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to org.apache.hadoop.io.Text
  at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)
  at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:226)
  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486)
  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439)
  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423)
  at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:560)
  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)
  at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)
  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)
  at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)
  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)
  at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:524)
  ... 10 more
{code}

A patch is on the way, but the short of it is that the LazyBinaryMapOI needs to return nulls if either the map or the lookup-key is null.

This is handled correctly for Text data, and for RCFiles using ColumnarSerDe.",,mithun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/14 15:21;mithun;HIVE-6389.patch;https://issues.apache.org/jira/secure/attachment/12632273/HIVE-6389.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,372309,,,,Mon Mar 03 18:06:24 UTC 2014,,,,,,,,,,"0|i1s5vj:",372613,,,,,,,,,,,,,,,,,,,,,"07/Feb/14 01:35;mithun;The fix (for trunk/) is attached.;;;","08/Feb/14 00:07;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12627529/Hive-6389.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5040 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1241/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1241/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12627529;;;","28/Feb/14 15:31;ashutoshc;Mithun, is the stack trace you posted above correct? 
From bug description I would have expected NPE from LazyBinaryColSerde on read side, but stack trace suggests some exception while writing data and that too from LazySimpleSerde. 
Also, looking at code in LazyBinaryMapOI, it doesn't look like that key is dereferenced in a manner which can throw NPE. So, not sure how your null catching of key will fix problem you described.
I think I am struggling to understand both problem as well as fix. If you can add the testcase in your patch, that will help a lot.;;;","03/Mar/14 02:36;mithun;Hey, Ashutosh. As a matter of fact, this stack trace is the result of running ""select mymap[ 'xyz' ] from mytable"", if mytable has null values for mymap. Although the bug is in the LazyBinaryObjectInspector for Maps, it doesn't manifest at the time of read.
The reason you're seeing a fail in LazySimpleSerde is because the results of the query are being serialized into String (i.e. to console).

The LazyBinaryMapOI returns -1 for NULL maps. WHen the LazySimpleSerde attempts to convert this Integer into Text, we get this bad-cast exception. The OI should have been returning nulls for null objects, like the ColumnarSerDe does.

The way I tested this is:
1. create table mytable_text( mymap map<string, string> ) stored as textfile;
2. echo ""\N\n\N\n\N"" > /tmp/mytable.txt && hdfs dfs -copyFromLocal /tmp/mytable.txt /user/hive/warehouse/mytable_text
3. create table mytable_rcfile( mymap map<string, string> ) stored as rcfile; -- LazyBinaryColumnarSerDe
4. insert overwrite table mytable_rcfile select mymap from mytable_text;
5. select mymap['blah'] from mytable_rcfile;

Steps 1-4 is simply to insert a null-map into an RCFile-based table.
Step 5 causes the null-map to be returned by LazyBinaryMapOI as '-1', etc.

This patch brings LazyBinaryMapOI's behaviour in line with LazyMapOI. (This is likely just a copy-paste error, from getMapSize().

;;;","03/Mar/14 03:44;ashutoshc;I see.  Thanks for explanation. Makes sense now.
Problem is map itself is null, I thought you are dealing with a case of null-key in a map and fixing it by if ( .. || key == null) check. Although key == null check is not required, because null will be correctly returned in that case (also evident in LazyMapOI). Fix is to return null instead of -1 for data == null case. Anyhow, fix looks good. 
+1 can you re-upload the patch to get Hive QA to re-run.;;;","03/Mar/14 15:20;mithun;Pulling patch. Will capitalize and resubmit.;;;","03/Mar/14 15:21;mithun;Renamed.;;;","03/Mar/14 17:09;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12632273/HIVE-6389.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5218 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hive.hcatalog.mapreduce.TestHCatMutablePartitioned.testHCatPartitionedTable
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1605/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1605/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12632273;;;","03/Mar/14 18:06;ashutoshc;Committed to trunk. Thanks, Mithun!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Newly added tests in TestJdbcDriver2 from HIVE-4395 is not running,HIVE-6383,12693516,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,prasadm,navis,navis,06/Feb/14 08:25,18/Feb/14 19:25,14/Jul/23 06:14,18/Feb/14 19:25,,,,,,,,,,0.13.0,,JDBC,,,,0,,,"Newly added tests are not marked with @Test annotation and seemed not running. In my try after adding the annotation, testFetchFirstQuery is failed. [~prasadm] Could you check this?",,navis,prasadm,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/14 20:09;prasadm;HIVE-6383.1.patch;https://issues.apache.org/jira/secure/attachment/12628318/HIVE-6383.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,372101,,,,Tue Feb 18 19:25:33 UTC 2014,,,,,,,,,,"0|i1s4lz:",372406,,,,,,,,,,,,,,,,,,,,,"06/Feb/14 16:04;prasadm;[~navis] Thanks for pointing that out. I will take a look.;;;","11/Feb/14 20:10;prasadm;Added test annotations and a minor update to the test. Verified that the fetchFirst test passes with the patch.;;;","12/Feb/14 21:55;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12628318/HIVE-6383.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5093 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_revoke_table_priv
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority2
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1294/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1294/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12628318;;;","13/Feb/14 19:05;thejas;+1;;;","18/Feb/14 19:23;prasadm;Thanks Thejas!
The failed tests seems to be unrelated. The patch only changes TestJdbcDriver2.java test and not not touched any other code.;;;","18/Feb/14 19:25;prasadm;Patch committed to trunk. 
Thanks Thejas for the review!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PATCHED_BLOB encoding in ORC will corrupt data in some cases,HIVE-6382,12693495,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,prasanth_j,prasanth_j,prasanth_j,06/Feb/14 04:19,31/Jul/14 18:44,14/Jul/23 06:14,21/Feb/14 02:48,0.13.0,,,,,,,,,0.13.0,,Serializers/Deserializers,,,,0,orcfile,,"In PATCHED_BLOB encoding (added in HIVE-4123), gapVsPatchList is an array of long that stores gap (g) between the values that are patched and the patch value (p). The maximum distance of gap can be 511 that require 8 bits to encode. And patch values can take more than 56 bits. When patch values take more than 56 bits, p + g will become > 64 bits which cannot be packed to a long. This will result in data corruption under the case where patch values are > 56 bits. 

Stack trace will look like:
{code}
Caused by: java.lang.ArrayIndexOutOfBoundsException: 3
at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.preparePatchedBlob(RunLengthIntegerWriterV2.java:593)
at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.determineEncoding(RunLengthIntegerWriterV2.java:541)
at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.write(RunLengthIntegerWriterV2.java:746)
at org.apache.hadoop.hive.ql.io.orc.WriterImpl$IntegerTreeWriter.write(WriterImpl.java:744)
at org.apache.hadoop.hive.ql.io.orc.WriterImpl$StructTreeWriter.write(WriterImpl.java:1320)
at org.apache.hadoop.hive.ql.io.orc.WriterImpl.addRow(WriterImpl.java:1849)
at org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat$OrcRecordWriter.write(OrcOutputFormat.java:75)
at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:638)
at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:501)
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:842)
at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:88)
at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:501)
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:842)
at org.apache.hadoop.hive.ql.exec.ExtractOperator.processOp(ExtractOperator.java:45)
at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:501)
at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:249)
... 7 more
{code}",,hagleitn,leftyl,prasanth_j,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6347,,,,,,,,,,,,,,HIVE-6369,HIVE-5970,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/14 04:22;prasanth_j;HIVE-6382.1.patch;https://issues.apache.org/jira/secure/attachment/12627290/HIVE-6382.1.patch","06/Feb/14 20:44;prasanth_j;HIVE-6382.2.patch;https://issues.apache.org/jira/secure/attachment/12627461/HIVE-6382.2.patch","19/Feb/14 20:52;prasanth_j;HIVE-6382.3.patch;https://issues.apache.org/jira/secure/attachment/12629886/HIVE-6382.3.patch","19/Feb/14 23:55;prasanth_j;HIVE-6382.4.patch;https://issues.apache.org/jira/secure/attachment/12629934/HIVE-6382.4.patch","20/Feb/14 00:29;prasanth_j;HIVE-6382.5.patch;https://issues.apache.org/jira/secure/attachment/12629940/HIVE-6382.5.patch","20/Feb/14 01:45;prasanth_j;HIVE-6382.6.patch;https://issues.apache.org/jira/secure/attachment/12629962/HIVE-6382.6.patch",,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,372080,,,,Wed Jul 02 07:50:50 UTC 2014,,,,,,,,,,"0|i1s4hb:",372385,,,,,,,,,,,,,,,,,,,,,"06/Feb/14 04:22;prasanth_j;Initial version of patch.;;;","06/Feb/14 04:24;prasanth_j;HIVE-6347 adds hive configuration to ORC reader interface. It is required for this patch to determine whether to skip corrupt data or throw exception.;;;","06/Feb/14 04:25;prasanth_j;Still have to verify if this patch fixes HIVE-6369.;;;","06/Feb/14 04:27;prasanth_j;Making it patch available for HIVE QA to run precommit tests.;;;","06/Feb/14 17:51;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12627290/HIVE-6382.1.patch

{color:green}SUCCESS:{color} +1 5039 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1220/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1220/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12627290;;;","06/Feb/14 20:44;prasanth_j;In this patch, added a safeguard length of 1 to gap and patch list array to avoid off-by-one errors or throwing exception.;;;","06/Feb/14 23:56;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12627461/HIVE-6382.2.patch

{color:green}SUCCESS:{color} +1 5039 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1226/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1226/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12627461;;;","19/Feb/14 20:52;prasanth_j;Added configuration to orc readers to support skipping of corrupted data.;;;","19/Feb/14 23:55;prasanth_j;Addressed [~sershe]'s review comments.;;;","20/Feb/14 00:24;sershe;wrt 64 bits that's what I meant by relying on internals of that func... maybe it can check and throw assertion error if it is >56 but not 64... anyway that's nit.
Otherwise +1;;;","20/Feb/14 00:29;prasanth_j;Gotcha! Fixed it in this patch.;;;","20/Feb/14 01:45;prasanth_j;Earlier patch was not compiling clean as conf object of some orc reader interface was missing. Fixed it in this patch.;;;","20/Feb/14 15:35;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12629962/HIVE-6382.6.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5172 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_map_operators
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1425/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1425/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12629962;;;","20/Feb/14 18:02;prasanth_j;The test failure does not seem to be related to this patch. ;;;","21/Feb/14 02:48;hagleitn;Committed to trunk. Thanks [~prasanth_j] and [~sershe]!;;;","10/Apr/14 11:39;leftyl;For the record:  this adds the configuration parameter *hive.exec.orc.skip.corrupt.data* to HiveConf.java and hive-default.xml.template.;;;","02/Jul/14 07:50;leftyl;*hive.exec.orc.skip.corrupt.data* is documented in the wiki:

* [Configuration Properties -- hive.exec.orc.skip.corrupt.data | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.orc.skip.corrupt.data];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bin/hcat script won't launch - uses invalid $HIVE_HOME,HIVE-6381,12693479,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,06/Feb/14 02:04,07/Feb/14 19:52,14/Jul/23 06:14,07/Feb/14 19:52,0.12.0,,,,,,,,,0.13.0,,HCatalog,,,,0,,,"fails with

ekoifman:hcatalog ekoifman$ bin/hcat
/Users/ekoifman/dev/hive/packaging/target/apache-hive-0.13.0-SNAPSHOT-bin/apache-hive-0.13.0-SNAPSHOT-bin/hcatalog/bin/hcat: Cannot find lib dir within HIVE_HOME : ./lib",,ekoifman,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/14 02:59;ekoifman;HIVE-6381.patch;https://issues.apache.org/jira/secure/attachment/12627280/HIVE-6381.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,372064,,,,Fri Feb 07 19:52:45 UTC 2014,,,,,,,,,,"0|i1s4dj:",372368,,,,,,,,,,,,,,,,,,,,,"06/Feb/14 02:59;ekoifman;no pre commit tests;;;","06/Feb/14 14:48;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12627280/HIVE-6381.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5035 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_dyn_part
org.apache.hadoop.hive.common.type.TestDecimal128.testHighPrecisionDecimal128Multiply
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1217/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1217/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12627280;;;","06/Feb/14 21:41;thejas;+1;;;","07/Feb/14 19:52;thejas;Patch committed to trunk.
Thanks for the contribution Eugene!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make HADOOP_HOME setting consistent between hive and webhcat_config.sh,HIVE-6377,12693452,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,05/Feb/14 22:14,07/Feb/14 19:54,14/Jul/23 06:14,07/Feb/14 19:54,0.12.0,,,,,,,,,0.13.0,,WebHCat,,,,0,,,,,brocknoland,ekoifman,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/14 03:13;ekoifman;HIVE-6377.patch;https://issues.apache.org/jira/secure/attachment/12627284/HIVE-6377.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,372037,,,,Fri Feb 07 19:54:41 UTC 2014,,,,,,,,,,"0|i1s47j:",372341,,,,,,,,,,,,,,,,,,,,,"06/Feb/14 03:10;ekoifman;no pre commi test;;;","06/Feb/14 03:13;ekoifman;no pre commit test;;;","06/Feb/14 16:20;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12627284/HIVE-6377.patch

{color:green}SUCCESS:{color} +1 5035 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1218/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1218/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12627284;;;","06/Feb/14 16:24;brocknoland;To disable tests it must be in all caps. Also we are pushing all patches through precommits now regardless of the *expected* build impact.;;;","06/Feb/14 21:43;thejas;+1;;;","07/Feb/14 19:54;thejas;Patch committed to trunk.
Thanks for the contribution Eugene!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable hive to work with tez on secure cluster,HIVE-6376,12693408,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,05/Feb/14 18:46,06/Feb/14 23:02,14/Jul/23 06:14,06/Feb/14 23:02,,,,,,,,,,tez-branch,,,,,,0,,,Need to pass the path objects for Tez to fetch credentials.,,hagleitn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6360,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/14 20:58;hagleitn;HIVE-6376.1.patch;https://issues.apache.org/jira/secure/attachment/12627214/HIVE-6376.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,371993,,,,Thu Feb 06 23:02:15 UTC 2014,,,,,,,,,,"0|i1s3xr:",372297,,,,,,,,,,,,,,,,,,,,,"06/Feb/14 23:02;hagleitn;Committed to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix CTAS for parquet,HIVE-6375,12693359,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,szehon,brocknoland,brocknoland,05/Feb/14 14:43,27/Jun/14 06:55,14/Jul/23 06:14,28/Feb/14 19:13,0.13.0,,,,,,,,,0.13.0,,,,,,0,Parquet,,"More details here:

https://github.com/Parquet/parquet-mr/issues/272",,brocknoland,glenn.strycker@gmail.com,kamrul,leftyl,qwertymaniac,szehon,tongjie,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-5783,,,,,,,,,,,,,,,,,HIVE-5803,HIVE-6938,,,,,,,,,,,,,,,,,,,,"19/Feb/14 21:57;szehon;HIVE-6375.2.patch;https://issues.apache.org/jira/secure/attachment/12629901/HIVE-6375.2.patch","21/Feb/14 01:34;szehon;HIVE-6375.3.patch;https://issues.apache.org/jira/secure/attachment/12630220/HIVE-6375.3.patch","23/Feb/14 22:06;szehon;HIVE-6375.4.patch;https://issues.apache.org/jira/secure/attachment/12630586/HIVE-6375.4.patch","19/Feb/14 00:34;szehon;HIVE-6375.patch;https://issues.apache.org/jira/secure/attachment/12629690/HIVE-6375.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,371944,,,,Fri Jun 27 06:55:33 UTC 2014,,,,,,,,,,"0|i1s3mv:",372248,,,,,,,,,,,,,,,,,,,,,"19/Feb/14 00:44;szehon;There is a bug in SemanticAnalyzer that leads to different column names used in FileSink and CreateTable operators.  Submitting a fix.;;;","19/Feb/14 00:49;szehon;As ParquetSerde is resolving columns by name, column rename will not work for parquet file as of now.  Renaming the JIRA to reflect this.;;;","19/Feb/14 15:48;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12629690/HIVE-6375.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5134 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas_hadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_ctas
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1410/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1410/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12629690;;;","19/Feb/14 17:59;xuefuz;Patch looks good. Minor comment on RB. The test diff needs to be fixed.;;;","19/Feb/14 21:57;szehon;Thanks Xuefu for review.  Incorporated feedback and fixed test output.  Seems select from srcbucket has some randomness to the result, as it is a bucketed table.;;;","19/Feb/14 22:32;kamrul;+1 
reviewed the patch.

CTAS for aver doesn't work for the same reason (HIVE-5803).
Hopefully, the patch will help avro as well.;;;","19/Feb/14 22:42;szehon;Yea, looks like a similar issue.;;;","21/Feb/14 00:19;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12629901/HIVE-6375.2.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5164 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_ctas
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into5
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_insert_into6
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1430/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1430/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12629901;;;","21/Feb/14 01:34;szehon;Test only fixes.  Last change did not completely guaratnee the test output, need to sort by both key,value.  Switched to another test table that has unique key,value pair.

Other tests insert_into5 and insert_into6 are known issues (HIVE-6479).  Final failure on minimr driver is not reproducible.;;;","23/Feb/14 09:15;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12630220/HIVE-6375.3.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5176 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_ctas
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1449/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1449/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12630220;;;","23/Feb/14 22:06;szehon;Looks like 'database:default' output was added to all test output files by recent commit HIVE-5958.  Regenerating my new test output due to the same.;;;","25/Feb/14 06:07;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12630586/HIVE-6375.4.patch

{color:green}SUCCESS:{color} +1 5177 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1483/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1483/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12630586;;;","25/Feb/14 19:23;szehon;[~xuefuz] Can you please take a look?  Thanks.;;;","25/Feb/14 19:59;xuefuz;+1;;;","28/Feb/14 19:13;xuefuz;Patch committed to trunk. Thanks to Szehon for the contribution.;;;","01/Mar/14 16:40;xuefuz;Previous commit didn't seem going thru. Tried again.;;;","26/Jun/14 07:12;leftyl;Support for CTAS is documented in the wiki:

* [Language Manual -- Parquet -- Limitations | https://cwiki.apache.org/confluence/display/Hive/Parquet#Parquet-Limitations]

If there's another JIRA ticket for column rename, please link it to this ticket.  For now, the wiki continues to cite this ticket for column rename.;;;","26/Jun/14 18:33;szehon;[~leftylev] I believe HIVE-6938 adds support for parquet column-rename (with a flag).  I added a link.;;;","27/Jun/14 06:55;leftyl;Great, I'll revise the Parquet doc.  Thanks [~szehon].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive job submitted with non-default name node (fs.default.name) doesn't process locations properly ,HIVE-6374,12693300,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,benjzh,benjzh,benjzh,05/Feb/14 08:08,13/Nov/14 19:42,14/Jul/23 06:14,13/May/14 16:39,0.11.0,0.12.0,0.13.0,,,,,,,0.14.0,,CLI,,,,0,,,"Create table/index/database and add partition DDL doesn't work properly if all following conditions are true:
- Metastore service is used
- fs.default.name is specified and it differs from the default one
- Location is not specified or specified as a not fully qualified URI

The root cause of this behavior is that Hive client doesn't pass configuration context to the metastore services which tries to resolve the paths. The fix is it too resolve the path in the Hive client if fs.default.name is specified and it differs from the default one (it is must easier then start passing the context, which would be a major change).

The CR will submitted shortly after tests are done
",Any,benjzh,gates,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/14 20:04;benjzh;Design of the fix HIVE-6374.docx;https://issues.apache.org/jira/secure/attachment/12627200/Design+of+the+fix+HIVE-6374.docx","02/Apr/14 12:59;benjzh;hive-6374.1.patch;https://issues.apache.org/jira/secure/attachment/12638250/hive-6374.1.patch","30/Apr/14 11:17;benjzh;hive-6374.3.patch;https://issues.apache.org/jira/secure/attachment/12642627/hive-6374.3.patch","30/Mar/14 12:04;benjzh;hive-6374.patch;https://issues.apache.org/jira/secure/attachment/12637707/hive-6374.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,371885,,,,Thu Nov 13 19:42:53 UTC 2014,,,,,,,,,,"0|i1s39r:",372189,,,,,,,,,,,,,,,,,,,,,"05/Feb/14 20:04;benjzh;More details and description of the fix;;;","30/Mar/14 14:57;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637707/hive-6374.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 5504 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_parallel_orderby
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority2
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testExecuteStatementAsync
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2045/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2045/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637707;;;","31/Mar/14 21:41;ashutoshc;[~benjzh] Can you create review board entry for this on : https://reviews.apache.org/r/new/;;;","02/Apr/14 12:59;benjzh;Test cleanup fixed (was not failing the test, but however it is nicer...);;;","02/Apr/14 15:14;benjzh;CR is available here: https://reviews.apache.org/r/19923/
Thanks to Ashutosh for reminding!;;;","05/Apr/14 07:55;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638250/hive-6374.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5549 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_parallel_orderby
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2120/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2120/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638250;;;","30/Apr/14 11:17;benjzh;Updated after sync with latest trunk
Fixed bug with wrong casing of table/index/db paths
Please review and vote!

(Note: the TestMinimrCliDriver tests fail even without the patch, therefore they are not related to this change);;;","04/May/14 22:52;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12642627/hive-6374.3.patch

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 5496 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dml
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/122/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/122/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12642627;;;","13/May/14 01:09;ashutoshc;+1;;;","13/May/14 16:39;ashutoshc;Committed to trunk. Thanks, Benjamin!;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestCliDriverMethods test can cause entire build to fail,HIVE-6373,12693282,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,05/Feb/14 03:46,07/Feb/14 19:43,14/Jul/23 06:14,07/Feb/14 19:41,,,,,,,,,,0.13.0,,Tests,,,,0,,,"TestCliDriverMethods has some tests which uses a custom SecurityManager  to intercept System.exit().  If the test fails early the security manager may not be unregistered and the process is unable to exit cleanly, which can cause the entire test run to fail with:

[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.16:test (default-test) on project hive-cli: ExecutionException; nested exception is java.util.concurrent.ExecutionException: org.apache.maven.surefire.booter.SurefireBooterForkException: Error occurred in starting fork, check output in log -> [Help 1]
",,jdere,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/14 03:49;jdere;HIVE-6373.1.patch;https://issues.apache.org/jira/secure/attachment/12627065/HIVE-6373.1.patch","07/Feb/14 19:40;thejas;HIVE-6373.2.patch;https://issues.apache.org/jira/secure/attachment/12627680/HIVE-6373.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,371867,,,,Fri Feb 07 19:43:14 UTC 2014,,,,,,,,,,"0|i1s35r:",372171,,,,,,,,,,,,,,,,,,,,,"05/Feb/14 03:49;jdere;patch v1;;;","05/Feb/14 21:45;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12627065/HIVE-6373.1.patch

{color:green}SUCCESS:{color} +1 5017 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1207/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1207/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12627065;;;","06/Feb/14 03:16;jdere;https://reviews.apache.org/r/17779/;;;","06/Feb/14 04:50;thejas;+1 . Just a minor comment about a comment.
;;;","07/Feb/14 19:40;thejas;HIVE-6373.2.patch - patch with modified comment
;;;","07/Feb/14 19:41;thejas;Patch committed to trunk. Thanks for the contribution Jason!
;;;","07/Feb/14 19:43;jdere;Thanks Thejas.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getDatabaseMajor/Minor version returns wrong values,HIVE-6372,12693273,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,szehon,szehon,szehon,05/Feb/14 02:02,22/Mar/14 06:30,14/Jul/23 06:14,13/Feb/14 22:17,,,,,,,,,,0.13.0,,JDBC,,,,0,,,"Currently getDatabaseMajorVersion returns 13, and getDatabaseMinorVersion returns 0.   The index is off by one.",,prasadm,szehon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/14 02:17;szehon;HIVE-6372.patch;https://issues.apache.org/jira/secure/attachment/12627054/HIVE-6372.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,371858,,,,Thu Feb 13 22:17:51 UTC 2014,,,,,,,,,,"0|i1s33r:",372162,,,,,,,,,,,,,,,,,,,,,"05/Feb/14 17:41;prasadm;+1
;;;","05/Feb/14 18:34;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12627054/HIVE-6372.patch

{color:green}SUCCESS:{color} +1 5010 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1203/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1203/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12627054;;;","13/Feb/14 22:17;prasadm;Patch committed to trunk. Thanks Szehon!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2 - Request serving thread should get class loader from existing SessionState,HIVE-6364,12693095,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,,jaideepdhok,jaideepdhok,04/Feb/14 12:37,23/Mar/14 16:56,14/Jul/23 06:14,23/Mar/14 16:56,,,,,,,,,,0.13.0,,HiveServer2,,,,0,,,"SessionState is created for each session in HS2. If we do any add jars, a class loader is set in the SessionState's conf object. This class loader should also be set in each thread that serves request of the same session.

Scenario (both requests are in the same session)-
{noformat}
// req 1
add jar foo.jar // Served by thread th1, this updates class loader and sets in SessionState.conf

// req2 served by th2, such that th1 != th2
CREATE TEMPORARY FUNCTION foo_udf AS 'some class in foo.jar' 
// This can throw class not found error, because although 
// the new thread (th2) gets the same session state as th1,
// the class loader is different (Thread.currentThread.getContextClassLoader()

{noformat}",,amareshwari,jaideepdhok,jdere,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-3969,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/14 12:57;jaideepdhok;HIVE-6364.1.patch;https://issues.apache.org/jira/secure/attachment/12626868/HIVE-6364.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,371681,,,,Sun Mar 23 16:56:30 UTC 2014,,,,,,,,,,"0|i1s20v:",371981,,,,,,,,,,,,,,,,,,,,,"04/Feb/14 12:56;jaideepdhok;Raised review board request - https://reviews.apache.org/r/17708/;;;","04/Feb/14 12:57;jaideepdhok;attaching patch;;;","05/Feb/14 14:11;jaideepdhok;There's another issue - In SessionState.registerJar class loader is obtained from the current thread. If the current thread was used during another session, then the current session can get some jars from that session.;;;","18/Mar/14 16:55;ashutoshc;Although, HIVE-3969 is marked as duplicate, I don't think it is a duplicate. This one fixes the problem of having right class loader for a thread serving the query, whereas HIVE-3969 talks about unloading registered jars. So, it seems there are two independent problem, both of which needs to be fixed.
[~jaideepdhok] would you like to rebase your patch.;;;","18/Mar/14 18:26;jdere;Hi Jaideep, when I tried debugging hiveserver2 due to HIVE-6672, it appeared that there was a thread running for each connection (session).  Non-SQL commands (such as ADD JAR), were being run within this session thread and so the classloader for the session thread had the JARs loaded.  When a SQL command was executed the session thread would start a new thread, and it appeared that this new thread was using the same classloader (and had the added JARs in the classloader's list of URLs). Were you seeing different behavior  in your testing (I was running this on Mac, I think with jdk 1.6, not sure if it would have been different)?

In the patch, the thread's classloader is getting set to the HiveConf's classloader .. where is the HiveConf's classloader getting set from? Do we need to worry about having to make sure this classloader is updated whenever a JAR is added to the classpath?;;;","19/Mar/14 00:42;jaideepdhok;[~ashutoshc] I will put up a new patch.
[~jdere] Add jar will always update the class loader. That's the current behaviour. I think the first class loader is set using the conf.getClassLoader method, if nothing is set it will return the default class loader.

;;;","19/Mar/14 21:37;ashutoshc;I think [~navis] 's patch on HIVE-3969 is more complete and subsumes current patch. So, I guess we should try to get that one and close this one. Although, [~jaideepdhok] 's [earlier point | https://issues.apache.org/jira/browse/HIVE-6364?focusedCommentId=13892132&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13892132] about jars on a thread being visible across session is still there. Though, I am not sure how will that negatively impact query execution and way to fix it. That probably can be taken up as follow-up of HIVE-3969;;;","23/Mar/14 16:56;ashutoshc;Fixed via HIVE-3969;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support union all on tez,HIVE-6362,12693035,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,04/Feb/14 02:42,17/Feb/14 07:58,14/Jul/23 06:14,17/Feb/14 07:58,,,,,,,,,,tez-branch,,,,,,0,,,,,hagleitn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6360,,"04/Feb/14 02:53;hagleitn;HIVE-6362.1.patch;https://issues.apache.org/jira/secure/attachment/12626815/HIVE-6362.1.patch","12/Feb/14 05:03;hagleitn;HIVE-6362.2.patch;https://issues.apache.org/jira/secure/attachment/12628433/HIVE-6362.2.patch","12/Feb/14 22:46;hagleitn;HIVE-6362.3.patch;https://issues.apache.org/jira/secure/attachment/12628595/HIVE-6362.3.patch","15/Feb/14 08:15;hagleitn;HIVE-6362.4.patch;https://issues.apache.org/jira/secure/attachment/12629211/HIVE-6362.4.patch","15/Feb/14 08:42;hagleitn;HIVE-6362.5.patch;https://issues.apache.org/jira/secure/attachment/12629213/HIVE-6362.5.patch","16/Feb/14 08:41;hagleitn;HIVE-6362.8.patch;https://issues.apache.org/jira/secure/attachment/12629259/HIVE-6362.8.patch","16/Feb/14 09:32;hagleitn;HIVE-6362.9.patch;https://issues.apache.org/jira/secure/attachment/12629260/HIVE-6362.9.patch",,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,371621,,,,Mon Feb 17 07:58:57 UTC 2014,,,,,,,,,,"0|i1s1nj:",371921,,,,,,,,,,,,,,,,,,,,,"04/Feb/14 02:53;hagleitn;Very basic first steps in .1;;;","12/Feb/14 05:03;hagleitn;.2 generates right plan and translates to Tez DAG. There's still some errors to be sorted out.;;;","15/Feb/14 08:42;hagleitn;rebased;;;","16/Feb/14 08:41;hagleitn;.8 is functional. Follow up things to fix. There's two stats aggr tasks in some cases, auto-merge is off with this.;;;","16/Feb/14 09:32;hagleitn;.9 has updated golden files;;;","17/Feb/14 07:58;hagleitn;Committed to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop 2.3 + Tez 0.3,HIVE-6360,12693000,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,03/Feb/14 23:46,12/Aug/14 01:55,14/Jul/23 06:14,02/Mar/14 02:25,,,,,,,,,,0.13.0,,,,,,0,,,"There are some things pending that rely on hadoop 2.3 or tez 0.3. These are not released yet, but will be soon. I'm proposing to collect these in the tez branch and do a merge back once these components have been released at that version.

The things depending on 0.3 or hadoop 2.3 are:

- Zero Copy read for ORC
- Unions in Tez
- Tez on secure clusters
- Changes to DagUtils to reflect tez 0.2 -> 0.3
- Prewarm containers",,apivovarov,gopalv,hagleitn,jnp,leftyl,thejas,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6376,,,,,,,,,,,,,,,,,HIVE-6347,HIVE-6524,HIVE-6501,HIVE-6498,HIVE-6391,HIVE-6493,HIVE-6533,HIVE-6534,HIVE-6442,HIVE-6443,HIVE-6444,HIVE-6525,,,,,,HIVE-6362,,HIVE-6255,HIVE-6346,"28/Feb/14 02:42;hagleitn;HIVE-6360.1.patch;https://issues.apache.org/jira/secure/attachment/12631654/HIVE-6360.1.patch","28/Feb/14 22:45;hagleitn;HIVE-6360.2.patch;https://issues.apache.org/jira/secure/attachment/12631860/HIVE-6360.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,371586,,,,Tue Aug 12 01:55:21 UTC 2014,,,,,,,,,,"0|i1s1g7:",371886,,,,,,,,,,,,,,,,,,,,,"28/Feb/14 12:03;thejas;+1 LGTM, pending some minor comments on rb, and tests passing.
;;;","28/Feb/14 19:09;jnp;+1;;;","28/Feb/14 22:45;hagleitn;.2 addresses review comments and add change to golden file for FileDumpTest.;;;","28/Feb/14 22:48;hagleitn;Ran tests locally. All passed except for minimr test udf_using, which is failing w or w/o patch.;;;","28/Feb/14 22:54;vikram.dixit;LGTM +1. Ran tests on my machine. ;;;","01/Mar/14 23:54;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12631860/HIVE-6360.2.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1575/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1575/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-project/2.3.0/hadoop-project-2.3.0.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-project/2.3.0/hadoop-project-2.3.0.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-project/2.3.0/hadoop-project-2.3.0.pom (38 KB at 2213.0 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-main/2.3.0/hadoop-main-2.3.0.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-main/2.3.0/hadoop-main-2.3.0.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-main/2.3.0/hadoop-main-2.3.0.pom (18 KB at 332.5 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-annotations/2.3.0/hadoop-annotations-2.3.0.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-annotations/2.3.0/hadoop-annotations-2.3.0.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-annotations/2.3.0/hadoop-annotations-2.3.0.pom (3 KB at 129.6 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.pom (14 KB at 855.4 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.pom
Downloading: http://repo.maven.apache.org/maven2/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.pom
Downloaded: http://repo.maven.apache.org/maven2/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.pom (3 KB at 148.7 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.pom
Downloading: http://repo.maven.apache.org/maven2/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.pom
Downloaded: http://repo.maven.apache.org/maven2/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.pom (3 KB at 194.0 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/sonatype/oss/oss-parent/3/oss-parent-3.pom
Downloading: http://repo.maven.apache.org/maven2/org/sonatype/oss/oss-parent/3/oss-parent-3.pom
Downloaded: http://repo.maven.apache.org/maven2/org/sonatype/oss/oss-parent/3/oss-parent-3.pom (4 KB at 192.5 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-auth/2.3.0/hadoop-auth-2.3.0.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-auth/2.3.0/hadoop-auth-2.3.0.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-auth/2.3.0/hadoop-auth-2.3.0.pom (6 KB at 311.5 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-hdfs/2.3.0/hadoop-hdfs-2.3.0.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-hdfs/2.3.0/hadoop-hdfs-2.3.0.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-hdfs/2.3.0/hadoop-hdfs-2.3.0.pom (26 KB at 1493.5 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-mapreduce-client-core/2.3.0/hadoop-mapreduce-client-core-2.3.0.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-core/2.3.0/hadoop-mapreduce-client-core-2.3.0.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-core/2.3.0/hadoop-mapreduce-client-core-2.3.0.pom (4 KB at 200.8 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-mapreduce-client/2.3.0/hadoop-mapreduce-client-2.3.0.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-mapreduce-client/2.3.0/hadoop-mapreduce-client-2.3.0.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-mapreduce-client/2.3.0/hadoop-mapreduce-client-2.3.0.pom (7 KB at 503.9 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-yarn-common/2.3.0/hadoop-yarn-common-2.3.0.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-common/2.3.0/hadoop-yarn-common-2.3.0.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-common/2.3.0/hadoop-yarn-common-2.3.0.pom (9 KB at 613.8 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-yarn/2.3.0/hadoop-yarn-2.3.0.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn/2.3.0/hadoop-yarn-2.3.0.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn/2.3.0/hadoop-yarn-2.3.0.pom (4 KB at 287.7 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-yarn-api/2.3.0/hadoop-yarn-api-2.3.0.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-api/2.3.0/hadoop-yarn-api-2.3.0.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-api/2.3.0/hadoop-yarn-api-2.3.0.pom (5 KB at 396.5 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.3.0/hadoop-mapreduce-client-jobclient-2.3.0.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.3.0/hadoop-mapreduce-client-jobclient-2.3.0.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.3.0/hadoop-mapreduce-client-jobclient-2.3.0.pom (6 KB at 44.9 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-mapreduce-client-common/2.3.0/hadoop-mapreduce-client-common-2.3.0.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-common/2.3.0/hadoop-mapreduce-client-common-2.3.0.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-common/2.3.0/hadoop-mapreduce-client-common-2.3.0.pom (4 KB at 54.2 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-yarn-client/2.3.0/hadoop-yarn-client-2.3.0.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-client/2.3.0/hadoop-yarn-client-2.3.0.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-client/2.3.0/hadoop-yarn-client-2.3.0.pom (5 KB at 448.9 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-yarn-server-common/2.3.0/hadoop-yarn-server-common-2.3.0.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server-common/2.3.0/hadoop-yarn-server-common-2.3.0.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server-common/2.3.0/hadoop-yarn-server-common-2.3.0.pom (6 KB at 493.1 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-yarn-server/2.3.0/hadoop-yarn-server-2.3.0.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server/2.3.0/hadoop-yarn-server-2.3.0.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server/2.3.0/hadoop-yarn-server-2.3.0.pom (2 KB at 138.4 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.3.0/hadoop-mapreduce-client-shuffle-2.3.0.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.3.0/hadoop-mapreduce-client-shuffle-2.3.0.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.3.0/hadoop-mapreduce-client-shuffle-2.3.0.pom (2 KB at 148.7 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.3.0/hadoop-yarn-server-nodemanager-2.3.0.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.3.0/hadoop-yarn-server-nodemanager-2.3.0.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.3.0/hadoop-yarn-server-nodemanager-2.3.0.pom (12 KB at 215.3 KB/sec)
Downloading: http://repository.apache.org/snapshots/org/apache/tez/tez-api/0.3.0-incubating-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/tez/tez-api/0.3.0-incubating-SNAPSHOT/tez-api-0.3.0-incubating-SNAPSHOT.pom
[WARNING] The POM for org.apache.tez:tez-api:jar:0.3.0-incubating-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/tez/tez-runtime-library/0.3.0-incubating-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/tez/tez-runtime-library/0.3.0-incubating-SNAPSHOT/tez-runtime-library-0.3.0-incubating-SNAPSHOT.pom
[WARNING] The POM for org.apache.tez:tez-runtime-library:jar:0.3.0-incubating-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/tez/tez-mapreduce/0.3.0-incubating-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/tez/tez-mapreduce/0.3.0-incubating-SNAPSHOT/tez-mapreduce-0.3.0-incubating-SNAPSHOT.pom
[WARNING] The POM for org.apache.tez:tez-mapreduce:jar:0.3.0-incubating-SNAPSHOT is missing, no dependency information available
Downloading: http://repository.apache.org/snapshots/org/apache/tez/tez-tests/0.3.0-incubating-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/tez/tez-tests/0.3.0-incubating-SNAPSHOT/tez-tests-0.3.0-incubating-SNAPSHOT.pom
[WARNING] The POM for org.apache.tez:tez-tests:jar:tests:0.3.0-incubating-SNAPSHOT is missing, no dependency information available
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-yarn-server-tests/2.3.0/hadoop-yarn-server-tests-2.3.0.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server-tests/2.3.0/hadoop-yarn-server-tests-2.3.0.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server-tests/2.3.0/hadoop-yarn-server-tests-2.3.0.pom (6 KB at 455.6 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.3.0/hadoop-yarn-server-resourcemanager-2.3.0.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.3.0/hadoop-yarn-server-resourcemanager-2.3.0.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.3.0/hadoop-yarn-server-resourcemanager-2.3.0.pom (10 KB at 757.8 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.3.0/hadoop-yarn-server-web-proxy-2.3.0.pom
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.3.0/hadoop-yarn-server-web-proxy-2.3.0.pom
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.3.0/hadoop-yarn-server-web-proxy-2.3.0.pom (5 KB at 47.2 KB/sec)
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-common/2.3.0/hadoop-common-2.3.0.jar
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-annotations/2.3.0/hadoop-annotations-2.3.0.jar
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar
Downloading: http://www.datanucleus.org/downloads/maven2/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar
Downloading: http://www.datanucleus.org/downloads/maven2/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-auth/2.3.0/hadoop-auth-2.3.0.jar
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-hdfs/2.3.0/hadoop-hdfs-2.3.0.jar
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-hdfs/2.3.0/hadoop-hdfs-2.3.0-tests.jar
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-mapreduce-client-core/2.3.0/hadoop-mapreduce-client-core-2.3.0.jar
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.3.0/hadoop-mapreduce-client-jobclient-2.3.0-tests.jar
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-mapreduce-client-common/2.3.0/hadoop-mapreduce-client-common-2.3.0.jar
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.3.0/hadoop-mapreduce-client-shuffle-2.3.0.jar
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-yarn-api/2.3.0/hadoop-yarn-api-2.3.0.jar
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-yarn-common/2.3.0/hadoop-yarn-common-2.3.0.jar
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-yarn-client/2.3.0/hadoop-yarn-client-2.3.0.jar
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-yarn-server-tests/2.3.0/hadoop-yarn-server-tests-2.3.0-tests.jar
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-yarn-server-common/2.3.0/hadoop-yarn-server-common-2.3.0.jar
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.3.0/hadoop-yarn-server-nodemanager-2.3.0.jar
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.3.0/hadoop-yarn-server-resourcemanager-2.3.0.jar
Downloading: http://www.datanucleus.org/downloads/maven2/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.3.0/hadoop-yarn-server-web-proxy-2.3.0.jar
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-common/2.3.0/hadoop-common-2.3.0.jar
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-annotations/2.3.0/hadoop-annotations-2.3.0.jar
Downloading: http://repo.maven.apache.org/maven2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar
Downloading: http://repo.maven.apache.org/maven2/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar
Downloading: http://repo.maven.apache.org/maven2/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-annotations/2.3.0/hadoop-annotations-2.3.0.jar (17 KB at 630.2 KB/sec)
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-auth/2.3.0/hadoop-auth-2.3.0.jar
Downloaded: http://repo.maven.apache.org/maven2/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar (19 KB at 601.9 KB/sec)
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-hdfs/2.3.0/hadoop-hdfs-2.3.0.jar
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-auth/2.3.0/hadoop-auth-2.3.0.jar (49 KB at 2119.6 KB/sec)
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-hdfs/2.3.0/hadoop-hdfs-2.3.0-tests.jar
Downloaded: http://repo.maven.apache.org/maven2/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar (528 KB at 5068.1 KB/sec)
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-core/2.3.0/hadoop-mapreduce-client-core-2.3.0.jar
Downloaded: http://repo.maven.apache.org/maven2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar (1563 KB at 6298.9 KB/sec)
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.3.0/hadoop-mapreduce-client-jobclient-2.3.0-tests.jar
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-common/2.3.0/hadoop-common-2.3.0.jar (2765 KB at 6999.7 KB/sec)
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-common/2.3.0/hadoop-mapreduce-client-common-2.3.0.jar
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-hdfs/2.3.0/hadoop-hdfs-2.3.0-tests.jar (2307 KB at 5654.2 KB/sec)
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.3.0/hadoop-mapreduce-client-shuffle-2.3.0.jar
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.3.0/hadoop-mapreduce-client-jobclient-2.3.0-tests.jar (1437 KB at 7040.7 KB/sec)
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-api/2.3.0/hadoop-yarn-api-2.3.0.jar
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.3.0/hadoop-mapreduce-client-shuffle-2.3.0.jar (22 KB at 1646.3 KB/sec)
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-common/2.3.0/hadoop-yarn-common-2.3.0.jar
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-common/2.3.0/hadoop-mapreduce-client-common-2.3.0.jar (646 KB at 8169.8 KB/sec)
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-client/2.3.0/hadoop-yarn-client-2.3.0.jar
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-client/2.3.0/hadoop-yarn-client-2.3.0.jar (94 KB at 2530.1 KB/sec)
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server-tests/2.3.0/hadoop-yarn-server-tests-2.3.0-tests.jar
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server-tests/2.3.0/hadoop-yarn-server-tests-2.3.0-tests.jar (42 KB at 2088.4 KB/sec)
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server-common/2.3.0/hadoop-yarn-server-common-2.3.0.jar
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-api/2.3.0/hadoop-yarn-api-2.3.0.jar (1264 KB at 6286.9 KB/sec)
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.3.0/hadoop-yarn-server-nodemanager-2.3.0.jar
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server-common/2.3.0/hadoop-yarn-server-common-2.3.0.jar (182 KB at 1075.9 KB/sec)
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.3.0/hadoop-yarn-server-resourcemanager-2.3.0.jar
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-common/2.3.0/hadoop-yarn-common-2.3.0.jar (1306 KB at 4890.2 KB/sec)
Downloading: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.3.0/hadoop-yarn-server-web-proxy-2.3.0.jar
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.3.0/hadoop-yarn-server-web-proxy-2.3.0.jar (26 KB at 1604.3 KB/sec)
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.3.0/hadoop-yarn-server-nodemanager-2.3.0.jar (464 KB at 4929.1 KB/sec)
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-core/2.3.0/hadoop-mapreduce-client-core-2.3.0.jar (1440 KB at 2184.1 KB/sec)
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-hdfs/2.3.0/hadoop-hdfs-2.3.0.jar (5649 KB at 7412.5 KB/sec)
Downloaded: http://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.3.0/hadoop-yarn-server-resourcemanager-2.3.0.jar (718 KB at 1276.2 KB/sec)
Downloading: http://repository.apache.org/snapshots/org/apache/tez/tez-api/0.3.0-incubating-SNAPSHOT/tez-api-0.3.0-incubating-SNAPSHOT.jar
Downloading: http://repository.apache.org/snapshots/org/apache/tez/tez-runtime-library/0.3.0-incubating-SNAPSHOT/tez-runtime-library-0.3.0-incubating-SNAPSHOT.jar
Downloading: http://repository.apache.org/snapshots/org/apache/tez/tez-mapreduce/0.3.0-incubating-SNAPSHOT/tez-mapreduce-0.3.0-incubating-SNAPSHOT.jar
Downloading: http://repository.apache.org/snapshots/org/apache/tez/tez-tests/0.3.0-incubating-SNAPSHOT/tez-tests-0.3.0-incubating-SNAPSHOT-tests.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [3.305s]
[INFO] Hive Ant Utilities ................................ SUCCESS [6.827s]
[INFO] Hive Shims Common ................................. SUCCESS [5.110s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [1.997s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [2.864s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [1.477s]
[INFO] Hive Shims 0.23 ................................... FAILURE [14.404s]
[INFO] Hive Shims ........................................ SKIPPED
[INFO] Hive Common ....................................... SKIPPED
[INFO] Hive Serde ........................................ SKIPPED
[INFO] Hive Metastore .................................... SKIPPED
[INFO] Hive Query Language ............................... SKIPPED
[INFO] Hive Service ...................................... SKIPPED
[INFO] Hive JDBC ......................................... SKIPPED
[INFO] Hive Beeline ...................................... SKIPPED
[INFO] Hive CLI .......................................... SKIPPED
[INFO] Hive Contrib ...................................... SKIPPED
[INFO] Hive HBase Handler ................................ SKIPPED
[INFO] Hive HCatalog ..................................... SKIPPED
[INFO] Hive HCatalog Core ................................ SKIPPED
[INFO] Hive HCatalog Pig Adapter ......................... SKIPPED
[INFO] Hive HCatalog Server Extensions ................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client ................. SKIPPED
[INFO] Hive HCatalog Webhcat ............................. SKIPPED
[INFO] Hive HCatalog HBase Storage Handler ............... SKIPPED
[INFO] Hive HWI .......................................... SKIPPED
[INFO] Hive ODBC ......................................... SKIPPED
[INFO] Hive Shims Aggregator ............................. SKIPPED
[INFO] Hive TestUtils .................................... SKIPPED
[INFO] Hive Packaging .................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 40.503s
[INFO] Finished at: Sat Mar 01 18:54:54 EST 2014
[INFO] Final Memory: 25M/61M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-shims-0.23: Could not resolve dependencies for project org.apache.hive.shims:hive-shims-0.23:jar:0.13.0-SNAPSHOT: The following artifacts could not be resolved: org.apache.tez:tez-api:jar:0.3.0-incubating-SNAPSHOT, org.apache.tez:tez-runtime-library:jar:0.3.0-incubating-SNAPSHOT, org.apache.tez:tez-mapreduce:jar:0.3.0-incubating-SNAPSHOT, org.apache.tez:tez-tests:jar:tests:0.3.0-incubating-SNAPSHOT: Could not find artifact org.apache.tez:tez-api:jar:0.3.0-incubating-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-shims-0.23
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12631860;;;","02/Mar/14 02:08;hagleitn;If you look at the actual run, tests have run and no new failures introduced. Same as in local run.;;;","02/Mar/14 02:25;hagleitn;Merged to trunk. Thanks [~vikram.dixit], [~thejas], [~jnp]!;;;","02/Mar/14 04:33;leftyl;This adds seven configuration parameters in HiveConf.java and hive-default.xml.template:

* HIVE-6347:  hive.exec.orc.zerocopy

* HIVE-6498:
**  hive.merge.tezfiles
**  hive.tez.input.format
**  hive.tez.container.size
**  hive.tez.java.opts
* HIVE-6391:
**  hive.prewarm.enabled
**  hive.prewarm.numcontainers

;;;","23/Apr/14 07:29;leftyl;The Tez configuration parameters are documented in the wiki:  https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-Tez.

The ORC configuration parameter (hive.exec.orc.zerocopy) still needs to be added to the wiki.;;;","09/Jun/14 05:36;leftyl;Now hive.exec.orc.zerocopy is also in the wiki.  I added it to the list of miscellaneous Tez parameters, but does it belong there or does it just rely on Hadoop 2.3?  (And if it relies on Hadoop 2.3, should that be mentioned in the description?)

* [Configuration Properties:  hive.exec.orc.zerocopy | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.orc.zerocopy]
* [Configuration Properties:  Tez | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-Tez];;;","09/Jun/14 06:45;gopalv;[~leftylev]: zero-copy relies on APIs introduced in hadoop-2.3 and remain available in all newer releases.;;;","09/Jun/14 08:50;leftyl;Thanks [~gopalv].  I've removed hive.exec.orc.zerocopy from the Tez list and changed its description to ""Use zerocopy reads with ORC. (This requires Hadoop 2.3 or later.)"";;;","12/Aug/14 01:14;apivovarov;Lefty, can you fix hive.hive.merge.tezfiles on wiki?;;;","12/Aug/14 01:55;leftyl;Oops!  That's a cut-&-paste error, thanks for noticing [~apivovarov].  It's fixed now and I made sure there aren't any other hive.hive.* parameters in the wiki.

* [Configuration Properties -- hive.merge.tezfiles | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.merge.tezfiles];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
beeline -f fails on scripts with tabs in them.,HIVE-6359,12692912,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,navis,cartershanklin,cartershanklin,03/Feb/14 18:13,12/Mar/14 09:20,14/Jul/23 06:14,10/Mar/14 14:35,,,,,,,,,,0.13.0,,,,,,0,,,"NO PRECOMMIT TESTS

On a recent trunk build I used beeline -f on a script with tabs in it.

Beeline rather unhelpfully attempts to perform tab expansion on the tabs and the query fails. Here's a screendump.

{code}
Connecting to jdbc:hive2://mymachine:10000/mydb
Connected to: Apache Hive (version 0.13.0-SNAPSHOT)
Driver: Hive JDBC (version 0.13.0-SNAPSHOT)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 0.13.0-SNAPSHOT by Apache Hive
0: jdbc:hive2://mymachine:10000/mydb> select  i_brand_id as brand_id, i_brand as brand,
. . . . . . . . . . . . . . . . . . . . . . .>  
Display all 560 possibilities? (y or n) 
. . . . . . . . . . . . . . . . . . . . . . .>  ager_id=36
. . . . . . . . . . . . . . . . . . . . . . .>  
Display all 560 possibilities? (y or n) 
. . . . . . . . . . . . . . . . . . . . . . .>  d d_moy=12
. . . . . . . . . . . . . . . . . . . . . . .>  
Display all 560 possibilities? (y or n) 
. . . . . . . . . . . . . . . . . . . . . . .>  d d_year=2001
. . . . . . . . . . . . . . . . . . . . . . .>         and ss_sold_date between '2001-12-01' and '2001-12-31'
. . . . . . . . . . . . . . . . . . . . . . .>  group by i_brand, i_brand_id
. . . . . . . . . . . . . . . . . . . . . . .>  order by ext_price desc, brand_id
. . . . . . . . . . . . . . . . . . . . . . .> limit 100 ;
Error: Error while compiling statement: FAILED: ParseException line 1:65 missing FROM at 'd_moy' near 'd' in from source (state=42000,code=40000)
Closing: org.apache.hive.jdbc.HiveConnection
{code}

The same query works fine if I replace tabs with some spaces.",,cartershanklin,leftyl,mdominguez@cloudera.com,navis,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-4268,,,,,,,,,,,,,,,,,,,,,"05/Feb/14 04:02;navis;HIVE-6359.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12627067/HIVE-6359.1.patch.txt","19/Feb/14 01:39;navis;HIVE-6359.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12629711/HIVE-6359.2.patch.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,371498,,,,Wed Mar 12 09:20:08 UTC 2014,,,,,,,,,,"0|i1s0wn:",371801,,,,,,,,,,,,,,,,,,,,,"05/Feb/14 18:37;xuefuz;[~navis] Thanks for working on this. When the patch is ready for review, could you please provide a review board entry?;;;","05/Feb/14 23:18;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12627067/HIVE-6359.1.patch.txt

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5036 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_disable_merge_for_bucketing
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1208/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1208/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12627067;;;","19/Feb/14 01:44;navis;[~xuefuz] I've missed your comment. Made review board link.;;;","19/Feb/14 15:04;xuefuz;[~navis] I just realiazed it's a simple change, but thanks for the review link.

+1;;;","10/Mar/14 14:35;thejas;Patch committed to trunk and 0.13 branch (It was already in the list maintained by Harish).
cc [~rhbutani]
;;;","12/Mar/14 07:27;leftyl;Should this bug fix be mentioned in the wiki, with version information?

* [HiveServer2 Clients:  Beeline Command Options |https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-BeelineCommandOptions];;;","12/Mar/14 07:34;thejas;Yes, I think we can put a note for the -f option about this bug in 0.12 and earlier, as this is not something that is easy to debug, and people could have tabs in their script files.
;;;","12/Mar/14 07:59;leftyl;Done, please review -f option in Beeline wikidoc (link 2 comments back).;;;","12/Mar/14 08:53;thejas;The wikidoc update LGTM
;;;","12/Mar/14 09:20;leftyl;Thanks, night owl.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
filterExpr not printed in explain for tablescan operators (ppd),HIVE-6358,12692803,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,03/Feb/14 09:10,05/Feb/14 22:31,14/Jul/23 06:14,05/Feb/14 22:31,,,,,,,,,,0.13.0,,,,,,0,,,,,hagleitn,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6267,,,,,,,,,"03/Feb/14 09:12;hagleitn;HIVE-6358.1.patch;https://issues.apache.org/jira/secure/attachment/12626621/HIVE-6358.1.patch","03/Feb/14 23:25;hagleitn;HIVE-6358.2.patch;https://issues.apache.org/jira/secure/attachment/12626773/HIVE-6358.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,371389,,,,Wed Feb 05 22:31:24 UTC 2014,,,,,,,,,,"0|i1s08n:",371692,,,,,,,,,,,,,,,,,,,,,"03/Feb/14 21:51;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626621/HIVE-6358.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4997 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_pushdown
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1169/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1169/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626621;;;","03/Feb/14 23:25;hagleitn;.2 fixes test from precommit (missed one golden file);;;","04/Feb/14 22:37;vikram.dixit;+1.;;;","05/Feb/14 22:31;hagleitn;Committed to trunk. Thanks for the review Vikram!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dependency injection in hbase storage handler is broken,HIVE-6356,12692792,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,xuefuz,navis,navis,03/Feb/14 08:04,01/Mar/14 16:28,14/Jul/23 06:14,01/Mar/14 16:28,,,,,,,,,,0.13.0,,HBase Handler,,,,0,,,"Dependent jars for hbase is not added to tmpjars, which is caused by the change of method signature(TableMapReduceUtil.addDependencyJars).",,navis,ndimiduk,sershe,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-3603,,,,,,,,,"03/Feb/14 08:05;navis;HIVE-6356.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12626614/HIVE-6356.1.patch.txt","04/Feb/14 00:09;navis;HIVE-6356.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12626785/HIVE-6356.2.patch.txt","13/Feb/14 01:34;navis;HIVE-6356.3.patch.txt;https://issues.apache.org/jira/secure/attachment/12628643/HIVE-6356.3.patch.txt","28/Feb/14 19:33;xuefuz;HIVE-6356.4.patch.txt;https://issues.apache.org/jira/secure/attachment/12631814/HIVE-6356.4.patch.txt","26/Feb/14 20:26;xuefuz;HIVE-6356.4.patch.txt;https://issues.apache.org/jira/secure/attachment/12631340/HIVE-6356.4.patch.txt","06/Feb/14 17:15;ndimiduk;HIVE-6356.addendum.00.patch;https://issues.apache.org/jira/secure/attachment/12627374/HIVE-6356.addendum.00.patch",,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,371378,,,,Sat Mar 01 16:28:48 UTC 2014,,,,,,,,,,"0|i1s067:",371681,,,,,,,,,,,,,,,,,,,,,"03/Feb/14 16:39;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626614/HIVE-6356.1.patch.txt

{color:green}SUCCESS:{color} +1 4997 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1165/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1165/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626614;;;","03/Feb/14 17:09;ashutoshc;Is htrace strictly required ? If so, than don't we need to make sure its jar is available at run-time (currently it seems we don't have it in our lib/ dir of package?) [~ndimiduk] Can you also take a look?;;;","03/Feb/14 18:20;ndimiduk;I stumbled into this recently as well. HTrace is now a required runtime dependency, even when it's not used. This patch is incorrect, however. Because Hive is using the mapred namespace classes, the correct API is to invoke o.a.h.hbase.mapred.TableMapReduceUtil#addDependencyJars(JobConf). This will wire in all of HBase's runtime dependencies for you, and also attempt to auto-detect additional dependencies based on the JobConf (output classes, partitioners, formats, etc). If you want more fine-grained control over these dependencies (as Pig did, see PIG-3285), there are additional static methods in the o.a.h.hbase.mapreduce.TableMapReduceUtil class.

For Hive's purpose, I think you'll be fine with just calling mapred.TableMapReduceUtil#addDependencyJars(JobConf). Having a smoke test that runs in pseudo-distributed mode would be helpful in verifying all requirements are met.;;;","04/Feb/14 00:06;navis;Right. I've forgot there are two version of TableMapReduceUtil. HIVE-3603 changed import clause of TableMapReduceUtil to mapred, which cause this problem. I'll fix this shortly after.;;;","04/Feb/14 08:37;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626785/HIVE-6356.2.patch.txt

{color:green}SUCCESS:{color} +1 4997 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1172/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1172/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626785;;;","04/Feb/14 18:22;ndimiduk;Patch 2 looks better. You should be able to do without explicitly requesting org.apache.hadoop.hbase.HBaseConfiguration.class as well; this should be included by the underlying HBase method.

+1 (for what it's worth);;;","04/Feb/14 22:55;ashutoshc;+1;;;","04/Feb/14 23:42;ashutoshc;Committed to trunk. Thanks, Navis!;;;","06/Feb/14 01:51;navis;Sadly, patch2 cannot resolve this issue. I should I have done some tests before updating the patch (the problem looked so obvious, sorry). o.a.h.hbase.mapred.TableMapReduceUtil#addDependencyJars(JobConf) is only works with older version of hbase.

Now back to the problem, o.a.h.hbase.mapreduce.TableMapReduceUtil takes Job as parameter rather than JobConf, which seemed not good idea thinking that it's mere a wrapper of JobConf. Anyway, we can resolve this with patch1 (copy needed class list from TableMapReduceUtil) or make new Job instance and copy values from it to original JobConf. Which one is better?;;;","06/Feb/14 17:15;ndimiduk;Please don't copy-paste the dependency list from HBase. The whole point of the addDependencyJars is to encapsulate that list in a single place that's managed by the HBase team.

The reason the method accepts a Job instead of just a Configuration is that it inspects the job to add additional dependencies, such as input/output formats. Please refresh your memory from the comments on HIVE-2379.

I'm attaching an addendum that should fix this. To be sure, I'll test it out on a real cluster later this afternoon. If the Hive team doesn't want the additional input/output format snooping, we can use an alternative method I introduced in HBASE-9165 for the benefit of PIG-3285.;;;","12/Feb/14 20:00;sershe;addendum looks good to me;;;","12/Feb/14 20:06;ashutoshc;[~navis] Can you reupload the patch with correct name so Hive QA picks it up.;;;","13/Feb/14 00:42;navis;addendum is basically same with already applied patch(patch2) and it's not fixing this problem. If copying is that bad, I should use second method to fix this.;;;","13/Feb/14 01:42;ndimiduk;Patch 3 is not the addendum. My mistake, I thought patch2 had been committed.

bq. it's not fixing this problem

What exactly is the problem it's not fixing? Is the job failing due to missing dependencies? Which dependencies? Are they on the classpath of the job client?

I think copying really is that bad; it means this integration will break every time HBase changes it's dependencies. Not often, but it does happen. Instead, use the provided method and the correct dependencies will be added at runtime, everytime. Is this behavior undesirable for some reason?

What do you mean ""second method""?;;;","13/Feb/14 03:46;navis;bq. Is the job failing due to missing dependencies
Yes, it fails with ClassNotFoundException. 
{code}
org.apache.hadoop.hbase.mapred.TableMapReduceUtil.addDependencyJars(jobConf)
org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.addDependencyJars(jobConf, HBaseStorageHandler.class)
{code}
This adds zookeeper, netty, guava, hadoop, hive-exec, hive-hbase-handler to tmpJars, missing hbase related classes. 

bq. I think copying really is that bad
Agree. Just wanted quick, easy fix.

bq. What do you mean ""second method""?
In my comment above, ""Anyway, we can resolve this with patch1 (copy needed class list from TableMapReduceUtil) or make new Job instance and copy values from it to original JobConf"". You can see it in patch3.
;;;","13/Feb/14 20:20;ndimiduk;Which version of HBase are you testing against? [mapred.TableMapReduceUtil#addDependencyJars(JobConf)|https://github.com/apache/hbase/blob/0.96/hbase-server/src/main/java/org/apache/hadoop/hbase/mapred/TableMapReduceUtil.java#L298-L316] calls the [same method|https://github.com/apache/hbase/blob/0.96/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java#L586-L612] from the mapreduce package, which in turn adds all the hbase jars, plus the hbase dependencies you mention.

ClassNotFoundException thrown at the job client or in the job? At the client means your local classpath is missing something. In the job means it wasn't packaged properly. What's the missing class/jar?;;;","14/Feb/14 07:12;navis;Ah, you are using hbase-0.96.1, which is applied HBASE-9165. I got it. Hive in trunk uses hbase-0.96.0 (addHBaseDependencyJars() does not exist in it).

Should we upgrade hbase to 0.96.1? I prefer to support both versions by applying patch3 but I'm good with any way.;;;","18/Feb/14 21:59;ndimiduk;With my HBase hat on, I'd say the upgrade to 0.96.1 us prudent for both the API improvements and the performance improvements. Not sure how Hive folks feel about this.;;;","18/Feb/14 22:04;ndimiduk;Even on 0.96.0, both [mapred.TableMapReduceUtil#addDependencyJars(JobConf)|https://github.com/apache/hbase/blob/hbase-0.96.0/hbase-server/src/main/java/org/apache/hadoop/hbase/mapred/TableMapReduceUtil.java#L258-L276] and [mapreduce.TableMapReduceUtil#addDependencyJars(Job)|https://github.com/apache/hbase/blob/hbase-0.96.0/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java#L514-L544] handle the hbase dependencies for you. I'm back to not understanding what issue you're having :);;;","18/Feb/14 22:05;ashutoshc;I am also in favor of bumping HBase version. We already have code complexity supporting multiple versions of Hadoop, we don't worsen that by supporting multiple HBase version.;;;","19/Feb/14 00:33;navis;bq. Even on 0.96.0, both mapred.TableMapReduceUtil#addDependencyJars(JobConf) and mapreduce.TableMapReduceUtil#addDependencyJars(Job) handle the hbase dependencies for you

Really wish to call the method but all we got is JobConf, not Job and there is no way just to wrap existing JobConf(copies in it) with Job. So in patch3, I've create dummy Job instance to call it and copied values(in ""tmpJars"") in it to original JobConf. I've mentioned this once already and even quoted again as you requested explanation. 

bq. o.a.h.hbase.mapreduce.TableMapReduceUtil takes Job as parameter rather than JobConf
bq. ""..or make new Job instance and copy values from it to original JobConf"". You can see it in patch3.

I'm ok with fixing this in anyway (upgrade hbase to 0.96.1 or applying patch3), though I prefer the later way to support hbase-0.96.0, which is still in use by many companies.;;;","19/Feb/14 00:48;ndimiduk;bq. all we got is JobConf, not Job

So why not call mapred.TableMapReduceUtil#addDependencyJars(JobConf) ? It does what you need; that's why it's there.;;;","19/Feb/14 00:54;navis;In hbase-0.96.0, mapred.TableMapReduceUtil#addDependencyJars(JobConf) does not call ""org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.addHBaseDependencyJars(job)"" which is not exist.

{code}
public static void addDependencyJars(JobConf job) throws IOException {
    org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.addDependencyJars(
      job,
      org.apache.zookeeper.ZooKeeper.class,
      org.jboss.netty.channel.ChannelFactory.class,
      com.google.common.base.Function.class,
      com.google.protobuf.Message.class,
      job.getMapOutputKeyClass(),
      job.getMapOutputValueClass(),
      job.getOutputKeyClass(),
      job.getOutputValueClass(),
      job.getPartitionerClass(),
      job.getClass(""mapred.input.format.class"", TextInputFormat.class, InputFormat.class),
      job.getClass(""mapred.output.format.class"", TextOutputFormat.class, OutputFormat.class),
      job.getCombinerClass());
  }
{code}
Again, this only 
bq. adds zookeeper, netty, guava, hadoop, hive-exec, hive-hbase-handler to tmpJars, missing hbase related classes.;;;","19/Feb/14 01:31;ndimiduk;Yes, of course; I now see the issue. My sincerest apologies for belaboring an invalid point :(

In the interest of supporting as many users as possible, patch v3 seems the most prudent approach, with a note about deferring back to HBase methods once Hive moves on to 0.98.0.;;;","19/Feb/14 21:56;ashutoshc;Marking this as Patch Available. Lets get this in. Code changes are rather small to bump up hbase version.;;;","25/Feb/14 20:16;xuefuz;[~ashutoshc] Could we put a close on this? I understood that patch v3 is committed to trunk. Do we still need addendum patch to be committed, in order to close this JIRA?;;;","26/Feb/14 17:42;xuefuz;If we all agree that patch v3 is better, should we revert the following commit (which is patch v2) and commit patch v3 instead?

commit 57467b4fd8cc5acb5250b635f86c3115691cf367
Author: Ashutosh Chauhan <hashutosh@apache.org>
Date:   Tue Feb 4 23:42:30 2014 +0000

    HIVE-6356 : Dependency injection in hbase storage handler is broken (Navis v
;;;","26/Feb/14 19:10;ashutoshc;I think v3 is required in addition to v2. I am fine with committing v3 (with a comment in the code that problem is fixed in 0.96.1 and later, so those changes will not be required once Hive bumps up its hbase version).;;;","26/Feb/14 20:07;xuefuz;Thanks for the clarification, [~ashutoshc]. I will update the patch v3 with the necessary comments.;;;","26/Feb/14 20:26;xuefuz;Patch #4 is basically #3 with necessary comments.;;;","27/Feb/14 06:51;navis;+1;;;","28/Feb/14 09:34;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12631340/HIVE-6356.4.patch.txt

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5180 tests executed
*Failed tests:*
{noformat}
org.apache.hcatalog.hbase.snapshot.lock.TestWriteLock.testRun
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1551/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1551/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12631340;;;","28/Feb/14 19:32;xuefuz;The above test seems transient. I manually re-ran it and it passed. Reattach the same patch (v4) for another run.;;;","01/Mar/14 15:58;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12631814/HIVE-6356.4.patch.txt

{color:green}SUCCESS:{color} +1 5185 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1566/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1566/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12631814;;;","01/Mar/14 16:28;xuefuz;Patch committed to trunk. Thanks to Navis, Ashutosh, et al.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Some index test golden files produce non-deterministic stats in explain,HIVE-6354,12692760,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,03/Feb/14 01:27,22/Mar/14 06:31,14/Jul/23 06:14,05/Feb/14 21:48,,,,,,,,,,0.13.0,,,,,,0,,,,,hagleitn,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6267,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/14 02:47;hagleitn;HIVE-6354.1.patch;https://issues.apache.org/jira/secure/attachment/12626589/HIVE-6354.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,371355,,,,Wed Feb 05 21:48:42 UTC 2014,,,,,,,,,,"0|i1s013:",371658,,,,,,,,,,,,,,,,,,,,,"03/Feb/14 02:47;hagleitn;Moving it to minimr seems to work for me locally;;;","03/Feb/14 11:35;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626589/HIVE-6354.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 4994 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1162/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1162/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626589;;;","03/Feb/14 23:27;hagleitn;failed tests are flaky - unrelated to this check in.;;;","04/Feb/14 21:21;vikram.dixit;+1. LGTM;;;","05/Feb/14 21:48;hagleitn;Committed to trunk. Thanks for the review Vikram!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update hadoop-2 golden files after HIVE-6267,HIVE-6353,12692759,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,03/Feb/14 01:26,06/Feb/14 00:50,14/Jul/23 06:14,06/Feb/14 00:49,,,,,,,,,,0.13.0,,,,,,0,,,HIVE-6267 changed explain with lots of changes to golden files. Separate jira because of number of files changed. ,,hagleitn,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6267,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/14 09:26;hagleitn;HIVE-6353.1.patch;https://issues.apache.org/jira/secure/attachment/12626622/HIVE-6353.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,371354,,,,Thu Feb 06 00:50:09 UTC 2014,,,,,,,,,,"0|i1s00v:",371657,,,,,,,,,,,,,,,,,,,,,"03/Feb/14 20:10;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626622/HIVE-6353.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4997 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1168/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1168/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626622;;;","03/Feb/14 23:26;hagleitn;failure is unrelated.;;;","04/Feb/14 21:21;vikram.dixit;+1;;;","06/Feb/14 00:50;hagleitn;Committed to trunk. Thanks for the review [~vikram.dixit].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Order by/Sort by in subquery,HIVE-6348,12692632,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,lirui,hagleitn,hagleitn,01/Feb/14 02:00,06/May/20 18:03,14/Jul/23 06:14,30/Jun/17 08:09,,,,,,,,,,3.0.0,,,,,,0,sub-query,,"select * from (select * from foo order by c asc) bar order by c desc;

in hive sorts the data set twice. The optimizer should probably remove any order by/sort by in the sub query unless you use 'limit '. Could even go so far as barring it at the semantic level.

",,akatose,cartershanklin,h_o,hagleitn,jcamachorodriguez,leftyl,lirui,vgarg,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CALCITE-2798,,,,,,,,,,,,,,,,,,,,,"31/May/17 11:00;lirui;HIVE-6348.1.patch;https://issues.apache.org/jira/secure/attachment/12870560/HIVE-6348.1.patch","02/Jun/17 11:22;lirui;HIVE-6348.2.patch;https://issues.apache.org/jira/secure/attachment/12870969/HIVE-6348.2.patch","08/Jun/17 08:43;lirui;HIVE-6348.3.patch;https://issues.apache.org/jira/secure/attachment/12872025/HIVE-6348.3.patch","27/Jun/17 08:00;lirui;HIVE-6348.4.patch;https://issues.apache.org/jira/secure/attachment/12874638/HIVE-6348.4.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,371227,,,,Tue May 22 23:58:07 UTC 2018,,,,,,,,,,"0|i1rz8v:",371530,,,,,,,,,,,,,,,,,,,,,"31/May/17 12:01;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12870560/HIVE-6348.1.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 134 failed/errored test(s), 10804 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join0] (batchId=82)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join15] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join20] (batchId=83)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join31] (batchId=42)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_rp_auto_join0] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_rp_union] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_rp_windowing] (batchId=37)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_union] (batchId=72)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_windowing] (batchId=51)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[concat_op] (batchId=70)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[correlationoptimizer14] (batchId=33)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[decimal_3] (batchId=25)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[dynamic_rdd_cache] (batchId=51)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[groupby_distinct_samekey] (batchId=54)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[groupby_grouping_sets_grouping] (batchId=3)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[identity_project_remove_skip] (batchId=47)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[input20] (batchId=12)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[input33] (batchId=57)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[input3_limit] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[limit_pushdown_negative] (batchId=38)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[list_bucket_dml_8] (batchId=68)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[macro_duplicate] (batchId=38)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[mapreduce2] (batchId=22)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[mergejoin] (batchId=56)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[multi_insert_gby2] (batchId=37)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[multi_insert_gby3] (batchId=71)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[orc_ppd_schema_evol_2b] (batchId=14)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[order_by_expr_1] (batchId=65)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[order_by_expr_2] (batchId=62)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[partcols1] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[partition_date] (batchId=14)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[partition_timestamp] (batchId=34)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[partition_varchar1] (batchId=62)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[pcr] (batchId=57)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[pointlookup2] (batchId=77)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[pointlookup3] (batchId=6)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd2] (batchId=25)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_join4] (batchId=54)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_udf_case] (batchId=41)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[reducesink_dedup] (batchId=22)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[rename_external_partition_location] (batchId=46)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[truncate_column] (batchId=78)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[truncate_column_buckets] (batchId=24)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[union_paren] (batchId=45)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_data_types] (batchId=72)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_decimal_3] (batchId=19)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_decimal_round] (batchId=34)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_decimal_round_2] (batchId=22)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_groupby_reduce] (batchId=53)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_interval_1] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_non_string_partition] (batchId=32)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_tablesample_rows] (batchId=49)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vectorization_12] (batchId=10)
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver[udf_row_sequence] (batchId=231)
org.apache.hadoop.hive.cli.TestContribNegativeCliDriver.testCliDriver[invalid_row_sequence] (batchId=234)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[mapreduce2] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[auto_join0] (batchId=161)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[auto_join30] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cbo_gby] (batchId=152)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cbo_rp_gby] (batchId=156)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cbo_rp_semijoin] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cbo_semijoin] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cbo_subq_not_in] (batchId=154)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cbo_udf_udaf] (batchId=144)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cbo_union] (batchId=158)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cbo_windowing] (batchId=154)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[explainuser_1] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[identity_project_remove_skip] (batchId=154)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[leftsemijoin_mr] (batchId=146)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[mergejoin] (batchId=155)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[mrr] (batchId=146)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[multi_column_in] (batchId=147)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[multi_column_in_single] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[orc_ppd_schema_evol_2b] (batchId=146)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[ppr_pushdown] (batchId=152)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[reduce_deduplicate_extended] (batchId=156)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[special_character_in_tabnames_1] (batchId=153)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[subquery_in] (batchId=156)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[subquery_notin] (batchId=157)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[subquery_scalar] (batchId=152)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_join] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_join_tests] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_joins_explain] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_schema_evolution] (batchId=160)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_smb_1] (batchId=158)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_smb_main] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_data_types] (batchId=158)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_decimal_3] (batchId=147)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_decimal_round] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_decimal_round_2] (batchId=148)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_groupby_grouping_sets_grouping] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_groupby_reduce] (batchId=155)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_interval_1] (batchId=146)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_join30] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_number_compare_projection] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vectorization_12] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[leftsemijoin_mr] (batchId=167)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[truncate_column_buckets] (batchId=167)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[vector_non_string_partition] (batchId=98)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query39] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query64] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query91] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query98] (batchId=232)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join0] (batchId=137)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join15] (batchId=107)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join20] (batchId=138)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join30] (batchId=113)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join31] (batchId=119)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[cbo_gby] (batchId=119)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[cbo_semijoin] (batchId=116)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[cbo_subq_not_in] (batchId=121)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[cbo_udf_udaf] (batchId=101)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[cbo_union] (batchId=133)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[dynamic_rdd_cache] (batchId=123)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[identity_project_remove_skip] (batchId=121)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[leftsemijoin_mr] (batchId=107)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[mapreduce2] (batchId=110)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[multi_insert_gby2] (batchId=116)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[multi_insert_gby3] (batchId=132)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[pcr] (batchId=125)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_in] (batchId=127)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[tez_join_tests] (batchId=135)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[tez_joins_explain] (batchId=116)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_data_types] (batchId=133)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorization_12] (batchId=104)
org.apache.hadoop.hive.ql.TestTxnCommands2.testOrcNoPPD (batchId=268)
org.apache.hadoop.hive.ql.TestTxnCommands2.testOrcPPD (batchId=268)
org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdate.testOrcNoPPD (batchId=280)
org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdate.testOrcPPD (batchId=280)
org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdateAndVectorization.testOrcNoPPD (batchId=277)
org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdateAndVectorization.testOrcPPD (batchId=277)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5488/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5488/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5488/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 134 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12870560 - PreCommit-HIVE-Build;;;","02/Jun/17 11:26;lirui;Fix some tests in v2 patch.
The plan is to disallow order/sort by w/o limit in sub query in semantic level. We somehow have lots of such queries in qtest, and so they need to be updated.;;;","02/Jun/17 12:58;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12870969/HIVE-6348.2.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 68 failed/errored test(s), 10813 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join0] (batchId=82)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join15] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join20] (batchId=83)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join31] (batchId=42)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_rp_auto_join0] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_rp_windowing] (batchId=37)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_windowing] (batchId=51)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[concat_op] (batchId=70)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[correlationoptimizer14] (batchId=33)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[dynamic_rdd_cache] (batchId=51)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[groupby_distinct_samekey] (batchId=54)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[identity_project_remove_skip] (batchId=47)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[input20] (batchId=12)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[input33] (batchId=57)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[input3_limit] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[limit_pushdown_negative] (batchId=38)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[mapreduce2] (batchId=22)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[multi_insert_gby2] (batchId=37)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[multi_insert_gby3] (batchId=71)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[partcols1] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd2] (batchId=25)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_join4] (batchId=54)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[reducesink_dedup] (batchId=22)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[truncate_column_buckets] (batchId=24)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_data_types] (batchId=72)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_tablesample_rows] (batchId=49)
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver[udf_row_sequence] (batchId=231)
org.apache.hadoop.hive.cli.TestContribNegativeCliDriver.testCliDriver[invalid_row_sequence] (batchId=234)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[mapreduce2] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[auto_join0] (batchId=161)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[auto_join30] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cbo_gby] (batchId=152)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cbo_rp_gby] (batchId=156)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cbo_rp_semijoin] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cbo_semijoin] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cbo_windowing] (batchId=154)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[explainuser_1] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[identity_project_remove_skip] (batchId=154)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[leftsemijoin_mr] (batchId=146)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[mrr] (batchId=146)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[multi_column_in] (batchId=147)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[multi_column_in_single] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[reduce_deduplicate_extended] (batchId=156)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[special_character_in_tabnames_1] (batchId=153)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_join_tests] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_joins_explain] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_data_types] (batchId=158)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_join30] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_number_compare_projection] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[leftsemijoin_mr] (batchId=167)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[truncate_column_buckets] (batchId=167)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join0] (batchId=137)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join15] (batchId=107)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join20] (batchId=138)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join30] (batchId=113)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join31] (batchId=119)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[cbo_gby] (batchId=119)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[cbo_semijoin] (batchId=116)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[dynamic_rdd_cache] (batchId=123)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[identity_project_remove_skip] (batchId=121)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[leftsemijoin_mr] (batchId=107)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[mapreduce2] (batchId=110)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[multi_insert_gby2] (batchId=116)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[multi_insert_gby3] (batchId=132)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[tez_join_tests] (batchId=135)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[tez_joins_explain] (batchId=116)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_data_types] (batchId=133)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5516/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5516/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5516/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 68 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12870969 - PreCommit-HIVE-Build;;;","05/Jun/17 06:58;lirui;The latest failures are due to the sub-query order/sort by in our tests. I'd like to get some feedbacks before updating them.
cc [~hagleitn], [~ashutoshc], [~xuefuz]. Do you think the proposal makes sense?;;;","05/Jun/17 16:38;xuefuz;I'm wondering if it makes more sense to optimize the query rather than banning it. While it might be dumb and inefficient, I don't quite see anything wrong in semantics.;;;","05/Jun/17 17:21;ashutoshc;Indeed optimizing away inner query sort (without limit) is much more user friendly then throwing up exception.;;;","05/Jun/17 17:39;vgarg;I agree with [~ashutoshc] and [~xuefuz]. If we do insist on letting users know about order by/sort by IMHO showing a warning and then proceeding with the query or optimizing to remove order by would be better.;;;","05/Jun/17 19:44;cartershanklin;I don't think banning is a good idea, there's just no way to know what will break in user's environments.;;;","06/Jun/17 01:07;lirui;Thanks guys for the suggestions. Yeah I agree ignoring such order/sort by is better. Do you think I can just remove it from the AST?;;;","06/Jun/17 02:01;xuefuz;[~lirui], I think it's better to remove it from operator tree and the optimization can be put as one of the optimization rules.;;;","06/Jun/17 05:54;vgarg;I think it's better to remove it in AST or during logical plan generation. Because once HiveSubqueryRemoveRule is executed, which is the very first rule, subquery will be rewritten into join and there is no way to figure out if original query had a subquery.;;;","07/Jun/17 11:09;lirui;Hi [~xuefuz], [~vgarg], I also think it's easier to do this in AST. Once we have the OP tree, I'm not sure how to tell whether an order by belongs to sub queries. Besides, I also noted CBO may introduce synthetic sub queries. So I'll write a patch to do it in AST.;;;","07/Jun/17 16:39;xuefuz;Sounds good, [~lirui], [~vgarg]. As this is an optimization, I thought it can be put together with all these optimization rules. As to HiveSubqueryRemoveRule, removing order-by can be done right before that. Nevertheless, whatever makes it easy to implement.;;;","07/Jun/17 17:38;ashutoshc;I am not sure doing it on AST is better (or easier). AST is not amenable to traverse nor does it contain semantic info. Easier (and correct) way IMO would be to write a rule on calcite operator tree which matches on HiveSort followed by HiveSort check there is no limit in that sort and than removes that HiveSort from tree. 
SubQueryRemove rule will remove subqueries from tree by than, but I dont think that will matter. Essentially, on an operator tree you are looking for redundant Sort operators.;;;","08/Jun/17 08:42;lirui;Hi [~ashutoshc], I didn't put it in calcite because it won't work if CBO is disabled.
My plan is to do it after we've done {{genResolvedParseTree}} on the AST, at which point parse info of each QB is generated. To remove the order by, we can simply remove the corresponding AST node. I feel it more complicated to do it in operator tree, e.g. we have to detect sorting RS in sub queries, and after the RS is removed (actually that's probably not the only OP that needs be removed), we may also have to do some modifications to upstream/downstream operators.
Upload patch v3 for test and demonstrate my idea.;;;","08/Jun/17 18:46;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12872025/HIVE-6348.3.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 48 failed/errored test(s), 10820 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[insert_overwrite_local_directory_1] (batchId=237)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join0] (batchId=83)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join15] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join20] (batchId=83)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join31] (batchId=42)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_rp_auto_join0] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[concat_op] (batchId=70)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[correlationoptimizer14] (batchId=33)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[dynamic_rdd_cache] (batchId=51)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[groupby_distinct_samekey] (batchId=54)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[identity_project_remove_skip] (batchId=47)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[input20] (batchId=12)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[input33] (batchId=57)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[input3_limit] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[limit_pushdown_negative] (batchId=38)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[multi_insert_gby2] (batchId=37)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[multi_insert_gby3] (batchId=71)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd2] (batchId=25)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_join4] (batchId=54)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[truncate_column_buckets] (batchId=24)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_tablesample_rows] (batchId=49)
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver[udf_row_sequence] (batchId=231)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[auto_join0] (batchId=161)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[auto_join30] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[explainuser_1] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[identity_project_remove_skip] (batchId=154)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[mrr] (batchId=146)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[reduce_deduplicate_extended] (batchId=156)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_join_tests] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_joins_explain] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_join30] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_number_compare_projection] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[truncate_column_buckets] (batchId=167)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query78] (batchId=232)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join0] (batchId=137)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join15] (batchId=107)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join20] (batchId=138)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join30] (batchId=113)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join31] (batchId=119)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[dynamic_rdd_cache] (batchId=123)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[identity_project_remove_skip] (batchId=121)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[multi_insert_gby2] (batchId=116)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[multi_insert_gby3] (batchId=132)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[tez_join_tests] (batchId=135)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[tez_joins_explain] (batchId=116)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5583/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5583/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5583/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 48 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12872025 - PreCommit-HIVE-Build;;;","11/Jun/17 19:45;vgarg;[~ashutoshc] Plan generated after subquery remove rule/de-correlation doesn't generate HiveSortLimit on HiveSortLimit e.g. for query {code:sql} select * from part where p_size IN (select p_size from part p where p.p_type <> part.p_name order by p_size) {code} plan just after decorrelation looks like
{code:sql}
HiveProject(p_partkey=[$0], p_name=[$1], p_mfgr=[$2], p_brand=[$3], p_type=[$4], p_size=[$5], p_container=[$6], p_retailprice=[$7], p_comment=[$8])
  HiveProject(p_partkey=[$0], p_name=[$1], p_mfgr=[$2], p_brand=[$3], p_type=[$4], p_size=[$5], p_container=[$6], p_retailprice=[$7], p_comment=[$8], BLOCK__OFFSET__INSIDE__FILE=[$9], INPUT__FILE__NAME=[$10], ROW__ID=[$11])
    LogicalJoin(condition=[AND(<>($1, $13), =($5, $12))], joinType=[inner])
      HiveTableScan(table=[[default.part]], table:alias=[part])
      HiveAggregate(group=[{0, 1}])
        HiveProject(p_size=[$0], p_type0=[$1])
          HiveProject(p_size=[$0], p_type0=[$13])
            HiveSortLimit(sort0=[$0], dir0=[ASC-nulls-first])
              HiveProject(p_size=[$5], p_partkey=[$0], p_name=[$1], p_mfgr=[$2], p_brand=[$3], p_type=[$4], p_size1=[$5], p_container=[$6], p_retailprice=[$7], p_comment=[$8], block__offset__inside__file=[$9], input__file__name=[$10], row__id=[$11], p_type0=[$4])
                LogicalFilter(condition=[IS NOT NULL($4)])
                  HiveTableScan(table=[[default.part]], table:alias=[p])
{code}
So you have one sort limit on right side of join.  One possible rule could be if top project doesn't project any column/expression from right side then remove HiveSortLimit from right side of join.;;;","11/Jun/17 19:46;vgarg;[~lirui] Latest patch looks fine. Can you add tests?;;;","12/Jun/17 09:52;lirui;Hi [~vgarg], it turns out we do need the sub query being sorted in some cases. An example is input20.q:
{code}
EXPLAIN
FROM (
  FROM src
  MAP src.key, src.key 
  USING 'cat'
  DISTRIBUTE BY key
  SORT BY key, value
) tmap
INSERT OVERWRITE TABLE dest1
REDUCE tmap.key, tmap.value
USING 'python input20_script.py'
AS key, value;
{code}
The query plan is:
{noformat}
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: src
            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
            Select Operator
              expressions: key (type: string), key (type: string)
              outputColumnNames: _col0, _col1
              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
              Transform Operator
                command: cat
                output info:
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: string), _col1 (type: string)
                  sort order: ++
                  Map-reduce partition columns: _col0 (type: string)
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: string), _col1 (type: string)
      Reduce Operator Tree:
        Select Operator
          expressions: VALUE._col0 (type: string), VALUE._col1 (type: string)
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
          Transform Operator
            command: python input20_script.py
            output info:
                input format: org.apache.hadoop.mapred.TextInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
            Select Operator
              expressions: UDFToInteger(_col0) (type: int), _col1 (type: string)
              outputColumnNames: _col0, _col1
              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
              File Output Operator
                compressed: false
                Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                table:
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.dest1
{noformat}
The script {{input20_script.py}} counts the number of times each <Key, Value> appears and expects the input to be sorted. I'll consider how to handle such a case.;;;","12/Jun/17 23:31;ashutoshc;Turns out we already have such rules. They are HiveSortRemoveRule and HiveSortMergeRule. First thing to verify is why they are not able to optimize trees in these case.
[~lirui] CBO is now always enabled. It can only be turned off if user set config to false explicitly, but that is true for any other config in system. Even your patch adds a config, so if thats off, user won't get this optimization either.;;;","14/Jun/17 04:37;vgarg;{{HiveSortRemoveRule}} removes HiveSortLimit only if it is created by HiveSortJoinReduceRule ""If it is not created by HiveSortJoinReduceRule, we cannot remove it"" {{HiveSortMergeRule}} seems to merge two HiveSortLimit into one which isn't the case here. So I don't think there is any existing rule covering this case.;;;","20/Jun/17 11:21;lirui;Hi [~ashutoshc], [~vgarg], either doing this in CBO or not is OK to me. But I'm not sure how to handle cases like input20.q. Let's consider the following two queries:
The orderBy should be removed in this one:
{code}
from (select key from src order by key) tmap insert overwrite table dest select tmap.key;
{code}
While it shouldn't be removed in this one:
{code}
from (select key,value from src order by key,value) tmap insert overwrite table dest1 reduce tmap.key, tmap.value using 'python input20_script.py';
{code}
The two queries have very similar ASTs. Any suggestions how can we distinguish them? Maybe we can skip the optimization in case of scripts and UDFs. Is it correct to expect a specific order from a sub query in the first place?;;;","21/Jun/17 02:35;ashutoshc;I am not sure why can't order by removed in these cases. There is no contract that scripts and UDFs will see data in any particular order. So, its perfectly alright to remove sorts in such cases.;;;","21/Jun/17 08:56;lirui;[~ashutoshc], do you mean it's OK to break cases like input20? If so, can we go with v3 patch? Currently CBO won't run if query contains scripts or sortBy clause.;;;","26/Jun/17 15:12;ashutoshc;I don't think its 'breaking'  those cases. So, change should be ok.
[~lirui] Seems like some of q.out files needs update. Can you please update those? ;;;","27/Jun/17 08:09;lirui;Patch v4 updates the affected tests.
# I modified {{input20_script.py}} so it doesn't expect ordered input and gives the same results.
# I disabled the optimization in {{correlationoptimizer14.q}} and {{reduce_deduplicate_extended.q}}. Otherwise most of the test cases in these two qfiles become pointless.
# For {{concat_op.q}}, I removed the collect_list UDF in output. Because the column order in the list is not guaranteed.

Other golden files only have changes in query plan, not the results. Will add an RB for this.;;;","27/Jun/17 21:01;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12874638/HIVE-6348.4.patch

{color:green}SUCCESS:{color} +1 due to 9 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 12 failed/errored test(s), 10848 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join36] (batchId=82)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_smb_main] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=99)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query16] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query94] (batchId=232)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testBootstrapFunctionReplication (batchId=216)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testCreateFunctionIncrementalReplication (batchId=216)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testCreateFunctionWithFunctionBinaryJarsOnHDFS (batchId=216)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5790/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5790/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5790/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 12 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12874638 - PreCommit-HIVE-Build;;;","28/Jun/17 09:40;lirui;Latest failures are not related. [~vgarg], [~ashutoshc] could you take a look? Thanks.;;;","28/Jun/17 22:13;vgarg;[~lirui] Your patch removes order by from queries such as {code:sql}select key from (select key from src order by key) sub limit 1{code} I don't think it is safe to remove order by in this case.;;;","28/Jun/17 22:15;vgarg;{{udf_row_sequence.q}} test is another example where order by is removed for query {code:sql}select key, row_sequence() as r from (select key from src order by key) x order by r{code} which I don't think is correct since row_sequence() is fixed for a given key depending upon its order;;;","28/Jun/17 22:30;vgarg;Looks like I am wrong. According to https://mariadb.com/kb/en/mariadb/why-is-order-by-in-a-from-subquery-ignored/  
bq. A ""table"" (and subquery in the FROM clause too) is - according to the SQL standard - an unordered set of rows. Rows in a table (or in a subquery in the FROM clause) do not come in any specific order. That's why the optimizer can ignore the ORDER BY clause that you have specified. In fact, SQL standard does not even allow the ORDER BY clause to appear in this subquery;;;","28/Jun/17 23:10;vgarg;+1 Looks good to me;;;","29/Jun/17 02:43;lirui;Thanks [~vgarg] for reviewing. Yeah I think it's in compliance with the standard, though may ""break"" assumptions of some UDFs.
I'll commit it if other guys have no further comments.;;;","29/Jun/17 04:21;cartershanklin;It's trivia at this point but ORDER BY in subquery is allowed by optional feature F851 in the SQL standard, but there is no associated guarantee of ordering that goes with it.;;;","29/Jun/17 04:40;vgarg;[~cartershanklin] Good to know (y);;;","30/Jun/17 08:09;lirui;Pushed to master. Thanks guys for the review!;;;","03/Jul/17 06:45;leftyl;Doc note:  This adds *hive.remove.orderby.in.subquery* to HiveConf.java, so it needs to be documented in Configuration Properties.

* [Configuration Properties -- Query Execution | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-QueryandDDLExecution]

Do we also need general documentation?  If so, should it go in the Subqueries doc or the SortBy doc?

* [Subqueries | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+SubQueries]
* [Sort By / Order By / etc. | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+SortBy]

Added a TODOC3.0 label.;;;","05/Jul/17 02:38;lirui;Thanks [~leftylev] for the doc note. I've updated the configuration properties and order/sort by docs.;;;","08/Jul/17 02:36;leftyl;Thank you for the documentation, Rui Li.  I removed the TODOC3.0 label.

Here are the doc links:

* [Configuration Properties -- hive.remove.orderby.in.subquery | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.remove.orderby.in.subquery]
* [SortBy -- Syntax of Order By | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+SortBy#LanguageManualSortBy-SyntaxofOrderBy]
* [SortBy -- Syntax of Sort By | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+SortBy#LanguageManualSortBy-SyntaxofSortBy];;;","22/May/18 23:58;vgarg;Hive 3.0.0 has been released so closing this jira.;;;",,,,,,,,,,,,,,,,,,,,,
ZeroCopy read path for ORC RecordReader,HIVE-6347,12692598,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,gopalv,gopalv,gopalv,31/Jan/14 21:34,20/Jan/16 21:03,14/Jul/23 06:14,26/Feb/14 08:51,tez-branch,,,,,,,,,tez-branch,,File Formats,,,,0,,,"ORC can use the new HDFS Caching APIs and the ZeroCopy readers to avoid extra data copies into memory while scanning files.

Implement ORC zcr codepath and a hive.orc.zerocopy flag.",,aajisaka,aperepel,brocknoland,gopalv,hagleitn,leftyl,sztanko,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-10047,HIVE-6346,,HIVE-6382,,,,,,,,,,,HDFS-4949,HIVE-6360,HDFS-5957,,,,,,,,,,,,,,,,,,,,,,"01/Feb/14 04:42;gopalv;HIVE-6347.1.patch;https://issues.apache.org/jira/secure/attachment/12626451/HIVE-6347.1.patch","15/Feb/14 03:06;gopalv;HIVE-6347.2-tez.patch;https://issues.apache.org/jira/secure/attachment/12629191/HIVE-6347.2-tez.patch","15/Feb/14 03:12;gopalv;HIVE-6347.3-tez.patch;https://issues.apache.org/jira/secure/attachment/12629192/HIVE-6347.3-tez.patch","18/Feb/14 06:35;gopalv;HIVE-6347.4-tez.patch;https://issues.apache.org/jira/secure/attachment/12629484/HIVE-6347.4-tez.patch","26/Feb/14 07:28;gopalv;HIVE-6347.5-tez.patch;https://issues.apache.org/jira/secure/attachment/12631167/HIVE-6347.5-tez.patch",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,371193,,,,Wed Jan 20 21:03:17 UTC 2016,,,,,,,,,,"0|i1rz1b:",371496,ZeroCopy readers for the ORC file format,,,,,,,,,,,,,,,,,,,,"01/Feb/14 04:42;gopalv;Patch which applies over HIVE-6346 on hive/tez branch;;;","01/Feb/14 12:46;leftyl;The patch adds *hive.orc.zerocopy* to HiveConf.java, so it also needs to document the flag in hive-default.xml.template.;;;","01/Feb/14 13:39;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626451/HIVE-6347.1.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1151/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1151/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:399:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_ORDER KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:399:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_GROUP KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:399:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_MAP LPAREN"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:399:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_INSERT KW_INTO"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:399:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_LATERAL KW_VIEW"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:524:5: 
Decision can match input such as ""{AMPERSAND..BITWISEXOR, DIV..DIVIDE, EQUAL..EQUAL_NS, GREATERTHAN..GREATERTHANOREQUALTO, KW_AND, KW_ARRAY, KW_BETWEEN..KW_BOOLEAN, KW_CASE, KW_DOUBLE, KW_FLOAT, KW_IF, KW_IN, KW_INT, KW_LIKE, KW_MAP, KW_NOT, KW_OR, KW_REGEXP, KW_RLIKE, KW_SMALLINT, KW_STRING..KW_STRUCT, KW_TINYINT, KW_UNIONTYPE, KW_WHEN, LESSTHAN..LESSTHANOREQUALTO, MINUS..NOTEQUAL, PLUS, STAR, TILDE}"" using multiple alternatives: 1, 3

As a result, alternative(s) 3 were disabled for that input
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-exec ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-exec ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-exec ---
[INFO] Compiling 1546 source files to /data/hive-ptest/working/apache-svn-trunk-source/ql/target/classes
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] Note: Some input files use or override a deprecated API.
[WARNING] Note: Recompile with -Xlint:deprecation for details.
[WARNING] Note: Some input files use unchecked or unsafe operations.
[WARNING] Note: Recompile with -Xlint:unchecked for details.
[INFO] 4 warnings 
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/SnappyCodec.java:[21,48] cannot find symbol
symbol  : class DirectDecompressorShim
location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/SnappyCodec.java:[23,48] cannot find symbol
symbol  : class DirectCompressionType
location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[51,48] cannot find symbol
symbol  : class ByteBufferPoolShim
location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[52,48] cannot find symbol
symbol  : class ZeroCopyReaderShim
location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[100,64] cannot find symbol
symbol  : class ByteBufferPoolShim
location: class org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[96,17] cannot find symbol
symbol  : class ZeroCopyReaderShim
location: class org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ZlibCodec.java:[27,48] cannot find symbol
symbol  : class DirectCompressionType
location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ZlibCodec.java:[28,48] cannot find symbol
symbol  : class DirectDecompressorShim
location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/SnappyCodec.java:[82,11] cannot find symbol
symbol  : variable DirectCompressionType
location: class org.apache.hadoop.hive.ql.io.orc.SnappyCodec
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/SnappyCodec.java:[94,5] cannot find symbol
symbol  : class DirectDecompressorShim
location: class org.apache.hadoop.hive.ql.io.orc.SnappyCodec
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/SnappyCodec.java:[95,32] cannot find symbol
symbol  : variable DirectCompressionType
location: class org.apache.hadoop.hive.ql.io.orc.SnappyCodec
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[153,5] method does not override or implement a method from a supertype
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[165,5] method does not override or implement a method from a supertype
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[228,45] cannot find symbol
symbol  : method getZeroCopyReader(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ByteBufferAllocatorPool)
location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ZlibCodec.java:[95,11] cannot find symbol
symbol  : variable DirectCompressionType
location: class org.apache.hadoop.hive.ql.io.orc.ZlibCodec
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ZlibCodec.java:[107,5] cannot find symbol
symbol  : class DirectDecompressorShim
location: class org.apache.hadoop.hive.ql.io.orc.ZlibCodec
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ZlibCodec.java:[108,32] cannot find symbol
symbol  : variable DirectCompressionType
location: class org.apache.hadoop.hive.ql.io.orc.ZlibCodec
[INFO] 17 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [4.645s]
[INFO] Hive Ant Utilities ................................ SUCCESS [6.787s]
[INFO] Hive Shims Common ................................. SUCCESS [3.329s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.245s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [2.600s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [1.428s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [3.198s]
[INFO] Hive Shims ........................................ SUCCESS [0.590s]
[INFO] Hive Common ....................................... SUCCESS [7.734s]
[INFO] Hive Serde ........................................ SUCCESS [8.743s]
[INFO] Hive Metastore .................................... SUCCESS [26.982s]
[INFO] Hive Query Language ............................... FAILURE [40.846s]
[INFO] Hive Service ...................................... SKIPPED
[INFO] Hive JDBC ......................................... SKIPPED
[INFO] Hive Beeline ...................................... SKIPPED
[INFO] Hive CLI .......................................... SKIPPED
[INFO] Hive Contrib ...................................... SKIPPED
[INFO] Hive HBase Handler ................................ SKIPPED
[INFO] Hive HCatalog ..................................... SKIPPED
[INFO] Hive HCatalog Core ................................ SKIPPED
[INFO] Hive HCatalog Pig Adapter ......................... SKIPPED
[INFO] Hive HCatalog Server Extensions ................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client ................. SKIPPED
[INFO] Hive HCatalog Webhcat ............................. SKIPPED
[INFO] Hive HCatalog HBase Storage Handler ............... SKIPPED
[INFO] Hive HWI .......................................... SKIPPED
[INFO] Hive ODBC ......................................... SKIPPED
[INFO] Hive Shims Aggregator ............................. SKIPPED
[INFO] Hive TestUtils .................................... SKIPPED
[INFO] Hive Packaging .................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1:52.063s
[INFO] Finished at: Sat Feb 01 08:39:12 EST 2014
[INFO] Final Memory: 55M/422M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-exec: Compilation failure: Compilation failure:
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/SnappyCodec.java:[21,48] cannot find symbol
[ERROR] symbol  : class DirectDecompressorShim
[ERROR] location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/SnappyCodec.java:[23,48] cannot find symbol
[ERROR] symbol  : class DirectCompressionType
[ERROR] location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[51,48] cannot find symbol
[ERROR] symbol  : class ByteBufferPoolShim
[ERROR] location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[52,48] cannot find symbol
[ERROR] symbol  : class ZeroCopyReaderShim
[ERROR] location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[100,64] cannot find symbol
[ERROR] symbol  : class ByteBufferPoolShim
[ERROR] location: class org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[96,17] cannot find symbol
[ERROR] symbol  : class ZeroCopyReaderShim
[ERROR] location: class org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ZlibCodec.java:[27,48] cannot find symbol
[ERROR] symbol  : class DirectCompressionType
[ERROR] location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ZlibCodec.java:[28,48] cannot find symbol
[ERROR] symbol  : class DirectDecompressorShim
[ERROR] location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/SnappyCodec.java:[82,11] cannot find symbol
[ERROR] symbol  : variable DirectCompressionType
[ERROR] location: class org.apache.hadoop.hive.ql.io.orc.SnappyCodec
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/SnappyCodec.java:[94,5] cannot find symbol
[ERROR] symbol  : class DirectDecompressorShim
[ERROR] location: class org.apache.hadoop.hive.ql.io.orc.SnappyCodec
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/SnappyCodec.java:[95,32] cannot find symbol
[ERROR] symbol  : variable DirectCompressionType
[ERROR] location: class org.apache.hadoop.hive.ql.io.orc.SnappyCodec
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[153,5] method does not override or implement a method from a supertype
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[165,5] method does not override or implement a method from a supertype
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[228,45] cannot find symbol
[ERROR] symbol  : method getZeroCopyReader(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ByteBufferAllocatorPool)
[ERROR] location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ZlibCodec.java:[95,11] cannot find symbol
[ERROR] symbol  : variable DirectCompressionType
[ERROR] location: class org.apache.hadoop.hive.ql.io.orc.ZlibCodec
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ZlibCodec.java:[107,5] cannot find symbol
[ERROR] symbol  : class DirectDecompressorShim
[ERROR] location: class org.apache.hadoop.hive.ql.io.orc.ZlibCodec
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ZlibCodec.java:[108,32] cannot find symbol
[ERROR] symbol  : variable DirectCompressionType
[ERROR] location: class org.apache.hadoop.hive.ql.io.orc.ZlibCodec
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-exec
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626451;;;","01/Feb/14 17:51;brocknoland;There are a number of cases there the if contains a negation
but there is an else condition. Thse should be swapped:

e.g

if (!bool) else => if (bool) else

{noformat}
+      if(zcr != null) {
+        while(len > 0) {
+          ByteBuffer partial = zcr.readBuffer(len, false);
+          result.add(new BufferChunk(partial, off));
+          int read = partial.remaining();
+          len -= read;
+          off += read;
+        }
+      } else {
{noformat}

{noformat}
+      if (ShimLoader.getHadoopShims().getDirectDecompressor(
+          DirectCompressionType.SNAPPY) != null) {
+        direct = Boolean.valueOf(true);
+      } else {
+        direct = Boolean.valueOf(false);
+      }
{noformat};;;","05/Feb/14 21:38;gopalv;[~leftylev]: I will add the docs into the hive-site.xml in the next rev - was waiting for HIVE-6346 to get pushed to branch.

[~brocknoland]: a != null is actually positive test, because it implies there exists something (i.e it's presence). That is how rest of the ORC code deals with optional feature/args (like SARGs).;;;","15/Feb/14 03:06;gopalv;Address comments on review board (& added docs);;;","16/Feb/14 19:55;gopalv;munmap() is async and delayed action;;;","26/Feb/14 08:51;hagleitn;Committed to branch. Thanks [~gopalv];;;","09/Jun/14 10:21;leftyl;*hive.exec.orc.zerocopy* is documented in hive-default.xml.template as of Hive 0.13.0 with the description ""Use zerocopy reads with ORC."" See HIVE-6360.

In the wiki its description has been expanded to ""Use zerocopy reads with ORC. (This requires Hadoop 2.3 or later.)""

* [Configuration Properties: hive.exec.orc.zerocopy | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.orc.zerocopy];;;","20/Jan/16 16:46;sztanko;Hello,
I am running my cluster on Hadoop 2.7.1 (checksum fc0a1a23fc1868e4d5ee7fa2b28a58a), using Hive 1.2.1 (checksum ab480aca41b24a9c3751b8c023338231) and hive.exec.orc.zerocopy tends to cause failures.
All my hive queries run fines, but once I enable zerocopy, it seems to have some problems with native libraries:

{code}
set hive.exec.orc.zerocopy = true;
<execute my query>
Hadoop job information for Stage-1: number of mappers: 316; number of reducers: 90
2016-01-20 16:37:54,479 Stage-1 map = 0%,  reduce = 0%
2016-01-20 16:38:21,061 Stage-1 map = 100%,  reduce = 100%
2016-01-20 16:39:21,246 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_1452780282075_23380 with errors
Error during job, obtaining debugging information...
Diagnostic Messages for this Task:
Error: java.io.IOException: java.lang.reflect.InvocationTargetException
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:266)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.<init>(HadoopShimsSecure.java:213)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getRecordReader(HadoopShimsSecure.java:333)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:719)
	at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.<init>(MapTask.java:169)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:432)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:252)
	... 11 more
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.compress.snappy.SnappyDecompressor.decompressBytesDirect()I
	at org.apache.hadoop.io.compress.snappy.SnappyDecompressor.decompressBytesDirect(Native Method)
	at org.apache.hadoop.io.compress.snappy.SnappyDecompressor.decompressDirect(SnappyDecompressor.java:305)
	at org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor.decompress(SnappyDecompressor.java:341)
	at org.apache.hadoop.hive.shims.ZeroCopyShims$DirectDecompressorAdapter.decompress(ZeroCopyShims.java:101)
	at org.apache.hadoop.hive.ql.io.orc.SnappyCodec.directDecompress(SnappyCodec.java:100)
	at org.apache.hadoop.hive.ql.io.orc.SnappyCodec.decompress(SnappyCodec.java:67)
	at org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.readHeader(InStream.java:214)
	at org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.read(InStream.java:227)
	at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readValues(RunLengthIntegerReaderV2.java:54)
	at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.next(RunLengthIntegerReaderV2.java:302)
	at org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory$StringDictionaryTreeReader.readDictionaryLengthStream(TreeReaderFactory.java:1674)
	at org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory$StringDictionaryTreeReader.startStripe(TreeReaderFactory.java:1654)
	at org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory$StringTreeReader.startStripe(TreeReaderFactory.java:1382)
	at org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory$StructTreeReader.startStripe(TreeReaderFactory.java:2040)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:795)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:986)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1019)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:205)
	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:539)
	at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat$VectorizedOrcRecordReader.<init>(VectorizedOrcInputFormat.java:71)
	at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.getRecordReader(VectorizedOrcInputFormat.java:156)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.createVectorizedReader(OrcInputFormat.java:1088)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1102)
	at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.<init>(CombineHiveRecordReader.java:67)
	... 16 more

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
{code}
;;;","20/Jan/16 20:38;gopalv;{code}
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.compress.snappy.SnappyDecompressor.decompressBytesDirect()I
{code}

Does look like the tasks are not able to locate the libhadoop.so binary (*or* the hadoop build was done without snappy-dev available).

Zero copy readers don't work if libhadoop.so is missing anyway.

But to fill in some later developments, turning on zerocopy=true needs cluster-wide configs to turn on YARN-1775.

The YARN memory counting counts memory-mapped files as container memory, so without that change you might see containers being killed for using too much memory as you scale past the terabyte levels.;;;","20/Jan/16 21:03;sztanko;Thanks, [~gopalv],
Seems like tasks are not able to locate the libhadoop, indeed. However, they were able to do that when zerocopy was disabled (I am using snappy compression in my orc tables) and perform well. libhadoop.so and libsnappy are there and I never had any problem with those. This makes me think that there is a correlation between finding native libraries and zerocopy.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Hadoop-2.4.0 shims to hive-tez,HIVE-6346,12692595,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,gopalv,gopalv,gopalv,31/Jan/14 21:30,05/Feb/14 00:15,14/Jul/23 06:14,05/Feb/14 00:15,tez-branch,,,,,,,,,tez-branch,,Shims,,,,0,,,The HadoopShims needs new shim functions to add extra HDFS Caching functionality which is not available in 2.2.0 branch.,,gopalv,hagleitn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6347,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6360,,,,"01/Feb/14 02:55;gopalv;HIVE-6346.1.patch;https://issues.apache.org/jira/secure/attachment/12626444/HIVE-6346.1.patch","01/Feb/14 03:08;gopalv;HIVE-6346.2.patch;https://issues.apache.org/jira/secure/attachment/12626445/HIVE-6346.2.patch","04/Feb/14 06:12;gopalv;HIVE-6346.3.patch;https://issues.apache.org/jira/secure/attachment/12626834/HIVE-6346.3.patch","05/Feb/14 00:14;hagleitn;HIVE-6346.4.patch;https://issues.apache.org/jira/secure/attachment/12627036/HIVE-6346.4.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,371190,,,,Wed Feb 05 00:15:22 UTC 2014,,,,,,,,,,"0|i1rz0n:",371493,Add ZeroCopy and DirectDecompressor shims to make use of the HDFS zero-copy reads.,,,,,,,,,,,,,,,,,,,,"01/Feb/14 02:55;gopalv;Patch to add 0.23C shims to hive-shims & hive-exec.

Applies to hive/tez branch & pulls apache-snapshots to build against 2.4.0-SNAPSHOT release.;;;","01/Feb/14 11:57;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626445/HIVE-6346.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4993 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1147/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1147/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626445;;;","03/Feb/14 22:29;hagleitn;[~t3rmin4t0r]Comment on rb. Looks good so far. Biggest question I have is whether we need another shim version for that. Seems we could just upgrade 23 when 2.4 is out.

Also: The test failure is unrelated. Tests have been successful.;;;","04/Feb/14 06:12;gopalv;Removed new Shim and rolled the feature-set into a package protected class.

That class is loaded only if the HDFS features are available during shim load.;;;","04/Feb/14 08:29;hagleitn;Looks good to me. Another couple of nits on rb. I can change those when I commit (if that's fine with you).;;;","04/Feb/14 15:51;gopalv;Sure.

I just did some testing with hadooo-2.2.x release with zero-copy reads config option turned on, with a build compiled against 2.4.0. The feature gets turned off automatically without any class loader errors.;;;","04/Feb/14 16:34;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626834/HIVE-6346.3.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1180/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1180/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims-0.20 ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/src/test/resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-0.20 ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims-0.20 ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims-0.20 ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims-0.20 ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/hive-shims-0.20-0.13.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-0.20 ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/hive-shims-0.20-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.20/0.13.0-SNAPSHOT/hive-shims-0.20-0.13.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.20/0.13.0-SNAPSHOT/hive-shims-0.20-0.13.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims Secure Common 0.13.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-common-secure ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims-common-secure ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/src/main/resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-common-secure ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims-common-secure ---
[INFO] Compiling 12 source files to /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/classes
[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/src/main/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java uses or overrides a deprecated API.
[WARNING] Note: Recompile with -Xlint:deprecation for details.
[WARNING] Note: Some input files use unchecked or unsafe operations.
[WARNING] Note: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims-common-secure ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/src/test/resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-common-secure ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims-common-secure ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims-common-secure ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims-common-secure ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/hive-shims-common-secure-0.13.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-common-secure ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/hive-shims-common-secure-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-common-secure/0.13.0-SNAPSHOT/hive-shims-common-secure-0.13.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-common-secure/0.13.0-SNAPSHOT/hive-shims-common-secure-0.13.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims 0.20S 0.13.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-0.20S ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims-0.20S ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/src/main/resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-0.20S ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims-0.20S ---
[INFO] Compiling 3 source files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/classes
[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java uses or overrides a deprecated API.
[WARNING] Note: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims-0.20S ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/src/test/resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-0.20S ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims-0.20S ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims-0.20S ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims-0.20S ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/hive-shims-0.20S-0.13.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-0.20S ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/hive-shims-0.20S-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.20S/0.13.0-SNAPSHOT/hive-shims-0.20S-0.13.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.20S/0.13.0-SNAPSHOT/hive-shims-0.20S-0.13.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims 0.23 0.13.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[WARNING] The POM for org.apache.hadoop:hadoop-common:jar:2.3.0-SNAPSHOT is missing, no dependency information available
[WARNING] The POM for org.apache.hadoop:hadoop-hdfs:jar:2.3.0-SNAPSHOT is missing, no dependency information available
[WARNING] The POM for org.apache.hadoop:hadoop-hdfs:jar:tests:2.3.0-SNAPSHOT is missing, no dependency information available
[WARNING] The POM for org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.3.0-SNAPSHOT is missing, no dependency information available
[WARNING] The POM for org.apache.hadoop:hadoop-mapreduce-client-jobclient:jar:tests:2.3.0-SNAPSHOT is missing, no dependency information available
[WARNING] The POM for org.apache.hadoop:hadoop-yarn-api:jar:2.3.0-SNAPSHOT is missing, no dependency information available
[WARNING] The POM for org.apache.hadoop:hadoop-yarn-common:jar:2.3.0-SNAPSHOT is missing, no dependency information available
[WARNING] The POM for org.apache.hadoop:hadoop-yarn-client:jar:2.3.0-SNAPSHOT is missing, no dependency information available
[WARNING] The POM for org.apache.hadoop:hadoop-yarn-server-tests:jar:tests:2.3.0-SNAPSHOT is missing, no dependency information available
Downloading: http://www.datanucleus.org/downloads/maven2/org/xerial/snappy/snappy-java/1.0.4.1/snappy-java-1.0.4.1.jar
Downloading: http://repo.maven.apache.org/maven2/org/xerial/snappy/snappy-java/1.0.4.1/snappy-java-1.0.4.1.jar
Downloaded: http://repo.maven.apache.org/maven2/org/xerial/snappy/snappy-java/1.0.4.1/snappy-java-1.0.4.1.jar (973 KB at 483.9 KB/sec)
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [4.722s]
[INFO] Hive Ant Utilities ................................ SUCCESS [8.607s]
[INFO] Hive Shims Common ................................. SUCCESS [2.649s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.161s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [2.985s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [1.527s]
[INFO] Hive Shims 0.23 ................................... FAILURE [5.301s]
[INFO] Hive Shims ........................................ SKIPPED
[INFO] Hive Common ....................................... SKIPPED
[INFO] Hive Serde ........................................ SKIPPED
[INFO] Hive Metastore .................................... SKIPPED
[INFO] Hive Query Language ............................... SKIPPED
[INFO] Hive Service ...................................... SKIPPED
[INFO] Hive JDBC ......................................... SKIPPED
[INFO] Hive Beeline ...................................... SKIPPED
[INFO] Hive CLI .......................................... SKIPPED
[INFO] Hive Contrib ...................................... SKIPPED
[INFO] Hive HBase Handler ................................ SKIPPED
[INFO] Hive HCatalog ..................................... SKIPPED
[INFO] Hive HCatalog Core ................................ SKIPPED
[INFO] Hive HCatalog Pig Adapter ......................... SKIPPED
[INFO] Hive HCatalog Server Extensions ................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client ................. SKIPPED
[INFO] Hive HCatalog Webhcat ............................. SKIPPED
[INFO] Hive HCatalog HBase Storage Handler ............... SKIPPED
[INFO] Hive HWI .......................................... SKIPPED
[INFO] Hive ODBC ......................................... SKIPPED
[INFO] Hive Shims Aggregator ............................. SKIPPED
[INFO] Hive TestUtils .................................... SKIPPED
[INFO] Hive Packaging .................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 31.032s
[INFO] Finished at: Tue Feb 04 11:34:09 EST 2014
[INFO] Final Memory: 24M/59M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-shims-0.23: Could not resolve dependencies for project org.apache.hive.shims:hive-shims-0.23:jar:0.13.0-SNAPSHOT: The following artifacts could not be resolved: org.apache.hadoop:hadoop-common:jar:2.3.0-SNAPSHOT, org.apache.hadoop:hadoop-hdfs:jar:2.3.0-SNAPSHOT, org.apache.hadoop:hadoop-hdfs:jar:tests:2.3.0-SNAPSHOT, org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.3.0-SNAPSHOT, org.apache.hadoop:hadoop-mapreduce-client-jobclient:jar:tests:2.3.0-SNAPSHOT, org.apache.hadoop:hadoop-yarn-api:jar:2.3.0-SNAPSHOT, org.apache.hadoop:hadoop-yarn-common:jar:2.3.0-SNAPSHOT, org.apache.hadoop:hadoop-yarn-client:jar:2.3.0-SNAPSHOT, org.apache.hadoop:hadoop-yarn-server-tests:jar:tests:2.3.0-SNAPSHOT: Could not find artifact org.apache.hadoop:hadoop-common:jar:2.3.0-SNAPSHOT -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-shims-0.23
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626834;;;","05/Feb/14 00:15;hagleitn;Committed to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Beeline outputs error message when HIVE_AUX_JARS_PATH is set,HIVE-6340,12692361,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,xuefuz,xuefuz,xuefuz,30/Jan/14 18:35,05/Feb/14 22:01,14/Jul/23 06:14,05/Feb/14 22:01,0.10.0,0.11.0,0.12.0,0.9.0,,,,,,0.13.0,,CLI,,,,0,,,"To reproduce:
{code}
xzhang@xzdt:~/apache/hive3$ export HIVE_AUX_JARS_PATH=foo
xzhang@xzdt:~/apache/hive3$ ./packaging/target/apache-hive-0.13.0-SNAPSHOT-bin/apache-hive-0.13.0-SNAPSHOT-bin/bin/beeline 
-hiveconf (No such file or directory)
hive.aux.jars.path=file:/foo (No such file or directory)
Beeline version 0.13.0-SNAPSHOT by Apache Hive
{code}

This is related to HIVE-6173 (hiveconf option is not supported for beeline), but it has additional problem in hive script where option -hiveconf should be --hiveconf (double dashes).",,mdominguez@cloudera.com,prasadm,qwertymaniac,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6173,,,,,,,,,,,,,,,HIVE-6173,,,,,,"01/Feb/14 03:29;xuefuz;HIVE-6340.patch;https://issues.apache.org/jira/secure/attachment/12626446/HIVE-6340.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,370956,,,,Wed Feb 05 22:01:58 UTC 2014,,,,,,,,,,"0|i1rxlb:",371261,,,,,,,,,,,,,,,,,,,,,"01/Feb/14 13:36;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626446/HIVE-6340.patch

{color:green}SUCCESS:{color} +1 4993 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1149/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1149/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626446;;;","04/Feb/14 18:14;prasadm;+1
Looks fine to me.;;;","05/Feb/14 22:01;xuefuz;Patch committed to trunk. Thanks to Prasad for the review.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HIVE-5279 deprecated UDAF class without explanation/documentation/alternative,HIVE-6331,12692009,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,larsfrancke,larsfrancke,larsfrancke,29/Jan/14 11:05,13/Nov/14 19:43,14/Jul/23 06:14,06/Apr/14 23:26,,,,,,,,,,0.14.0,,,,,,0,,,"HIVE-5279 added a @Deprecated annotation to the {{UDAF}} class. The comment in that class says {quote}UDAF classes are REQUIRED to inherit from this class.{quote}

One of these two needs to be updated. Either remove the annotation or document why it was deprecated and what to use instead.

Unfortunately [~navis] did not leave any documentation about his intentions.

I'm happy to provide a patch once I know the intentions.",,larsfrancke,leftyl,navis,swarnim,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/14 09:35;larsfrancke;HIVE-5279.1.patch;https://issues.apache.org/jira/secure/attachment/12631184/HIVE-5279.1.patch","17/Mar/14 09:40;larsfrancke;HIVE-6331.2.patch;https://issues.apache.org/jira/secure/attachment/12635056/HIVE-6331.2.patch","21/Mar/14 10:02;larsfrancke;HIVE-6331.3.patch;https://issues.apache.org/jira/secure/attachment/12635984/HIVE-6331.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,370599,,,,Thu Nov 13 19:43:28 UTC 2014,,,,,,,,,,"0|i1rvg7:",370909,,,,,,,,,,,,,,,,,,,,,"03/Feb/14 00:53;navis;UDAF was deprecated long time before HIVE-5279 and I just added the annotation along with the patch but I should have fixed the javadoc also. Currently, implementing GenericUDAFResolver2 or extending AbstractGenericUDAFResolver is the standard way to implement aggregation function.;;;","26/Feb/14 09:35;larsfrancke;Adds deprecation notice and cleans up Javadoc a bit.

Thanks for the hint Navis;;;","26/Feb/14 09:37;larsfrancke;I accidentally named the patch after the wrong issue but it's still correct for this one.;;;","17/Mar/14 05:13;leftyl;Trivial review comments:

1.  ""support"" should be ""supports"" since UDAF is singular:
{code}
+ * Optional for a UDAF class (by implementing these two methods, the user declares
+ * that the UDAF support partial aggregations):
{code}

2.  Second <li> should be indented like the first one:

{code}
+ * <ol>
+ *   <li>Implement a single method called {@code evaluatePartial} that returns the PARTIAL aggregation result.
+ * {@code evaluatePartial} should never return {@code null} or an Exception will be thrown.</li>
+ * <li>Implement a single method called {@code aggregatePartial} that takes a PARTIAL
{code};;;","17/Mar/14 09:40;larsfrancke;This new patch addresses Lefty's comments;;;","18/Mar/14 21:04;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635056/HIVE-6331.2.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5411 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1868/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1868/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635056;;;","20/Mar/14 18:17;leftyl;+1 ;;;","20/Mar/14 20:03;swarnim;Can we not remove the statement ""UDAF classes are REQUIRED to inherit from this class."" completely. IMHO, the statement ""Base class for all User-defined Aggregation Function (UDAF) classes."" is good enough indication that new UDAFs are required to inherit from this class. The previous statement with the deprecation statement makes things a little confusing.;;;","21/Mar/14 10:02;larsfrancke;Incorporated latest comments;;;","23/Mar/14 23:41;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12635984/HIVE-6331.3.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5442 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority2
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1932/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1932/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12635984;;;","25/Mar/14 20:34;xuefuz;[~lars_francke] I'm wondering if the test failures are related to your change. I guess not, but could you confirm?;;;","04/Apr/14 23:34;larsfrancke;The patch is a doc only patch and as such shouldn't affect any tests. They seem unrelated to me.;;;","06/Apr/14 23:26;xuefuz;Patch committed to trunk. Thanks Lars.;;;","13/Nov/14 19:43;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive script should not overwrite AUX_CLASSPATH with HIVE_AUX_JARS_PATH if the latter is set,HIVE-6328,12691944,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,xuefuz,xuefuz,xuefuz,29/Jan/14 02:52,17/May/14 08:36,14/Jul/23 06:14,01/Feb/14 03:19,0.10.0,0.12.0,0.8.0,0.9.0,,,,,,0.13.0,,,,,,0,,,"Hive script (bin/hive) replaces the value of AUX_CLASSPATH with the value of HIVE_AUX_JARS_PATH if HIVE_AUX_JARS_PATH is defined. This is not desirable because user uses the former to include additional classes when starting hive, while using the latter to specify additional jars that are needed to run MR jobs. The problem can be demonstrated with the script snippet:
{code}
elif [ ""${HIVE_AUX_JARS_PATH}"" != """" ]; then
  HIVE_AUX_JARS_PATH=`echo $HIVE_AUX_JARS_PATH | sed 's/,/:/g'`
  if $cygwin; then
      HIVE_AUX_JARS_PATH=`cygpath -p -w ""$HIVE_AUX_JARS_PATH""`
      HIVE_AUX_JARS_PATH=`echo $HIVE_AUX_JARS_PATH | sed 's/;/,/g'`
  fi
  AUX_CLASSPATH=${HIVE_AUX_JARS_PATH}
  AUX_PARAM=""file://$(echo ${HIVE_AUX_JARS_PATH} | sed 's/:/,file:\/\//g')""
fi
{code}

AUX_CLASSPATH should be respected regardless whether HIVE_AUX_JARS_PATH is defined.",,leftyl,prasadm,swarnim,vgumashta,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-5410,,,,,,,,,,,,,,,,,,,,,,,,"29/Jan/14 03:35;xuefuz;HIVE-6328.patch;https://issues.apache.org/jira/secure/attachment/12625783/HIVE-6328.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,370534,,,,Sat May 17 08:36:42 UTC 2014,,,,,,,,,,"0|i1rv1r:",370844,,,,,,,,,,,,,,,,,,,,,"29/Jan/14 06:02;vgumashta;+1 (non-binding). Thanks a lot for the patch!;;;","29/Jan/14 06:18;swarnim;+1(non-binding). Was able to do a quick verification to demonstrate the working.;;;","29/Jan/14 08:16;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625783/HIVE-6328.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4972 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1090/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1090/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625783;;;","29/Jan/14 16:30;xuefuz;The above test failure doesn't appear related to the changes here. Manually run that test passed. Patch is good for review.;;;","30/Jan/14 18:46;prasadm;+1

Looks fine to me. Thanks for taking care of the issue.;;;","01/Feb/14 03:19;xuefuz;Patch committed to trunk. Thanks to Prasad for the review.;;;","21/Mar/14 19:07;leftyl;Has anyone updated the wiki with information about this jira & related jiras (HIVE-2269, HIVE-3978, HIVE-5363, HIVE-5410)?;;;","17/May/14 08:36;leftyl;Ping.

> Has anyone updated the wiki with information about this jira & related jiras (HIVE-2269, HIVE-3978, HIVE-5363, HIVE-5410)?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split generation in ORC may generate wrong split boundaries because of unaccounted padded bytes,HIVE-6326,12691836,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,prasanth_j,prasanth_j,prasanth_j,28/Jan/14 21:19,19/Feb/14 22:25,14/Jul/23 06:14,19/Feb/14 02:50,0.13.0,,,,,,,,,0.13.0,,Serializers/Deserializers,,,,0,orcfile,,"HIVE-5091 added padding to ORC files to avoid ORC stripes straddling HDFS blocks. The length of this padded bytes are not stored in stripe information. OrcInputFormat.getSplits() uses stripeInformation.getLength() for split computation. stripeInformation.getLength() is sum of index length, data length and stripe footer length. It does not account for the length of padded bytes which may result in wrong split boundary.

The fix for this is to use the offset of next stripe as the length of current stripe which includes the padded bytes as well.",,gates,leftyl,omalley,prasanth_j,rajesh.balamohan,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-5091,,,,,,,,,,,,,,,,,,,,,"29/Jan/14 02:39;prasanth_j;HIVE-6326.1.patch;https://issues.apache.org/jira/secure/attachment/12625771/HIVE-6326.1.patch","11/Feb/14 23:17;prasanth_j;HIVE-6326.2.patch;https://issues.apache.org/jira/secure/attachment/12628363/HIVE-6326.2.patch","14/Feb/14 23:19;prasanth_j;HIVE-6326.3.patch;https://issues.apache.org/jira/secure/attachment/12629155/HIVE-6326.3.patch","15/Feb/14 01:19;prasanth_j;HIVE-6326.4.patch;https://issues.apache.org/jira/secure/attachment/12629175/HIVE-6326.4.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,370483,,,,Wed Feb 19 02:50:31 UTC 2014,,,,,,,,,,"0|i1ruqf:",370793,,,,,,,,,,,,,,,,,,,,,"29/Jan/14 02:39;prasanth_j;Making it patch available for precommit tests.;;;","29/Jan/14 04:54;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625771/HIVE-6326.1.patch

{color:green}SUCCESS:{color} +1 4971 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1089/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1089/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625771;;;","11/Feb/14 23:17;prasanth_j;Added more protection code towards missing or null stripe statistics to protect against unexpected behaviour for old orc files or orc files written out of hive.;;;","13/Feb/14 00:55;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12628363/HIVE-6326.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5086 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1296/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1296/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12628363;;;","14/Feb/14 23:19;prasanth_j;The earlier patch made split computation unnecessarily complicated. Thanks [~owen.omalley] for pointing out. Uploading a new one.;;;","15/Feb/14 00:17;omalley;You don't need the change to the first line, since it is functionally equivalent to the new version.;;;","15/Feb/14 00:20;omalley;You may also want to protect line 732 with

{code}
if (sarg != null &&
   stripeStats != null &&
   idx < stripeStats.size() &&
   !isStripeSatisfyPredicate(...) {
{code};;;","15/Feb/14 01:19;prasanth_j;Incorporated [~owen.omalley] review comments.;;;","15/Feb/14 05:50;omalley;+1;;;","16/Feb/14 17:34;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12629175/HIVE-6326.4.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5120 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_truncate_column_buckets
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1342/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1342/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12629175;;;","18/Feb/14 21:16;prasanth_j;Test failures are not related to this issue.;;;","19/Feb/14 02:50;vikram.dixit;Committed to trunk. Thanks Prasanth!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix file_with_header_footer_negative.q,HIVE-6322,12691691,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,28/Jan/14 18:40,13/Nov/14 19:39,14/Jul/23 06:14,14/Mar/14 23:38,,,,,,,,,,0.14.0,,Tests,,,,0,,,"Looks like HIVE-6310 improperly fixed test file_with_header_footer_negative.q, I think the issue with that test was that it was using the wrong path for its input files (../data/files, rather than ../../data/files for the mavenized build)",,brocknoland,jdere,shuainie,thejas,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6323,,,,,,,,,,,,,,,,,,,,,"28/Jan/14 18:41;jdere;HIVE-6322.1.patch;https://issues.apache.org/jira/secure/attachment/12625622/HIVE-6322.1.patch","29/Jan/14 21:09;jdere;HIVE-6322.2.patch;https://issues.apache.org/jira/secure/attachment/12625971/HIVE-6322.2.patch","14/Mar/14 23:04;jdere;HIVE-6322.3.patch;https://issues.apache.org/jira/secure/attachment/12634860/HIVE-6322.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,370436,,,,Thu Nov 13 19:39:42 UTC 2014,,,,,,,,,,"0|i1ruin:",370757,,,,,,,,,,,,,,,,,,,,,"28/Jan/14 18:43;brocknoland;+1 pending test;;;","28/Jan/14 18:51;xuefuz;Well, it's interesting. The test result didn't reflect the error. BTW, HIVE-6310 didn't attempt to fix what is described here.;;;","28/Jan/14 21:24;shuainie;HIVE-6310 cover this problem by changing the expected output file to the wrong result. This should be the right fix.;;;","29/Jan/14 21:09;jdere;Looks like precommit test never ran, uploading patch again.;;;","30/Jan/14 16:14;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625971/HIVE-6322.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4972 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1114/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1114/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625971;;;","30/Jan/14 22:39;jdere;I doubt the failure in root_dir_external_table.q is related is related to the changes in this patch.;;;","14/Mar/14 23:04;jdere;Was about to commit this but had to update the diffs, as there are new pre/post hooks in the qfile output due to some of the auth changes. Patch v3 has the new qfile output changes.  I'll go ahead and commit if there are no objections.;;;","14/Mar/14 23:38;jdere;Committed to trunk.;;;","13/Nov/14 19:39;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hiveserver2 --help says Unrecognized option: -h,HIVE-6321,12691616,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,Fixed,,ragarwal,ragarwal,28/Jan/14 11:34,13/Nov/14 19:42,14/Jul/23 06:14,21/Mar/14 05:20,0.12.0,,,,,,,,,0.14.0,,HiveServer2,,,,0,,,"{code}
$ hiveserver2 --help
Starting HiveServer2
Unrecognized option: -h
usage: hiveserver2
 -H,--help                        Print help information
    --hiveconf <property=value>   Use value for given property
{code}",,ragarwal,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6709,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jan/14 11:35;ragarwal;HIVE-6321.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12625564/HIVE-6321.1.patch.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,370361,,,,Thu Nov 13 19:42:54 UTC 2014,,,,,,,,,,"0|i1rtxz:",370662,,,,,,,,,,,,,,,,,,,,,"28/Jan/14 21:12;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625564/HIVE-6321.1.patch.txt

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 4972 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1071/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1071/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625564;;;","21/Mar/14 05:20;ashutoshc;Fixed via HIVE-6709;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row-based ORC reader with PPD turned on dies on BufferUnderFlowException/IndexOutOfBoundsException ,HIVE-6320,12691533,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,prasanth_j,gopalv,gopalv,28/Jan/14 00:53,01/Aug/14 23:59,14/Jul/23 06:14,06/Feb/14 00:49,0.13.0,,,,,,,,,0.13.0,,Serializers/Deserializers,,,,0,orcfile,,"ORC data reader crashes out on a BufferUnderflowException, while trying to read data row-by-row with the predicate push-down enabled on current trunk.

*Stack trace:*
{code}
Caused by: java.nio.BufferUnderflowException
	at java.nio.Buffer.nextGetIndex(Buffer.java:472)
	at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:117)
	at org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.read(InStream.java:207)
	at org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readInts(SerializationUtils.java:450)
	at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readDirectValues(RunLengthIntegerReaderV2.java:240)
	at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readValues(RunLengthIntegerReaderV2.java:53)
	at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.next(RunLengthIntegerReaderV2.java:288)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$IntTreeReader.next(RecordReaderImpl.java:510)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StructTreeReader.next(RecordReaderImpl.java:1581)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.next(RecordReaderImpl.java:2707)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$OrcRecordReader.next(OrcInputFormat.java:125)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$OrcRecordReader.next(OrcInputFormat.java:101)
{code}
 OR it could be
{code}
Caused by: java.lang.IndexOutOfBoundsException
        at java.nio.ByteBuffer.wrap(ByteBuffer.java:352)
        at org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.readHeader(InStream.java:180)
        at org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.read(InStream.java:197)
        at org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readInts(SerializationUtils.java:450)
        at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readDirectValues(RunLengthIntegerReaderV2.java:252)
        at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readValues(RunLengthIntegerReaderV2.java:59)
        at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.next(RunLengthIntegerReaderV2.java:300)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$LongTreeReader.next(RecordReaderImpl.java:475)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StructTreeReader.next(RecordReaderImpl.java:1159)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.next(RecordReaderImpl.java:2198)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$OrcRecordReader.next(OrcInputFormat.java:108)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$OrcRecordReader.next(OrcInputFormat.java:57)
        at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:274)
        ... 15 more
{code}

The query run is 

{code}
set hive.vectorized.execution.enabled=false;
set hive.optimize.index.filter=true;

insert overwrite directory '/tmp/foo' select * from lineitem where l_orderkey is not null;
{code}

*Reason:*
The issue is related to generating the disk range boundaries. If two adjacent row groups have same compressed block offset then the worst case slop that was added to the end offset will contain only the current compression block. In some cases the values towards the end of this compression block will stretch beyond the boundary to fetch values causing BufferUnderFlowException or IndexOutOfBoundsException.",,gopalv,hagleitn,omalley,prasanth_j,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/14 21:10;prasanth_j;HIVE-6320.1.patch;https://issues.apache.org/jira/secure/attachment/12626175/HIVE-6320.1.patch","04/Feb/14 02:13;prasanth_j;HIVE-6320.2.patch;https://issues.apache.org/jira/secure/attachment/12626807/HIVE-6320.2.patch","03/Feb/14 23:15;prasanth_j;HIVE-6320.2.patch;https://issues.apache.org/jira/secure/attachment/12626768/HIVE-6320.2.patch","04/Feb/14 05:42;prasanth_j;HIVE-6320.3.patch;https://issues.apache.org/jira/secure/attachment/12626832/HIVE-6320.3.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,370278,,,,Thu Feb 06 00:49:26 UTC 2014,,,,,,,,,,"0|i1rtfr:",370579,,,,,,,,,,,,,,,,,,,,,"30/Jan/14 21:10;prasanth_j;The issue was related to generating the disk range boundaries. If two adjacent row groups have same compressed block offset then the worst case slop that was added to the end offset will contain only the current compression block. In some cases the values towards the end of this compression block will stretch beyond the boundary to fetch values causing BufferUnderFlowException. 

The attached patch extends this worst case slop boundary to safely accommodate the adjacent compression block.;;;","30/Jan/14 21:11;prasanth_j;Making it patch available for precommit tests.;;;","31/Jan/14 14:02;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626175/HIVE-6320.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4980 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1127/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1127/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626175;;;","31/Jan/14 18:57;prasanth_j;The test failures seems to be unrelated.;;;","03/Feb/14 22:53;omalley;Actually, you always need the next 2 compression blocks regardless of whether the compression blocks are the same for the two row groups.

The rest of the patch looks good.;;;","03/Feb/14 23:15;prasanth_j;Addressed [~owen.omalley] and [~gopalv]'s code review comments.;;;","04/Feb/14 02:13;prasanth_j;HIVE QA did not pickup the patch. Uploading again.;;;","04/Feb/14 05:24;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626807/HIVE-6320.2.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 4997 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_turnoff_hadoop20
org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testPartialPlanCompressed
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1171/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1171/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626807;;;","04/Feb/14 05:42;prasanth_j;Fixed TestRecordReaderImpl.java test failure. Other failures seems unrelated.;;;","04/Feb/14 11:47;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626832/HIVE-6320.3.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4997 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.common.type.TestDecimal128.testHighPrecisionDecimal128Multiply
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1174/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1174/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626832;;;","04/Feb/14 18:58;prasanth_j;The failed test is not related to this bug.;;;","05/Feb/14 00:16;omalley;+1;;;","06/Feb/14 00:49;hagleitn;Committed to trunk. Thanks [~prasanth_j] and [~owen.omalley].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MetaStoreDirectSql ctor should not throw,HIVE-6315,12691436,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,sershe,sershe,sershe,27/Jan/14 18:44,06/Feb/14 19:16,14/Jul/23 06:14,06/Feb/14 19:16,0.13.0,,,,,,,,,0.13.0,,,,,,0,,,"There are unprotected places in the ctor that may throw in some rare circumstances, it should never throw but rather self-disable.",,roshan_naik,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/14 18:13;sershe;HIVE-6315.01.patch;https://issues.apache.org/jira/secure/attachment/12626920/HIVE-6315.01.patch","27/Jan/14 18:53;sershe;HIVE-6315.patch;https://issues.apache.org/jira/secure/attachment/12625400/HIVE-6315.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,370181,,,,Thu Feb 06 19:16:51 UTC 2014,,,,,,,,,,"0|i1rsvb:",370483,,,,,,,,,,,,,,,,,,,,,"27/Jan/14 18:53;sershe;Simple patch. Also removed outdated todo that was already addressed;;;","27/Jan/14 19:12;roshan_naik;I see the following exception with this patch applied:

org.datanucleus.api.jdo.exceptions.ClassNotPersistenceCapableException: The class ""org.apache.hadoop.hive.metastore.model.MVersionTable"" is not persistable. This means that it either hasnt been enhanced, or that the enhanced version of the file is not in the CLASSPATH (or is hidden by an unenhanced version), or the Meta-Data/annotations for the class are not found.
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:380)
	at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:732)
	at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752)
	at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6025)
	at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:5935)
	at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:5913)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hive.metastore.RetryingRawStore.invoke(RetryingRawStore.java:122)
	at com.sun.proxy.$Proxy7.verifySchema(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:389)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:427)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:314)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:274)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4175)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:115)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:98)
	at org.apache.hive.streaming.TestStreaming.setup(TestStreaming.java:45)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:157)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:77)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:195)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:63)
NestedThrowablesStackTrace:
The class ""org.apache.hadoop.hive.metastore.model.MVersionTable"" is not persistable. This means that it either hasnt been enhanced, or that the enhanced version of the file is not in the CLASSPATH (or is hidden by an unenhanced version), or the Meta-Data/annotations for the class are not found.
org.datanucleus.exceptions.ClassNotPersistableException: The class ""org.apache.hadoop.hive.metastore.model.MVersionTable"" is not persistable. This means that it either hasnt been enhanced, or that the enhanced version of the file is not in the CLASSPATH (or is hidden by an unenhanced version), or the Meta-Data/annotations for the class are not found.
	at org.datanucleus.ExecutionContextImpl.assertClassPersistable(ExecutionContextImpl.java:5565)
	at org.datanucleus.ExecutionContextImpl.persistObjectInternal(ExecutionContextImpl.java:2030)
	at org.datanucleus.ExecutionContextImpl.persistObjectWork(ExecutionContextImpl.java:1972)
	at org.datanucleus.ExecutionContextImpl.persistObject(ExecutionContextImpl.java:1820)
	at org.datanucleus.ExecutionContextThreadedImpl.persistObject(ExecutionContextThreadedImpl.java:217)
	at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:727)
	at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752)
	at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6025)
	at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:5935)
	at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:5913)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hive.metastore.RetryingRawStore.invoke(RetryingRawStore.java:122)
	at com.sun.proxy.$Proxy7.verifySchema(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:389)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:427)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:314)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:274)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4175)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:115)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:98)
	at org.apache.hive.streaming.TestStreaming.setup(TestStreaming.java:45)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:157)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:77)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:195)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:63);;;","27/Jan/14 19:37;sershe;This is not related to this patch. This patch makes some code not throw, that's it.
Is it on the same setup where the ctor originally failed? Maybe the setup is broken and JDO/DataNucleus don't work at all?
Is this on Hive trunk, or does it have changes on top?;;;","28/Jan/14 03:53;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625400/HIVE-6315.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 4961 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_import_exported_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_reducers_power_two
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_load_hdfs_file_with_space_in_the_name
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_file_with_header_footer_negative
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1051/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1051/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625400;;;","28/Jan/14 19:19;sershe;[~ashutoshc] can you do a quick review?;;;","28/Jan/14 19:35;ashutoshc;+1;;;","28/Jan/14 22:36;sershe;Will commit tomorrow;;;","04/Feb/14 18:13;sershe;rebase patch after recent changes. I didn't have committer access last time I checked, let me try to commit this again after HiveQA runs;;;","04/Feb/14 21:27;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626920/HIVE-6315.01.patch

{color:green}SUCCESS:{color} +1 4997 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1186/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1186/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626920;;;","06/Feb/14 19:16;ashutoshc;Committed to trunk. Thanks, Sergey!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The logging (progress reporting) is too verbose,HIVE-6314,12691406,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,navis,sds,sds,27/Jan/14 16:14,06/Jan/15 17:46,14/Jul/23 06:14,28/Mar/14 20:36,,,,,,,,,,,,,,,,0,logger,,"The progress report is issued every second even when no progress have been made:
{code}
2014-01-27 10:35:55,209 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.68 sec
2014-01-27 10:35:56,678 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.68 sec
2014-01-27 10:35:59,344 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.68 sec
2014-01-27 10:36:01,268 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.67 sec
2014-01-27 10:36:03,149 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.67 sec
{code}
This pollutes the logs and the screen, and people do not appreciate it as much as the designers might have thought ([How do I limit log verbosity of hive?|http://stackoverflow.com/q/20849289/850781], [controlling the level of verbosity in Hive|http://stackoverflow.com/q/14121543/850781]).
It would be nice to be able to control the level of verbosity (but *not* by the {{-v}} switch!):
# Make sure that the progress report is only issued where there is something new to report; or
# Remove all the progress messages; or
# Make sure that progress is reported only every X sec (instead of every 1 second)",,gates,navis,rhbutani,sds,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jan/14 06:45;navis;HIVE-6314.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12625533/HIVE-6314.1.patch.txt","26/Mar/14 17:39;rhbutani;HIVE-6314.2.patch;https://issues.apache.org/jira/secure/attachment/12636958/HIVE-6314.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,370151,,,,Fri Mar 28 20:36:13 UTC 2014,,,,,,,,,,"0|i1rson:",370453,,,,,,,,,,,,,,,,,,,,,"28/Jan/14 06:48;navis;It was a bug. Hive should not log when there was no progress. 

set polling interval value bigger than 1000 (1sec, default) by set hive.exec.counters.pull.interval=<interval msec>;;;","28/Jan/14 19:37;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625533/HIVE-6314.1.patch.txt

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4932 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1070/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1070/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625533;;;","26/Mar/14 17:39;rhbutani;[~navis] patch didn't apply anymore; have updated it. Please check the update.;;;","26/Mar/14 23:51;gates;Ran tests on the latest patch, all looks good.;;;","27/Mar/14 11:45;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12636958/HIVE-6314.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5491 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1979/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1979/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12636958;;;","27/Mar/14 15:33;rhbutani;+1;;;","28/Mar/14 20:36;rhbutani;committed to trunk and 0.13
thanks Navis;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minimr tests in hadoop-1 hangs on shutdown,HIVE-6313,12691326,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,Fixed,navis,navis,navis,27/Jan/14 07:56,13/Nov/14 19:42,14/Jul/23 06:14,28/May/14 02:56,,,,,,,,,,0.14.0,,Tests,,,,0,,,It takes minutes after all tests run waiting for all task trackers shutdown. Just shutting down JobTracker after killing pending jobs seemed enough.,,navis,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/14 07:57;navis;HIVE-6313.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12625332/HIVE-6313.1.patch.txt","02/Apr/14 08:58;navis;HIVE-6313.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12638217/HIVE-6313.2.patch.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,370071,,,,Thu Nov 13 19:42:15 UTC 2014,,,,,,,,,,"0|i1rs6v:",370373,,,,,,,,,,,,,,,,,,,,,"27/Jan/14 10:26;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625332/HIVE-6313.1.patch.txt

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 4958 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_import_exported_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_reducers_power_two
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_load_hdfs_file_with_space_in_the_name
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_file_with_header_footer_negative
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1044/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1044/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625332;;;","02/Apr/14 23:40;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12638217/HIVE-6313.2.patch.txt

{color:green}SUCCESS:{color} +1 5541 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2084/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2084/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12638217;;;","27/May/14 05:02;navis;[~ashutoshc] Could you review this? The delay is a little annoying. ;;;","27/May/14 05:32;ashutoshc;+1;;;","28/May/14 02:56;ashutoshc;Committed to trunk. Thanks, Navis!;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
doAs with plain sasl auth should be session aware,HIVE-6312,12691303,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,navis,navis,navis,27/Jan/14 02:08,30/Dec/14 23:11,14/Jul/23 06:14,14/Mar/14 18:08,,,,,,,,,,0.13.0,,HiveServer2,,,,1,,,TUGIContainingProcessor creates new Subject for each invocation which induces FileSystem leakage when cache is enable(true by default).,,mdominguez@cloudera.com,navis,prasadm,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6663,,,,HIVE-6484,HIVE-4501,HIVE-5447,,,,,HIVE-9234,,,HIVE-6245,,,,,,,,,,,,,,,,,,,,,"27/Jan/14 02:10;navis;HIVE-6312.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12625302/HIVE-6312.1.patch.txt","28/Jan/14 01:20;navis;HIVE-6312.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12625494/HIVE-6312.2.patch.txt","13/Mar/14 15:01;thejas;HIVE-6312.3.patch.txt;https://issues.apache.org/jira/secure/attachment/12634445/HIVE-6312.3.patch.txt","14/Mar/14 08:12;thejas;HIVE-6312.4.patch.txt;https://issues.apache.org/jira/secure/attachment/12634672/HIVE-6312.4.patch.txt",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,370048,,,,Fri Mar 14 18:08:26 UTC 2014,,,,,,,,,,"0|i1rs1r:",370350,,,,,,,,,,,,,,,,,,,,,"27/Jan/14 02:12;navis;Running preliminary test;;;","27/Jan/14 04:10;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625302/HIVE-6312.1.patch.txt

{color:red}ERROR:{color} -1 due to 12 failed/errored test(s), 4914 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_import_exported_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_load_hdfs_file_with_space_in_the_name
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_parallel_orderby
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_file_with_header_footer_negative
org.apache.hive.beeline.TestBeeLineWithArgs.testEmbeddedBeelineConnection
org.apache.hive.jdbc.TestJdbcDriver2.org.apache.hive.jdbc.TestJdbcDriver2
org.apache.hive.service.auth.TestPlainSaslHelper.testDoAsSetting
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testExecuteStatement
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testExecuteStatementAsync
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testGetFunctions
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testOpenSession
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1037/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1037/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 12 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625302;;;","27/Jan/14 21:39;vgumashta;Hi [~navis]. Thanks a lot for the patch! Can you also upload it to rb since it's easier to read there?;;;","28/Jan/14 01:25;navis;[~vaibhavgumashta] Intended to make review board entry when all tests passed. Made one with on-going patch and thanks for your interest.;;;","28/Jan/14 18:03;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625494/HIVE-6312.2.patch.txt

{color:green}SUCCESS:{color} +1 4960 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1068/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1068/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625494;;;","12/Mar/14 10:33;vgumashta;[~navis] Left some minor comments on rb. How about we get rid of TUGIContainingProcessor class too since it won't be used anymore?;;;","13/Mar/14 12:43;thejas;The patch does not apply cleanly on trunk anymore. It would be great to have this in hive 0.13, since we are running out of time, I will go ahead and rebase it.
;;;","13/Mar/14 15:01;thejas;HIVE-6312.3.patch.txt - rebased patch, address my own minor comments .
The changes in the original patch look good. [~navis] can you please review my rebase changes ?
;;;","13/Mar/14 15:03;thejas;+1 to original changes. Will consider it a complete +1 once I get a review from [~navis] or another committer.
I am unable to upload to the existing reviewboard .
;;;","13/Mar/14 15:15;thejas;I have kicked off a pre-commit test build.
;;;","14/Mar/14 03:39;navis;[~thejas] Sorry for the delay. Look good to me. +1;;;","14/Mar/14 03:48;vgumashta;[~thejas] Should we also get rid of TUGIContainingProcessor?;;;","14/Mar/14 05:54;prasadm;[~thejas] The TUGIContainingProcessor related changes are in fact already in trunk via HIVE-5155. You might want to rebase the patch to see if there are conflicts.;;;","14/Mar/14 06:06;thejas;[~prasadm] Yes, the patch from Navis didn't apply on post HIVE-5155 trunk, that is why I had to rebase it. HIVE-6312.3.patch.txt is the rebased patch I uploaded. It is not in the reviewboard link that Navis created, because I can't upload it there.

;;;","14/Mar/14 06:28;prasadm;ah ok. I only looked at the review board and not the latest patch. sorry about that.
Updated changes look fine to me.

+1
;;;","14/Mar/14 06:57;thejas;[~vgumashta] Yes, I think it makes sense to remove TUGIContainingProcessor. I will create a followup patch for it.
;;;","14/Mar/14 07:05;thejas;Thanks for pointing that out Vaibhav. Created HIVE-6663;;;","14/Mar/14 08:06;navis;[~thejas] How about just removing it in here?;;;","14/Mar/14 08:12;thejas;HIVE-6312.4.patch.txt - removes TUGIContainingProcessor;;;","14/Mar/14 08:16;thejas;Navis, I included it in a different jira because I wasn't sure if I would get a +1 in time. Somehow thought that its late in night there (Seoul?) and you might not be around ! :)
;;;","14/Mar/14 08:43;navis;It's about the time to go home :)
I've checked TUGIContainingProcessor is not included in any other class or document. Thanks again for a review.;;;","14/Mar/14 17:30;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12634672/HIVE-6312.4.patch.txt

{color:green}SUCCESS:{color} +1 5394 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1773/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1773/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12634672;;;","14/Mar/14 18:08;thejas;Patch committed to trunk and 0.13 branch (this is in the 0.13 jira list).
Thanks Navis!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix a few minimr test failures,HIVE-6310,12691286,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,xuefuz,xuefuz,xuefuz,26/Jan/14 19:15,22/Mar/14 06:33,14/Jul/23 06:14,28/Jan/14 15:01,0.13.0,,,,,,,,,0.13.0,,Testing Infrastructure,,,,0,,,"These test cases are:
{code}
ql/src/test/queries/clientpositive/import_exported_table.q
ql/src/test/queries/clientpositive/load_hdfs_file_with_space_in_the_name.q
ql/src/test/queries/clientpositive/root_dir_external_table.q
{code}

They are failing because of existing hdfs:///tmp/test, possible left over by other tests.",,brocknoland,jdere,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/14 17:19;xuefuz;HIVE-6310.1.patch;https://issues.apache.org/jira/secure/attachment/12625381/HIVE-6310.1.patch","27/Jan/14 23:03;xuefuz;HIVE-6310.2.patch;https://issues.apache.org/jira/secure/attachment/12625469/HIVE-6310.2.patch","26/Jan/14 19:18;xuefuz;HIVE-6310.patch;https://issues.apache.org/jira/secure/attachment/12625283/HIVE-6310.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,370031,,,,Tue Jan 28 15:01:44 UTC 2014,,,,,,,,,,"0|i1rrxz:",370333,,,,,,,,,,,,,,,,,,,,,"26/Jan/14 19:52;brocknoland;+1 pending tests;;;","26/Jan/14 21:10;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625283/HIVE-6310.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 4958 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_import_exported_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_reducers_power_two
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_file_with_header_footer_negative
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1035/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1035/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625283;;;","27/Jan/14 17:19;xuefuz;Test failures above seems strange. All of them passed when being manually started. file_with_header_footer_negative.q seems suffering the same problem as the previous patch was to address. Thus, the new patch, #1, includes fix for that too.;;;","27/Jan/14 19:13;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625381/HIVE-6310.1.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 4961 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_import_exported_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_reducers_power_two
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1045/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1045/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625381;;;","27/Jan/14 23:03;xuefuz;Patch #2 updated with fix for import_exported_table.q. Still having no clue why the other two tests are failing. Will have to exclude them if they keep failing.;;;","28/Jan/14 08:50;jdere;For test file_with_header_footer_negative.q, it looks like the issue is that the .q file has bad paths in it. Try with these changes and you don't need to update the .q.out file:

{noformat}
-dfs -copyFromLocal ../data/files/header_footer_table_1 hdfs:///tmp/test/header_footer_table_1;
+dfs -copyFromLocal ../../data/files/header_footer_table_1 hdfs:///tmp/test/header_footer_table_1;
 
-dfs -copyFromLocal ../data/files/header_footer_table_2 hdfs:///tmp/test/header_footer_table_2;
+dfs -copyFromLocal ../../data/files/header_footer_table_2 hdfs:///tmp/test/header_footer_table_2;
{noformat};;;","28/Jan/14 13:24;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625469/HIVE-6310.2.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 4961 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_reducers_power_two
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1059/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1059/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625469;;;","28/Jan/14 14:24;brocknoland;I checked on the slave nodes and there is plenty of space.

{noformat}
[hiveptest@ip-10-74-94-52 ~]$ df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/xvde1             40G  4.7G   33G  13% /
{noformat}

Below is the error messages for the two failed tests. Both times there is a runtime exception followed by some hadoop exceptions.

http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-1059/failed/TestMinimrCliDriver-infer_bucket_sort_reducers_power_two.q/source/itests/qtest/target/tmp/history/done/version-1/localhost_1390912810566_/2014/01/28/000000/job_20140128044010447_0005_1390912971033_hiveptest_--+Test+join+on+three+tables+on+d...c.value%2529%2528Stage

{noformat}
Task TASKID=""task_20140128044010447_0005_r_000000"" TASK_TYPE=""REDUCE"" START_TIME=""1390912978019"" SPLITS="""" .
Task TASKID=""task_20140128044010447_0005_r_000001"" TASK_TYPE=""REDUCE"" START_TIME=""1390912978081"" SPLITS="""" .
Task TASKID=""task_20140128044010447_0005_r_000002"" TASK_TYPE=""REDUCE"" START_TIME=""1390912978117"" SPLITS="""" .
ReduceAttempt TASK_TYPE=""REDUCE"" TASKID=""task_20140128044010447_0005_r_000000"" TASK_ATTEMPT_ID=""attempt_20140128044010447_0005_r_000000_0"" START_TIME=""1390912978026"" TRACKER_NAME=""tracker_host0\.foo\.com:localhost/127\.0\.0\.1:60535"" HTTP_PORT=""53633"" LOCALITY=""OFF_SWITCH"" AVATAAR=""VIRGIN"" .
ReduceAttempt TASK_TYPE=""REDUCE"" TASKID=""task_20140128044010447_0005_r_000000"" TASK_ATTEMPT_ID=""attempt_20140128044010447_0005_r_000000_0"" TASK_STATUS=""FAILED"" FINISH_TIME=""1390912983112"" HOSTNAME=""host0\.foo\.com"" ERROR=""java\.lang\.IllegalArgumentException: Null user
	at org\.apache\.hadoop\.security\.UserGroupInformation\.createRemoteUser(UserGroupInformation\.java:887)
	at org\.apache\.hadoop\.mapred\.Child\.main(Child\.java:241)
"" .
MapAttempt TASK_TYPE=""MAP"" TASKID=""task_20140128044010447_0005_m_000000"" TASK_ATTEMPT_ID=""attempt_20140128044010447_0005_m_000000_0"" START_TIME=""1390912974953"" TRACKER_NAME=""tracker_host0\.foo\.com:localhost/127\.0\.0\.1:60535"" HTTP_PORT=""53633"" LOCALITY=""RACK_LOCAL"" AVATAAR=""VIRGIN"" .
MapAttempt TASK_TYPE=""MAP"" TASKID=""task_20140128044010447_0005_m_000000"" TASK_ATTEMPT_ID=""attempt_20140128044010447_0005_m_000000_0"" TASK_STATUS=""FAILED"" FINISH_TIME=""1390912977965"" HOSTNAME=""host0\.foo\.com"" ERROR=""Map output lost, rescheduling: getMapOutput(attempt_20140128044010447_0005_m_000000_0,2) failed :
org\.apache\.hadoop\.util\.DiskChecker$DiskErrorException: Could not find taskTracker/hiveptest/jobcache/job_20140128044010447_0005/attempt_20140128044010447_0005_m_000000_0/output/file\.out\.index in any of the configured local directories
	at org\.apache\.hadoop\.fs\.LocalDirAllocator$AllocatorPerContext\.getLocalPathToRead(LocalDirAllocator\.java:429)
	at org\.apache\.hadoop\.fs\.LocalDirAllocator\.getLocalPathToRead(LocalDirAllocator\.java:160)
	at org\.apache\.hadoop\.mapred\.TaskTracker$MapOutputServlet\.doGet(TaskTracker\.java:4053)
	at javax\.servlet\.http\.HttpServlet\.service(HttpServlet\.java:707)
	at javax\.servlet\.http\.HttpServlet\.service(HttpServlet\.java:820)
	at org\.mortbay\.jetty\.servlet\.ServletHolder\.handle(ServletHolder\.java:511)
	at org\.mortbay\.jetty\.servlet\.ServletHandler$CachedChain\.doFilter(ServletHandler\.java:1221)
	at org\.apache\.hadoop\.http\.HttpServer$QuotingInputFilter\.doFilter(HttpServer\.java:914)
	at org\.mortbay\.jetty\.servlet\.ServletHandler$CachedChain\.doFilter(ServletHandler\.java:1212)
	at org\.mortbay\.jetty\.servlet\.ServletHandler\.handle(ServletHandler\.java:399)
	at org\.mortbay\.jetty\.security\.SecurityHandler\.handle(SecurityHandler\.java:216)
	at org\.mortbay\.jetty\.servlet\.SessionHandler\.handle(SessionHandler\.java:182)
	at org\.mortbay\.jetty\.handler\.ContextHandler\.handle(ContextHandler\.java:766)
	at org\.mortbay\.jetty\.webapp\.WebAppContext\.handle(WebAppContext\.java:450)
	at org\.mortbay\.jetty\.handler\.ContextHandlerCollection\.handle(ContextHandlerCollection\.java:230)
	at org\.mortbay\.jetty\.handler\.HandlerWrapper\.handle(HandlerWrapper\.java:152)
	at org\.mortbay\.jetty\.Server\.handle(Server\.java:322)
	at org\.mortbay\.jetty\.HttpConnection\.handleRequest(HttpConnection\.java:542)
	at org\.mortbay\.jetty\.HttpConnection$RequestHandler\.headerComplete(HttpConnection\.java:928)
	at org\.mortbay\.jetty\.HttpParser\.parseNext(HttpParser\.java:549)
	at org\.mortbay\.jetty\.HttpParser\.parseAvailable(HttpParser\.java:212)
	at org\.mortbay\.jetty\.HttpConnection\.handle(HttpConnection\.java:404)
	at org\.mortbay\.io\.nio\.SelectChannelEndPoint\.run(SelectChannelEndPoint\.java:410)
	at org\.mortbay\.thread\.QueuedThreadPool$PoolThread\.run(QueuedThreadPool\.java:582)
"" .
{noformat}

http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-1059/failed/TestMinimrCliDriver-external_table_with_space_in_location_path.q-infer_bucket_sort_merge.q-auto_sortmerge_join_16.q-and-1-more/source/itests/qtest/target/tmp/history/done/version-1/localhost_1390912806507_/2014/01/28/000000/job_20140128044006356_0003_1390912970454_hiveptest_select+%250Aa.key+%252C+%250Aa.value+%252C+%250Ab.value...b.key%2529%2528Stage

{noformat}
MapAttempt TASK_TYPE=""MAP"" TASKID=""task_20140128044006356_0003_m_000003"" TASK_ATTEMPT_ID=""attempt_20140128044006356_0003_m_000003_0"" START_TIME=""1390912978252"" TRACKER_NAME=""tracker_host2\.foo\.com:localhost/127\.0\.0\.1:58706"" HTTP_PORT=""37104"" LOCALITY=""RACK_LOCAL"" AVATAAR=""VIRGIN"" .
MapAttempt TASK_TYPE=""MAP"" TASKID=""task_20140128044006356_0003_m_000003"" TASK_ATTEMPT_ID=""attempt_20140128044006356_0003_m_000003_0"" TASK_STATUS=""FAILED"" FINISH_TIME=""1390912987227"" HOSTNAME=""host2\.foo\.com"" ERROR=""java\.lang\.NullPointerException
	at org\.apache\.hadoop\.hive\.ql\.io\.HiveInputFormat\.init(HiveInputFormat\.java:257)
	at org\.apache\.hadoop\.hive\.ql\.io\.HiveInputFormat\.pushProjectionsAndFilters(HiveInputFormat\.java:439)
	at org\.apache\.hadoop\.hive\.ql\.io\.HiveInputFormat\.pushProjectionsAndFilters(HiveInputFormat\.java:432)
	at org\.apache\.hadoop\.hive\.ql\.io\.BucketizedHiveInputFormat\.getRecordReader(BucketizedHiveInputFormat\.java:71)
	at org\.apache\.hadoop\.mapred\.MapTask$TrackedRecordReader\.<init>(MapTask\.java:191)
	at org\.apache\.hadoop\.mapred\.MapTask\.runOldMapper(MapTask\.java:412)
	at org\.apache\.hadoop\.mapred\.MapTask\.run(MapTask\.java:366)
	at org\.apache\.hadoop\.mapred\.Child$4\.run(Child\.java:255)
	at java\.security\.AccessController\.doPrivileged(Native Method)
	at javax\.security\.auth\.Subject\.doAs(Subject\.java:396)
	at org\.apache\.hadoop\.security\.UserGroupInformation\.doAs(UserGroupInformation\.java:1190)
	at org\.apache\.hadoop\.mapred\.Child\.main(Child\.java:249)
"" .
MapAttempt TASK_TYPE=""MAP"" TASKID=""task_20140128044006356_0003_m_000006"" TASK_ATTEMPT_ID=""attempt_20140128044006356_0003_m_000006_0"" START_TIME=""1390912978652"" TRACKER_NAME=""tracker_host1\.foo\.com:localhost/127\.0\.0\.1:59914"" HTTP_PORT=""45568"" LOCALITY=""RACK_LOCAL"" AVATAAR=""VIRGIN"" .
MapAttempt TASK_TYPE=""MAP"" TASKID=""task_20140128044006356_0003_m_000006"" TASK_ATTEMPT_ID=""attempt_20140128044006356_0003_m_000006_0"" TASK_STATUS=""FAILED"" FINISH_TIME=""1390912987977"" HOSTNAME=""host1\.foo\.com"" ERROR=""org\.apache\.hadoop\.util\.DiskChecker$DiskErrorException: Could not find any valid local directory for output/spill0\.out
	at org\.apache\.hadoop\.fs\.LocalDirAllocator$AllocatorPerContext\.getLocalPathForWrite(LocalDirAllocator\.java:381)
	at org\.apache\.hadoop\.fs\.LocalDirAllocator\.getLocalPathForWrite(LocalDirAllocator\.java:146)
	at org\.apache\.hadoop\.fs\.LocalDirAllocator\.getLocalPathForWrite(LocalDirAllocator\.java:127)
	at org\.apache\.hadoop\.mapred\.MapOutputFile\.getSpillFileForWrite(MapOutputFile\.java:121)
	at org\.apache\.hadoop\.mapred\.MapTask$MapOutputBuffer\.sortAndSpill(MapTask\.java:1397)
	at org\.apache\.hadoop\.mapred\.MapTask$MapOutputBuffer\.flush(MapTask\.java:1303)
	at org\.apache\.hadoop\.mapred\.MapTask\.runOldMapper(MapTask\.java:431)
	at org\.apache\.hadoop\.mapred\.MapTask\.run(MapTask\.java:366)
	at org\.apache\.hadoop\.mapred\.Child$4\.run(Child\.java:255)
	at java\.security\.AccessController\.doPrivileged(Native Method)
	at javax\.security\.auth\.Subject\.doAs(Subject\.java:396)
	at org\.apache\.hadoop\.security\.UserGroupInformation\.doAs(UserGroupInformation\.java:1190)
	at org\.apache\.hadoop\.mapred\.Child\.main(Child\.java:249)
{noformat};;;","28/Jan/14 14:28;xuefuz;Having made some progress after all. Two failure remains. 

infer_bucket_sort_reducers_power_two:
{code}
Begin query: infer_bucket_sort_reducers_power_two.q
Error during job, obtaining debugging information...
Job Tracking URL: http://localhost:37148/jobdetails.jsp?jobid=job_20140128044010447_0005
Examining task ID: task_20140128044010447_0005_m_000002 (and more) from job job_20140128044010447_0005
Examining task ID: task_20140128044010447_0005_m_000001 (and more) from job job_20140128044010447_0005

Task with the most failures(5): 
-----
Task ID:
  task_20140128044010447_0005_m_000000

URL:
  http://localhost:50030/taskdetails.jsp?jobid=job_20140128044010447_0005&tipid=task_20140128044010447_0005_m_000000
-----
Diagnostic Messages for this Task:
java.lang.Throwable: Child Error
	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)
Caused by: java.io.IOException: Task process exit with nonzero status of 1.
	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:258)

java.lang.Throwable: Child Error
	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)
Caused by: java.io.IOException: Task process exit with nonzero status of 1.
	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:258)


Exception: Client Execution failed with error code = 2 running 

-- Test join on three tables on different keys, should be bucketed and sorted by latter key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1') 
SELECT a.key, c.value FROM src a JOIN src b ON (a.key = b.key) JOIN src c ON (b.value = c.value)
{code}

auto_sortmerge_join_16.q:

{code}
java.lang.Throwable: Child Error
	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)
Caused by: java.io.IOException: Task process exit with nonzero status of 1.
	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:258)


Exception: Client Execution failed with error code = 2 running  

select 
a.key , 
a.value , 
b.value , 
'day1' as day, 
1 as pri 
from 
( 
select 
key, 
value 
from bucket_big where day='day1'
) a 
left outer join 
( 
select 
key, 
value
from bucket_small 
where pri between 1 and 2
) b 
on 
(a.key = b.key) 

See ./ql/target/tmp/log/hive.log or ./itests/qtest/target/tmp/log/hive.log, or check ./ql/target/surefire-reports or ./itests/qtest/target/surefire-reports/ for specific test cases logs.
junit.framework.AssertionFailedError: Client Execution failed with error code = 2 running  

select 
a.key , 
a.value , 
b.value , 
'day1' as day, 
1 as pri 
from 
( 
select 
key, 
value 
from bucket_big where day='day1'
) a 
left outer join 
( 
select 
key, 
value
from bucket_small 
where pri between 1 and 2
) b 
on 
(a.key = b.key) 

See ./ql/target/tmp/log/hive.log or ./itests/qtest/target/tmp/log/hive.log, or check ./ql/target/surefire-reports or ./itests/qtest/target/surefire-reports/ for specific test cases logs.
	at junit.framework.Assert.fail(Assert.java:50)
	at org.apache.hadoop.hive.ql.QTestUtil.failed(QTestUtil.java:1716)
	at org.apache.hadoop.hive.cli.TestMinimrCliDriver.runTest(TestMinimrCliDriver.java:156)
	at org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16(TestMinimrCliDriver.java:126)
...
{code}

Since I'm able to find the task log, I have no idea what caused the task failure. [~navis] suggested memory error though. It will be great if any one knows where to pick up the task log. I plan to check in the patch and exclude the two test cases from ptest to avoid further noises.

[~brocknoland] Could you take another look of the patch?;;;","28/Jan/14 14:30;brocknoland;+1 on the latest patch.

bq. suggested memory error though

I don't see any disk space or memory issues on the hosts.;;;","28/Jan/14 14:31;xuefuz;BTW, the two failed tests had no problem passing if started manually, which makes the problem more interesting.;;;","28/Jan/14 14:39;xuefuz;Thanks, Brock. I'm going to commit the patch right way.

Could the errors [~brocknoland] saw be due to HIVE-6309?

;;;","28/Jan/14 14:41;brocknoland;The tests run as hadoop-1 while I think that is hadoop-2?;;;","28/Jan/14 14:46;xuefuz;Okay. Never mind then.;;;","28/Jan/14 15:01;xuefuz;Patch committed to trunk. The remaining two failures were excluded in ptest property file.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive incorrectly removes TaskAttempt output files if MRAppMaster fails once,HIVE-6309,12691274,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,chenchun,chenchun,chenchun,26/Jan/14 17:10,30/Jan/14 18:59,14/Jul/23 06:14,30/Jan/14 18:59,,,,,,,,,,0.13.0,,,,,,0,,,"We recently upgrade to hadoop2.2 and sometimes find some tables lost several data files after a mid night ETL process. We find that these MapReduce jobs which generate the partial tables have something in common that the MRAppMaster of which all had failed once and the tables all left only a single data file 000000_1000.

The following log in hive.log give us some clues of what's going on with the incorrectly deleted data files.
{code}
$ grep 'hive_2014-01-24_12-33-18_507_6790415670781610350' hive.log
2014-01-24 12:52:43,140 WARN  exec.Utilities (Utilities.java:removeTempOrDuplicateFiles(1535)) - Duplicate taskid file removed: hdfs://hadoop00.lf.sankuai.com:9000/tmp/hive-scratch/hive-sankuai/hive_2014-01-24_12-33-18_507_6790415670781610350/_tmp.-ext-10000.intermediate/000001_1000 with length 824627293. Existing file: hdfs://hadoop00.lf.sankuai.com:9000/tmp/hive-scratch/hive-sankuai/hive_2014-01-24_12-33-18_507_6790415670781610350/_tmp.-ext-10000.intermediate/000000_1000 with length 824860643
2014-01-24 12:52:43,142 WARN  exec.Utilities (Utilities.java:removeTempOrDuplicateFiles(1535)) - Duplicate taskid file removed: hdfs://hadoop00.lf.sankuai.com:9000/tmp/hive-scratch/hive-sankuai/hive_2014-01-24_12-33-18_507_6790415670781610350/_tmp.-ext-10000.intermediate/000002_1000 with length 824681826. Existing file: hdfs://hadoop00.lf.sankuai.com:9000/tmp/hive-scratch/hive-sankuai/hive_2014-01-24_12-33-18_507_6790415670781610350/_tmp.-ext-10000.intermediate/000000_1000 with length 824860643
2014-01-24 12:52:43,149 WARN  exec.Utilities (Utilities.java:removeTempOrDuplicateFiles(1535)) - Duplicate taskid file removed: hdfs://hadoop00.lf.sankuai.com:9000/tmp/hive-scratch/hive-sankuai/hive_2014-01-24_12-33-18_507_6790415670781610350/_tmp.-ext-10000.intermediate/000003_1000 with length 824830450. Existing file: hdfs://hadoop00.lf.sankuai.com:9000/tmp/hive-scratch/hive-sankuai/hive_2014-01-24_12-33-18_507_6790415670781610350/_tmp.-ext-10000.intermediate/000000_1000 with length 824860643
2014-01-24 12:52:43,151 WARN  exec.Utilities (Utilities.java:removeTempOrDuplicateFiles(1535)) - Duplicate taskid file removed: hdfs://hadoop00.lf.sankuai.com:9000/tmp/hive-scratch/hive-sankuai/hive_2014-01-24_12-33-18_507_6790415670781610350/_tmp.-ext-10000.intermediate/000004_1000 with length 824753882. Existing file: hdfs://hadoop00.lf.sankuai.com:9000/tmp/hive-scratch/hive-sankuai/hive_2014-01-24_12-33-18_507_6790415670781610350/_tmp.-ext-10000.intermediate/000000_1000 with length 824860643
{code}

We find that it's because nextAttemptNumber in hadoop2.2 is bigger than 1000 and hive doesn't correctly extract task id from filename. See the following code in org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.java
and ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
{code}
// org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.java
    // All the new TaskAttemptIDs are generated based on MR
    // ApplicationAttemptID so that attempts from previous lives don't
    // over-step the current one. This assumes that a task won't have more
    // than 1000 attempts in its single generation, which is very reasonable.
    nextAttemptNumber = (appAttemptId - 1) * 1000;

// ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
   /**
    * The first group will contain the task id. The second group is the optional extension. The file
    * name looks like: ""0_0"" or ""0_0.gz"". There may be a leading prefix (tmp_). Since getTaskId() can
    * return an integer only - this should match a pure integer as well. {1,3} is used to limit
    * matching for attempts #'s 0-999.
    */
   private static final Pattern FILE_NAME_TO_TASK_ID_REGEX =
       Pattern.compile(""^.*?([0-9]+)(_[0-9]{1,3})?(\\..*)?$"");
{code}

And with the bellow reasons,  extract this value for attempt numbers >= 1000 : 
{code}
>>> re.match(""^.*?([0-9]+)(_[0​-9])?(\\..*)?$"", 'part-r-000000_2').group(1)
'000000'
>>> re.match(""^.*?([0-9]+)(_[0​-9])?(\\..*)?$"", 'part-r-000000_1001').group(1)
'1001'
{code}",hadoop 2.2,ankibakshi,chenchun,prasadm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-2309,,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/14 17:15;chenchun;HIVE-6309.patch;https://issues.apache.org/jira/secure/attachment/12625277/HIVE-6309.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,370019,,,,Thu Jan 30 18:59:23 UTC 2014,,,,,,,,,,"0|i1rrvb:",370321,,,,,,,,,,,,,,,,,,,,,"26/Jan/14 19:26;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625277/HIVE-6309.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 4958 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_import_exported_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_load_hdfs_file_with_space_in_the_name
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_file_with_header_footer_negative
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1034/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1034/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625277;;;","27/Jan/14 02:39;chenchun;I don't think the failed tests are related. Review https://reviews.apache.org/r/17377/;;;","30/Jan/14 00:27;ashutoshc;+1;;;","30/Jan/14 18:59;ashutoshc;Committed to trunk. Thanks, Chun!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
COLUMNS_V2 Metastore table not populated for tables created without an explicit column list.,HIVE-6308,12691180,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ychena,alex.behm,alex.behm,25/Jan/14 02:54,30/Jan/15 01:34,14/Jul/23 06:14,28/Jan/15 02:52,0.10.0,,,,,,,,,1.1.0,,Database/Schema,,,,2,,,"Consider this example table:

CREATE TABLE avro_test
ROW FORMAT SERDE
'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED as INPUTFORMAT
'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT
'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
TBLPROPERTIES (
'avro.schema.url'='file:///path/to/the/schema/test_serializer.avsc');

When I try to run an ANALYZE TABLE for computing column stats on any of the columns, then I get:

org.apache.hadoop.hive.ql.metadata.HiveException: NoSuchObjectException(message:Column o_orderpriority for which stats gathering is requested doesn't exist.)
    at org.apache.hadoop.hive.ql.metadata.Hive.updateTableColumnStatistics(Hive.java:2280)
    at org.apache.hadoop.hive.ql.exec.ColumnStatsTask.persistTableStats(ColumnStatsTask.java:331)
    at org.apache.hadoop.hive.ql.exec.ColumnStatsTask.execute(ColumnStatsTask.java:343)
    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:138)
    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:66)
    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1383)
    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1169)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:982)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)
    at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)
    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)
    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:412)
    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:613)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.hadoop.util.RunJar.main(RunJar.java:208)

The root cause appears to be that the COLUMNS_V2 table in the Metastore isn't populated properly during the table creation.",,alex.behm,brocknoland,erwaman,julienlehuen,mdominguez@cloudera.com,rem120,richardatcloudera,skye,szehon,wilbur.yang,ychena,Yibing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/15 20:05;ychena;HIVE-6308.1.patch;https://issues.apache.org/jira/secure/attachment/12694386/HIVE-6308.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,369923,,,,Wed Jan 28 15:42:25 UTC 2015,,,,,,,,,,"0|i1rra7:",370225,,,,,,,,,,,,,,,,,,,,,"23/Jul/14 06:40;julienlehuen;This is *critical* for Impala users! We want to be able to perform our Metastore operations via Hive's Thrift server, but this would create unhealthy tables for which we can not run ANALYZE. Such table's statistics cannot computed by Impala (statistics are absolutely necessary for Impala to run joins in an optimal manner), and this means that we need to create table via Impala over JDBC. This is less than ideal, and this issue is the only blocker from letting us create via Hive over Thrift.

Related IMPALA tickets: [IMPALA-867|https://issues.cloudera.org/browse/IMPALA-867], [IMPALA-1104|https://issues.cloudera.org/browse/IMPALA-1104];;;","15/Aug/14 01:01;wilbur.yang;I can't seem to reproduce this. Should there be data loaded into the table?;;;","24/Jan/15 20:05;ychena;Need code review;;;","25/Jan/15 03:05;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12694386/HIVE-6308.1.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 7366 tests executed
*Failed tests:*
{noformat}
TestCustomAuthentication - did not produce a TEST-*.xml file
org.apache.hive.jdbc.TestSSL.testSSLFetchHttp
org.apache.hive.spark.client.TestSparkClient.testJobSubmission
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2514/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2514/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-2514/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12694386 - PreCommit-HIVE-TRUNK-Build;;;","26/Jan/15 00:43;ychena;The test failures are not related to the change.;;;","27/Jan/15 06:49;szehon;+1, thanks for adding unit test;;;","28/Jan/15 02:52;szehon;Committed to trunk.  Thanks Yongzhi for debugging and finding the fix!;;;","28/Jan/15 15:42;ychena;Thank you Szehon!

This fix treats creating Avro tables without col defs in hive the same as creating table with all col defs. 
This fix does not address this kind of avro tables created before the fix.

Tested with hive command:  analyze table compute statistics for column. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test use of quoted identifiers in user/role names,HIVE-6305,12691138,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,thejas,thejas,24/Jan/14 22:21,13/Nov/14 19:41,14/Jul/23 06:14,16/Jul/14 19:20,,,,,,,,,,0.14.0,,Authorization,,,,0,TODOC13,,"Tests need to be added to verify that quoted identifiers can be used with user and role names.

For example - 
{code}
 grant all on x to user `user-qa`; 
show grant user `user-qa` on table x; 
{code}
",,jdere,leftyl,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6013,,,,,,,,,,,,,,,,,,,,,"15/Jul/14 20:18;jdere;HIVE-6305.1.patch;https://issues.apache.org/jira/secure/attachment/12655851/HIVE-6305.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,369881,,,,Thu Nov 13 19:41:12 UTC 2014,,,,,,,,,,"0|i1rr0v:",370183,,,,,,,,,,,,,,,,,,,,,"16/Jul/14 00:03;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12655851/HIVE-6305.1.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5718 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_temp_table
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_join_hash
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/800/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/800/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-800/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12655851;;;","16/Jul/14 00:11;jdere;Patch only added new q file tests, the other failures are not related.;;;","16/Jul/14 00:52;thejas;+1;;;","16/Jul/14 19:20;thejas;Patch committed to trunk. Thanks for the contribution Jason!
;;;","17/Jul/14 02:55;leftyl;This jira just adds tests so it doesn't need any user doc, but quoted identifiers for names should be documented.  Which jira would that be?;;;","17/Jul/14 18:38;thejas;The quoted identifier support was added as part HIVE-6013 by [~rhbutani] . This is just testing that it works with role names as well. If i remember right, HIVE-6013 talks only about column names because only that was tested as that part of the patch.

Yes, we should document that role names can also be quoted identifier.;;;","17/Jul/14 20:23;leftyl;Good, HIVE-6013 is already documented so it's easy to add this to the wiki.  Thanks, Thejas.

One question:  why don't these tests specify hive.support.quoted.identifiers?  Oh, nevermind, its default setting is column.;;;","23/Jul/14 05:37;leftyl;Another question:  If hive.support.quoted.identifiers is set to none, can back-ticks be used for regular expressions in user & role names?  I'll try to dodge the issue in the doc, for now.

Presumably quoted user/role names can have Unicode characters, since the test's role name has a space (create role `src Role2`).  But the SQL Std Auth doc says user names are managed outside of Hive, so maybe Unicode doesn't work for them.

And presumably this applies to both SQL standard based and default authorization, so I updated both docs with identical information:

* [SQL Standard Based Hive Authorization -- Names of Users and Roles | https://cwiki.apache.org/confluence/display/Hive/SQL+Standard+Based+Hive+Authorization#SQLStandardBasedHiveAuthorization-NamesofUsersandRoles]
* [Hive Authorization -- Names of Users and Roles | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Authorization#LanguageManualAuthorization-NamesofUsersandRoles]

But if Unicode is permitted in user/role names, then the description of hive.support.quoted.identifiers should be revised to include them.  Or even if Unicode isn't permitted, the parameter should mention user and role names.

* [Configuration Properties -- hive.support.quoted.identifiers | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.support.quoted.identifiers];;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_json_object throw java.lang.IllegalStateException: No match found exception.,HIVE-6301,12690952,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,pensz,pensz,pensz,24/Jan/14 01:42,07/Feb/14 00:39,14/Jul/23 06:14,07/Feb/14 00:39,0.10.0,0.11.0,0.12.0,,,,,,,0.13.0,,,,,,0,,,"In fact, you can find the bug in code.

https://github.com/apache/hive/blob/branch-0.10/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFJson.java#L190

Miss call _mKey.matches()_ before use _mKey.group(1)_

The bug still exists in the newest version.

So, we met such exception in some query:


{quote}

2014-01-23 11:08:19,869 FATAL ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public org.apache.hadoop.io.Text org.apache.hadoop.hive.ql.udf.UDFJson.evaluate(java.lang.String,java.lang.String)  on object org.apache.hadoop.hive.ql.udf.UDFJson@c7056d5 of class org.apache.hadoop.hive.ql.udf.UDFJson with arguments {{ .... }:java.lang.String, $.6:java.lang.String} of size 2
Caused by: java.lang.IllegalStateException: No match found
	at java.util.regex.Matcher.group(Matcher.java:468)
	at org.apache.hadoop.hive.ql.udf.UDFJson.extract(UDFJson.java:190)
	at org.apache.hadoop.hive.ql.udf.UDFJson.evaluate(UDFJson.java:154)
	... 24 more

{quote}",,navis,pensz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/14 02:00;pensz;HIVE-6301-0.10.patch;https://issues.apache.org/jira/secure/attachment/12624973/HIVE-6301-0.10.patch","24/Jan/14 02:04;pensz;HIVE-6301-0.11.patch;https://issues.apache.org/jira/secure/attachment/12624974/HIVE-6301-0.11.patch","24/Jan/14 02:11;pensz;HIVE-6301-0.12.patch;https://issues.apache.org/jira/secure/attachment/12624978/HIVE-6301-0.12.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,369692,,,,Fri Feb 07 00:39:08 UTC 2014,,,,,,,,,,"0|i1rpun:",369993,,,,,,,,,,,,,,,,,,,,,"24/Jan/14 02:00;pensz;for hive 0.10 .;;;","24/Jan/14 02:04;pensz;for hive 0.11.;;;","06/Feb/14 01:57;navis;+1;;;","07/Feb/14 00:39;navis;Committed to trunk. Thanks for contribution, Zhiwen Sun. I've enrolled you on hive contributor list and feel free assign issues to youself.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add config flag to turn off fetching partition stats,HIVE-6298,12690937,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,24/Jan/14 00:14,23/Mar/14 17:31,14/Jul/23 06:14,03/Feb/14 22:12,,,,,,,,,,0.13.0,,,,,,0,,,,,hagleitn,leftyl,prasanth_j,sershe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6300,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/14 00:15;hagleitn;HIVE-6298.1.patch;https://issues.apache.org/jira/secure/attachment/12624956/HIVE-6298.1.patch","30/Jan/14 18:54;hagleitn;HIVE-6298.2.patch;https://issues.apache.org/jira/secure/attachment/12626145/HIVE-6298.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,369677,,,,Sun Mar 23 07:34:59 UTC 2014,,,,,,,,,,"0|i1rprr:",369980,,,,,,,,,,,,,,,,,,,,,"24/Jan/14 00:16;hagleitn;For debugging purposes it's helpful to be able to suppress loading of the stats for all partitions of a table when the stats annotation is being used. ;;;","24/Jan/14 00:26;prasanth_j;LGTM.;;;","24/Jan/14 01:09;leftyl;Please document *hive.stats.fetch.partition.stats* in hive-default.xml.template, then I'll add it to the wiki for release 0.13.

If you want to be thorough, you can also make these changes to hive-default.xml.template:

* change the default value of *hive.stats.key.prefix.max.length* from 200 to 150 
* document 8 config properties that have been added since release 0.12:
** hive.stats.max.variable.length
** hive.stats.list.num.entries
** hive.stats.map.num.entries
** hive.stats.map.parallelism
** hive.stats.fetch.column.stats
** hive.stats.avg.row.size
** hive.stats.join.factor
** hive.stats.deserialization.factor

These changes were found by comparing files in branch 0.12 & trunk (as of January 13th).  I've learned my lesson:  from now on I'll check stats patches for new configs.;;;","24/Jan/14 01:18;prasanth_j;My apologies. Its my mistake. I should have added these to hive-default.xml.template. All these were added as part of HIVE-5369 and its subtasks. I created another subtask to fix them here HIVE-6300. Will post a patch soon. I will document ""hive.stats.fetch.partition.stats"" as well in the new JIRA.;;;","24/Jan/14 08:17;hagleitn;Thanks for taking up adding this particular flag as well, [~prasanth_j]. [~leftylev] thanks for the reminder. I wasn't sure that this particular flag needs to be documented (mostly for developers when debugging), but I think you're right. It should get added.;;;","24/Jan/14 18:20;sershe;lgtm;;;","25/Jan/14 04:00;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12624956/HIVE-6298.1.patch

{color:red}ERROR:{color} -1 due to 13 failed/errored test(s), 4963 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_groupby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_select
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_union
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_import_exported_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_load_hdfs_file_with_space_in_the_name
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_file_with_header_footer_negative
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1008/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1008/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 13 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12624956;;;","28/Jan/14 21:23;sershe;+1;;;","30/Jan/14 18:54;hagleitn;rebased to trunk;;;","31/Jan/14 10:46;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626145/HIVE-6298.2.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 4980 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1125/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1125/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626145;;;","03/Feb/14 22:12;hagleitn;Committed to trunk. Thanks [~sershe], [~prasanth_j] and [~leftylev] for the reviews!;;;","23/Mar/14 07:34;leftyl;This needs a fix version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
batchSize computation in Vectorized ORC reader can cause BufferUnderFlowException when PPD is enabled,HIVE-6287,12690834,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,prasanth_j,prasanth_j,prasanth_j,23/Jan/14 18:18,31/Jul/14 18:46,14/Jul/23 06:14,31/Jan/14 21:18,0.13.0,,,,,,,,,0.13.0,,Vectorization,,,,0,orcfile,vectorization,"nextBatch() method that computes the batchSize is only aware of stripe boundaries. This will not work when predicate pushdown (PPD) in ORC is enabled as PPD works at row group level (stripe contains multiple row groups). By default, row group stride is 10000. When PPD is enabled, some row groups may get eliminated. After row group elimination, disk ranges are computed based on the selected row groups. If batchSize computation is not aware of this, it will lead to BufferUnderFlowException (reading beyond disk range). Following scenario should illustrate it more clearly

{code}
|--------------------------------- STRIPE 1 ------------------------------------|
|-- row grp 1 --|-- row grp 2 --|-- row grp 3 --|-- row grp 4 --|-- row grp 5 --|
                |--------- diskrange 1 ---------|               |- diskrange 2 -|
                                                ^
                                             (marker)   
{code}

diskrange1 will have 20000 rows and diskrange 2 will have 10000 rows. Since nextBatch() was not aware of row groups and hence the diskranges, it tries to read 1024 values from the end of diskrange 1 where it should only read 20000 % 1024 = 544 values. This will result in BufferUnderFlowException.

To fix this, a marker is placed at the end of each range and batchSize is computed accordingly. {code}batchSize = Math.min(VectorizedRowBatch.DEFAULT_SIZE, (markerPosition - rowInStripe));{code}

Stack trace will look like:
{code}
Caused by: java.nio.BufferUnderflowException
	at java.nio.Buffer.nextGetIndex(Buffer.java:492)
	at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:135)
	at org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.read(InStream.java:207)
	at org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readFloat(SerializationUtils.java:70)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$FloatTreeReader.nextVector(RecordReaderImpl.java:673)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StructTreeReader.nextVector(RecordReaderImpl.java:1615)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.nextBatch(RecordReaderImpl.java:2883)
	at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat$VectorizedOrcRecordReader.next(VectorizedOrcInputFormat.java:94)
	... 15 more
{code}",,ehans,hagleitn,prasanth_j,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/14 00:09;prasanth_j;HIVE-6287.1.patch;https://issues.apache.org/jira/secure/attachment/12624955/HIVE-6287.1.patch","24/Jan/14 19:16;prasanth_j;HIVE-6287.2.patch;https://issues.apache.org/jira/secure/attachment/12625092/HIVE-6287.2.patch","29/Jan/14 19:02;prasanth_j;HIVE-6287.3.patch;https://issues.apache.org/jira/secure/attachment/12625925/HIVE-6287.3.patch","28/Jan/14 19:49;prasanth_j;HIVE-6287.3.patch;https://issues.apache.org/jira/secure/attachment/12625645/HIVE-6287.3.patch","30/Jan/14 19:30;prasanth_j;HIVE-6287.4.patch;https://issues.apache.org/jira/secure/attachment/12626157/HIVE-6287.4.patch","23/Jan/14 18:20;prasanth_j;HIVE-6287.WIP.patch;https://issues.apache.org/jira/secure/attachment/12624853/HIVE-6287.WIP.patch",,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,369573,,,,Fri Jan 31 21:18:00 UTC 2014,,,,,,,,,,"0|i1rp5r:",369878,,,,,,,,,,,,,,,,,,,,,"23/Jan/14 18:20;prasanth_j;Uploading WIP patch. Q file tests are yet to be added.;;;","24/Jan/14 00:09;prasanth_j;Added q file tests.;;;","24/Jan/14 00:10;prasanth_j;Making it patch available for HIVE QA to pickup.;;;","24/Jan/14 00:17;prasanth_j;Attaching RB link.;;;","24/Jan/14 18:08;ehans;I think that by PPD you mean predicate pushdown. This was not immediately obvious to me. I edited it into the description. It's a good idea to define acronyms on first use. Thanks!;;;","24/Jan/14 19:16;prasanth_j;Reuploading the same patch for HIVE QA to pick up.

Thanks [~ehans] for the update to description.;;;","25/Jan/14 23:00;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625092/HIVE-6287.2.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 4959 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_vectorization_ppd
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_import_exported_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_reducers_power_two
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_load_hdfs_file_with_space_in_the_name
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_file_with_header_footer_negative
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1025/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1025/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625092;;;","28/Jan/14 19:49;prasanth_j;Addressed [~gopalv]'s review comment. ;;;","28/Jan/14 19:49;prasanth_j;Patch number should be .3. Reuploading it.;;;","28/Jan/14 23:19;hagleitn;Assuming tests are passing: +1 LGTM;;;","29/Jan/14 19:02;prasanth_j;Reuploading patch again for precommit tests.;;;","30/Jan/14 09:38;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625925/HIVE-6287.3.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 4973 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_vectorization_ppd
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1109/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1109/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625925;;;","30/Jan/14 19:30;prasanth_j;The test failure was related to rounding issues in sum(). Its producing unpredictable results on different OSes. Fixed the failure by typecasting to int in this patch.;;;","31/Jan/14 12:24;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626157/HIVE-6287.4.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 4981 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1126/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1126/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626157;;;","31/Jan/14 18:56;prasanth_j;The test failures seems to be unrelated.;;;","31/Jan/14 21:18;hagleitn;Committed to trunk. Thanks [~prasanth_j]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Network resource leak with HiveClientCache when using HCatInputFormat,HIVE-6268,12690534,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sushanth,sushanth,sushanth,23/Jan/14 02:52,12/Nov/14 01:44,14/Jul/23 06:14,31/Jan/14 03:08,0.12.0,,,,,,,,,0.13.0,,HCatalog,,,,0,,,"HCatInputFormat has a cache feature that allows HCat to cache hive client connections to the metastore, so as to not keep reinstantiating a new hive server every single time. This uses a guava cache of hive clients, which only evicts entries from cache on the next write, or by manually managing the cache.

So, in a single threaded case, where we reuse the hive client, the cache works well, but in a massively multithreaded case, where each thread might perform one action, and then is never used, there are no more writes to the cache, and all the clients stay alive, thus keeping ports open.",,leftyl,sushanth,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6332,,,,,,,,,,,,,,,,HIVE-8830,,,,,,,,"29/Jan/14 19:19;sushanth;HIVE-6268.2.patch;https://issues.apache.org/jira/secure/attachment/12625934/HIVE-6268.2.patch","29/Jan/14 21:11;sushanth;HIVE-6268.3.patch;https://issues.apache.org/jira/secure/attachment/12625972/HIVE-6268.3.patch","23/Jan/14 17:56;sushanth;HIVE-6268.patch;https://issues.apache.org/jira/secure/attachment/12624843/HIVE-6268.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,369570,,,,Mon Feb 03 18:50:41 UTC 2014,,,,,,,,,,"0|i1rp53:",369875,,,,,,,,,,,,,,,,,,,,,"23/Jan/14 17:55;sushanth;To fix this, we can do two things :

a) Provide a jobconf parameter that allows users to disable usage of the cache altogether. Useful for massively multithreaded cases.
b) In the cases that use a cache, we should spawn a separate maintenance thread that will prune and expire from time to time.

Attaching a patch which does both of the above.;;;","23/Jan/14 18:15;sushanth;Note also, that the default behaviour is to continue to have the cache enabled, so as to not surprise any users. The new parameter is called ""hcatalog.hive.client.cache.disabled"", and would need to be set to ""true"" by the HCat user to disable the cache.;;;","24/Jan/14 00:07;sushanth;[~thejas]/[~daijy], could I bother either of you for a review?;;;","24/Jan/14 02:21;thejas;[~sushanth] Can you please create a reviewboard link and also document the configuration in release notes of jira?
;;;","24/Jan/14 02:22;sushanth;Reviewboard link : https://reviews.apache.org/r/17297/;;;","24/Jan/14 09:12;leftyl;Where should *hcatalog.hive.client.cache.disabled* be documented, besides the release notes?

AFAIK, none of the configuration properties in HCatConstants.java are mentioned in the wiki.  HCatConstants itself is only mentioned a couple of times in ""Notification for a New Partition"":  https://cwiki.apache.org/confluence/display/Hive/HCatalog+Notification#HCatalogNotification-NotificationforaNewPartition.;;;","25/Jan/14 19:32;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12624843/HIVE-6268.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 4958 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_import_exported_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_load_hdfs_file_with_space_in_the_name
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_file_with_header_footer_negative
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1022/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1022/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12624843;;;","29/Jan/14 19:14;sushanth;(Attaching updated patch);;;","29/Jan/14 21:11;sushanth;One more modification to try and have the first interval where it checks whether to expire from cache or not be early if the user has set a short timeout.;;;","29/Jan/14 21:15;sushanth;@Lefty : The parameters documentation for HCat, I'm afraid is lacking. I will take that on as a todo. I've created another hive jira and  assigned it to myself : https://issues.apache.org/jira/browse/HIVE-6332;;;","30/Jan/14 07:22;thejas;+1;;;","30/Jan/14 11:18;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625972/HIVE-6268.3.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4972 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1110/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1110/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625972;;;","31/Jan/14 03:08;thejas;Patch committed to trunk. Thanks for the contribution Sushanth!
;;;","31/Jan/14 09:08;leftyl;*hcatalog.hive.client.cache.disabled* will be documented with HIVE-6332:  HCatConstants Documentation needed.

Should it also be documented here in a release note?  (I'd say yes, because HIVE-6332 might not get done in time for release 0.13.0.);;;","03/Feb/14 18:50;sushanth;Hi Lefty, Yes, I think we should document it in a release note. I'm planning on finishing HIVE-6332 in a week but it's good to have it in a release note as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Explain explain,HIVE-6267,12690530,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,23/Jan/14 02:25,04/Feb/14 00:34,14/Jul/23 06:14,03/Feb/14 05:48,,,,,,,,,,0.13.0,,,,,,0,,,"I've gotten feedback over time saying that it's very difficult to grok our explain command. There's supposedly a lot of information that mainly matters to developers or the testing framework. Comparing it to other major DBs it does seem like we're packing way more into explain than other folks.

I've gone through the explain checking, what could be done to improve readability. Here's a list of things I've found:

- AST (unreadable in it's ""lisp"" syntax, not really required for end users)
- Vectorization (enough to display once per task and only when true)
- Expressions representation is very lengthy, could be much more compact
- ""if not exists"" on DDL (enough to display only on true, or maybe not at all)
- bucketing info (enough if displayed only if table is actually bucketed)
- external flag (show only if external)
- GlobalTableId (don't need in plain explain, maybe in extended)
- Position of big table (already clear from plan)
- Stats always (Most DBs mostly only show stats in explain, that gives a sense of what the planer thinks will happen)
- skew join (only if true should be enough)
- limit doesn't show the actual limit
- ""Alias -> Map Operator tree"" -> alias is duplicated in TableScan operator
- tag is only useful at runtime (move to explain extended)
- Some names are camel case or abbreviated, clearer if full name
- Tez is missing vertex map (aka edges)
- explain formatted (json) is broken right now (swallows some information)

Since changing explain results in many golden file updates, i'd like to take a stab at all of these at once.",,gates,hagleitn,navis,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6353,HIVE-6354,,,,,,,,,,,,,,,,,,,HIVE-6358,,,,,,,,"23/Jan/14 02:27;hagleitn;HIVE-6267.1.partial;https://issues.apache.org/jira/secure/attachment/12624561/HIVE-6267.1.partial","30/Jan/14 07:53;hagleitn;HIVE-6267.2.partial;https://issues.apache.org/jira/secure/attachment/12626075/HIVE-6267.2.partial","31/Jan/14 06:07;hagleitn;HIVE-6267.3.partial;https://issues.apache.org/jira/secure/attachment/12626264/HIVE-6267.3.partial","02/Feb/14 10:38;hagleitn;HIVE-6267.4.patch;https://issues.apache.org/jira/secure/attachment/12626526/HIVE-6267.4.patch","02/Feb/14 13:46;hagleitn;HIVE-6267.5.patch;https://issues.apache.org/jira/secure/attachment/12626529/HIVE-6267.5.patch","02/Feb/14 22:04;hagleitn;HIVE-6267.6.patch;https://issues.apache.org/jira/secure/attachment/12626566/HIVE-6267.6.patch","03/Feb/14 00:53;hagleitn;HIVE-6267.7.patch.gz;https://issues.apache.org/jira/secure/attachment/12626578/HIVE-6267.7.patch.gz","03/Feb/14 01:24;hagleitn;HIVE-6267.8.patch;https://issues.apache.org/jira/secure/attachment/12626583/HIVE-6267.8.patch",,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,369389,,,,Tue Feb 04 00:34:04 UTC 2014,,,,,,,,,,"0|i1ro0f:",369692,,,,,,,,,,,,,,,,,,,,,"23/Jan/14 02:26;hagleitn;Here's a draft. Still have to run and evaluate golden files.;;;","23/Jan/14 02:27;hagleitn;Oops. Wrong file. 6267.1 is the right one.;;;","30/Jan/14 07:53;hagleitn;.2 fixes bug in previous patch. Overriding toStringTree for ASTNodes to change the explain output was not such a hot idea (used for row resolvers).;;;","31/Jan/14 06:07;hagleitn;Fixed another issue. Removing any duplicates from being printed in explain is too aggressive for some things.;;;","31/Jan/14 19:00;hagleitn;https://reviews.apache.org/r/17597/;;;","01/Feb/14 02:20;vikram.dixit;LGTM +1 (Pending tests run).;;;","02/Feb/14 10:40;hagleitn;.4 contains all the changed golden files.;;;","02/Feb/14 12:38;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626526/HIVE-6267.4.patch

{color:red}ERROR:{color} -1 due to 36 failed/errored test(s), 4997 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binarysortable_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_like_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_auto
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformatCTAS
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_alter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_create_table_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_functions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_tblproperties
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_top_level
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_unset_table_view_property
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1152/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1152/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 36 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626526;;;","02/Feb/14 13:46;hagleitn;some fixes in .5;;;","02/Feb/14 15:50;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626529/HIVE-6267.5.patch

{color:red}ERROR:{color} -1 due to 24 failed/errored test(s), 4997 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binarysortable_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_like_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_auto
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_functions
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1153/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1153/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 24 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626529;;;","03/Feb/14 00:13;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626566/HIVE-6267.6.patch

{color:red}ERROR:{color} -1 due to 103 failed/errored test(s), 4997 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join14_hadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join18_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join27
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join29
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_reordering_values
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_without_localtask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binarysortable_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_auto
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join14_hadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join18_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join27
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join29
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32_lessSize
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join35
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join36
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join37
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join38
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join39
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join40
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join41
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_alt_syntax
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_cond_pushdown_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_filters_overlap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_hive_626
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_merging
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_nullsafe
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_reorder
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_reorder2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_reorder3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_reorder4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_star
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_thrift
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_vc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_join1
org.apache.hadoop.hive.ql.TestMTQueries.testMTQueries1
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1154/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1154/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 103 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626566;;;","03/Feb/14 00:27;hagleitn;Tests look good now. I've run different subsets of the golden files (upload limit) to prove that everything is looking good. There's 3 index tests that fail because their stats are unreliable. I will take out the ""explains"" temporarily and fix in different jira. This is basically exposing a problem - not something that was introduced in this patch.;;;","03/Feb/14 00:29;hagleitn;By ""unreliable"" I mean that they produce different results when run multiple times in sequence in terms of data size of the stats.;;;","03/Feb/14 00:53;hagleitn;.7 has all changes (compressed);;;","03/Feb/14 01:22;hagleitn;Committed to trunk. Thanks for the review Vikram!

I'm keeping this open and kick off another test run to make sure everything is going right, since this changes a lot of files.;;;","03/Feb/14 01:24;hagleitn;.8 is just to kick a test run and verify.;;;","03/Feb/14 04:57;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626583/HIVE-6267.8.patch

{color:green}SUCCESS:{color} +1 4997 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1157/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1157/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626583;;;","03/Feb/14 05:48;hagleitn;Committed to trunk. Test ran: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1157/

No failures.;;;","03/Feb/14 08:31;navis;It's breaking all of pending patches. Is this that good?;;;","03/Feb/14 11:00;hagleitn;Sorry [~navis]. I tried to keep the disruption as minimal as possible. I collected all things I thought need fixing together before making the changes. Then, I waited until the weekend and a time when the queue is empty and tried to get everything back in shape before people start working again. 

I think it's worth it, otherwise I wouldn't have spent so much time on it. As I mentioned above, I've gotten feedback multiple times about seeing if we can improve explain. Unfortunately, that means tons of golden files. If you can think of a better way I can back out and try again. But it's not clear to me how to avoid changing that many golden files, since we rely on it so heavily in the q files..;;;","04/Feb/14 00:34;navis;[~hagleitn] Generally looks good to me. Simple and concise but things like ""Position of Big Table"" should not be removed, imho. I think I was a little pissed off yesterday, which was Sunday for you but Monday for me. I apologize for the rudeness.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unbalanced number of HiveParser msgs.push/msgs.pop calls when doing lookahead,HIVE-6264,12690352,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,22/Jan/14 19:13,23/Mar/14 17:33,14/Jul/23 06:14,29/Jan/14 23:42,,,,,,,,,,0.13.0,,,,,,0,,,"HiveParser pushes/pops messages describing the current parse rule like so:
{noformat}
joinSource
@init { gParent.msgs.push(""join source""); }
@after { gParent.msgs.pop(); }
...
{noformat}

The ANTLR generated code for the init/after actions looks like this:

{noformat}
         gParent.msgs.push(""join source""); 
...
            if ( state.backtracking==0 ) { gParent.msgs.pop(); }
{noformat}

If we have a parse rule that does some lookahead, the message is always pushed onto the message stack since the init action has no check of state.backtracking.  But that message is never popped because the after action does check state.backtracking. As a result there can be a bunch of parser context messages added to the stack which are never taken off.
",,jdere,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/14 19:24;jdere;HIVE-6264.1.patch;https://issues.apache.org/jira/secure/attachment/12624402/HIVE-6264.1.patch","24/Jan/14 02:59;jdere;HIVE-6264.2.patch;https://issues.apache.org/jira/secure/attachment/12624985/HIVE-6264.2.patch","28/Jan/14 18:57;jdere;HIVE-6264.3.patch;https://issues.apache.org/jira/secure/attachment/12625625/HIVE-6264.3.patch","29/Jan/14 09:41;jdere;HIVE-6264.4.patch;https://issues.apache.org/jira/secure/attachment/12625831/HIVE-6264.4.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,369305,,,,Wed Jan 29 23:42:25 UTC 2014,,,,,,,,,,"0|i1rni7:",369610,,,,,,,,,,,,,,,,,,,,,"22/Jan/14 19:14;jdere;Ran into this while trying some parser changes.  Thanks to [~rhbutani] for finding the issue.;;;","22/Jan/14 19:24;jdere;Patch v1. Add new pushMsg()/popMsg() methods in HiveParser which will check state.backtracking.;;;","22/Jan/14 19:34;jdere;https://reviews.apache.org/r/17200/;;;","24/Jan/14 02:59;jdere;re-uploading patch to kick off pre commit tests;;;","25/Jan/14 12:37;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12624985/HIVE-6264.2.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 4958 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_import_exported_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_reducers_power_two
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_load_hdfs_file_with_space_in_the_name
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_reduce_deduplicate
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_file_with_header_footer_negative
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1014/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1014/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12624985;;;","25/Jan/14 15:25;rhbutani;looks good. Can you check why the 5 minimr tests are failing.;;;","26/Jan/14 18:28;jdere;Looks like those tests are all failing due to the current issues in the minimr tests (HIVE-6293). I'll kick off another precommit run when these are resolved.;;;","28/Jan/14 18:57;jdere;Try running precommit test again, looks like the issues with Minimr tests have been fixed (HIVE-6310).;;;","28/Jan/14 19:37;rhbutani;+1;;;","29/Jan/14 09:41;jdere;upload patch again to run pre commit tests;;;","29/Jan/14 21:43;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625831/HIVE-6264.4.patch

{color:green}SUCCESS:{color} +1 4972 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1101/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1101/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625831;;;","29/Jan/14 23:42;rhbutani;thanks Jason;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid sending input files multiple times on Tez,HIVE-6263,12690216,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,22/Jan/14 07:55,25/Jan/14 05:14,14/Jul/23 06:14,25/Jan/14 05:14,,,,,,,,,,0.13.0,,,,,,0,,,Input paths can be recontructed from the plan. No need to send them in the job conf as well.,,hagleitn,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/14 07:56;hagleitn;HIVE-6263.1.patch;https://issues.apache.org/jira/secure/attachment/12624297/HIVE-6263.1.patch","23/Jan/14 02:14;hagleitn;HIVE-6263.2.patch;https://issues.apache.org/jira/secure/attachment/12624462/HIVE-6263.2.patch","23/Jan/14 22:35;hagleitn;HIVE-6263.3.patch;https://issues.apache.org/jira/secure/attachment/12624934/HIVE-6263.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,369170,,,,Sat Jan 25 05:14:42 UTC 2014,,,,,,,,,,"0|i1rmof:",369475,,,,,,,,,,,,,,,,,,,,,"23/Jan/14 02:14;hagleitn;Re-uploading to trigger precommit.;;;","23/Jan/14 22:35;hagleitn;Third time's the charm.;;;","24/Jan/14 00:21;vikram.dixit;LGTM +1 (assuming tests pass).;;;","25/Jan/14 00:21;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12624934/HIVE-6263.3.patch

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 4949 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_groupby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_select
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_union
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1007/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1007/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12624934;;;","25/Jan/14 04:38;hagleitn;Test failures are unrelated.;;;","25/Jan/14 05:14;hagleitn;Committed to trunk. Thanks for the review Vikram!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unnecessary copies of schema + table desc from serialized plan,HIVE-6262,12690214,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,22/Jan/14 07:50,29/Jan/14 06:51,14/Jul/23 06:14,29/Jan/14 06:51,,,,,,,,,,0.13.0,,,,,,0,,,"Currently for a partitioned table the following are true:

- for each partitiondesc we send a copy of the corresponding tabledesc
- for each partitiondesc we send two copies of the schema (in different formats).

Obviously we need to send different schemas if they are required by schema evolution, but in our case we'll always end up with multiple copies.

The effect can be dramatic. The reductions by removing those on partitioned tables easily be can be 8-10x in size. Plans themselves can be 10s to 100s of mb (even with kryo). The size difference also plays out in every task on the cluster we run.",,hagleitn,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/14 07:51;hagleitn;HIVE-6262.1.patch;https://issues.apache.org/jira/secure/attachment/12624296/HIVE-6262.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,369168,,,,Wed Jan 29 06:51:58 UTC 2014,,,,,,,,,,"0|i1rmnz:",369473,,,,,,,,,,,,,,,,,,,,,"23/Jan/14 02:13;hagleitn;Tests have successfully run: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/994/testReport/ (the 7 failures are unrelated). Unfortunately jira was down when the tests completed (so no auto update);;;","27/Jan/14 21:53;vikram.dixit;This is really good in terms of memory efficiency. LGTM +1.;;;","29/Jan/14 06:51;hagleitn;Committed to trunk. Thanks for the review Vikram!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update metadata.q.out file for tez (after change to .q file),HIVE-6261,12690212,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,22/Jan/14 07:43,25/Jan/14 04:25,14/Jul/23 06:14,25/Jan/14 04:25,,,,,,,,,,0.13.0,,,,,,0,,,,,hagleitn,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/14 07:45;hagleitn;HIVE-6261.1.patch;https://issues.apache.org/jira/secure/attachment/12624293/HIVE-6261.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,369166,,,,Fri Jan 24 00:21:49 UTC 2014,,,,,,,,,,"0|i1rmnj:",369471,,,,,,,,,,,,,,,,,,,,,"22/Jan/14 07:44;hagleitn;Same .q file is used on tez as well, we're missing an update to the golden file.;;;","22/Jan/14 22:16;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12624293/HIVE-6261.1.patch

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 4949 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_groupby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_select
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_union
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/993/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/993/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12624293;;;","22/Jan/14 23:32;hagleitn;Failures are unrelated. The hadoop-1 build isn't even touching the file changed in this patch.;;;","23/Jan/14 20:31;vikram.dixit;We need to investigate as to why in tez we have an extra stage for a couple of queries. We can take that up in another jira.;;;","24/Jan/14 00:19;hagleitn;I've created: https://issues.apache.org/jira/browse/HIVE-6295 for that.;;;","24/Jan/14 00:21;vikram.dixit;Cool. +1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compress plan when sending via RPC (Tez),HIVE-6260,12690211,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,22/Jan/14 07:37,25/Jan/14 04:37,14/Jul/23 06:14,25/Jan/14 04:37,,,,,,,,,,0.13.0,,,,,,0,,,When trying to send plan via RPC it's helpful to compress the payload. That way more potential plans can be sent (size limit).,,hagleitn,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/14 07:39;hagleitn;HIVE-6260.1.patch;https://issues.apache.org/jira/secure/attachment/12624290/HIVE-6260.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,369165,,,,Sat Jan 25 04:37:41 UTC 2014,,,,,,,,,,"0|i1rmnb:",369470,,,,,,,,,,,,,,,,,,,,,"22/Jan/14 20:43;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12624290/HIVE-6260.1.patch

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 4949 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_groupby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_select
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_union
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/992/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/992/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12624290;;;","22/Jan/14 21:29;hagleitn;Failures are unrelated. That seems to be a problem with the build. Tests failed on precommit runs of other jiras, also locally happens with or without the patch.;;;","23/Jan/14 20:12;vikram.dixit;LGTM. +1;;;","25/Jan/14 04:37;hagleitn;Committed to trunk. Thanks for the review Vikram!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add ability to specify delimiter in HCatalog Java API to create tables - HCatCreateTableDesc,HIVE-6251,12690139,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,21/Jan/14 22:56,06/Feb/14 17:03,14/Jul/23 06:14,05/Feb/14 18:19,0.13.0,,,,,,,,,0.13.0,,HCatalog,WebHCat,,,0,,,"Per https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-Create/Drop/TruncateTablerow_format, the following is supported when creating a table.

{code}
  : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char]
        [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]
        [NULL DEFINED AS char] (Note: Only available starting with Hive 0.13)
  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]
{code}

Need to add support for specifying 4 delimiters plus escape and NULL in HCatCreateTableDesc. create(String dbName, String tableName, List<HCatFieldSchema> columns) API.",,ekoifman,gates,leftyl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/14 22:47;ekoifman;HIVE-6251.2.patch;https://issues.apache.org/jira/secure/attachment/12625463/HIVE-6251.2.patch","23/Jan/14 18:15;ekoifman;HIVE-6251.patch;https://issues.apache.org/jira/secure/attachment/12624850/HIVE-6251.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,369096,,,,Thu Feb 06 17:03:26 UTC 2014,,,,,,,,,,"0|i1rm7z:",369401,Added ability to set serde parameters from webhcat-java client.,,,,,,,,,,,,,,,,,,,,"28/Jan/14 00:34;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625463/HIVE-6251.2.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 4961 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_import_exported_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_merge
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_load_hdfs_file_with_space_in_the_name
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_file_with_header_footer_negative
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1050/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1050/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625463;;;","28/Jan/14 02:35;ekoifman;The changes were all under webhcat/java-client - there is no way that can affect the tests listed above;;;","05/Feb/14 18:19;gates;Patch checked in.  Thanks Eugene.;;;","06/Feb/14 11:08;leftyl;Does this need any user doc?;;;","06/Feb/14 17:03;ekoifman;I think generated JavaDoc will be sufficient here;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2 JDBC SSL binary client transport should not call a TTransport#open ,HIVE-6249,12690133,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vgumashta,vgumashta,vgumashta,21/Jan/14 22:34,28/Jan/14 21:24,14/Jul/23 06:14,28/Jan/14 21:24,0.13.0,,,,,,,,,0.13.0,,HiveServer2,JDBC,,,0,,,Thrift's TSSLTransportFactory#getClientSocket already returns an open socket. HiveConnection#openTransport should call open only on sockets which are not bound.,,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/14 22:55;vgumashta;HIVE-6249.1.patch;https://issues.apache.org/jira/secure/attachment/12624210/HIVE-6249.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,369090,,,,Tue Jan 28 21:24:29 UTC 2014,,,,,,,,,,"0|i1rm6n:",369395,,,,,,,,,,,,,,,,,,,,,"22/Jan/14 05:10;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12624210/HIVE-6249.1.patch

{color:green}SUCCESS:{color} +1 4943 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/979/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/979/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12624210;;;","23/Jan/14 16:08;vgumashta;cc [~thejas];;;","27/Jan/14 21:15;vgumashta;Review board: https://reviews.apache.org/r/17420/;;;","28/Jan/14 02:59;thejas;+1;;;","28/Jan/14 21:24;thejas;Patch committed to trunk. Thanks for the contribution Vaibhav!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sign(a) UDF is not supported for decimal type,HIVE-6246,12690073,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,xuefuz,xuefuz,xuefuz,21/Jan/14 18:06,10/Feb/14 21:32,14/Jul/23 06:14,30/Jan/14 21:18,0.12.0,,,,,,,,,0.13.0,,UDF,,,,0,,,"java.sql.SQLException: Error while compiling statement: FAILED: SemanticException [Error 10014]: Line 1:86 Wrong arguments 'a': No matching method for class org.apache.hadoop.hive.ql.udf.UDFSign with (decimal(38,10)). Possible choices: _FUNC_(double)
",,kamrul,leftyl,prasadm,swarnim,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6327,HIVE-6385,,HIVE-2693,,,,,,,,,,,,,,,,,,,,,"27/Jan/14 21:49;xuefuz;HIVE-6246.1.patch;https://issues.apache.org/jira/secure/attachment/12625450/HIVE-6246.1.patch","27/Jan/14 21:48;xuefuz;HIVE-6246.1.patch;https://issues.apache.org/jira/secure/attachment/12625448/HIVE-6246.1.patch","23/Jan/14 18:29;xuefuz;HIVE-6246.patch;https://issues.apache.org/jira/secure/attachment/12624861/HIVE-6246.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,369030,,,,Thu Jan 30 21:21:30 UTC 2014,,,,,,,,,,"0|i1rltj:",369335,,,,,,,,,,,,,,,,,,,,,"23/Jan/14 20:49;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12624861/HIVE-6246.patch

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 4950 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_groupby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_select
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_union
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1001/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1001/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12624861;;;","23/Jan/14 20:58;xuefuz;It looks like that the above reported failures are not caused by this patch, as they are failing in other pre-commit test runs.;;;","23/Jan/14 20:59;xuefuz;RB: https://reviews.apache.org/r/17262/;;;","23/Jan/14 22:57;kamrul;Left comments in RB.;;;","27/Jan/14 20:35;swarnim;Non committer +1;;;","27/Jan/14 21:48;xuefuz;Patch #1 has no real code changes: comments and method name.;;;","28/Jan/14 05:29;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625450/HIVE-6246.1.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 4962 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_import_exported_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_load_hdfs_file_with_space_in_the_name
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_file_with_header_footer_negative
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1052/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1052/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625450;;;","28/Jan/14 05:43;xuefuz;Above failures are related to HIVE-6310, having nothing to do with the patch here.;;;","28/Jan/14 08:44;leftyl;When this is committed, it needs to be documented in the wiki here:  [Mathematical UDFs |https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types#LanguageManualTypes-MathematicalUDFs] --  just add sign(a) to the list.;;;","29/Jan/14 19:45;prasadm;+1
;;;","30/Jan/14 21:18;xuefuz;Patch committed to trunk. Thanks to Prasad for the review.
Wiki will be updated accordingly.;;;","30/Jan/14 21:21;xuefuz;Wiki is updated, listing sign as one of supported functions for decimal.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HS2 creates DBs/Tables with wrong ownership when HMS setugi is true,HIVE-6245,12689941,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vkorukanti,ctang,ctang,21/Jan/14 03:24,15/Oct/15 17:41,14/Jul/23 06:14,28/Aug/14 23:30,0.12.0,0.13.0,,,,,,,,0.14.0,,HiveServer2,,,,0,,,"The case with following settings is valid but does not work correctly in current HS2:
==
hive.server2.authentication=NONE (or LDAP)
hive.server2.enable.doAs= true
hive.metastore.sasl.enabled=false
hive.metastore.execute.setugi=true
==
Ideally, HS2 is able to impersonate the logged in user (from Beeline, or JDBC application) and create DBs/Tables with user's ownership.
",,apivovarov,brett_s_r,brocknoland,ctang,daisuke.kobayashi,erwaman,mdominguez@cloudera.com,navis,qwertymaniac,serega_sheypak,sho.shimauchi,thejas,vgumashta,vkorukanti,yufeldman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6312,,,HIVE-12188,HIVE-7890,,,,,,,,,,,,,,,,,,HIVE-7807,,"16/May/14 01:27;vkorukanti;HIVE-6245.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12645154/HIVE-6245.2.patch.txt","20/May/14 03:04;vkorukanti;HIVE-6245.3.patch.txt;https://issues.apache.org/jira/secure/attachment/12645717/HIVE-6245.3.patch.txt","20/Aug/14 22:31;vkorukanti;HIVE-6245.4.patch;https://issues.apache.org/jira/secure/attachment/12663248/HIVE-6245.4.patch","28/Aug/14 07:26;vkorukanti;HIVE-6245.5.patch;https://issues.apache.org/jira/secure/attachment/12664846/HIVE-6245.5.patch","21/Jan/14 04:15;ctang;HIVE-6245.patch;https://issues.apache.org/jira/secure/attachment/12624065/HIVE-6245.patch",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,368907,,,,Thu Nov 13 19:42:41 UTC 2014,,,,,,,,,,"0|i1rl1z:",369211,,,,,,,,,,,,,,,,,,,,,"21/Jan/14 04:15;ctang;Fixes include:
1. be able to open an impersonation session in an non-kerberized HS2
2. when working with non-kerberized HMS but with hive.metastore.execute.setugi set to true, remember to close the ThreadLocal Hive object thus avoiding using a stale HMS connection in a new session.;;;","22/Jan/14 22:33;ctang;[~thejas] Could you take a look at this JIRA and comment? I noticed that the change in HIVE-4356 (remove duplicate impersonation parameters for hiveserver2) made HS2 doAs (impersonation) only work with Kerberos env. Thanks;;;","07/Apr/14 09:52;serega_sheypak;We do run ALTER TABLE ADD PARTITION ... LOCATION 'hdfs://bla-bla' through JDBC as oozie-java-action. Sometimes it works, sometimes it doesn't work. HiveServer2 'looses' passed username and sets username='anonymous' and refuses to add partition because user named 'anonymous' has no access right to partition location.
Whe does it happen from time to time? For example we have an oozie coordinator which adds 24 partitions during a day. It can work for a week and then suddenly fails with this 'anonymous' user.
;;;","14/Apr/14 21:55;thejas;This issue should be fixed through the changes in HIVE-6312 (which changed non-kerberos codepath to be same as kerberos code path in this aspect), and HIVE-6864.
;;;","14/Apr/14 22:08;thejas;[~ctang.ma] Apologies for not responding to your comments and patch sooner!
;;;","15/Apr/14 02:50;ctang;[~thejas] No worries, as long as it has been addressed in recent HIVE-6312 and HIVE-6864. I am going to verify them with my case as well. Thanks!;;;","15/Apr/14 03:24;thejas;Marking as duplicate as fixes were part of different jiras.
Please re-open if you still see the same issue.
;;;","16/May/14 01:22;vkorukanti;This looks like still a problem on trunk. I tried on latest trunk. Problem seems be that {{sessionHive}} object in {{HiveSessionImplwithUGI}} is never initialized if the authentication mechanism is not {{KERBEROS}}. Currently {{sessionHive}} is initialized in {{HiveSessionImplwithUGI.setDelegationToken}} only if the delegation token is not null. Delegation token is not null when authentication mechanism is {{KERBEROS}}. As {{sessionHive}} is null when {{HiveSessionImplwithUGI.acquire()}} is called a {{Hive}} object with MetaStoreClient of this session user is not set. So whatever the worker thread has {{Hive}} object in its thread variable, it will get used.

To repro it consistently set the following parameters in hive-site.xml and restart hiveserver2. And try creating tables as two different users.
{code}
hive.server2.thrift.min.worker.threads=1;
hive.server2.thrift.max.worker.threads=1;
{code};;;","16/May/14 01:27;vkorukanti;Attached a patch which fixed the issue. It doesn't have any test case. I am in the process of figuring out how to write a test case where both MetaStore and HiveServer2 are running.

[~vgumashta] Appreciate if you have any feedback on fix.;;;","16/May/14 20:46;thejas;Reopening the jira based on Venki's report.
;;;","20/May/14 03:04;vkorukanti;Attaching a patch with testcase.;;;","20/May/14 03:10;vkorukanti;Adding review board link;;;","20/May/14 21:09;vgumashta;[~vkorukanti] Thanks for the patch. I'll look at it now. As a workaround, you can use HS2 with embedded metastore. ;;;","21/May/14 03:23;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12645717/HIVE-6245.3.patch.txt

{color:red}ERROR:{color} -1 due to 13 failed/errored test(s), 5527 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency
org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus
org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/250/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/250/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-250/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 13 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12645717;;;","21/May/14 04:37;vkorukanti;Failures are not related to the change.;;;","05/Aug/14 02:32;apivovarov;I applied patch HIVE-6245.3.patch.txt to HDP-2.1.3.
hive.server2.authentication=NONE
""create table ..."" creates hdfs folder with logged in user ownership
;;;","05/Aug/14 18:16;apivovarov;Found one issue with patch #3
hcatalog can not be buit

[ERROR] /home/alex/workLC/hive-release-2.1.3/hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/TestPermsGrp.java:[79,19] method startMetaStore in class org.apache.hadoop.hive.metastore.MetaStoreUtils cannot be applied to given types;
[ERROR] required: int,org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge,org.apache.hadoop.hive.conf.HiveConf
[ERROR] found: int,org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge
[ERROR] reason: actual and formal argument lists differ in length
[ERROR] /home/alex/workLC/hive-release-2.1.3/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatPartitionPublish.java:[108,19] method startMetaStore in class org.apache.hadoop.hive.metastore.MetaStoreUtils cannot be applied to given types;
[ERROR] required: int,org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge,org.apache.hadoop.hive.conf.HiveConf
[ERROR] found: int,org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge
[ERROR] reason: actual and formal argument lists differ in length
[ERROR] /home/alex/workLC/hive-release-2.1.3/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestHCatPartitionPublish.java:[101,19] method startMetaStore in class org.apache.hadoop.hive.metastore.MetaStoreUtils cannot be applied to given types;
[ERROR] required: int,org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge,org.apache.hadoop.hive.conf.HiveConf
[ERROR] found: int,org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge
[ERROR] reason: actual and formal argument lists differ in length
[ERROR] /home/alex/workLC/hive-release-2.1.3/hcatalog/core/src/test/java/org/apache/hcatalog/cli/TestPermsGrp.java:[82,19] method startMetaStore in class org.apache.hadoop.hive.metastore.MetaStoreUtils cannot be applied to given types;
[ERROR] required: int,org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge,org.apache.hadoop.hive.conf.HiveConf
[ERROR] found: int,org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge
[ERROR] reason: actual and formal argument lists differ in length


;;;","05/Aug/14 20:09;apivovarov;Skip my last comment pls.
Hive build works fine with HIVE-6245.3.patch.txt;;;","20/Aug/14 22:31;vkorukanti;Attaching new patch.

Rebased on latest trunk, found out that the test is not working anymore. Had to do some changes to MiniHS2 also HIVE-7807.;;;","21/Aug/14 01:11;navis;[~thejas] Could you review this? ;;;","22/Aug/14 06:53;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12663248/HIVE-6245.4.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6116 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_join
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_opt_vectorization
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/451/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/451/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-451/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12663248;;;","27/Aug/14 18:51;brocknoland;I have been working on a patch to fix the same thing in HIVE-7890. I tested this patch and it works for me so +1 from my side.

Since we've hit this a number of times I actually think we should commit both this one and HIVE-7890.;;;","28/Aug/14 06:53;thejas;Looks good. I just have some minor comments, added them in reviewboard.
;;;","28/Aug/14 07:26;vkorukanti;Attaching patch v5. Addressed review comments.;;;","28/Aug/14 10:21;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12664846/HIVE-6245.5.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6127 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_opt_vectorization
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/542/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/542/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-542/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12664846;;;","28/Aug/14 15:51;vkorukanti;Ran 2 failed tests locally and both pass successfully.;;;","28/Aug/14 16:03;apivovarov;testCliDriver_dynpart_sort_opt_vectorization  failed in several previous builds as well.  (538-541)

TestHiveServer2.testConnection failed in build 540 too
http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/540/testReport/

Your build 542 does not have any additional failed test methods.;;;","28/Aug/14 16:17;vkorukanti;Actually testCliDriver_dynpart_sort_opt_vectorization still fails locally, for some reason it didn't run previously might be due to incorrect maven args in my test run.;;;","28/Aug/14 16:54;thejas;+1;;;","28/Aug/14 23:30;brocknoland;Thank you so much Venki! I have committed this to trunk!;;;","04/Sep/14 06:03;thejas;[~venki387] The test in TestHS2ImpersonationWithRemoteMS seems to be failing frequently. Would you be able to take a look at it ?
;;;","04/Sep/14 06:40;vkorukanti;[~thejas] Sorry about that. I see in all Hive QA runs where the test failed, second connection is failed to establish. There isn't any info in hive.log to narrow down. I will try to repro locally and update (HIVE-7942 is created for tracking).;;;","13/Nov/14 19:42;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
JOBS testsuite in WebHCat E2E tests does not work correctly in secure mode,HIVE-6233,12689885,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,deepesh,deepesh,deepesh,20/Jan/14 21:34,12/Feb/14 22:40,14/Jul/23 06:14,12/Feb/14 22:40,0.13.0,,,,,,,,,0.13.0,,Tests,WebHCat,,,0,,,"JOBS testsuite performs operations with two users test.user.name and test.other.user.name. In Kerberos secure mode it should kinit as the respective user.
NO PRECOMMIT TESTS",,daijy,deepesh,dschorow,ekoifman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/14 22:01;deepesh;HIVE-6233.patch;https://issues.apache.org/jira/secure/attachment/12624019/HIVE-6233.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,368851,Reviewed,,,Wed Feb 12 22:40:33 UTC 2014,,,,,,,,,,"0|i1rkpj:",369155,,,,,,,,,,,,,,,,,,,,,"20/Jan/14 22:01;deepesh;Attaching a patch for review with the following changes:
- kinit with relevant user between individual tests
- rolled hcat-authorization and jobstatus tests into test-multi-users target in build.xml;;;","12/Feb/14 21:54;ekoifman;can ""test-multi-users"" target be run in non-secure cluster and still have it test something meaningful?
If so, then the changes look good.  If not, I'm concerned that moving tests from ""test"" will make them be run less often during normal dev cycle...
;;;","12/Feb/14 22:00;deepesh;Thanks [~ekoifman] for the review! Yes, the test can be run on non-secure clusters as well and does provide value in testing multi-user scenarios for validating user/group permissions.;;;","12/Feb/14 22:15;ekoifman;+1;;;","12/Feb/14 22:40;daijy;Patch committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when switching to Tez execution mode after session has been initialized,HIVE-6231,12689860,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,20/Jan/14 19:34,22/Jan/14 07:35,14/Jul/23 06:14,22/Jan/14 07:35,,,,,,,,,,0.13.0,,,,,,0,,,,,hagleitn,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/14 19:36;hagleitn;HIVE-6231.1.patch;https://issues.apache.org/jira/secure/attachment/12623989/HIVE-6231.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,368827,,,,Wed Jan 22 07:35:48 UTC 2014,,,,,,,,,,"0|i1rkk7:",369131,,,,,,,,,,,,,,,,,,,,,"20/Jan/14 19:36;hagleitn;We're dynamically creating a session in TezTask if there is none yet. There's a bug in that though that causes NPE when opening the newly created session.;;;","20/Jan/14 19:39;vikram.dixit;LGTM +1 pending test run.;;;","20/Jan/14 23:11;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12623989/HIVE-6231.1.patch

{color:green}SUCCESS:{color} +1 4943 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/965/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/965/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12623989;;;","22/Jan/14 07:35;hagleitn;Committed to trunk. Thanks for the review Vikram!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stats are missing sometimes (regression from HIVE-5936),HIVE-6229,12689693,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,navis,navis,navis,19/Jan/14 05:53,23/Mar/14 08:10,14/Jul/23 06:14,22/Jan/14 01:25,,,,,,,,,,0.13.0,,Statistics,,,,0,,,"if prefix length is smaller than hive.stats.key.prefix.max.length but length of prefix + postfix is bigger than that, stats are missed.",,leftyl,navis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6144,HIVE-6205,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/14 06:32;navis;HIVE-6229.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12623851/HIVE-6229.1.patch.txt","20/Jan/14 11:25;navis;HIVE-6229.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12623937/HIVE-6229.2.patch.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,368660,,,,Sun Mar 23 08:10:36 UTC 2014,,,,,,,,,,"0|i1rjj3:",368964,,,,,,,,,,,,,,,,,,,,,"19/Jan/14 08:24;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12623851/HIVE-6229.1.patch.txt

{color:red}ERROR:{color} -1 due to 22 failed/errored test(s), 4943 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats19
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_smb_mapjoin_8
org.apache.hadoop.hive.ql.parse.TestParse.testParse_case_sensitivity
org.apache.hadoop.hive.ql.parse.TestParse.testParse_groupby1
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input1
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input2
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input3
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input5
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input6
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input7
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input9
org.apache.hadoop.hive.ql.parse.TestParse.testParse_input_testsequencefile
org.apache.hadoop.hive.ql.parse.TestParse.testParse_join1
org.apache.hadoop.hive.ql.parse.TestParse.testParse_join2
org.apache.hadoop.hive.ql.parse.TestParse.testParse_join3
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample2
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample3
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample5
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample6
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample7
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/959/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/959/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 22 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12623851;;;","19/Jan/14 18:32;ashutoshc;+1;;;","20/Jan/14 05:16;navis;[~ashutoshc] SourceTask in StatsWork is not valid when the task is exchanged or removed (auto.convert.join, etc.), which fails to access stats from StatsTask when using counter type. I think it would be better to fix that with this altogether.;;;","20/Jan/14 05:19;ashutoshc;I see. Yeah, would be good to fix that too.;;;","20/Jan/14 13:30;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12623937/HIVE-6229.2.patch.txt

{color:green}SUCCESS:{color} +1 4943 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/962/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/962/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12623937;;;","22/Jan/14 01:25;navis;Committed to trunk. Thanks for the review Ashutosh!;;;","22/Jan/14 10:39;leftyl;This adds hive.stats.key.prefix.reserve.length to HiveConf.java and hive-default.xml.template.

I can document it in the wiki (for Hive 0.13.0) along with hive.stats.key.prefix.max.length, but hive.stats.key.prefix has an ""internal usage only"" comment so are these other two hive.stats.key.prefix.* config params also internal use only?;;;","23/Jan/14 06:15;navis;""hive.stats.key.prefix"" is the key for JobConf instance used only for partial scan command. I think the name of config is a little misleading. (It's not related with other two similar-named configurations);;;","23/Jan/14 09:32;leftyl;Okay, I'll document the other two and there shouldn't be any confusion in the wikidoc because hive.stats.key.prefix won't be documented with them.  Thanks.;;;","23/Mar/14 08:10;leftyl;*hive.stats.key.prefix.reserve.length* is now documented in the wiki, but its description needs review and clarification:

{quote}
Reserved length for postfix of statistics key. Currently only meaningful for counter type statistics which should keep the length of the full statistics key smaller than the maximum length configured by hive.stats.key.prefix.max.length. For counter type statistics, it should be bigger than the length of LB spec if exists.
{quote}

What does ""LB spec"" mean?  All I could find by googling was the Ljung–Box test.  Does ""if exists"" mean ""if the LB spec exists""?  And what is ""it"" in ""it should be bigger"" -- this parameter's setting or the length of the full statistics key?

* [Configuration Properties:  hive.stats.key.prefix.reserve.length |https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.stats.key.prefix.reserve.length];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WebHCat E2E test JOBS_7 fails,HIVE-6227,12689611,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,deepesh,deepesh,deepesh,18/Jan/14 02:58,20/Jan/14 18:19,14/Jul/23 06:14,20/Jan/14 07:03,0.13.0,,,,,,,,,0.13.0,,Tests,,,,0,,,"WebHCat E2E test JOBS_7 fails while verifying the job status of a TempletonControllerJob and its child pig job. The filter currently is such that only pig jobs are looked at, it should also include TempletonControllerJob.",,daijy,deepesh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/14 03:00;deepesh;HIVE-6227.patch;https://issues.apache.org/jira/secure/attachment/12623770/HIVE-6227.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,368578,Reviewed,,,Mon Jan 20 18:19:05 UTC 2014,,,,,,,,,,"0|i1rj0v:",368882,,,,,,,,,,,,,,,,,,,,,"18/Jan/14 03:00;deepesh;Attaching the patch for review.;;;","18/Jan/14 03:36;daijy;+1;;;","18/Jan/14 09:35;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12623770/HIVE-6227.patch

{color:green}SUCCESS:{color} +1 4942 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/955/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/955/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12623770;;;","20/Jan/14 07:03;daijy;Patch committed to trunk.;;;","20/Jan/14 18:19;deepesh;Thanks [~daijy] for review and commit.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unneeded tez dependencies from hive,HIVE-6224,12689578,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,vikram.dixit,vikram.dixit,vikram.dixit,17/Jan/14 22:18,23/Mar/14 17:35,14/Jul/23 06:14,20/Jan/14 04:11,0.13.0,,,,,,,,,0.13.0,,Build Infrastructure,Tez,,,0,,,"After re-organization of some of the classes in tez, we no longer need to depend on certain packages. Removing these from the shims and from the tests dependencies.",,hagleitn,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/14 22:38;vikram.dixit;HIVE-6224.1.patch;https://issues.apache.org/jira/secure/attachment/12623740/HIVE-6224.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,368545,,,,Mon Jan 20 04:11:27 UTC 2014,,,,,,,,,,"0|i1ritj:",368849,,,,,,,,,,,,,,,,,,,,,"18/Jan/14 00:13;hagleitn;+1 will commit when tests pass.;;;","18/Jan/14 04:55;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12623740/HIVE-6224.1.patch

{color:green}SUCCESS:{color} +1 4942 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/953/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/953/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12623740;;;","20/Jan/14 04:11;hagleitn;Committed to trunk. Thanks Vikram!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stats for row-count not getting updated with Tez insert + dbclass=counter,HIVE-6218,12689387,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,hagleitn,gopalv,gopalv,16/Jan/14 22:47,12/Feb/14 05:21,14/Jul/23 06:14,12/Feb/14 05:21,0.13.0,,,,,,,,,0.13.0,,Statistics,Tez,,,0,,,"Inserting data into hive with Tez,  the stats on row-count is not getting updated when using the counter dbclass.

To reproduce, run ""ANALYZE TABLE store_sales COMPUTE STATISTICS;"" with tez as the execution engine.",,gopalv,hagleitn,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6295,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jan/14 06:37;hagleitn;HIVE-6218.1.patch;https://issues.apache.org/jira/secure/attachment/12625802/HIVE-6218.1.patch","30/Jan/14 02:11;hagleitn;HIVE-6218.2.patch;https://issues.apache.org/jira/secure/attachment/12626042/HIVE-6218.2.patch","10/Feb/14 21:42;hagleitn;HIVE-6218.3.patch;https://issues.apache.org/jira/secure/attachment/12628066/HIVE-6218.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,368354,,,,Wed Feb 12 05:21:18 UTC 2014,,,,,,,,,,"0|i1rhnb:",368659,,,,,,,,,,,,tez,,,,,,,,,"24/Jan/14 08:22;hagleitn;Looked into this. Turns out that the problem is that we're running analyze as MR via Tez' yarn runner. That one drops the required counters on the floor. Best fix is to probably just do the stats computation directly in Tez. I'll get on that.;;;","29/Jan/14 06:41;hagleitn;rb: https://reviews.apache.org/r/17484/;;;","29/Jan/14 14:59;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625802/HIVE-6218.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 4972 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1097/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1097/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625802;;;","30/Jan/14 02:11;hagleitn;Fix issues w/ unit tests in .2;;;","31/Jan/14 09:07;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626042/HIVE-6218.2.patch

{color:green}SUCCESS:{color} +1 4980 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1124/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1124/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626042;;;","06/Feb/14 21:55;vikram.dixit;+1;;;","10/Feb/14 21:42;hagleitn;.3 is rebased. Needed to update the golden files since the output has changed since this patch was created.;;;","12/Feb/14 00:52;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12628066/HIVE-6218.3.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5086 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_revoke_table_priv
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1275/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1275/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12628066;;;","12/Feb/14 05:21;hagleitn;Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update datanucleus.connectionPoolingType to BoneCP in hive-default.xml.template,HIVE-6216,12689324,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,Fixed,vgumashta,vgumashta,vgumashta,16/Jan/14 19:02,31/Jan/14 12:27,14/Jul/23 06:14,16/Jan/14 22:11,0.13.0,,,,,,,,,0.13.0,,Configuration,,,,0,,,"BoneCP should be the recommended default. We're upgrading the BoneCP version here: [HIVE-6170|https://issues.apache.org/jira/browse/HIVE-6170].",,hsubramaniyan,leftyl,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6170,,,,,,,,,,,,,,,,,,,,,"16/Jan/14 19:06;vgumashta;HIVE-6216.1.patch;https://issues.apache.org/jira/secure/attachment/12623443/HIVE-6216.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,368291,,,,Fri Jan 31 12:27:29 UTC 2014,,,,,,,,,,"0|i1rh9b:",368596,,,,,,,,,,,,,,,,,,,,,"16/Jan/14 19:06;vgumashta;cc [~ashutoshc];;;","16/Jan/14 19:44;hsubramaniyan;+1;;;","16/Jan/14 20:57;leftyl;When this is committed I'll fix the wiki (with version information). ;;;","16/Jan/14 21:19;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12623443/HIVE-6216.1.patch

{color:green}SUCCESS:{color} +1 4927 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/938/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/938/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12623443;;;","16/Jan/14 21:57;ashutoshc;+1;;;","16/Jan/14 22:11;ashutoshc;Committed to trunk. Thanks, Vaibhav!;;;","31/Jan/14 12:27;leftyl;Updated the wiki -- search for ""datanucleus.connection"" in [Configuration Properties:  MetaStore|https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-MetaStore].

{quote}
datanucleus.connectionPoolingType
Default Value: DBCP in Hive 0.7 to 0.11; BoneCP in 0.12 and later 
Added In: 0.7
Uses a BoneCP connection pool for JDBC metastore in release 0.12 and later (HIVE-4807), or a DBCP connection pool in releases 0.7 to 0.11.
{quote}

Question:  Is the value case-sensitive?  It's ""BoneCP"" in hive-default.xml.template but ""BONECP"" in HiveConf.java.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WebHCat job status E2E tests fail in presence of other jobs,HIVE-6211,12689170,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,deepesh,deepesh,deepesh,16/Jan/14 05:07,16/Jan/14 05:58,14/Jul/23 06:14,16/Jan/14 05:58,0.13.0,,,,,,,,,0.13.0,,Testing Infrastructure,,,,0,,,"Some job status related system tests in the WebHCat E2E testsuite fail intermittently when other MR jobs are run in the cluster running the tests.
The testsuite during verification should improve to handle the above situation.
NO PRECOMMIT TESTS",,daijy,deepesh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/14 05:09;deepesh;HIVE-6211.patch;https://issues.apache.org/jira/secure/attachment/12623311/HIVE-6211.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,368137,Reviewed,,,Thu Jan 16 05:58:58 UTC 2014,,,,,,,,,,"0|i1rgbb:",368442,,,,,,,,,,,,,,,,,,,,,"16/Jan/14 05:09;deepesh;Attaching the patch for review.;;;","16/Jan/14 05:58;daijy;+1. Patch committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'LOAD DATA INPATH ... OVERWRITE ..' doesn't overwrite current data,HIVE-6209,12689142,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,szehon,szehon,szehon,16/Jan/14 01:02,03/Mar/14 08:55,14/Jul/23 06:14,31/Jan/14 23:16,0.12.0,,,,,,,,,0.13.0,,,,,,0,,,"In case where user loads data into table using overwrite, using a different file, it is not being overwritten.

{code}
$ hdfs dfs -cat /tmp/data
aaa
bbb
ccc
$ hdfs dfs -cat /tmp/data2
ddd
eee
fff
$ hive
hive> create table test (id string); 
hive> load data inpath '/tmp/data' overwrite into table test;
hive> select * from test;
aaa
bbb
ccc
hive> load data inpath '/tmp/data2' overwrite into table test;
hive> select * from test;
aaa
bbb
ccc
ddd
eee
fff
{code}

It seems it is broken by HIVE-3756 which added another condition to whether ""rmr"" should be run on old directory, and skips in this case.

There is a workaround of set fs.hdfs.impl.disable.cache=true; 
which sabotages this condition, but this condition should be removed in long-term.",,daisuke.kobayashi,glenn.strycker@gmail.com,prasadm,sho.shimauchi,szehon,tzenmyo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-5926,,,,,,,,,,,,,,,,,,,,,,HIVE-3756,,,,,,,,,"22/Jan/14 03:36;szehon;HIVE-6209.1.patch;https://issues.apache.org/jira/secure/attachment/12624268/HIVE-6209.1.patch","17/Jan/14 18:43;szehon;HIVE-6209.patch;https://issues.apache.org/jira/secure/attachment/12623703/HIVE-6209.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,368109,,,,Fri Jan 31 23:16:00 UTC 2014,,,,,,,,,,"0|i1rg53:",368414,,,,,,,,,,,,,,,,,,,,,"16/Jan/14 01:46;szehon;Attaching a fix.;;;","17/Jan/14 18:43;szehon;Attaching again, not sure why pre commit tests not triggered.;;;","17/Jan/14 22:37;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12623703/HIVE-6209.patch

{color:green}SUCCESS:{color} +1 4940 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/950/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/950/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12623703;;;","20/Jan/14 22:52;prasadm;Looks fine to me. Some minor suggestions on the reviewboard.;;;","22/Jan/14 03:36;szehon;Adding a unit test, as per review comments.;;;","22/Jan/14 14:30;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12624268/HIVE-6209.1.patch

{color:green}SUCCESS:{color} +1 4944 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/987/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/987/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12624268;;;","22/Jan/14 17:17;prasadm;Thanks for addressing the suggestions. Looks fine to me.

+1
;;;","31/Jan/14 23:16;prasadm;Patch committed to trunk. Thanks for your contribution Szehon!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
user-defined aggregate functions cannot be used as windowing function,HIVE-6208,12689090,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,jdere,jdere,jdere,15/Jan/14 21:07,17/Jan/14 05:29,14/Jul/23 06:14,17/Jan/14 05:29,,,,,,,,,,0.13.0,,UDF,,,,1,,,"Function registry does a pass to register all GenericUDAFs as window functions. However any aggregate functions added after this (such as a user-added temporary function) don't work as window functions:


hive> create temporary function mysum as 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum';
OK
Time taken: 0.0050 seconds
hive> explain select mysum(key) over () from src;                                               
FAILED: NullPointerException null


java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.parse.PTFTranslator.translate(PTFTranslator.java:354)
	at org.apache.hadoop.hive.ql.parse.PTFTranslator.translate(PTFTranslator.java:194)
	at org.apache.hadoop.hive.ql.parse.WindowingComponentizer.next(WindowingComponentizer.java:86)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genWindowingPlan(SemanticAnalyzer.java:10721)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:7904)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:7862)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:8678)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8904)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:310)
	at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:65)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:310)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:440)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:996)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1039)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:932)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:922)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:424)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
",,erwaman,jdere,rhbutani,SteveWaggoner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/14 21:52;jdere;HIVE-6208.1.patch;https://issues.apache.org/jira/secure/attachment/12623239/HIVE-6208.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,368057,,,,Fri Jan 17 05:29:01 UTC 2014,,,,,,,,,,"0|i1rfun:",368363,,,,,,,,,,,,,,,,,,,,,"15/Jan/14 21:08;jdere;Also should not be getting NPE here if the window function cannot be found.;;;","15/Jan/14 21:58;jdere;RB at https://reviews.apache.org/r/16921/;;;","16/Jan/14 03:51;rhbutani;+1;;;","16/Jan/14 03:55;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12623239/HIVE-6208.1.patch

{color:green}SUCCESS:{color} +1 4929 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/925/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/925/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12623239;;;","17/Jan/14 05:29;rhbutani;thanks Jason;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
alter <table> partition column throws NPE in authorization,HIVE-6205,12688954,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,navis,navis,navis,15/Jan/14 07:34,25/Jan/14 20:28,14/Jul/23 06:14,25/Jan/14 20:28,,,,,,,,,,0.13.0,,Authorization,,,,0,,,"alter table alter_coltype partition column (dt int);
{noformat}
2014-01-15 15:53:40,364 ERROR ql.Driver (SessionState.java:printError(457)) - FAILED: NullPointerException null
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.Driver.doAuthorization(Driver.java:599)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:479)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:996)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1039)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:932)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:922)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:424)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:197)
{noformat}

Operation for TOK_ALTERTABLE_ALTERPARTS is not defined.",,navis,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6229,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/14 07:39;navis;HIVE-6205.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12623084/HIVE-6205.1.patch.txt","16/Jan/14 02:08;navis;HIVE-6205.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12623293/HIVE-6205.2.patch.txt","17/Jan/14 00:39;navis;HIVE-6205.3.patch.txt;https://issues.apache.org/jira/secure/attachment/12623543/HIVE-6205.3.patch.txt","22/Jan/14 01:39;navis;HIVE-6205.4.patch.txt;https://issues.apache.org/jira/secure/attachment/12624251/HIVE-6205.4.patch.txt","24/Jan/14 01:02;navis;HIVE-6205.5.patch.txt;https://issues.apache.org/jira/secure/attachment/12624966/HIVE-6205.5.patch.txt",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,367921,,,,Sat Jan 25 20:28:47 UTC 2014,,,,,,,,,,"0|i1rf0v:",368228,,,,,,,,,,,,,,,,,,,,,"15/Jan/14 07:39;navis;Running preliminary test;;;","15/Jan/14 16:27;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12623084/HIVE-6205.1.patch.txt

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/923/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/923/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/contrib/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/contrib/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/contrib/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/contrib/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-contrib ---
[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/contrib/target/test-classes
[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/contrib/src/test/org/apache/hadoop/hive/contrib/serde2/TestRegexSerDe.java uses or overrides a deprecated API.
[WARNING] Note: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-contrib ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-contrib ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/contrib/target/hive-contrib-0.13.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-contrib ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/contrib/target/hive-contrib-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-contrib/0.13.0-SNAPSHOT/hive-contrib-0.13.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/contrib/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-contrib/0.13.0-SNAPSHOT/hive-contrib-0.13.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive HBase Handler 0.13.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-hbase-handler ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-hbase-handler ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/main/resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-hbase-handler ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-hbase-handler ---
[INFO] Compiling 18 source files to /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/target/classes
[WARNING] Note: Some input files use or override a deprecated API.
[WARNING] Note: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-hbase-handler ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/test/resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hbase-handler ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hbase-handler ---
[INFO] Compiling 4 source files to /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/target/test-classes
[WARNING] Note: Some input files use or override a deprecated API.
[WARNING] Note: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hbase-handler ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hbase-handler ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/target/hive-hbase-handler-0.13.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hbase-handler ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/target/hive-hbase-handler-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hbase-handler/0.13.0-SNAPSHOT/hive-hbase-handler-0.13.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hbase-handler/0.13.0-SNAPSHOT/hive-hbase-handler-0.13.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive HCatalog 0.13.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-hcatalog ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/hcatalog (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-hcatalog ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hcatalog ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hcatalog ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hcatalog/hive-hcatalog/0.13.0-SNAPSHOT/hive-hcatalog-0.13.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive HCatalog Core 0.13.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-hcatalog-core ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/core (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-hcatalog-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/core/src/main/resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-hcatalog-core ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-hcatalog-core ---
[INFO] Compiling 144 source files to /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/core/target/classes
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] Note: Some input files use or override a deprecated API.
[WARNING] Note: Recompile with -Xlint:deprecation for details.
[WARNING] Note: Some input files use unchecked or unsafe operations.
[WARNING] Note: Recompile with -Xlint:unchecked for details.
[INFO] 4 warnings 
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java:[78,80] cannot find symbol
symbol  : variable TOK_ALTERTABLE_ALTERPARTS_MERGEFILES
location: class org.apache.hadoop.hive.ql.parse.HiveParser
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/core/src/main/java/org/apache/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java:[81,80] cannot find symbol
symbol  : variable TOK_ALTERTABLE_ALTERPARTS_MERGEFILES
location: class org.apache.hadoop.hive.ql.parse.HiveParser
[INFO] 2 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [4.646s]
[INFO] Hive Ant Utilities ................................ SUCCESS [8.090s]
[INFO] Hive Shims Common ................................. SUCCESS [2.583s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.159s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [2.632s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [1.428s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [3.399s]
[INFO] Hive Shims ........................................ SUCCESS [0.595s]
[INFO] Hive Common ....................................... SUCCESS [7.545s]
[INFO] Hive Serde ........................................ SUCCESS [8.691s]
[INFO] Hive Metastore .................................... SUCCESS [25.228s]
[INFO] Hive Query Language ............................... SUCCESS [54.811s]
[INFO] Hive Service ...................................... SUCCESS [6.657s]
[INFO] Hive JDBC ......................................... SUCCESS [1.399s]
[INFO] Hive Beeline ...................................... SUCCESS [1.064s]
[INFO] Hive CLI .......................................... SUCCESS [2.005s]
[INFO] Hive Contrib ...................................... SUCCESS [1.648s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.837s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.469s]
[INFO] Hive HCatalog Core ................................ FAILURE [1.821s]
[INFO] Hive HCatalog Pig Adapter ......................... SKIPPED
[INFO] Hive HCatalog Server Extensions ................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client ................. SKIPPED
[INFO] Hive HCatalog Webhcat ............................. SKIPPED
[INFO] Hive HCatalog HBase Storage Handler ............... SKIPPED
[INFO] Hive HWI .......................................... SKIPPED
[INFO] Hive ODBC ......................................... SKIPPED
[INFO] Hive Shims Aggregator ............................. SKIPPED
[INFO] Hive TestUtils .................................... SKIPPED
[INFO] Hive Packaging .................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 2:22.706s
[INFO] Finished at: Wed Jan 15 11:27:31 EST 2014
[INFO] Final Memory: 53M/454M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-hcatalog-core: Compilation failure: Compilation failure:
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java:[78,80] cannot find symbol
[ERROR] symbol  : variable TOK_ALTERTABLE_ALTERPARTS_MERGEFILES
[ERROR] location: class org.apache.hadoop.hive.ql.parse.HiveParser
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/core/src/main/java/org/apache/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java:[81,80] cannot find symbol
[ERROR] symbol  : variable TOK_ALTERTABLE_ALTERPARTS_MERGEFILES
[ERROR] location: class org.apache.hadoop.hive.ql.parse.HiveParser
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-hcatalog-core
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12623084;;;","16/Jan/14 02:08;navis;Fixed build fail;;;","16/Jan/14 16:36;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12623293/HIVE-6205.2.patch.txt

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 4927 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_coltype
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_partition_coltype_2columns
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/933/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/933/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12623293;;;","22/Jan/14 11:25;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12624251/HIVE-6205.4.patch.txt

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4943 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveOperationType.checkHiveOperationTypeMatch
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/984/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/984/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12624251;;;","25/Jan/14 03:47;thejas;+1;;;","25/Jan/14 15:59;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12624966/HIVE-6205.5.patch.txt

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 4958 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_import_exported_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_load_hdfs_file_with_space_in_the_name
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_file_with_header_footer_negative
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1016/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1016/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12624966;;;","25/Jan/14 20:28;thejas;Patch committed to trunk. Thanks for the contribution Navis!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Privileges of role granted indrectily to user is not applied,HIVE-6203,12688940,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,navis,navis,navis,15/Jan/14 05:18,21/Mar/14 15:51,14/Jul/23 06:14,21/Mar/14 15:51,,,,,,,,,,0.13.0,,Authorization,,,,0,,,"For example, 
{noformat}
create role r1;
create role r2;
grant select on table eq to role r1;
grant role r1 to role r2;
grant role r2 to user admin;
select * from eq limit 5;
{noformat}

admin -> r2 -> r1 -> SEL on table eq
but user admin fails to access table eq",,ashutoshc,cdrome,navis,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6122,,,,,,,HIVE-5954,,,,,,,,,,HIVE-5954,,,,,,,,,,,,,,,,,,,,,"12/Feb/14 09:23;navis;HIVE-6203.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12628456/HIVE-6203.1.patch.txt","14/Feb/14 06:02;navis;HIVE-6203.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12628956/HIVE-6203.2.patch.txt","17/Feb/14 01:49;navis;HIVE-6203.3.patch.txt;https://issues.apache.org/jira/secure/attachment/12629297/HIVE-6203.3.patch.txt","17/Feb/14 09:04;navis;HIVE-6203.4.patch.txt;https://issues.apache.org/jira/secure/attachment/12629343/HIVE-6203.4.patch.txt",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,367907,,,,Fri Mar 21 15:51:31 UTC 2014,,,,,,,,,,"0|i1rexr:",368214,,,,,,,,,,,,,,,,,,,,,"12/Feb/14 09:24;navis;Preliminary test;;;","13/Feb/14 04:00;thejas;There is already get_privilege_set that returns privileges for the roles as well. The right behavior for it is to return the privileges through the indirect roles as well. I think it is better to re-use that instead of adding another thrift api.
It should be straightforward to change that to finally call a version of list_roles that also looks at indirect roles.
;;;","13/Feb/14 10:21;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12628456/HIVE-6203.1.patch.txt

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1305/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1305/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-1305/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseSerDe.java'
Reverted 'hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStorageHandler.java'
Reverted 'hbase-handler/src/java/org/apache/hadoop/hive/hbase/LazyHBaseRow.java'
Reverted 'hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseCompositeKey.java'
Reverted 'hbase-handler/pom.xml'
Reverted 'itests/util/pom.xml'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyObjectBase.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyObject.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/LazySimpleStructObjectInspector.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyStruct.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/columnar/ColumnarStructBase.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryObject.java'
Reverted 'serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryStruct.java'
++ egrep -v '^X|^Performing status on external'
++ svn status --no-ignore
++ awk '{print $2}'
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target hbase-handler/src/test/results/positive/hbase_custom_key.q.out hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestHBaseKeyFactory.java hbase-handler/src/test/queries/positive/hbase_custom_key.q hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseLazyObjectFactory.java hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseKeyFactory.java testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/storage-handlers/hbase/target hcatalog/server-extensions/target hcatalog/core/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target hwi/target common/target common/src/gen contrib/target service/target serde/target serde/src/java/org/apache/hadoop/hive/serde2/StructObjectBaseInspector.java serde/src/java/org/apache/hadoop/hive/serde2/StructObject.java beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1567876.

At revision 1567876.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12628456;;;","14/Feb/14 06:05;navis;Yes, it's for minimizing the code difference. If test passes, I can rewrite it as you suggested.;;;","14/Feb/14 22:49;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12628956/HIVE-6203.2.patch.txt

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1330/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1330/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-1330/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'conf/hive-default.xml.template'
Reverted 'itests/hive-unit/src/test/java/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java'
Reverted 'itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java'
Reverted 'common/src/test/org/apache/hadoop/hive/conf/TestHiveConf.java'
Reverted 'common/src/test/org/apache/hadoop/hive/conf/TestHiveLogging.java'
Reverted 'common/src/test/org/apache/hadoop/hive/conf/TestHiveConfRestrictList.java'
Reverted 'common/src/java/org/apache/hadoop/hive/conf/HiveConf.java'
Reverted 'common/pom.xml'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/plan/DDLWork.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/VariableSubstitution.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveUtils.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFile.java'
++ egrep -v '^X|^Performing status on external'
++ awk '{print $2}'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/storage-handlers/hbase/target hcatalog/server-extensions/target hcatalog/core/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target hwi/target common/target common/src/gen common/src/java/org/apache/hadoop/hive/ant common/src/java/org/apache/hadoop/hive/conf/Validator.java common/src/java/org/apache/hive/common/util/SystemVariables.java contrib/target service/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target ql/src/test/results/clientpositive/show_conf.q.out ql/src/test/queries/clientpositive/show_conf.q ql/src/java/org/apache/hadoop/hive/ql/plan/ShowConfDesc.java
+ svn update
U    pom.xml
A    ql/src/test/queries/clientnegative/authorization_disallow_transform.q
A    ql/src/test/results/clientnegative/authorization_disallow_transform.q.out
A    ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/DisallowTransformHook.java
U    ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java

Fetching external item into 'hcatalog/src/test/e2e/harness'
Updated external to revision 1568539.

Updated to revision 1568539.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12628956;;;","17/Feb/14 01:49;navis;Rebased to trunk;;;","17/Feb/14 08:47;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12629297/HIVE-6203.3.patch.txt

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1355/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1355/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-1355/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
++ egrep -v '^X|^Performing status on external'
++ awk '{print $2}'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/storage-handlers/hbase/target hcatalog/server-extensions/target hcatalog/core/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target hwi/target common/target common/src/gen service/target contrib/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target
+ svn update
A    ql/src/test/queries/clientpositive/vector_coalesce.q
A    ql/src/test/results/clientpositive/vector_coalesce.q.out
U    ql/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java
U    ql/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java
U    ql/src/java/org/apache/hadoop/hive/ql/exec/vector/LongColumnVector.java
U    ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java
U    ql/src/java/org/apache/hadoop/hive/ql/exec/vector/DoubleColumnVector.java
A    ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorCoalesce.java
U    ql/src/java/org/apache/hadoop/hive/ql/exec/vector/ColumnVector.java
U    ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java

Fetching external item into 'hcatalog/src/test/e2e/harness'
Updated external to revision 1568905.

Updated to revision 1568905.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12629297;;;","17/Feb/14 09:04;navis;Conflicted again. Rebased to trunk.;;;","17/Feb/14 20:03;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12629343/HIVE-6203.4.patch.txt

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5128 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1364/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1364/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12629343;;;","17/Feb/14 23:57;thejas;[~navis] Please let me know if you would like me to create a patch to fix existing get_privilege_set function instead of adding new thrift api. I will be able to create one by tomorrow. That should require less code changes.
;;;","22/Feb/14 08:34;thejas;[~navis] I have uploaded a patch to HIVE-5954 that updates get_privilege_set to indirect privileges through roles. Can you please review that one ? We won't need an additional thrift API with that patch.

;;;","21/Mar/14 15:51;ashutoshc;Fixed via HIVE-5954;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Hive custom SerDe cannot load DLL added by ""ADD FILE"" command",HIVE-6200,12688901,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,shuainie,shuainie,shuainie,15/Jan/14 00:51,13/Nov/14 19:41,14/Jul/23 06:14,27/Mar/14 15:51,,,,,,,,,,0.14.0,,,,,,0,,,"When custom SerDe need to load a DLL file added using ""ADD FILE"" command in HIVE, the loading fail with exception like ""java.lang.UnsatisfiedLinkError:C:\tmp\admin2_6996@headnode0_201401100431_resources\hello.dll: Access is denied"". 
The reason is when FileSystem creating local copy of the file, the permission of local file is set to default as ""666"". DLL file need ""execute"" permission to be loaded successfully.
Similar scenario also happens when Hadoop localize files in distributed cache. The solution in Hadoop is to add ""execute"" permission to the file after localizationl.",,gates,shanyu,shuainie,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/14 01:05;shuainie;HIVE-6200.1.patch;https://issues.apache.org/jira/secure/attachment/12623035/HIVE-6200.1.patch","25/Mar/14 17:01;shuainie;HIVE-6200.2.patch;https://issues.apache.org/jira/secure/attachment/12636732/HIVE-6200.2.patch","26/Mar/14 20:46;shuainie;HIVE-6200.3.patch;https://issues.apache.org/jira/secure/attachment/12636997/HIVE-6200.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,367868,,,,Thu Nov 13 19:41:35 UTC 2014,,,,,,,,,,"0|i1rep3:",368175,,,,,,,,,,,,,,,,,,,,,"15/Jan/14 03:19;thejas;As this seems to be required only on windows, how about adding a check if it is windows before changing the permission ?
;;;","15/Jan/14 03:41;shuainie;Hi [~thejas]. I think this problem is not limited to Windows. On both Windows and Linux, when creating the copy file in local FS, Hadoop will set the permission of the file to default permission (hard coded to 666). In the implementation of distributed cache in Hadoop, it uses the same way to add ""execute"" permission for the file regardless of the OS at TrackerDistributedCacheManager.downloadCacheObject().;;;","15/Jan/14 11:43;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12623035/HIVE-6200.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4925 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/918/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/918/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12623035;;;","15/Jan/14 18:53;thejas;+1;;;","15/Jan/14 21:43;shuainie;Validated the failed test in HIVE QA result. It is not related to the change made in this JIRA.;;;","15/Jan/14 21:48;shanyu;+1;;;","24/Mar/14 21:21;ashutoshc;Changing of permissions should be moved in downloadResource() method, such that we change permissions only when files are downloaded. Because, int that case we are guaranteed to successfully complete chmod operation. Otherwise, we may try to change permissions of local files (which may not be allowed since we may not own them).

Also, patch needs a rebase.;;;","25/Mar/14 17:01;shuainie;Thanks for the commend [~ashutoshc]. Updated the patch;;;","25/Mar/14 17:10;ashutoshc;+1;;;","26/Mar/14 20:16;gates;Build failed with
{code}
[ERROR] COMPILATION ERROR :
[INFO] -------------------------------------------------------------
[ERROR] /grid/0/hortonal/ptest/working_dir/working/apache-github-source/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java:[788,9] cannot find symbol
symbol  : variable FileUtil
location: class org.apache.hadoop.hive.ql.session.SessionState
[INFO] 1 error
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Hive .............................................. SUCCESS [52.644s]
[INFO] Hive Ant Utilities ................................ SUCCESS [8.526s]
[INFO] Hive Shims Common ................................. SUCCESS [18.758s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [20.058s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.372s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [1.443s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [48.812s]
[INFO] Hive Shims ........................................ SUCCESS [0.227s]
[INFO] Hive Common ....................................... SUCCESS [2.576s]
[INFO] Hive Serde ........................................ SUCCESS [3.588s]
[INFO] Hive Metastore .................................... SUCCESS [19.850s]
[INFO] Hive Query Language ............................... FAILURE [23.695s]
{code};;;","26/Mar/14 20:46;shuainie;Sorry, HIVE-6200.2.patch is the wrong incomplete patch. Update with the new one HIVE-6200.3.patch;;;","27/Mar/14 15:38;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12636997/HIVE-6200.3.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5491 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_dyn_part
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1983/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1983/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12636997;;;","27/Mar/14 15:51;ashutoshc;Committed to trunk. Thanks, Shuaishuai!;;;","13/Nov/14 19:41;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support top level union all statements,HIVE-6189,12688505,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,13/Jan/14 08:01,03/Mar/16 11:54,14/Jul/23 06:14,15/Jan/14 23:53,,,,,,,,,,0.13.0,,,,,,0,,,"I've always wondered why union all has to be in subqueries in hive.

After looking at it, problems are:

- Hive Parser:
  - Union happens at the wrong place (insert ... select ... union all select ...) is parsed as (insert select) union select.
  - There are many rewrite rules in the parser to force any query into the a from - insert -select form. No doubt for historical reasons.
- Plan generation/semantic analysis assumes top level ""TOK_QUERY"" and not top level ""TOK_UNION"".

The rewrite rules don't work when we move the ""UNION ALL"" into the select statements. However, it's not hard to do that in code.",,hagleitn,leftyl,navis,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-9002,,,,,,,,,,,,,,,,HIVE-1398,,,,,"13/Jan/14 08:14;hagleitn;HIVE-6189.1.patch;https://issues.apache.org/jira/secure/attachment/12622596/HIVE-6189.1.patch","13/Jan/14 20:04;hagleitn;HIVE-6189.2.patch;https://issues.apache.org/jira/secure/attachment/12622691/HIVE-6189.2.patch","15/Jan/14 00:52;hagleitn;HIVE-6189.3.patch;https://issues.apache.org/jira/secure/attachment/12623031/HIVE-6189.3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,367524,,,,Tue May 27 13:43:58 UTC 2014,,,,,,,,,,"0|i1rclj:",367832,,,,,,,,,,,,,,,,,,,,,"13/Jan/14 12:56;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12622596/HIVE-6189.1.patch

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 4917 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_subq_insert
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample2
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample3
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample4
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample5
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample6
org.apache.hadoop.hive.ql.parse.TestParse.testParse_sample7
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/881/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/881/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12622596;;;","13/Jan/14 20:04;hagleitn;.2 addresses the failures.;;;","13/Jan/14 20:06;hagleitn;RB: https://reviews.apache.org/r/16818/;;;","14/Jan/14 01:04;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12622691/HIVE-6189.2.patch

{color:green}SUCCESS:{color} +1 4924 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/891/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/891/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12622691;;;","14/Jan/14 02:11;navis;+1;;;","14/Jan/14 08:21;leftyl;Apparently the nonsupport of top-level UNION ALL never got documented, so for starters that needs to be fixed in the wiki for Hive 0.12.0 and earlier.  Then we'll need documentation for this ticket, which could go in a release note or directly into the wiki:  [Union Syntax|https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Union].;;;","15/Jan/14 00:52;hagleitn;Thanks [~navis] for the review. I've added .3 because [~rhbutani] showed me a much better way to do this. He showed me how I can do all the necessary rewrites in the grammar without having to mess with the tree in semantic analysis. This is much cleaner.;;;","15/Jan/14 01:47;navis;I'm pretty sure Harish is more proficient than me for this kind of works. Good to hear that hive supports top level union! ;;;","15/Jan/14 10:04;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12623031/HIVE-6189.3.patch

{color:green}SUCCESS:{color} +1 4925 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/916/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/916/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12623031;;;","15/Jan/14 18:58;hagleitn;[~leftylev] - the documentation in the link you sent looks good. We could specify that unions can be used in views, insert, and ctas statements, but this I'm thinking that's almost self explanatory. As for hive .12 and below - the restriction was that unions could only be used within a subquery. I.e.: 

""select_statement union all select_statement union all ..."" had to be written as ""select * from (select_statement union all select_statement union all ...) unionresult""

Ditto for CTAS, insert, create/alter view as.

Does that make sense?;;;","15/Jan/14 22:56;rhbutani;+1;;;","15/Jan/14 23:53;hagleitn;Committed to trunk. Thanks for reviewing [~rhbutani] and [~navis]!;;;","27/May/14 13:43;leftyl;[~hagleitn], how's this wiki revision?

* [Language Manual - Union | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Union]
* [diffs | https://cwiki.apache.org/confluence/pages/diffpages.action?pageId=27362049&originalId=41812730];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add test to verify that DESCRIBE TABLE works with quoted table names,HIVE-6187,12688361,Bug,Closed,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,cwsteinbach,amok,amok,10/Jan/14 23:54,27/Jan/15 09:21,14/Jul/23 06:14,13/May/14 16:15,0.10.0,,,,,,,,,0.14.0,,,,,,1,TODOC14,,"Backticks around tables named after special keywords, such as items, allow us to create, drop, and alter the table. For example
{code:sql}
CREATE TABLE foo.`items` (bar INT);
DROP TABLE foo.`items`;
ALTER TABLE `items` RENAME TO `items_`;
{code}
However, we cannot call
{code:sql}
DESCRIBE foo.`items`;
DESCRIBE `items`;
{code}
The DESCRIBE query does not permit backticks to surround table names. The error returned is
{code:sql}
FAILED: SemanticException [Error 10001]: Table not found `items`
{code} ",,amok,cwsteinbach,dzanter,erwaman,leftyl,navis,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-2949,,,HIVE-6013,,,,,,,,,,,,,,,,,,,,,"12/May/14 09:40;cwsteinbach;HIVE-6187.1.patch;https://issues.apache.org/jira/secure/attachment/12644392/HIVE-6187.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,367380,,,,Tue Jan 27 09:21:34 UTC 2015,,,,,,,,,,"0|i1rbpz:",367689,,,,,,,,,,,,,,,,,,,,,"14/Jan/14 04:16;navis;It's working on trunk.;;;","21/Feb/14 06:33;amok;Weird. The Hive we're using is provided in CDH 4.5.0-1.cdh4.5.0.p0.30 from Cloudera.;;;","12/May/14 09:38;cwsteinbach;I can confirm that this functionality is currently working on trunk, and also that it's broken in the 0.12.0 release. I'm not sure when it was fixed, and there doesn't appear to be any test coverage that will prevent someone from breaking it again in the future.
;;;","12/May/14 09:40;cwsteinbach;Attaching a patch that adds several quoted testcases to describe_table.q.;;;","12/May/14 16:05;ashutoshc;+1;;;","12/May/14 21:02;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12644392/HIVE-6187.1.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 5504 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_stats_counter
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/178/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/178/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12644392;;;","13/May/14 16:15;ashutoshc;Committed to trunk. Thanks, Carl!;;;","13/Jun/14 11:05;leftyl;This fix should be documented in the wiki for 0.14.0.

* [Language Manual -- DDL -- Describe | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-Describe];;;","13/Nov/14 19:40;thejas;This has been fixed in 0.14 release. Please open new jira if you see any issues.
;;;","27/Jan/15 09:21;leftyl;Documentation done, please review the last information box (""Bug fixed in Hive 0.13.0 — quoted identifiers"") in this section:

* [Language Manual -- DDL -- Describe Table/View/Column | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-DescribeTable/View/Column];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DDLTask is inconsistent in creating a table and adding a partition when dealing with location,HIVE-6185,12688335,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,xuefuz,xuefuz,xuefuz,10/Jan/14 21:41,14/Jan/14 00:47,14/Jul/23 06:14,14/Jan/14 00:47,0.12.0,,,,,,,,,0.13.0,,Query Processor,,,,0,,,"When creating a table, Hive uses URI to represent location:
{code}
    if (crtTbl.getLocation() != null) {
      tbl.setDataLocation(new Path(crtTbl.getLocation()).toUri());
    }
{code}
When adding a partition, Hive uses Path to represent location:
{code}
      // set partition path relative to table
      db.createPartition(tbl, addPartitionDesc.getPartSpec(), new Path(tbl
                    .getPath(), addPartitionDesc.getLocation()), addPartitionDesc.getPartParams(),
                    addPartitionDesc.getInputFormat(),
                    addPartitionDesc.getOutputFormat(),
                    addPartitionDesc.getNumBuckets(),
                    addPartitionDesc.getCols(),
                    addPartitionDesc.getSerializationLib(),
                    addPartitionDesc.getSerdeParams(),
                    addPartitionDesc.getBucketCols(),
                    addPartitionDesc.getSortCols());
{code}

This disparity makes the values stored in metastore be encoded differently, causing problems w.r.t. special character as demonstrated in HIVE-5446. As a result, the code dealing with location for table is different for partition, creating maintenance burden.

We need to standardize it to Path to be in line with other Path related cleanup effort.",,kamrul,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/14 19:30;xuefuz;HIVE-6185.1.patch;https://issues.apache.org/jira/secure/attachment/12622513/HIVE-6185.1.patch","12/Jan/14 01:49;xuefuz;HIVE-6185.2.patch;https://issues.apache.org/jira/secure/attachment/12622525/HIVE-6185.2.patch","13/Jan/14 19:55;xuefuz;HIVE-6185.3.patch;https://issues.apache.org/jira/secure/attachment/12622690/HIVE-6185.3.patch","11/Jan/14 04:09;xuefuz;HIVE-6185.patch;https://issues.apache.org/jira/secure/attachment/12622492/HIVE-6185.patch","11/Jan/14 03:46;xuefuz;HIVE-6185.patch;https://issues.apache.org/jira/secure/attachment/12622490/HIVE-6185.patch",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,367354,,,,Tue Jan 14 00:47:59 UTC 2014,,,,,,,,,,"0|i1rbk7:",367663,,,,,,,,,,,,,,,,,,,,,"11/Jan/14 14:31;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12622492/HIVE-6185.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/863/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/863/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/itests (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-it ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-it ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-it ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-it/0.13.0-SNAPSHOT/hive-it-0.13.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - Custom Serde 0.13.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-custom-serde ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/itests/custom-serde (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-it-custom-serde ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/custom-serde/src/main/resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-it-custom-serde ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-it-custom-serde ---
[INFO] Compiling 8 source files to /data/hive-ptest/working/apache-svn-trunk-source/itests/custom-serde/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-it-custom-serde ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/custom-serde/src/test/resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-it-custom-serde ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/custom-serde/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/custom-serde/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/custom-serde/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/custom-serde/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-it-custom-serde ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-it-custom-serde ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-it-custom-serde ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/itests/custom-serde/target/hive-it-custom-serde-0.13.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-it-custom-serde ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/custom-serde/target/hive-it-custom-serde-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-it-custom-serde/0.13.0-SNAPSHOT/hive-it-custom-serde-0.13.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/custom-serde/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-it-custom-serde/0.13.0-SNAPSHOT/hive-it-custom-serde-0.13.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - HCatalog Unit Tests 0.13.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-hcatalog-it-unit ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-hcatalog-it-unit ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/src/main/resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-hcatalog-it-unit ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-hcatalog-it-unit ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-hcatalog-it-unit ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/src/test/resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hcatalog-it-unit ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hcatalog-it-unit ---
[INFO] Compiling 7 source files to /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/test-classes
[WARNING] Note: Some input files use or override a deprecated API.
[WARNING] Note: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hcatalog-it-unit ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hcatalog-it-unit ---
[WARNING] JAR will be empty - no content was marked for inclusion!
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-0.13.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-jar-plugin:2.2:test-jar (default) @ hive-hcatalog-it-unit ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-0.13.0-SNAPSHOT-tests.jar
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hcatalog-it-unit ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/0.13.0-SNAPSHOT/hive-hcatalog-it-unit-0.13.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/0.13.0-SNAPSHOT/hive-hcatalog-it-unit-0.13.0-SNAPSHOT.pom
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-0.13.0-SNAPSHOT-tests.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/0.13.0-SNAPSHOT/hive-hcatalog-it-unit-0.13.0-SNAPSHOT-tests.jar
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - Testing Utilities 0.13.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-util ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/itests/util (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-it-util ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/itests/util/src/main/resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-it-util ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-it-util ---
[INFO] Compiling 41 source files to /data/hive-ptest/working/apache-svn-trunk-source/itests/util/target/classes
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] Note: Some input files use or override a deprecated API.
[WARNING] Note: Recompile with -Xlint:deprecation for details.
[INFO] 2 warnings 
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsNotSubdirectoryOfTableHook.java:[47,18] cannot find symbol
symbol  : method getPartitionPath()
location: class org.apache.hadoop.hive.ql.metadata.Partition
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java:[46,18] cannot find symbol
symbol  : method getPartitionPath()
location: class org.apache.hadoop.hive.ql.metadata.Partition
[INFO] 2 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive Integration - Parent ......................... SUCCESS [4.620s]
[INFO] Hive Integration - Custom Serde ................... SUCCESS [11.056s]
[INFO] Hive Integration - HCatalog Unit Tests ............ SUCCESS [5.939s]
[INFO] Hive Integration - Testing Utilities .............. FAILURE [3.795s]
[INFO] Hive Integration - Unit Tests ..................... SKIPPED
[INFO] Hive Integration - Test Serde ..................... SKIPPED
[INFO] Hive Integration - QFile Tests .................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 27.238s
[INFO] Finished at: Sat Jan 11 09:30:55 EST 2014
[INFO] Final Memory: 28M/85M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-it-util: Compilation failure: Compilation failure:
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsNotSubdirectoryOfTableHook.java:[47,18] cannot find symbol
[ERROR] symbol  : method getPartitionPath()
[ERROR] location: class org.apache.hadoop.hive.ql.metadata.Partition
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java:[46,18] cannot find symbol
[ERROR] symbol  : method getPartitionPath()
[ERROR] location: class org.apache.hadoop.hive.ql.metadata.Partition
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-it-util
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12622492;;;","11/Jan/14 21:35;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12622513/HIVE-6185.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4917 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_exim_14_managed_location_over_existing
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/866/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/866/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12622513;;;","12/Jan/14 01:49;xuefuz;Patch #2 fixed the above test failure.;;;","12/Jan/14 01:53;xuefuz;RB: https://reviews.apache.org/r/16806/;;;","12/Jan/14 03:55;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12622525/HIVE-6185.2.patch

{color:green}SUCCESS:{color} +1 4917 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/871/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/871/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12622525;;;","12/Jan/14 22:34;kamrul;Patch looks good!
Few comments:
1. In Partition::setBucketCount(), 
FileSystem fs = FileSystem.get(getDataLocation().toUri(), Hive.get().getConf())
can be rewritten as (to make it consistent for other places):
FileSystem fs = getDataLocation().getFileSystem(Hive.get().getConf());

2. Same thing in SamplePruner:: limitPrune()
FileSystem fs = FileSystem.get(part.getDataLocation().toUri(), Hive.get() .getConf());
can be rewritten as 
FileSystem fs = part.getDataLocation().getFileSystem(Hive.get().getConf());

3. In Partition.java

A new method ""public Path getDataLocation() "" is introduced. Is it replacing ""public Path getPartitionPath() "" or  ""final public URI getDataLocation()""? If it is the later one, do we need to keep the ""final"" modifier?
 ;;;","13/Jan/14 16:34;ashutoshc;+1, left a minor comment on RB.;;;","13/Jan/14 19:55;xuefuz;Patch #3 incorporated the review feedback.;;;","13/Jan/14 23:32;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12622690/HIVE-6185.3.patch

{color:green}SUCCESS:{color} +1 4924 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/889/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/889/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12622690;;;","14/Jan/14 00:47;ashutoshc;Committed to trunk. Thanks, Xuefu!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in SessionManager.stop() in HiveServer2,HIVE-6184,12688207,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,navis,jaideepdhok,jaideepdhok,10/Jan/14 09:34,15/Jan/14 23:43,14/Jul/23 06:14,15/Jan/14 23:43,,,,,,,,,,0.13.0,,HiveServer2,,,,0,,,"The conf setting hive.server2.async.exec.shutdown.timeout is set to a long value (10L) in HiveConf.java, but it is read using getIntVar in SessionManager.stop. 

Instead it should be read as - 
{code}
      long timeout = hiveConf.getLongVar(ConfVars.HIVE_SERVER2_ASYNC_EXEC_SHUTDOWN_TIMEOUT);
{code}

Current code will either cause an assertion error if assertions are enabled, or it would return the timeout as -1 if the property is not set in hive-site.xml

Workaround is to explicitly set the property in hive-site.xml",,jaideepdhok,navis,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/14 01:25;navis;HIVE-6184.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12622760/HIVE-6184.1.patch.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,367226,,,,Wed Jan 15 23:43:31 UTC 2014,,,,,,,,,,"0|i1rarz:",367535,,,,,,,,,,,,,,,,,,,,,"14/Jan/14 01:25;navis;integer type seemed enough for seconds.;;;","14/Jan/14 15:03;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12622760/HIVE-6184.1.patch.txt

{color:green}SUCCESS:{color} +1 4924 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/900/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/900/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12622760;;;","15/Jan/14 00:41;thejas;+1;;;","15/Jan/14 23:43;thejas;Patch committed to trunk. Thanks for the contribution Navis!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive JDBC Driver connection fail when no default database passed in the connection string,HIVE-6180,12688156,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,prasadm,sravya,sravya,09/Jan/14 22:49,17/Jan/14 18:08,14/Jul/23 06:14,17/Jan/14 18:08,0.13.0,,,,,,,,,0.13.0,,HiveServer2,,,,0,,,"Connection fails with connection url ""jdbc:hive2://hostname:port"" without dbname after Hive-4256. There is a check in jdbc/HiveConnection.java : 
{noformat}
if(dbName!=null)
  stmt.execute(""use ""+dbName);
{noformat}

But looks like dbName is """" instead of null.",,prasadm,sravya,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-4256,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/14 00:09;prasadm;HIVE-6180.1.patch;https://issues.apache.org/jira/secure/attachment/12622741/HIVE-6180.1.patch","16/Jan/14 23:40;prasadm;HIVE-6180.2.patch;https://issues.apache.org/jira/secure/attachment/12623535/HIVE-6180.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,367176,,,,Fri Jan 17 18:08:26 UTC 2014,,,,,,,,,,"0|i1ragv:",367485,,,,,,,,,,,,,,,,,,,,,"14/Jan/14 11:59;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12622741/HIVE-6180.1.patch

{color:green}SUCCESS:{color} +1 4924 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/898/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/898/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12622741;;;","14/Jan/14 17:36;xuefuz;Look good. Minor question/comment on RB.;;;","14/Jan/14 19:28;xuefuz;+1 pengding test;;;","16/Jan/14 23:40;prasadm;Attaching updated patch;;;","17/Jan/14 04:52;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12623535/HIVE-6180.2.patch

{color:green}SUCCESS:{color} +1 4938 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/943/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/943/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12623535;;;","17/Jan/14 18:08;xuefuz;Patch committed to trunk. Thanks to Prasad.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix keyword KW_REANME which was intended to be KW_RENAME,HIVE-6177,12688030,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,Fixed,navis,navis,navis,09/Jan/14 09:53,14/Jan/14 02:18,14/Jul/23 06:14,14/Jan/14 02:18,,,,,,,,,,0.13.0,,Query Processor,,,,0,,,http://stackoverflow.com/questions/20952865/very-strange-keyword-reanme-in-apache-hive,,brocknoland,navis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/14 09:54;navis;HIVE-6177.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12622144/HIVE-6177.1.patch.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,367036,,,,Tue Jan 14 02:18:55 UTC 2014,,,,,,,,,,"0|i1r9lz:",367346,,,,,,,,,,,,,,,,,,,,,"09/Jan/14 17:35;brocknoland;+1;;;","09/Jan/14 22:08;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12622144/HIVE-6177.1.patch.txt

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4904 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/842/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/842/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12622144;;;","14/Jan/14 02:18;navis;Committed to trunk. Thanks Brock.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Beeline ""set varible"" doesn't show the value of the variable as Hive CLI",HIVE-6174,12687988,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,xuefuz,xuefuz,xuefuz,09/Jan/14 02:54,15/Jan/14 23:42,14/Jul/23 06:14,15/Jan/14 17:27,0.10.0,0.11.0,0.12.0,,,,,,,0.13.0,,CLI,,,,0,,,"Currently it displays nothing.
{code}
0: jdbc:hive2://> set env:TERM; 
0: jdbc:hive2://> 
{code}

In contrast,  Hive CLI displays the value of the variable.
{code}
hive> set env:TERM; 
env:TERM=xterm
{code}",,kamrul,prasadm,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/14 19:58;xuefuz;HIVE-5174.3.patch;https://issues.apache.org/jira/secure/attachment/12622514/HIVE-5174.3.patch","10/Jan/14 21:05;xuefuz;HIVE-6174.2.patch;https://issues.apache.org/jira/secure/attachment/12622443/HIVE-6174.2.patch","10/Jan/14 20:42;xuefuz;HIVE-6174.patch;https://issues.apache.org/jira/secure/attachment/12622436/HIVE-6174.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,366995,,,,Wed Jan 15 17:27:19 UTC 2014,,,,,,,,,,"0|i1r9cv:",367305,,,,,,,,,,,,,,,,,,,,,"10/Jan/14 20:47;xuefuz;With the patch, the console now displays for both remote mode and embedded mode:
{code}
0: jdbc:hive2://localhost:10000> set env:TERM;
[HiveQueryResultSet/next] 0
+-----------------+
|       set       |
+-----------------+
| env:TERM=xterm  |
+-----------------+
{code};;;","10/Jan/14 21:05;xuefuz;Patch #2 added test case for Beeline as well.;;;","11/Jan/14 05:03;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12622443/HIVE-6174.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4919 tests executed
*Failed tests:*
{noformat}
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testGetVariableValue
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/855/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/855/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12622443;;;","11/Jan/14 23:17;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12622514/HIVE-5174.3.patch

{color:green}SUCCESS:{color} +1 4919 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/868/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/868/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12622514;;;","12/Jan/14 01:57;xuefuz;RB: https://reviews.apache.org/r/16807/;;;","12/Jan/14 22:39;kamrul;+1
Looks very straight forward.;;;","14/Jan/14 06:39;prasadm;+1
Looks fine to me.;;;","15/Jan/14 17:27;xuefuz;Patch committed to trunk. Thanks to Prasad for the review.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Whitespaces and comments on Tez,HIVE-6172,12687978,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,09/Jan/14 00:29,05/Jul/14 04:01,14/Jul/23 06:14,09/Jan/14 01:39,tez-branch,,,,,,,,,tez-branch,,,,,,0,,,,,hagleitn,leftyl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-4660,HIVE-6098,,,,,,,,,,HIVE-6636,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/14 00:30;hagleitn;HIVE-6172.1.patch;https://issues.apache.org/jira/secure/attachment/12622077/HIVE-6172.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,366985,,,,Sat Jul 05 04:01:47 UTC 2014,,,,,,,,,,"0|i1r9an:",367295,,,,,,,,,,,,,,,,,,,,,"09/Jan/14 01:39;hagleitn;Committed to branch.;;;","09/Jan/14 11:12;leftyl;This patch documents hive.jar.directory & hive.user.install.directory in hive-default.xml.template.

(This comment helps search for them.);;;","05/Jul/14 04:01;leftyl;The wiki documents *hive.jar.directory* and *hive.user.install.directory* here:

* [Configuration Properties -- hive.jar.directory | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.jar.directory]
* [Configuration Properties -- hive.user.install.directory | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.user.install.directory];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade to the latest version of bonecp,HIVE-6170,12687966,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hsubramaniyan,hsubramaniyan,hsubramaniyan,08/Jan/14 23:32,31/Jan/14 12:13,14/Jul/23 06:14,16/Jan/14 21:57,,,,,,,,,,0.13.0,,,,,,0,,,,,hsubramaniyan,szehon,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6216,,,HIVE-4807,,,,,,,,,,,,,,,,,,,,,"15/Jan/14 00:35;hsubramaniyan;HIVE-6170.1.patch;https://issues.apache.org/jira/secure/attachment/12623026/HIVE-6170.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,366973,,,,Thu Jan 16 21:57:13 UTC 2014,,,,,,,,,,"0|i1r97z:",367283,,,,,,,,,,,,,,,,,,,,,"15/Jan/14 00:34;hsubramaniyan;There is a need to upgrade the bonecp to the latest release version since some database connectivity issues have been resolved with the latest release. Patch uploaded;;;","15/Jan/14 05:24;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12623026/HIVE-6170.1.patch

{color:green}SUCCESS:{color} +1 4925 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/914/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/914/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12623026;;;","15/Jan/14 08:23;vgumashta;+1 (non-binding).

cc [~ashutoshc];;;","15/Jan/14 15:42;ashutoshc;+1;;;","16/Jan/14 21:57;ashutoshc;Committed to trunk. Thanks, Hari!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update tez specific golden files after merge,HIVE-6169,12687944,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,08/Jan/14 21:29,08/Jan/14 22:08,14/Jul/23 06:14,08/Jan/14 22:08,tez-branch,,,,,,,,,tez-branch,,,,,,0,,,,,hagleitn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-4660,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/14 22:07;hagleitn;HIVE-6169.2.patch;https://issues.apache.org/jira/secure/attachment/12622041/HIVE-6169.2.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,366951,,,,Wed Jan 08 22:08:14 UTC 2014,,,,,,,,,,"0|i1r933:",367261,,,,,,,,,,,,,,,,,,,,,"08/Jan/14 22:08;hagleitn;Committed to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix some javadoc issues on Tez branch,HIVE-6168,12687937,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,08/Jan/14 20:33,08/Jan/14 20:39,14/Jul/23 06:14,08/Jan/14 20:39,tez-branch,,,,,,,,,tez-branch,,,,,,0,,,,,hagleitn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-4660,HIVE-6098,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/14 20:35;hagleitn;HIVE-6168.1.patch;https://issues.apache.org/jira/secure/attachment/12622025/HIVE-6168.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,366944,,,,Wed Jan 08 20:39:14 UTC 2014,,,,,,,,,,"0|i1r91r:",367255,,,,,,,,,,,,,,,,,,,,,"08/Jan/14 20:39;hagleitn;Committed to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JsonSerDe is too strict about table schema,HIVE-6166,12687871,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sushanth,sushanth,sushanth,08/Jan/14 14:59,18/Mar/15 14:45,14/Jul/23 06:14,13/Jan/14 16:38,0.12.0,,,,,,,,,0.13.0,,HCatalog,Serializers/Deserializers,,,0,,,"JsonSerDe is too strict when it comes to schema, erroring out if it finds a subfield with a key name that does not map to an appropriate type/schema of a table, or an inner-struct schema.

Thus, if a schema specifies ""s:struct<a:int,b:string>,k:int"" and we pass it data that looks like the following:

{noformat}
{ ""x"" : ""abc"" , ""s"" : { ""a"" : 2 , ""b"" : ""blah"", ""c"": ""woo"" } }
{noformat}

This should still pass, and the record should be read as if it were 

{noformat}
{ ""s"" : { ""a"" : 2 , ""b"" : ""blah""}, k :  null }
{noformat}

This will allow the JsonSerDe to be used with a wider set of data where the data does not map too finely to the declared table schema.

Note, we are still strict about a couple of things:

a) If there is a declared schema column, then the type cannot vary, that is still considered an error. i.e., if the hive table schema says k1 is a boolean, it cannot magically change into an int or a struct, say, for eg.
b) The JsonSerDe still attempts to map hive internal column names - i.e. if the data contains a column named ""_col2"", then, if ""_col2"" is not declared directly in the schema, it will map to column position 2 in that schema/subschema, rather than ignoring the field. This is so that tables created with CTAS will still work. ",,sushanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-9962,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/14 02:20;sushanth;HIVE-6166.2.patch;https://issues.apache.org/jira/secure/attachment/12622088/HIVE-6166.2.patch","11/Jan/14 01:08;sushanth;HIVE-6166.3.patch;https://issues.apache.org/jira/secure/attachment/12622475/HIVE-6166.3.patch","08/Jan/14 15:00;sushanth;HIVE-6166.patch;https://issues.apache.org/jira/secure/attachment/12621982/HIVE-6166.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,366877,,,,Mon Jan 13 16:38:44 UTC 2014,,,,,,,,,,"0|i1r8mv:",367188,,,,,,,,,,,,,,,,,,,,,"08/Jan/14 15:00;sushanth;Patch attached.;;;","08/Jan/14 17:04;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12621982/HIVE-6166.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4904 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/829/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/829/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12621982;;;","08/Jan/14 23:38;sushanth;The test failure marked above seems unconnected with this issue, and the new test added by this patch passed:

http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/829/testReport/org.apache.hive.hcatalog.data/TestJsonSerDe/testLooseJsonReadability/?

Existing tests using the JsonSerDe also passed.

http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/829/testReport/org.apache.hive.hcatalog.data/TestJsonSerDe/;;;","09/Jan/14 02:19;sushanth;Cancelling and updating patch, improving test and fixing an expression.;;;","09/Jan/14 02:20;sushanth;Updated patch.;;;","09/Jan/14 09:35;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12622088/HIVE-6166.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4905 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_leftsemijoin_mr
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/835/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/835/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12622088;;;","10/Jan/14 04:17;ashutoshc;For better readability and better performance, instead of try-catch block, its better to do 
{code}
Integer fpos = s.getPosition(fieldName);
if (null == fpos) {
} else {}
{code}
Though I am +1 even without that change.;;;","11/Jan/14 00:58;sushanth;Agreed, updating.;;;","11/Jan/14 01:08;sushanth;Attached patch with suggested change.;;;","11/Jan/14 01:08;sushanth;Made patch available to rerun tests with latest addition.;;;","11/Jan/14 07:55;ashutoshc;+1;;;","11/Jan/14 11:18;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12622475/HIVE-6166.3.patch

{color:green}SUCCESS:{color} +1 4918 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/860/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/860/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12622475;;;","13/Jan/14 16:38;ashutoshc;Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unify HivePreparedStatement from jdbc:hive and jdbc:hive2,HIVE-6165,12687860,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,,lethum,lethum,08/Jan/14 14:12,12/Feb/15 23:40,14/Jul/23 06:14,27/Oct/14 18:47,,,,,,,,,,1.1.0,,HiveServer2,JDBC,,,0,,,"org.apache.hadoop.hive.jdbc.HivePreparedStatement.class from the hive jdbc driver and org.apache.hive.jdbc.HivePreparedStatement.class from the hive2 jdbc drivers contain lots of duplicate code. 

Especially hive-HivePreparedStatement supports ""setObject"", while the hive2 version does not.

Share more code between the two to avoid duplicate work and to make sure that both support the broadest possible feature set.
",,analog.sony,hagleitn,lethum,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/14 19:49;xuefuz;HIVE-6165.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12676157/HIVE-6165.1.patch.txt","21/Jul/14 02:26;navis;HIVE-6165.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12656810/HIVE-6165.1.patch.txt","24/Oct/14 03:37;hagleitn;HIVE-6165.2.patch;https://issues.apache.org/jira/secure/attachment/12676822/HIVE-6165.2.patch","24/Oct/14 17:29;xuefuz;HIVE-6165.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12676946/HIVE-6165.2.patch.txt","23/Oct/14 21:29;xuefuz;HIVE-6165.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12676735/HIVE-6165.2.patch.txt",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,366866,,,,Mon Oct 27 18:47:05 UTC 2014,,,,,,,,,,"0|i1r8kf:",367177,,,,,,,,,,,,,,,,,,,,,"08/Jan/14 17:16;xuefuz;My understanding is that the old JDBC dirver is deprecated or close to be deprecated, and we probably don't want to make any changes unless absolutely necessary. Thus, I think the duplication is acceptable, and we are only moving JDBC driver 2 forward.;;;","09/Jan/14 13:47;lethum;That makes sense. Maybe the functionality implemented in the old JDBC driver but missing in JDBC driver 2 (setObject and maybe others) can be ported by simply copying the code.;;;","09/Jan/14 17:32;xuefuz;I think ""copying"" is fine.;;;","21/Jul/14 06:14;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12656810/HIVE-6165.1.patch.txt

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 5734 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_fail_8
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/868/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/868/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-868/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12656810;;;","21/Oct/14 19:49;xuefuz;Reattach the same patch to trigger the test, as the patch is old.;;;","21/Oct/14 23:21;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12676157/HIVE-6165.1.patch.txt

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1378/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1378/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1378/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]
+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-TRUNK-Build-1378/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'common/src/java/org/apache/hadoop/hive/conf/HiveConf.java'
Reverted 'ql/src/test/results/clientnegative/authorization_disallow_transform.q.out'
Reverted 'ql/src/test/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/TestSQLStdHiveAccessControllerHS2.java'
Reverted 'ql/src/test/queries/clientnegative/authorization_disallow_transform.q'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAccessController.java'
++ awk '{print $2}'
++ egrep -v '^X|^Performing status on external'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit-hadoop2/target itests/hive-minikdc/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/core/target hcatalog/streaming/target hcatalog/server-extensions/target hcatalog/hcatalog-pig-adapter/target hcatalog/webhcat/svr/dependency-reduced-pom.xml hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target accumulo-handler/target hwi/target common/target common/src/gen common/src/java/org/apache/hadoop/hive/conf/HiveConf.java.orig service/target contrib/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target ql/src/test/org/apache/hadoop/hive/ql/security/authorization/plugin/TestSQLStdHiveAccessControllerHS2.java ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/SettableConfigUpdater.java
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1633483.

At revision 1633483.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12676157 - PreCommit-HIVE-TRUNK-Build;;;","23/Oct/14 05:02;xuefuz;[~lethum], could you please rebase the patch?;;;","23/Oct/14 21:29;xuefuz;Rebased the patch with latest trunk.;;;","24/Oct/14 02:28;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12676735/HIVE-6165.2.patch.txt

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1425/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1425/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1425/

Messages:
{noformat}
**** This message was trimmed, see log for full details ****

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN KW_CASE KW_IF"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:68:4: 
Decision can match input such as ""LPAREN LPAREN KW_IF"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:115:5: 
Decision can match input such as ""KW_CLUSTER KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:127:5: 
Decision can match input such as ""KW_PARTITION KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:138:5: 
Decision can match input such as ""KW_DISTRIBUTE KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:149:5: 
Decision can match input such as ""KW_SORT KW_BY LPAREN"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:166:7: 
Decision can match input such as ""STAR"" using multiple alternatives: 1, 2

As a result, alternative(s) 2 were disabled for that input
warning(200): IdentifiersParser.g:179:5: 
Decision can match input such as ""KW_STRUCT"" using multiple alternatives: 4, 6

As a result, alternative(s) 6 were disabled for that input
warning(200): IdentifiersParser.g:179:5: 
Decision can match input such as ""KW_ARRAY"" using multiple alternatives: 2, 6

As a result, alternative(s) 6 were disabled for that input
warning(200): IdentifiersParser.g:179:5: 
Decision can match input such as ""KW_UNIONTYPE"" using multiple alternatives: 5, 6

As a result, alternative(s) 6 were disabled for that input
warning(200): IdentifiersParser.g:261:5: 
Decision can match input such as ""KW_NULL"" using multiple alternatives: 1, 8

As a result, alternative(s) 8 were disabled for that input
warning(200): IdentifiersParser.g:261:5: 
Decision can match input such as ""KW_FALSE"" using multiple alternatives: 3, 8

As a result, alternative(s) 8 were disabled for that input
warning(200): IdentifiersParser.g:261:5: 
Decision can match input such as ""KW_TRUE"" using multiple alternatives: 3, 8

As a result, alternative(s) 8 were disabled for that input
warning(200): IdentifiersParser.g:261:5: 
Decision can match input such as ""KW_DATE StringLiteral"" using multiple alternatives: 2, 3

As a result, alternative(s) 3 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_CLUSTER KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_UNION KW_ALL"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_SORT KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_INSERT KW_INTO"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_LATERAL KW_VIEW"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""KW_BETWEEN KW_MAP LPAREN"" using multiple alternatives: 8, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_DISTRIBUTE KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_INSERT KW_OVERWRITE"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_ORDER KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_GROUP KW_BY"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:393:5: 
Decision can match input such as ""{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_MAP LPAREN"" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:518:5: 
Decision can match input such as ""{AMPERSAND..BITWISEXOR, DIV..DIVIDE, EQUAL..EQUAL_NS, GREATERTHAN..GREATERTHANOREQUALTO, KW_AND, KW_ARRAY, KW_BETWEEN..KW_BOOLEAN, KW_CASE, KW_DOUBLE, KW_FLOAT, KW_IF, KW_IN, KW_INT, KW_LIKE, KW_MAP, KW_NOT, KW_OR, KW_REGEXP, KW_RLIKE, KW_SMALLINT, KW_STRING..KW_STRUCT, KW_TINYINT, KW_UNIONTYPE, KW_WHEN, LESSTHAN..LESSTHANOREQUALTO, MINUS..NOTEQUAL, PLUS, STAR, TILDE}"" using multiple alternatives: 1, 3

As a result, alternative(s) 3 were disabled for that input
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-exec ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-exec ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-exec ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-exec ---
[INFO] Compiling 1988 source files to /data/hive-ptest/working/apache-svn-trunk-source/ql/target/classes
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinBytesTableContainer.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinBytesTableContainer.java: Recompile with -Xlint:deprecation for details.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Query.java: Some input files use unchecked or unsafe operations.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Query.java: Recompile with -Xlint:unchecked for details.
[INFO] 4 warnings 
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java:[97,9] method does not override or implement a method from a supertype
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/stats/HiveRelMdRowCount.java:[192,15] method classifyFilters in class org.eigenbase.relopt.RelOptUtil cannot be applied to given types;
  required: org.eigenbase.rel.RelNode,java.util.List<org.eigenbase.rex.RexNode>,org.eigenbase.rel.JoinRelType,boolean,boolean,boolean,java.util.List<org.eigenbase.rex.RexNode>,java.util.List<org.eigenbase.rex.RexNode>,java.util.List<org.eigenbase.rex.RexNode>,org.eigenbase.util.Holder<org.eigenbase.rel.JoinRelType>,boolean
  found: org.eigenbase.rel.JoinRelBase,java.util.List<org.eigenbase.rex.RexNode>,org.eigenbase.rel.JoinRelType,boolean,boolean,boolean,java.util.List<org.eigenbase.rex.RexNode>,java.util.List<org.eigenbase.rex.RexNode>,java.util.List<org.eigenbase.rex.RexNode>
  reason: actual and formal argument lists differ in length
[INFO] 2 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [11.878s]
[INFO] Hive Shims Common ................................. SUCCESS [6.784s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [3.731s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [4.713s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.286s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [6.534s]
[INFO] Hive Shims ........................................ SUCCESS [2.014s]
[INFO] Hive Common ....................................... SUCCESS [11.148s]
[INFO] Hive Serde ........................................ SUCCESS [19.497s]
[INFO] Hive Metastore .................................... SUCCESS [37.527s]
[INFO] Hive Ant Utilities ................................ SUCCESS [1.681s]
[INFO] Hive Query Language ............................... FAILURE [1:04.366s]
[INFO] Hive Service ...................................... SKIPPED
[INFO] Hive Accumulo Handler ............................. SKIPPED
[INFO] Hive JDBC ......................................... SKIPPED
[INFO] Hive Beeline ...................................... SKIPPED
[INFO] Hive CLI .......................................... SKIPPED
[INFO] Hive Contrib ...................................... SKIPPED
[INFO] Hive HBase Handler ................................ SKIPPED
[INFO] Hive HCatalog ..................................... SKIPPED
[INFO] Hive HCatalog Core ................................ SKIPPED
[INFO] Hive HCatalog Pig Adapter ......................... SKIPPED
[INFO] Hive HCatalog Server Extensions ................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client ................. SKIPPED
[INFO] Hive HCatalog Webhcat ............................. SKIPPED
[INFO] Hive HCatalog Streaming ........................... SKIPPED
[INFO] Hive HWI .......................................... SKIPPED
[INFO] Hive ODBC ......................................... SKIPPED
[INFO] Hive Shims Aggregator ............................. SKIPPED
[INFO] Hive TestUtils .................................... SKIPPED
[INFO] Hive Packaging .................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 2:54.824s
[INFO] Finished at: Thu Oct 23 22:28:25 EDT 2014
[INFO] Final Memory: 101M/787M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-exec: Compilation failure: Compilation failure:
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java:[97,9] method does not override or implement a method from a supertype
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/stats/HiveRelMdRowCount.java:[192,15] method classifyFilters in class org.eigenbase.relopt.RelOptUtil cannot be applied to given types;
[ERROR] required: org.eigenbase.rel.RelNode,java.util.List<org.eigenbase.rex.RexNode>,org.eigenbase.rel.JoinRelType,boolean,boolean,boolean,java.util.List<org.eigenbase.rex.RexNode>,java.util.List<org.eigenbase.rex.RexNode>,java.util.List<org.eigenbase.rex.RexNode>,org.eigenbase.util.Holder<org.eigenbase.rel.JoinRelType>,boolean
[ERROR] found: org.eigenbase.rel.JoinRelBase,java.util.List<org.eigenbase.rex.RexNode>,org.eigenbase.rel.JoinRelType,boolean,boolean,boolean,java.util.List<org.eigenbase.rex.RexNode>,java.util.List<org.eigenbase.rex.RexNode>,java.util.List<org.eigenbase.rex.RexNode>
[ERROR] reason: actual and formal argument lists differ in length
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-exec
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12676735 - PreCommit-HIVE-TRUNK-Build;;;","24/Oct/14 03:37;hagleitn;Reuploading patch .2 after fixing build breakage.;;;","24/Oct/14 10:05;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12676822/HIVE-6165.2.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 6563 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_correctness
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_correctness
org.apache.hive.hcatalog.streaming.TestStreaming.testInterleavedTransactionBatchCommits
org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Json
org.apache.hive.minikdc.TestJdbcWithMiniKdc.testNegativeTokenAuth
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1439/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1439/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1439/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12676822 - PreCommit-HIVE-TRUNK-Build;;;","24/Oct/14 17:29;xuefuz;Reload the same patch to re-run test.;;;","24/Oct/14 18:21;hagleitn;[~xuefuz] - i've already done that. the last Hive QA entry is for .2 patch (i stripped the .txt by mistake).;;;","24/Oct/14 18:28;xuefuz;Yeah. I knew. However, the test failures seems unrelated, but I'm not quite sure. Thus, I liked to have another run to confirm. Thanks for pointing it out thought.;;;","25/Oct/14 09:03;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12676946/HIVE-6165.2.patch.txt

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 6578 tests executed
*Failed tests:*
{noformat}
org.apache.hive.minikdc.TestJdbcWithMiniKdc.testNegativeTokenAuth
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1450/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1450/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1450/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12676946 - PreCommit-HIVE-TRUNK-Build;;;","25/Oct/14 15:26;xuefuz;+1;;;","27/Oct/14 18:47;xuefuz;Patch committed to trunk. Thanks to Helmut for the patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Hive build on Windows failed with datanucleus enhancer error ""command line is too long""",HIVE-6164,12687808,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,shanyu,shanyu,shanyu,08/Jan/14 05:36,20/Jan/14 18:38,14/Jul/23 06:14,20/Jan/14 18:38,0.13.0,,,,,,,,,0.13.0,,Build Infrastructure,,,,0,,,"Build hive 0.13 against hadoop 2.0 on Windows always fail:
mvn install -Phadoop-2
...
[ERROR] --------------------
[ERROR]  Standard error from the DataNucleus tool + org.datanucleus.enhancer.Dat
aNucleusEnhancer :
[ERROR] --------------------
[ERROR] The command line is too long.",,shanyu,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/14 05:42;shanyu;HIVE-6164.patch;https://issues.apache.org/jira/secure/attachment/12621926/HIVE-6164.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,366812,,,,Mon Jan 20 18:38:54 UTC 2014,,,,,,,,,,"0|i1r88f:",367123,,,,,,,,,,,,,,,,,,,,,"08/Jan/14 05:40;shanyu;For more information, please refer to:
http://www.datanucleus.org/servlet/jira/browse/NUCMAVEN-44;;;","08/Jan/14 05:42;shanyu;patch attached.;;;","08/Jan/14 12:22;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12621926/HIVE-6164.patch

{color:green}SUCCESS:{color} +1 4903 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/828/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/828/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12621926;;;","17/Jan/14 03:14;thejas;+1;;;","20/Jan/14 18:38;thejas;Patch committed to trunk.
Thanks for the contribution Shanyu!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""multiple SLF4J bindings"" warning messages when running hive CLI on Hadoop 2.0",HIVE-6162,12687803,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,shanyu,shanyu,shanyu,08/Jan/14 04:58,12/Aug/19 08:06,14/Jul/23 06:14,20/Jan/14 18:36,0.12.0,,,,,,,,,0.13.0,,CLI,,,,0,,,"On Hadoop 2.0, when running hive command line, we saw warnings like this:

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/C:/myhdp/hadoop-2.1.2.2.0.6.0-0000/share/hado
op/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/C:/myhdp/hive-0.12.0.2.0.6.0-0000/lib/slf4j-l
og4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]",,koutlasandeep1993,morgandechile,rajasekhar.gade,shanyu,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-21688,HIVE-21157,,HIVE-9496,,,,,,,,,,,,,,,,,,,,,"09/Jan/14 07:33;shanyu;HIVE-6162.patch;https://issues.apache.org/jira/secure/attachment/12622129/HIVE-6162.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,366807,,,,Mon Aug 12 08:03:41 UTC 2019,,,,,,,,,,"0|i1r87b:",367118,,,,,,,,,,,,,,,,,,,,,"08/Jan/14 05:47;shanyu;Patch attached.;;;","09/Jan/14 07:33;shanyu;Exclude slf4j jars from the hive package. At runtime, hive always uses slf4j jars from hadoop shared library.;;;","09/Jan/14 17:15;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12622129/HIVE-6162.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4904 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/840/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/840/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12622129;;;","16/Jan/14 23:35;shanyu;The failure has nothing to do with my changes.;;;","17/Jan/14 03:17;thejas;+1;;;","20/Jan/14 18:36;thejas;Patch committed to trunk. Thanks for the contribution Shanyu!
;;;","04/Mar/16 16:46;rajasekhar.gade;Hello All, I am installing Hive 2.0.0 with Hadoop 2.7.2 in ubuntu 15.10,

I am getting an error while i start Hive through CLI.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive/lib/hive-jdbc-2.0.0-standalone.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-2.0.0.jar!/hive-log4j2.properties
Exception in thread ""main"" java.lang.RuntimeException: Hive metastore database is not initialized. Please use schematool (e.g. ./schematool -initSchema -dbType ...) to create the schema. If needed, don't forget to include the option to auto-create the underlying database in your JDBC connection string (e.g. ?createDatabaseIfNotExist=true for mysql)


I tried to apply this patch but failed to understand , Please help me.;;;","27/Dec/16 17:27;morgandechile;somebody did actually fix this ???

I am in 2.7.3 and having the same issue 

File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 293, in _call
    raise ExecutionFailed(err_msg, code, out, err)
resource_management.core.exceptions.ExecutionFailed: Execution of 'export HIVE_CONF_DIR=/usr/hdp/current/hive-metastore/conf/conf.server ; /usr/hdp/current/hive-server2-hive2/bin/schematool -initSchema -dbType mysql -userName hive -passWord [PROTECTED] -verbose' returned 1. SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/2.5.3.0-37/hive2/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.5.3.0-37/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

;;;","12/Aug/19 08:03;koutlasandeep1993;Hi team,

i am having the same error, can you please tell me where to place the patch.

is it to maven, if yes can you please provide in which file we need to.

it would be a great help.

 

Regards,

sandeep kumar k;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive uses deprecated hadoop configuration in Hadoop 2.0,HIVE-6159,12687735,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,shanyu,shanyu,shanyu,07/Jan/14 21:05,23/Jul/14 07:59,14/Jul/23 06:14,17/Jan/14 03:20,0.12.0,,,,,,,,,0.13.0,,Configuration,,,,0,,,"Build hive against hadoop 2.0. Then run hive CLI, you'll see deprecated configurations warnings like this:

13/12/14 01:00:51 INFO Configuration.deprecation: mapred.input.dir.recursive is
 deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.max.split.size is depre
 cated. Instead, use mapreduce.input.fileinputformat.split.maxsize
 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size is depre
 cated. Instead, use mapreduce.input.fileinputformat.split.minsize
 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size.per.rack
 is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.r
 ack
 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size.per.node
 is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.n
 ode
 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.reduce.tasks is depreca
 ted. Instead, use mapreduce.job.reduces
 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.reduce.tasks.speculativ
 e.execution is deprecated. Instead, use mapreduce.reduce.speculative
",,lagomorph,leftyl,mdominguez@cloudera.com,qwertymaniac,shanyu,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6049,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/14 18:49;shanyu;HIVE-6159-v2.patch;https://issues.apache.org/jira/secure/attachment/12622013/HIVE-6159-v2.patch","13/Jan/14 02:21;shanyu;HIVE-6159-v3.patch;https://issues.apache.org/jira/secure/attachment/12622570/HIVE-6159-v3.patch","14/Jan/14 01:36;shanyu;HIVE-6159-v4.patch;https://issues.apache.org/jira/secure/attachment/12622773/HIVE-6159-v4.patch","14/Jan/14 02:36;shanyu;HIVE-6159.4.patch;https://issues.apache.org/jira/secure/attachment/12622775/HIVE-6159.4.patch","15/Jan/14 22:15;shanyu;HIVE-6159.5.patch;https://issues.apache.org/jira/secure/attachment/12623246/HIVE-6159.5.patch","08/Jan/14 05:49;shanyu;HIVE-6159.patch;https://issues.apache.org/jira/secure/attachment/12621929/HIVE-6159.patch",,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,366735,,,,Fri Jan 17 03:20:01 UTC 2014,,,,,,,,,,"0|i1r7rb:",367046,,,,,,,,,,,,,,,,,,,,,"08/Jan/14 05:49;shanyu;Patch attached.

Use hadoop shims to provide hadoop property name.

Also removed ""mapred.job.tracker"" because it is not used anywhere in hive project.
;;;","08/Jan/14 10:48;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12621929/HIVE-6159.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4903 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_overridden_confs
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/827/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/827/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12621929;;;","08/Jan/14 18:49;shanyu;Attach a new patch fixing the unit test failure.;;;","10/Jan/14 22:00;shanyu;review created and linked to this jira.;;;","11/Jan/14 02:05;thejas;Canceling and submitting patch to run unit tests.
[~shanyu] The changes look good to me . 
One case I am concerned is where user continues to use the old config value (in hive-site.xml or using ""-hiveconf .."").  Can you add a unit test case to verify that the right thing happens ? eg - set the value of one of these properties in hive conf using old value, try reading back using new value and check if you get the updated value.

Have you tried running unit tests with hadoop2 with this change? The precommit tests run only run with hadoop 1.

;;;","13/Jan/14 02:21;shanyu;Thanks [~thejas] for reviewing the patch. I added a new test case to make sure users can still use deprecated properties in hive-site.xml and we can read the configuration in HiveConf with new key. New patch attached.

I have run unit tests with hadoop-2 and this patch doesn't introduce any new failures.;;;","13/Jan/14 20:01;thejas;Shanyu, can you please update the reviewboard link as well ?
;;;","13/Jan/14 21:50;shanyu;Just updated review board.;;;","14/Jan/14 01:36;shanyu;v4 patch attached.;;;","14/Jan/14 01:57;thejas;+1;;;","14/Jan/14 01:59;thejas;Shanyu,
Can you please attach the file with one of the accepted name formats as documented in https://cwiki.apache.org/confluence/display/Hive/Hive+PreCommit+Patch+Testing and then cancel patch and make it available again ?
(	example - HIVE-6159.4.patch );;;","14/Jan/14 02:36;shanyu;.4 patch attached;;;","14/Jan/14 15:04;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12622775/HIVE-6159.4.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/901/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/901/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims-0.23 ---
[INFO] Compiling 3 source files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/classes
[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java uses or overrides a deprecated API.
[WARNING] Note: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims-0.23 ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/src/test/resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-0.23 ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims-0.23 ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims-0.23 ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims-0.23 ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/hive-shims-0.23-0.13.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-0.23 ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/hive-shims-0.23-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.23/0.13.0-SNAPSHOT/hive-shims-0.23-0.13.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.23/0.13.0-SNAPSHOT/hive-shims-0.23-0.13.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Shims 0.13.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/aggregator (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/aggregator/src/main/resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/aggregator/src/test/resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/aggregator/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/aggregator/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/aggregator/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/aggregator/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims ---
[WARNING] JAR will be empty - no content was marked for inclusion!
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/aggregator/target/hive-shims-0.13.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/aggregator/target/hive-shims-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-shims/0.13.0-SNAPSHOT/hive-shims-0.13.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/aggregator/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims/0.13.0-SNAPSHOT/hive-shims-0.13.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Common 0.13.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-common ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/common (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (generate-version-annotation) @ hive-common ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-source (add-source) @ hive-common ---
[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/common/src/gen added.
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-common ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-common ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-common ---
[INFO] Compiling 35 source files to /data/hive-ptest/working/apache-svn-trunk-source/common/target/classes
[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/common/src/java/org/apache/hadoop/hive/common/ObjectPair.java uses unchecked or unsafe operations.
[WARNING] Note: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-common ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 4 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-common ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/common/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/common/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/common/target/tmp/conf
     [copy] Copying 5 files to /data/hive-ptest/working/apache-svn-trunk-source/common/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-common ---
[INFO] Compiling 13 source files to /data/hive-ptest/working/apache-svn-trunk-source/common/target/test-classes
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/common/src/test/org/apache/hadoop/hive/conf/TestHiveConf.java:[101,18] cannot find symbol
symbol  : method addDeprecation(java.lang.String,java.lang.String)
location: class org.apache.hadoop.conf.Configuration
[INFO] 1 error
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [4.741s]
[INFO] Hive Ant Utilities ................................ SUCCESS [6.721s]
[INFO] Hive Shims Common ................................. SUCCESS [3.300s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.048s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [2.774s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [1.442s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [3.414s]
[INFO] Hive Shims ........................................ SUCCESS [0.624s]
[INFO] Hive Common ....................................... FAILURE [7.132s]
[INFO] Hive Serde ........................................ SKIPPED
[INFO] Hive Metastore .................................... SKIPPED
[INFO] Hive Query Language ............................... SKIPPED
[INFO] Hive Service ...................................... SKIPPED
[INFO] Hive JDBC ......................................... SKIPPED
[INFO] Hive Beeline ...................................... SKIPPED
[INFO] Hive CLI .......................................... SKIPPED
[INFO] Hive Contrib ...................................... SKIPPED
[INFO] Hive HBase Handler ................................ SKIPPED
[INFO] Hive HCatalog ..................................... SKIPPED
[INFO] Hive HCatalog Core ................................ SKIPPED
[INFO] Hive HCatalog Pig Adapter ......................... SKIPPED
[INFO] Hive HCatalog Server Extensions ................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client ................. SKIPPED
[INFO] Hive HCatalog Webhcat ............................. SKIPPED
[INFO] Hive HCatalog HBase Storage Handler ............... SKIPPED
[INFO] Hive HWI .......................................... SKIPPED
[INFO] Hive ODBC ......................................... SKIPPED
[INFO] Hive Shims Aggregator ............................. SKIPPED
[INFO] Hive TestUtils .................................... SKIPPED
[INFO] Hive Packaging .................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 35.100s
[INFO] Finished at: Tue Jan 14 10:04:33 EST 2014
[INFO] Final Memory: 29M/71M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hive-common: Compilation failure
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/common/src/test/org/apache/hadoop/hive/conf/TestHiveConf.java:[101,18] cannot find symbol
[ERROR] symbol  : method addDeprecation(java.lang.String,java.lang.String)
[ERROR] location: class org.apache.hadoop.conf.Configuration
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-common
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12622775;;;","15/Jan/14 22:17;shanyu;Remove the unit test “testDeprecatedProperties()” because the addDeprecation() method isn’t available for hdoop-1. We would have to add the method to shims which I don’t think worth the effort.

The deprecation feature is unit tested thoroughly in Hadoop and in my opinion we shouldn’t add these in hive project. 
;;;","15/Jan/14 23:19;thejas;Sounds good to me . +1
;;;","16/Jan/14 00:35;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12623246/HIVE-6159.5.patch

{color:green}SUCCESS:{color} +1 4927 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/924/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/924/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12623246;;;","17/Jan/14 03:20;thejas;Patch committed to trunk. Thanks for the contribution Shanyu!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fetching column stats slower than the 101 during rush hour,HIVE-6157,12687716,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,sershe,hagleitn,hagleitn,07/Jan/14 19:45,01/Aug/14 06:20,14/Jul/23 06:14,30/Jan/14 00:24,0.13.0,,,,,,,,,0.13.0,,,,,,0,,,"""hive.stats.fetch.column.stats"" controls whether the column stats for a table are fetched during explain (in Tez: during query planning). On my setup (1 table 4000 partitions, 24 columns) the time spent in semantic analyze goes from ~1 second to ~66 seconds when turning the flag on. 65 seconds spent fetching column stats...

The reason is probably that the APIs force you to make separate metastore calls for each column in each partition. That's probably the first thing that has to change. The question is if in addition to that we need to cache this in the client or store the stats as a single blob in the database to further cut down on the time. However, the way it stands right now column stats seem unusable.

",,akolb,hagleitn,prasanth_j,sershe,shreepadma,sushanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6289,HIVE-6292,,,,,,,,,,,,,HIVE-4301,,,,,,,,,,,,,,,,HIVE-4301,,,,,"22/Jan/14 03:28;sershe;HIVE-6157.01.patch;https://issues.apache.org/jira/secure/attachment/12624267/HIVE-6157.01.patch","21/Jan/14 23:13;sershe;HIVE-6157.01.patch;https://issues.apache.org/jira/secure/attachment/12624225/HIVE-6157.01.patch","24/Jan/14 18:22;sershe;HIVE-6157.03.patch;https://issues.apache.org/jira/secure/attachment/12625080/HIVE-6157.03.patch","23/Jan/14 19:54;sershe;HIVE-6157.03.patch;https://issues.apache.org/jira/secure/attachment/12624888/HIVE-6157.03.patch","23/Jan/14 19:54;sershe;HIVE-6157.nogen.patch;https://issues.apache.org/jira/secure/attachment/12624887/HIVE-6157.nogen.patch","21/Jan/14 23:13;sershe;HIVE-6157.nogen.patch;https://issues.apache.org/jira/secure/attachment/12624224/HIVE-6157.nogen.patch","18/Jan/14 07:27;sershe;HIVE-6157.prelim.patch;https://issues.apache.org/jira/secure/attachment/12623775/HIVE-6157.prelim.patch",,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,366716,,,,Thu Jan 30 00:24:37 UTC 2014,,,,,,,,,,"0|i1r7n3:",367027,,,,,,,,,,,,,,,,,,,,,"17/Jan/14 20:19;sershe;Ok, this took rather longer than expected... initially I tried to make stat fetching part of partition pruning, this can be added as an extra optimization if necessary as this requires too many API changes all over the place.
The alternative is simple, getting stat calls are all batched. New APIs on thrift use req/resp pattern; requests contain db, table, column list, and partition list (for partitions). The request returns whatever it can find (rather than the full list with some nulls, like the old APIs that built lists using individual calls to metastore). The code then uses this. 
On metastore there's both JDO and SQL path for speed.
Also, cleaned up some stuff in StatOptimizer and StatsUtil that was generally suboptimal.;;;","17/Jan/14 20:19;sershe;Patch coming today barring something surprising happens;;;","18/Jan/14 07:27;sershe;Well, it looks like I cannot defeat datanucleus today... SQL path seems to work, although I didn't run all the tests. Let me comment out and check for now.;;;","20/Jan/14 04:52;shreepadma;Currently, the API fetches statistics for a given column. hive.stats.fetch.column.stats fetches stats for all columns for all partitions in all tables. Bad idea. HIVE-4301 was filed to support a bulk fetch API so that stats for all columns for all partitions in multiple tables can be fetched with a single call. Feel free to pick up HIVE-4301.;;;","20/Jan/14 17:20;sershe;Sorry, was not aware of that JIRA. Among other things, this patch adds bulk APIs. They do not support multiple tables as of now, though. Stats are currently fetched on the level of one column (stat optimizer) or one table (table scan stuff), so making use of multi-table API would require more extensive changes on the client (optimizer) side.;;;","21/Jan/14 23:13;sershe;first patch. There's one TODO# left where I think some validation code is dead, need to see if any tests fail with it.

Other than that many tests I ran passed, let's see what HiveQA says;;;","21/Jan/14 23:17;sershe;nogen patch omits generated code and is posted to https://reviews.apache.org/r/17162/;;;","21/Jan/14 23:30;sershe;hmm, q.out file change may have not been a planned inclusion, let me dbl check;;;","22/Jan/14 03:28;sershe;HiveQA won't pick the patch; same file;;;","22/Jan/14 08:17;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12624267/HIVE-6157.01.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 4943 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_coltype
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadataonly1
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/980/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/980/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12624267;;;","23/Jan/14 06:24;hagleitn;Patch looks good. Some questions/comments on RB. What about the test failures? They seem relevant.;;;","23/Jan/14 19:54;sershe;RB feedback, also test fixes (02 fixed those but I couldn't add it to jira). I am running the tez test now;;;","23/Jan/14 21:31;sershe;metadata_only_queries.q.out got modified for tez test, but it appears that the changes come from unrelated patches. Because it doesn't run in HiveQA it didn't get updated at some point. Not including in this patch... the rest passed;;;","23/Jan/14 22:38;hagleitn;I have HIVE-6261 open for the metadata_only_queries;;;","24/Jan/14 18:22;sershe;exact same patch, HiveQA won't run;;;","25/Jan/14 03:01;hagleitn;[~prasanth_j] do you want to also take a look? This would effect the stats annotation too.;;;","25/Jan/14 09:12;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625080/HIVE-6157.03.patch

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 4958 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_import_exported_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_reducers_power_two
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_load_hdfs_file_with_space_in_the_name
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_file_with_header_footer_negative
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1010/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1010/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625080;;;","27/Jan/14 20:02;prasanth_j;Mostly looks good. Left minor comments on RB.;;;","28/Jan/14 00:41;sershe;Some of the test failures are also happening in other jiras and some I cannot repro... let me address recent RB feedback and rerun QA;;;","28/Jan/14 19:36;hagleitn;LGTM +1;;;","28/Jan/14 19:40;sershe;on some other jira with harmless patch all but one of the same tests failed so I assume they are all broken. I'll take a look at bucketmapjoin6;;;","28/Jan/14 22:42;sershe;cannot repro on multiple machines and I see bucketmapjoin6 also fails in some jiras... probably flaky. Will commit tomorrow.;;;","30/Jan/14 00:24;hagleitn;Committed to trunk. Thanks Sergey!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2 returns a detailed error message to the client only when the underlying exception is a HiveSQLException,HIVE-6154,12687567,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vgumashta,vgumashta,vgumashta,07/Jan/14 01:07,08/Jan/14 21:58,14/Jul/23 06:14,08/Jan/14 21:58,0.13.0,,,,,,,,,0.13.0,,HiveServer2,,,,0,,,"In ThriftCLIService, if any of the API calls error out, the server API tries to send back a detailed error response by trying to extract information from the caught exception e like - HiveSQLException.toTStatus(e). However, within the HiveSQLException#toTStatus method, the detailed error info (sqlState, errorCode, errorMessage) is sent back only if the caught exception is an instance of HiveSQLException, otherwise only the error status is set. For other exceptions as well, we can set the error message in the response in addition to setting the error status to help in debugging.",,lanyuyang,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/14 02:14;vgumashta;HIVE-6154.1.patch;https://issues.apache.org/jira/secure/attachment/12621739/HIVE-6154.1.patch","07/Jan/14 20:48;vgumashta;HIVE-6154.2.patch;https://issues.apache.org/jira/secure/attachment/12621852/HIVE-6154.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,366568,,,,Wed Jan 08 21:58:47 UTC 2014,,,,,,,,,,"0|i1r6pz:",366879,,,,,,,,,,,,,,,,,,,,,"07/Jan/14 13:11;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12621739/HIVE-6154.1.patch

{color:green}SUCCESS:{color} +1 4880 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/820/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/820/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12621739;;;","07/Jan/14 19:58;thejas;Looks good. A nit - I think it would be bit more easier to read this way -

{code}
     if (e instanceof HiveSQLException) {
       return ((HiveSQLException)e).toTStatus();
     }
    TStatus tStatus = new TStatus(TStatusCode.ERROR_STATUS);
    tStatus.setErrorMessage(e.getMessage());
    return tStatus;
{code}
;;;","07/Jan/14 20:48;vgumashta;[~thejas] Done! Thanks for taking a look.;;;","07/Jan/14 20:56;thejas;+1;;;","07/Jan/14 23:28;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12621852/HIVE-6154.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4903 tests executed
*Failed tests:*
{noformat}
org.apache.hcatalog.hbase.snapshot.lock.TestWriteLock.testRun
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/822/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/822/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12621852;;;","08/Jan/14 21:58;thejas;Patch committed to trunk.
Thanks for the contribution Vaibhav!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Vikram to the committer's list and Gunther to PMC list.,HIVE-6153,12687541,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,vikram.dixit,vikram.dixit,vikram.dixit,06/Jan/14 22:26,11/Jan/14 11:00,14/Jul/23 06:14,07/Jan/14 20:28,,,,,,,,,,,,,,,,0,,,,,hagleitn,leftyl,sershe,vikram.dixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6158,,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/14 22:30;vikram.dixit;HIVE-6153.patch;https://issues.apache.org/jira/secure/attachment/12621701/HIVE-6153.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,366542,,,,Sat Jan 11 11:00:37 UTC 2014,,,,,,,,,,"0|i1r6k7:",366853,,,,,,,,,,,,,,,,,,,,,"06/Jan/14 22:34;sershe;the patch also appears to add Günther ;;;","06/Jan/14 22:37;vikram.dixit;Yes. Gunther's name was missing from the PMC list. Changed description to reflect the same.;;;","06/Jan/14 22:46;sershe;non-binding +1 :);;;","06/Jan/14 22:48;hagleitn;+1 LGTM;;;","07/Jan/14 09:23;leftyl;The description should say ""NO PRECOMMIT TESTS"" and Günther's name should be removed from the committer list.  For an example, see [HIVE-5644|https://issues.apache.org/jira/browse/HIVE-5644].  I'm going to add a note explaining that PMC members are also committers.

(Thanks for the umlaut, [~sershe].);;;","07/Jan/14 11:32;leftyl;But wait ... there's more!  Günther needs an empty entry for the ""roles"" column, like this:

{code}
+        <td>Hortonworks</td> <!-- organization -->
+        <td></td> <!-- roles -->
{code}

See HIVE-5742 for more examples:  https://issues.apache.org/jira/secure/attachment/12611991/HIVE-5472.patch.;;;","07/Jan/14 20:22;vikram.dixit;[~leftylev] I had committed this patch yesterday evening before your comments came in. I am sorry I jumped the gun and went ahead since it was a trivial patch and I had the +1. However, after looking at your comments, I found more such things in the page. I will raise another jira to fix it.

Thanks
Vikram.;;;","07/Jan/14 20:28;vikram.dixit;Committed to hive site.;;;","07/Jan/14 20:54;vikram.dixit;HIVE-6158 raised for follow up.;;;","08/Jan/14 02:40;leftyl;That expains one mystery but raises another -- when I added my comments in the wee hours, this JIRA was still marked as unresolved and yet my download of the Hive site included your changes.  So apparently I can't rely on the JIRA status being current, but at least the system isn't committing patches automatically.  Hm, it wasn't just a display buffer problem because I hadn't received email about it either.  Most peculiar.

Have you patched HIVE-6158 yet?  I can't trust what I'm seeing on the JIRA.  If you prefer, I can revise my patch on HIVE-6155 to include the HIVE-6158 changes.  Or (here's a cooperative thought) you could revise my patch.;;;","08/Jan/14 03:11;vikram.dixit;Hi Lefty,

I got a bit eager on this one considering it was trivial and missed updating the jira. My bad! I closed this issue today morning. I will be more careful from now onwards.

I haven't worked on HIVE-6158 yet. I would upload the patch if I had completed it and waited for a +1 and 24hrs thence before committing and closing it. (I learned my lesson :).) You can go ahead and revise your patch on HIVE-6155 to include changes from the jira HIVE-6158.

Thanks
Vikram.;;;","11/Jan/14 11:00;leftyl;Okay, patch 2 for HIVE-6155 has the changes for HIVE-6158.  And thanks for the offline tutorial on how to commit patches.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
insert query fails on hdfs federation + viewfs,HIVE-6152,12687536,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,06/Jan/14 22:08,14/Sep/15 04:14,14/Jul/23 06:14,29/Jan/14 18:02,,,,,,,,,,0.13.0,,,,,,0,,,"This is because Hive first writes data to /tmp/ and than moves from /tmp to final destination. In federated HDFS recommendation is to mount /tmp on a separate nameservice, which is usually different than /user. Since renames across different mount points are not supported, this fails. ",,in-chief,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7529,,,HIVE-11809,,,,,,,,,,,,,,,,,,,,,"06/Jan/14 22:29;ashutoshc;HIVE-6152.1.patch;https://issues.apache.org/jira/secure/attachment/12621700/HIVE-6152.1.patch","07/Jan/14 22:09;ashutoshc;HIVE-6152.2.patch;https://issues.apache.org/jira/secure/attachment/12621869/HIVE-6152.2.patch","10/Jan/14 16:44;ashutoshc;HIVE-6152.3.patch;https://issues.apache.org/jira/secure/attachment/12622402/HIVE-6152.3.patch","10/Jan/14 20:02;ashutoshc;HIVE-6152.4.patch;https://issues.apache.org/jira/secure/attachment/12622432/HIVE-6152.4.patch","29/Jan/14 02:51;ashutoshc;HIVE-6152.5.patch;https://issues.apache.org/jira/secure/attachment/12625776/HIVE-6152.5.patch",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,366537,,,,Mon Jul 28 07:25:50 UTC 2014,,,,,,,,,,"0|i1r6j3:",366848,,,,,,,,,,,,,,,,,,,,,"06/Jan/14 22:29;ashutoshc;Initial patch;;;","07/Jan/14 08:26;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12621700/HIVE-6152.1.patch

{color:red}ERROR:{color} -1 due to 563 failed/errored test(s), 4361 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_char1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_char2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_numbuckets_partitioned_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_numbuckets_partitioned_table2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_coltype
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_rename_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_rename_partition_authorization
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_table_serde2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_varchar1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_varchar2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_filter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_groupby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_select
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_archive_excludeHadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_archive_multi
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join14_hadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_reordering_values
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_nullable_fields
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ba_table1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ba_table2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ba_table3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ba_table_udfs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ba_table_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binary_output_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binary_table_bincolserde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_binary_table_colserde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_groupby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketmapjoin5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_case_sensitivity
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cast1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_nested_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_udf1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_column_access_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_columnarserde_create_shortcut
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_combine1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_combine3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_concatenate_inherit_table_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cp_mj_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_escape
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_genericudf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_insert_outputformat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_like
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_like_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_udaf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_create_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas_char
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas_hadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas_varchar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_custom_input_output_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_database_drop
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_date_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ddltime
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_disallow_incompatible_type_change_off
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_database_removes_partition_dirs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_drop_table_removes_partition_dirs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_enforce_order
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fileformat_mix
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fileformat_sequencefile
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_fileformat_text
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_join_breaktask2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_global_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1_map_nomap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby1_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_noskew_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3_map_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby3_noskew_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby4_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby4_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby4_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby5_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby5_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby5_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby6_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby6_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby6_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7_map_multi_single_reducer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby7_noskew_multi_single_reducer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby8_map
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby8_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby8_noskew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_complex_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_complex_types_multi_single_reducer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_cube1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_sets2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_map_ppr_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_multi_insert_common_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_multi_single_reducer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_multi_single_reducer2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_multi_single_reducer3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_position
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_ppr_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_rollup1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_test_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auth
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_empty
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_file_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables_compact
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_multiple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_self_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_unused
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_update
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_auto
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_compression
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compact_binary_search
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_compression
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_creation
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_stale
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_stale_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_convert_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_grouping_operators
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_multi_insert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_innerjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input11_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input12_hadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input14_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input1_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input35
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input36
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input38
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input39_hadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input3_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input41
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input44
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input49
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_columnarserde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_dynamicserde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_lazyserde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_testsequencefile
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_testxpath
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_testxpath2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert1_overwrite_partitions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert2_overwrite_partitions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_compressed
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insertexternal1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join14_hadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join27
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join29
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32_lessSize
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join35
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join36
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join37
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join38
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join39
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_map_ppr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_rc
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lateral_view_cp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lineage1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_binary_data
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_overwrite
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_loadpart1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_hook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapreduce8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_metadata_only_queries
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_gby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_gby2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_gby3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_lateral_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_insert_move_tasks_share_dependencies
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_newline
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nomore_ambiguous_table_col
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nonreserved_keywords_insert_into1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_notable_alias1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_notable_alias2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_notable_alias3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_null_column
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullformat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_create
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_dictionary_threshold
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_diff_part_cols
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_empty_files
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_empty_strings
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ends_with_nulls
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parallel
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partcols1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_date2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_schema1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_serde_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_varchar1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_varchar2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_vs_table_metadata
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_pcr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_constant_expr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_multi_insert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppr_pushdown2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_protectmode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ptf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_push_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quote1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_alter
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_quotedid_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rand_partitionpruner2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_bigdata
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_columnar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_default_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_lazydecompress
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_merge3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_merge4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_null_value
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_toleratecorruptions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rename_external_partition_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rename_partition_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rename_table_location
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample_islocalmode_hook_hadoop20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_unquote_and
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_unquote_not
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_unquote_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_indexes_edge_cases
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_show_indexes_syntax
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoinopt18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_aggregator_error_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_empty_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_invalidation
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_noscan_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_only_null
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_publisher_error_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_multiinsert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_table_access_keys_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_tablename_with_select
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_test_boolean_whereclause
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_lazy
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_timestamp_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_touch
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_merge
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_percentile_approx_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_10_trims
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_bitmap_and
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_bitmap_or
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_concat_insert1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_concat_insert2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_concat_ws
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_get_json_object
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_length
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_printf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_reverse
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_round_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_sentences
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_translate
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_json_tuple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_parse_url_tuple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union29
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union34
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_lateralview
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_view
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_nested_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_varchar_udf1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_part_project
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_context
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_rcfile_columnar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_timestamp_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_virtual_column
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_expressions
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_fileformat_base64
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes2
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes3
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes4
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes5
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver_serde_typedbytes_null
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_binary_map_queries
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_binary_map_queries_prefix
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_single_sourced_multi_insert
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats2
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats3
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats_empty_partition
org.apache.hadoop.hive.cli.TestHBaseMinimrCliDriver.testCliDriver_hbase_bulk
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket4
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket5
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers2
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_disable_merge_for_bucketing
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_groupby2
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_bucketed_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_map_operators
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_merge
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_join1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_list_bucket_dml_10
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_reduce_deduplicate
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_remote_script
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_smb_mapjoin_8
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_truncate_column_buckets
org.apache.hadoop.hive.ql.TestMTQueries.testMTQueries1
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/816/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/816/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 563 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12621700;;;","08/Jan/14 04:27;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12621869/HIVE-6152.2.patch

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 4903 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_rename_partition
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats19
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats3
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_list_bucket_dml_10
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_rename_partition_failure
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_rename_partition_failure2
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_alter_rename_partition_failure3
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/824/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/824/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12621869;;;","10/Jan/14 16:44;ashutoshc;Rebased patch which has alter and list bucketing test cases fixed. Cannot repro stats fail locally.;;;","10/Jan/14 18:55;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12622402/HIVE-6152.3.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 4917 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats19
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats3
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_list_bucket_dml_10
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/851/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/851/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12622402;;;","11/Jan/14 03:28;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12622432/HIVE-6152.4.patch

{color:green}SUCCESS:{color} +1 4917 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/854/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/854/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12622432;;;","13/Jan/14 21:02;thejas;I think it will be better to use same code path with and without viewfs. That change should work for non viewfs case as well. That way we don't have another untested combination.
;;;","14/Jan/14 22:13;ashutoshc;RB request : https://reviews.apache.org/r/16796/;;;","29/Jan/14 02:51;ashutoshc;Rebased on latest trunk. I think for first phase, we can keep it this way.  Later on, we can bring it on common path.;;;","29/Jan/14 03:06;thejas;+1;;;","29/Jan/14 11:37;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12625776/HIVE-6152.5.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4972 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1094/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1094/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12625776;;;","29/Jan/14 18:02;ashutoshc;Committed to trunk. ;;;","28/Jul/14 07:25;in-chief;https://issues.apache.org/jira/browse/HIVE-6152;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLIService should use debug log level instead of info,HIVE-6151,12687534,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Fixed,vgumashta,vgumashta,vgumashta,06/Jan/14 21:48,28/Jan/14 21:24,14/Jul/23 06:14,28/Jan/14 21:24,,,,,,,,,,0.13.0,,,,,,0,,,"The info level pollutes the production log.

cc [~hsubramaniyan] - thanks for spotting this.",,thejas,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/14 21:53;vgumashta;HIVE-6151.1.patch;https://issues.apache.org/jira/secure/attachment/12621686/HIVE-6151.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,366535,,,,Tue Jan 28 21:24:10 UTC 2014,,,,,,,,,,"0|i1r6in:",366846,,,,,,,,,,,,,,,,,,,,,"07/Jan/14 00:45;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12621686/HIVE-6151.1.patch

{color:green}SUCCESS:{color} +1 4875 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/814/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/814/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12621686;;;","28/Jan/14 01:42;thejas;+1
;;;","28/Jan/14 21:24;thejas;Patch committed to trunk. Thanks for the contribution Vaibhav!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Beeline ignores HIVE_OPTS,HIVE-6142,12687371,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,,vgumashta,vgumashta,05/Jan/14 21:05,09/Jan/14 03:15,14/Jul/23 06:14,09/Jan/14 03:15,0.13.0,,,,,,,,,0.13.0,,HiveServer2,JDBC,,,0,,,"For example, exporting HIVE_OPTS=""-hiveconf javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=/users/vgumashta/hdfs/hive/metastore_db;create=true"" doesn't set the javax.jdo.option.ConnectionURL when starting beeline. Need to do it again with the set command.",,vgumashta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-5677,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,366370,,,,Thu Jan 09 03:15:31 UTC 2014,,,,,,,,,,"0|i1r5hz:",366681,,,,,,,,,,,,,,,,,,,,,"09/Jan/14 03:15;vgumashta;Duplicate;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tez: Add some additional comments to clarify intent,HIVE-6138,12687279,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,03/Jan/14 23:47,03/Jan/14 23:51,14/Jul/23 06:14,03/Jan/14 23:51,,,,,,,,,,tez-branch,,,,,,0,,,,,hagleitn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-4660,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/14 23:49;hagleitn;HIVE-6138.1.patch;https://issues.apache.org/jira/secure/attachment/12621410/HIVE-6138.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,366277,,,,Fri Jan 03 23:51:44 UTC 2014,,,,,,,,,,"0|i1r4xj:",366588,,,,,,,,,,,,,,,,,,,,,"03/Jan/14 23:51;hagleitn;Committed to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix merge error on tez branch (TestCompareCliDriver),HIVE-6135,12687229,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,03/Jan/14 19:16,03/Jan/14 19:22,14/Jul/23 06:14,03/Jan/14 19:22,,,,,,,,,,tez-branch,,,,,,0,,,,,hagleitn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-4660,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/14 19:21;hagleitn;HIVE-6135.1.patch;https://issues.apache.org/jira/secure/attachment/12621370/HIVE-6135.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,366224,,,,Fri Jan 03 19:22:29 UTC 2014,,,,,,,,,,"0|i1r4lr:",366535,,,,,,,,,,,,,,,,,,,,,"03/Jan/14 19:22;hagleitn;Committed to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
alter exchange is implemented in inverted manner,HIVE-6129,12687094,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,navis,navis,navis,03/Jan/14 00:45,12/Dec/16 02:27,14/Jul/23 06:14,27/Mar/14 15:27,,,,,,,,,,0.13.0,,,,,,0,,,"see https://issues.apache.org/jira/browse/HIVE-4095?focusedCommentId=13819885&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13819885

alter exchange should be implemented accord to document in https://cwiki.apache.org/confluence/display/Hive/Exchange+Partition. i.e 

{code}
alter table T1 exchange partition (ds='1') with table T2"" 
{code}
should be (after creating T1@ds=1) 
{quote}
moves the data from T2 to T1@ds=1.... 
{quote}",,erwaman,gates,leftyl,lirui,navis,rhbutani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6133,,,,,,,,,,,HIVE-4095,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/14 04:22;navis;HIVE-6129.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12621248/HIVE-6129.1.patch.txt","26/Mar/14 17:25;rhbutani;HIVE-6129.2.patch;https://issues.apache.org/jira/secure/attachment/12636955/HIVE-6129.2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,366089,,,,Mon Dec 12 02:27:36 UTC 2016,,,,,,,,,,"0|i1r3rr:",366400,,,,,,,,,,,,,,,,,,,,,"03/Jan/14 23:02;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12621248/HIVE-6129.1.patch.txt

{color:green}SUCCESS:{color} +1 4874 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/799/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/799/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12621248;;;","26/Mar/14 17:25;rhbutani;[~navis] this patch was not applying anymore. Have regenned the .q.out files. Please verify.;;;","26/Mar/14 23:51;gates;Ran tests on the latest patch, all looks good.;;;","27/Mar/14 01:50;rhbutani;+1;;;","27/Mar/14 09:51;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12636955/HIVE-6129.2.patch

{color:green}SUCCESS:{color} +1 5491 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1977/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1977/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12636955;;;","27/Mar/14 15:27;rhbutani;committed to trunk and 0.13;;;","17/Aug/16 03:45;lirui;I wonder if we should update the doc for this. According to doc, T1 is source and T2 is target.;;;","12/Dec/16 02:27;leftyl;See documentation comments on HIVE-4095.

* doc comments:
** [Rick Moritz | https://issues.apache.org/jira/browse/HIVE-4095?focusedCommentId=15424579&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15424579]
** [Lefty Leverenz | https://issues.apache.org/jira/browse/HIVE-4095?focusedCommentId=15740751&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15740751]
* wikidocs:
** [Exchange Partition | https://cwiki.apache.org/confluence/display/Hive/Exchange+Partition]
** [DDL -- Exchange Partition | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-ExchangePartition];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add tez variables to hive-default.xml,HIVE-6128,12687017,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,02/Jan/14 18:36,08/Aug/14 16:14,14/Jul/23 06:14,08/Jan/14 20:42,,,,,,,,,,tez-branch,,,,,,0,,,"hive.orc.splits.include.file.footer
hive.orc.cache.stripe.details.size
",,hagleitn,leftyl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-4660,,,HIVE-6098,,,,,,,,,,,,,,,,,,,,,"08/Jan/14 19:46;hagleitn;HIVE-6128.1.patch;https://issues.apache.org/jira/secure/attachment/12622019/HIVE-6128.1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,366012,,,,Fri Aug 08 16:14:54 UTC 2014,,,,,,,,,,"0|i1r3a7:",366319,,,,,,,,,,,,,,,,,,,,,"02/Jan/14 18:37;hagleitn;+ hive.orc.compute.splits.num.threads;;;","08/Jan/14 20:42;hagleitn;Committed to branch.;;;","02/Feb/14 12:33;leftyl;Added to the Configuration Properties wikidoc for Hive 0.13.0.

* [hive.orc.splits.include.file.footer |https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.orc.splits.include.file.footer]
* [hive.orc.cache.stripe.details.size |https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.orc.cache.stripe.details.size]
* [hive.orc.compute.splits.num.threads |https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.orc.compute.splits.num.threads]

The descriptions don't mention Tez.  Should they?  For example, ""Only effective when hive.execution.engine=tez"" (or are they also useful for MR execution)?;;;","08/Aug/14 16:14;leftyl;[~hagleitn], are these three ORC parameters also Tez parameters?  If so, I can list them with other cross-references in the Tez section of Configuration Properties.

* [Configuration Properties -- Tez | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-Tez];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tez: Refactoring changes,HIVE-6125,12686924,Bug,Resolved,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,hagleitn,hagleitn,02/Jan/14 02:46,12/Mar/17 09:08,14/Jul/23 06:14,06/Jan/14 21:36,,,,,,,,,,0.13.0,,,,,,0,,,"In order to facilitate merge back I've separated out all the changes that don't require Tez. These changes introduce new interfaces, move code etc. In preparation of the Tez specific classes. This should help show what changes have been made that affect the MR codepath as well.",,hagleitn,hitesh,leftyl,thejas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-6098,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jan/14 02:56;hagleitn;HIVE-6125.1.patch;https://issues.apache.org/jira/secure/attachment/12621026/HIVE-6125.1.patch","03/Jan/14 18:35;hagleitn;HIVE-6125.2.patch;https://issues.apache.org/jira/secure/attachment/12621356/HIVE-6125.2.patch","03/Jan/14 21:00;hagleitn;HIVE-6125.3.patch;https://issues.apache.org/jira/secure/attachment/12621386/HIVE-6125.3.patch","03/Jan/14 23:37;hagleitn;HIVE-6125.4.patch;https://issues.apache.org/jira/secure/attachment/12621408/HIVE-6125.4.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,365919,,,,Sun Mar 12 09:08:24 UTC 2017,,,,,,,,,,"0|i1r2pj:",366226,,,,,,,,,,,,,,,,,,,,,"02/Jan/14 02:56;hagleitn;Still large. Mostly due to the fact that the TestParse xml files have changed...;;;","02/Jan/14 03:53;hagleitn;RB: https://reviews.apache.org/r/16557/;;;","02/Jan/14 07:49;leftyl;Patch 1 adds three config params to HiveConf.java:

* hive.orc.splits.include.file.footer
* hive.orc.cache.stripe.details.size
* hive.orc.compute.splits.num.threads

But it doesn't define them in hive-default.xml.template.  Just sayin'.;;;","02/Jan/14 18:36;hagleitn;Thanks for the feedback [~leftylev]. I'll create new jira for that. Definitely useful to have them in hive-default.xml as another means of documentation.;;;","02/Jan/14 18:37;hagleitn;Captured in HIVE-6128;;;","03/Jan/14 07:40;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12621026/HIVE-6125.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4873 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/787/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/787/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12621026;;;","03/Jan/14 18:29;hagleitn;failure seems unrelated. passes locally.;;;","03/Jan/14 18:34;hagleitn;This one is without golden files (easier to navigate): https://reviews.apache.org/r/16611/;;;","03/Jan/14 18:35;hagleitn;No change in .2 other than rebasing it.;;;","03/Jan/14 21:00;hagleitn;.3 is another rebase;;;","03/Jan/14 21:58;thejas;LGTM.  Just some minor comments in review board, otherwise +1.

;;;","03/Jan/14 23:37;hagleitn;.4 addresses the review comments.;;;","04/Jan/14 00:33;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12621386/HIVE-6125.3.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4874 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/802/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/802/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12621386;;;","04/Jan/14 05:13;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12621408/HIVE-6125.4.patch

{color:green}SUCCESS:{color} +1 4875 tests passed

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/804/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/804/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12621408;;;","06/Jan/14 21:36;hagleitn;Committed to trunk.;;;","10/Aug/14 04:03;leftyl;This adds three hive.orc.* configuration parameters to HiveConf.java:  *hive.orc.splits.include.file.footer*, *hive.orc.cache.stripe.details.size*, and *hive.orc.compute.splits.num.threads*.

Their descriptions are added in HIVE-6128 and they are documented in the wiki here:

* [Configuration Properties -- hive.orc.splits.include.file.footer |https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.orc.splits.include.file.footer]
* [Configuration Properties -- hive.orc.cache.stripe.details.size |https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.orc.cache.stripe.details.size]
* [Configuration Properties -- hive.orc.compute.splits.num.threads |https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.orc.compute.splits.num.threads];;;","12/Mar/17 09:08;leftyl;HIVE-16133 removes *hive.orc.cache.stripe.details.size* in release 2.2.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
